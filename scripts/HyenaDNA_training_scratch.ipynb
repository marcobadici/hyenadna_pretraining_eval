{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kqGmLLAtCem6",
        "outputId": "fc927b56-e8a9-4c47-b418-b5e18ecb0b86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.3.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3132, in _compute_dependencies\n",
            "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3132, in <listcomp>\n",
            "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3124, in reqs_for_extra\n",
            "    if not req.marker or req.marker.evaluate({'extra': extra}):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 300, in evaluate\n",
            "    current_environment = default_environment()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 264, in default_environment\n",
            "    \"platform_machine\": platform.machine(),\n",
            "  File \"/usr/lib/python3.10/platform.py\", line 940, in machine\n",
            "    def machine():\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1622, in _log\n",
            "    record = self.makeRecord(self.name, level, fn, lno, msg, args,\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1591, in makeRecord\n",
            "    rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 309, in __init__\n",
            "    if (args and len(args) == 1 and isinstance(args[0], collections.abc.Mapping)\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: transformers==4.26.1 in /usr/local/lib/python3.10/dist-packages (4.26.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.1) (2024.6.2)\n",
            "Requirement already satisfied: genomic-benchmarks in /usr/local/lib/python3.10/dist-packages (0.0.9)\n",
            "Requirement already satisfied: biopython>=1.79 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (1.83)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (2.31.0)\n",
            "Requirement already satisfied: pip>=20.0.1 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (23.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (2.0.3)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (6.0.1)\n",
            "Requirement already satisfied: gdown>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (5.1.0)\n",
            "Requirement already satisfied: yarl in /usr/local/lib/python3.10/dist-packages (from genomic-benchmarks) (1.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.2.0->genomic-benchmarks) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.2.0->genomic-benchmarks) (3.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->genomic-benchmarks) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->genomic-benchmarks) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->genomic-benchmarks) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->genomic-benchmarks) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->genomic-benchmarks) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->genomic-benchmarks) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->genomic-benchmarks) (2024.6.2)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.10/dist-packages (from yarl->genomic-benchmarks) (6.0.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.4->genomic-benchmarks) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.2.0->genomic-benchmarks) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->genomic-benchmarks) (1.7.1)\n",
            "Requirement already satisfied: OmegaConf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from OmegaConf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from OmegaConf) (6.0.1)\n"
          ]
        }
      ],
      "source": [
        "#@title Installs\n",
        "!pip install einops\n",
        "!pip install torchvision\n",
        "!pip install transformers==4.26.1\n",
        "!pip install genomic-benchmarks\n",
        "!pip install OmegaConf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL9n6_s9Cem8"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"HyenaDNA training & inference example (Public)\n",
        "\n",
        "This code is adapted from the original colab tutorial on HyenaDNA. Check that out for an easier entry point into the code.\n",
        "\n",
        "We provide the code here as an example for those who want something outside collab, with Huggingface integration.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1wyVEQd4R3HYLTUOXEEQmp_I8aNC_aLhL\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#@title Imports\n",
        "# for HyenaDNA specifically\n",
        "import torch\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "from einops import rearrange\n",
        "from typing import Optional\n",
        "from functools import partial\n",
        "from torch import Tensor\n",
        "from torchvision.ops import StochasticDepth\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Sequence, Union\n",
        "\n",
        "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
        "\n",
        "\n",
        "\"\"\"# HyenaDNA\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#@title Hyena layer\n",
        "\n",
        "\n",
        "def fftconv(u, k, D):\n",
        "    \"\"\"\n",
        "    We apply a convolution through the fourier domain (from the Convolution Theorem)\n",
        "\n",
        "    \"\"\"\n",
        "    seqlen = u.shape[-1]\n",
        "    fft_size = 2 * seqlen\n",
        "\n",
        "    k_f = torch.fft.rfft(k, n=fft_size) / fft_size\n",
        "    u_f = torch.fft.rfft(u.to(dtype=k.dtype), n=fft_size)\n",
        "\n",
        "    if len(u.shape) > 3: k_f = k_f.unsqueeze(1)\n",
        "    y = torch.fft.irfft(u_f * k_f, n=fft_size, norm='forward')[..., :seqlen]\n",
        "\n",
        "    out = y + u * D.unsqueeze(-1)\n",
        "    return out.to(dtype=u.dtype)\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def mul_sum(q, y):\n",
        "    return (q * y).sum(dim=1)\n",
        "\n",
        "class OptimModule(nn.Module):\n",
        "    \"\"\" Interface for Module that allows registering buffers/parameters with configurable optimizer hyperparameters \"\"\"\n",
        "\n",
        "    def register(self, name, tensor, lr=None, wd=0.0):\n",
        "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
        "\n",
        "        if lr == 0.0:\n",
        "            self.register_buffer(name, tensor)\n",
        "        else:\n",
        "            self.register_parameter(name, nn.Parameter(tensor))\n",
        "\n",
        "            optim = {}\n",
        "            if lr is not None: optim[\"lr\"] = lr\n",
        "            if wd is not None: optim[\"weight_decay\"] = wd\n",
        "            setattr(getattr(self, name), \"_optim\", optim)\n",
        "\n",
        "\n",
        "class Sin(nn.Module):\n",
        "    \"\"\"The Sin activation function for the Hyena Filter function.\"\"\"\n",
        "    def __init__(self, dim, w=10, train_freq=True):\n",
        "        super().__init__()\n",
        "        self.freq = nn.Parameter(w * torch.ones(1, dim)) if train_freq else w * torch.ones(1, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sin(self.freq * x)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(OptimModule):\n",
        "    def __init__(self, emb_dim: int, seq_len: int, lr_pos_emb: float=1e-5, **kwargs):\n",
        "        \"\"\"Complex exponential positional embeddings for Hyena filters.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        # The time embedding fed to the filteres is normalized so that t_f = 1\n",
        "        t = torch.linspace(0, 1, self.seq_len)[None, :, None] # 1, L, 1\n",
        "\n",
        "        if emb_dim > 1:\n",
        "            bands = (emb_dim - 1) // 2\n",
        "        # To compute the right embeddings we use the \"proper\" linspace\n",
        "        t_rescaled = torch.linspace(0, seq_len - 1, seq_len)[None, :, None]\n",
        "        w = 2 * math.pi * t_rescaled / seq_len # 1, L, 1\n",
        "\n",
        "        f = torch.linspace(1e-4, bands - 1, bands)[None, None]\n",
        "        z = torch.exp(-1j * f * w)\n",
        "        z = torch.cat([t, z.real, z.imag], dim=-1)\n",
        "        self.register(\"z\", z, lr=lr_pos_emb)\n",
        "        self.register(\"t\", t, lr=0.0)\n",
        "\n",
        "    def forward(self, L):\n",
        "        return self.z[:, :L], self.t[:, :L]\n",
        "\n",
        "\n",
        "class ExponentialModulation(OptimModule):\n",
        "    \"\"\"The window function applied to the output of the (MLP) filter function.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        fast_decay_pct=0.3,\n",
        "        slow_decay_pct=1.5,\n",
        "        target=1e-2,\n",
        "        modulation_lr=0.0,\n",
        "        modulate: bool=True,\n",
        "        shift: float = 0.05,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.modulate = modulate\n",
        "        self.shift = shift\n",
        "        max_decay = math.log(target) / fast_decay_pct\n",
        "        min_decay = math.log(target) / slow_decay_pct\n",
        "        deltas = torch.linspace(min_decay, max_decay, d_model)[None, None]\n",
        "        self.register(\"deltas\", deltas, lr=modulation_lr)\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        if self.modulate:\n",
        "            decay = torch.exp(-t * self.deltas.abs())\n",
        "            x = x * (decay + self.shift)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HyenaFilter(OptimModule):\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model,\n",
        "            emb_dim=3, # dim of input to MLP, augments with positional encoding\n",
        "            order=16, # width of the implicit MLP\n",
        "            fused_fft_conv=False,\n",
        "            seq_len=1024,\n",
        "            lr=1e-3,\n",
        "            lr_pos_emb=1e-5,\n",
        "            dropout=0.0,\n",
        "            w=1, # frequency of periodic activations\n",
        "            wd=0, # weight decay of kernel parameters\n",
        "            bias=True,\n",
        "            num_inner_mlps=2,\n",
        "            normalized=False,\n",
        "            **kwargs\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Implicit long filter with modulation.\n",
        "\n",
        "        Args:\n",
        "            d_model: number of channels in the input\n",
        "            emb_dim: dimension of the positional encoding (`emb_dim` - 1) // 2 is the number of bands\n",
        "            order: width of the FFN\n",
        "            num_inner_mlps: number of inner linear layers inside filter MLP\n",
        "\n",
        "        Note:\n",
        "            filter_dropout is not implemented\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.use_bias = bias\n",
        "        self.fused_fft_conv = fused_fft_conv\n",
        "        self.bias = nn.Parameter(torch.randn(self.d_model))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        act = Sin(dim=order, w=w)\n",
        "        self.emb_dim = emb_dim\n",
        "        assert emb_dim % 2 != 0 and emb_dim >= 3, \"emb_dim must be odd and greater or equal to 3 (time, sine and cosine)\"\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.pos_emb = PositionalEmbedding(emb_dim, seq_len, lr_pos_emb)\n",
        "\n",
        "        self.implicit_filter = nn.Sequential(\n",
        "            nn.Linear(emb_dim, order),\n",
        "            act,\n",
        "        )\n",
        "        for i in range(num_inner_mlps):\n",
        "            self.implicit_filter.append(nn.Linear(order, order))\n",
        "            self.implicit_filter.append(act)\n",
        "\n",
        "        self.implicit_filter.append(nn.Linear(order, d_model, bias=False))\n",
        "\n",
        "        self.modulation = ExponentialModulation(d_model, **kwargs)\n",
        "\n",
        "        self.normalized = normalized\n",
        "        for c in self.implicit_filter.children():\n",
        "            for name, v in c.state_dict().items():\n",
        "                optim = {\"weight_decay\": wd, \"lr\": lr}\n",
        "                setattr(getattr(c, name), \"_optim\", optim)\n",
        "\n",
        "    def filter(self, L, *args, **kwargs):\n",
        "        z, t = self.pos_emb(L)\n",
        "        h = self.implicit_filter(z)\n",
        "        h = self.modulation(t, h)\n",
        "        return h\n",
        "\n",
        "    def forward(self, x, L, k=None, bias=None, *args, **kwargs):\n",
        "        if k is None: k = self.filter(L)\n",
        "\n",
        "        # Ensure compatibility with filters that return a tuple\n",
        "        k = k[0] if type(k) is tuple else k\n",
        "\n",
        "        y = fftconv(x, k, bias)\n",
        "        return y\n",
        "\n",
        "\n",
        "class HyenaOperator(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model,\n",
        "            l_max,\n",
        "            order=2,\n",
        "            filter_order=64,\n",
        "            dropout=0.0,\n",
        "            filter_dropout=0.0,\n",
        "            **filter_args,\n",
        "        ):\n",
        "        r\"\"\"\n",
        "        Hyena operator described in the paper https://arxiv.org/pdf/2302.10866.pdf\n",
        "\n",
        "        Args:\n",
        "            d_model (int): Dimension of the input and output embeddings (width of the layer)\n",
        "            l_max: (int): Maximum input sequence length. Defaults to None\n",
        "            order: (int): Depth of the Hyena recurrence. Defaults to 2\n",
        "            dropout: (float): Dropout probability. Defaults to 0.0\n",
        "            filter_dropout: (float): Dropout probability for the filter. Defaults to 0.0\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.l_max = l_max\n",
        "        self.order = order\n",
        "        inner_width = d_model * (order + 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.in_proj = nn.Linear(d_model, inner_width)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.short_filter = nn.Conv1d(\n",
        "            inner_width,\n",
        "            inner_width,\n",
        "            3,\n",
        "            padding=2,\n",
        "            groups=inner_width\n",
        "        )\n",
        "        self.filter_fn = HyenaFilter(\n",
        "            d_model * (order - 1),\n",
        "            order=filter_order,\n",
        "            seq_len=l_max,\n",
        "            channels=1,\n",
        "            dropout=filter_dropout,\n",
        "            **filter_args\n",
        "        )\n",
        "\n",
        "    def forward(self, u, *args, **kwargs):\n",
        "        l = u.size(-2)\n",
        "        l_filter = min(l, self.l_max)\n",
        "        u = self.in_proj(u)\n",
        "        u = rearrange(u, 'b l d -> b d l')\n",
        "\n",
        "        uc = self.short_filter(u)[...,:l_filter]\n",
        "        *x, v = uc.split(self.d_model, dim=1)\n",
        "\n",
        "        k = self.filter_fn.filter(l_filter)[0]\n",
        "        k = rearrange(k, 'l (o d) -> o d l', o=self.order - 1)\n",
        "        bias = rearrange(self.filter_fn.bias, '(o d) -> o d', o=self.order - 1)\n",
        "\n",
        "        for o, x_i in enumerate(reversed(x[1:])):\n",
        "            v = self.dropout(v * x_i)\n",
        "            v = self.filter_fn(v, l_filter, k=k[o], bias=bias[o])\n",
        "\n",
        "        y = rearrange(v * x[0], 'b d l -> b l d')\n",
        "\n",
        "        y = self.out_proj(y)\n",
        "        return y\n",
        "\n",
        "#@title Self-Attention (alternative)\n",
        "\n",
        "\"\"\"\n",
        "If you'd like to try the HyenaDNA model using attention instead, you can. ie,\n",
        "use a regular decoder only Transformer.\n",
        "\"\"\"\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Implement the scaled dot product attention with softmax.\n",
        "    Arguments\n",
        "    ---------\n",
        "        softmax_scale: The temperature to use for the softmax attention.\n",
        "                      (default: 1/sqrt(d_keys) where d_keys is computed at\n",
        "                      runtime)\n",
        "        attention_dropout: The dropout rate to apply to the attention\n",
        "                           (default: 0.0)\n",
        "    \"\"\"\n",
        "    def __init__(self, causal=False, softmax_scale=None, attention_dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.causal = causal\n",
        "        self.softmax_scale = softmax_scale\n",
        "        self.dropout_p = attention_dropout\n",
        "\n",
        "    def forward(self, qkv, causal=None, key_padding_mask=None):\n",
        "        \"\"\"Implements the multihead softmax attention.\n",
        "        Arguments\n",
        "        ---------\n",
        "            qkv: The tensor containing the query, key, and value. (B, S, 3, H, D)\n",
        "            causal: if passed, will override self.causal\n",
        "            key_padding_mask: boolean mask to apply to the attention weights. True means to keep,\n",
        "                False means to mask out. (B, S)\n",
        "        \"\"\"\n",
        "        batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
        "        causal = self.causal if causal is None else causal\n",
        "        q, k, v = qkv.unbind(dim=2)\n",
        "        softmax_scale = self.softmax_scale or 1.0 / math.sqrt(q.shape[-1])\n",
        "        scores = torch.einsum('bthd,bshd->bhts', q, k * softmax_scale)\n",
        "        if key_padding_mask is not None:\n",
        "            padding_mask = torch.full((batch_size, seqlen), -10000.0, dtype=scores.dtype,\n",
        "                                      device=scores.device)\n",
        "            padding_mask.masked_fill_(key_padding_mask, 0.0)\n",
        "            # TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)\n",
        "            scores = scores + rearrange(padding_mask, 'b s -> b 1 1 s')\n",
        "        if causal:\n",
        "            # \"triu_tril_cuda_template\" not implemented for 'BFloat16'\n",
        "            # So we have to construct the mask in float\n",
        "            causal_mask = torch.triu(torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1)\n",
        "            # TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)\n",
        "            scores = scores + causal_mask.to(dtype=scores.dtype)\n",
        "        attention = torch.softmax(scores, dim=-1, dtype=v.dtype)\n",
        "        attention_drop = F.dropout(attention, self.dropout_p if self.training else 0.0)\n",
        "        output = torch.einsum('bhts,bshd->bthd', attention_drop, v)\n",
        "        return output\n",
        "\n",
        "class MHA(nn.Module):\n",
        "    \"\"\"Multi-head self-attention and cross-attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, bias=True, dropout=0.0,\n",
        "                 softmax_scale=None, causal=False, layer_idx=None, dwconv=False,return_residual=False,device=None, dtype=None) -> None:\n",
        "        \"\"\"\n",
        "            return_residual: whether to return the input x along with the output. This is for\n",
        "                performance reason: for post-norm architecture, returning the input allows us\n",
        "                to fuse the backward of nn.Linear with the residual connection.\n",
        "        \"\"\"\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.causal = causal\n",
        "        self.layer_idx = layer_idx\n",
        "        self.dwconv = dwconv\n",
        "        self.return_residual = return_residual\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        assert self.embed_dim % num_heads == 0, \"self.kdim must be divisible by num_heads\"\n",
        "        self.head_dim = self.embed_dim // num_heads\n",
        "\n",
        "        linear_cls = nn.Linear\n",
        "        linear_resid_cls = LinearResidual\n",
        "        inner_attn_cls =  SelfAttention\n",
        "\n",
        "        if not self.return_residual:\n",
        "            self.Wqkv = linear_cls(embed_dim, 3 * embed_dim, bias=bias, **factory_kwargs)\n",
        "        else:\n",
        "            self.Wqkv = linear_resid_cls(embed_dim, 3 * embed_dim, bias=bias, **factory_kwargs)\n",
        "        if self.dwconv:\n",
        "            self.dwconv_qkv = nn.Conv1d(3 * embed_dim, 3 * embed_dim, kernel_size=3, padding=2,\n",
        "                                        groups=3 * embed_dim)\n",
        "\n",
        "        self.inner_attn = inner_attn_cls(causal=causal, softmax_scale=softmax_scale,\n",
        "                                         attention_dropout=dropout)\n",
        "\n",
        "        # output projection always have the bias (for now)\n",
        "        self.out_proj = linear_cls(embed_dim, embed_dim, **factory_kwargs)\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n",
        "                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n",
        "                is the is the sum of the sequence lengths in the batch.\n",
        "            cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n",
        "                of the sequences in the batch, used to index into x. Only applicable when using\n",
        "                FlashAttention.\n",
        "            max_seqlen: int. Maximum sequence length in the batch.\n",
        "            key_padding_mask: boolean mask, True means to keep, False means to mask out.\n",
        "                (batch, seqlen). Only applicable when not using FlashAttention.\n",
        "            mixer_subset: for cross-attention only. If not None, will take a subset of x\n",
        "                before applying the query projection. Useful for e.g., ViT where we only care\n",
        "                about the CLS token in the last layer.\n",
        "            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n",
        "            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n",
        "        \"\"\"\n",
        "\n",
        "        kwargs = ({'key_padding_mask': key_padding_mask, **kwargs})\n",
        "\n",
        "        if not self.return_residual:\n",
        "            qkv = self.Wqkv(x)\n",
        "        else:\n",
        "            qkv, x = self.Wqkv(x)\n",
        "        if self.dwconv:\n",
        "            qkv = rearrange(self.dwconv_qkv(rearrange(qkv, 'b s d -> b d s'))[..., :-2],\n",
        "                            'b d s -> b s d').contiguous()\n",
        "        qkv = rearrange(qkv, '... (three h d) -> ... three h d', three=3, d=self.head_dim)\n",
        "\n",
        "        context = self.inner_attn(qkv, **kwargs)\n",
        "\n",
        "        out = self.out_proj(rearrange(context, '... h d -> ... (h d)'))\n",
        "        return out if not self.return_residual else (out, x)\n",
        "\n",
        "#@title MLP layer\n",
        "\n",
        "\"\"\"\n",
        "The MLP layer after the mixer layer (HyenaOperator).\n",
        "\"\"\"\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, activation=F.gelu,\n",
        "                 return_residual=False, device=None, dtype=None):\n",
        "        \"\"\"\n",
        "        From https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/modules/mlp.py\n",
        "        \"\"\"\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.return_residual = return_residual\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features, **factory_kwargs)\n",
        "        self.activation = activation\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features, **factory_kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.fc1(x)\n",
        "        y = self.activation(y)\n",
        "        y = self.fc2(y)\n",
        "        return y if not self.return_residual else (y, x)\n",
        "\n",
        "#@title Block layer (Hyena + MLP layers)\n",
        "\n",
        "\"\"\"\n",
        "A block consists of a Mixer layer (Hyena or attention), and a MLP layer.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class LinearResidual(nn.Linear):\n",
        "    \"\"\"Wrap nn.Linear to return the residual as well. For compatibility with FusedDense.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return super().forward(input), input\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, mixer_cls=None, mlp_cls=None, norm_cls=nn.LayerNorm,\n",
        "                 dropout_cls=nn.Dropout, prenorm=True, resid_dropout1=0., resid_dropout2=0.,\n",
        "                 drop_path1=0., drop_path2=0.,\n",
        "                 return_residual=False,\n",
        "                 residual_in_fp32=False):\n",
        "        \"\"\"\n",
        "        From https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/modules/block.py\n",
        "        For prenorm=True, this Block has a slightly different structure compared to a regular\n",
        "        prenorm Transformer block.\n",
        "        The standard block is: LN -> MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add.\n",
        "        [Ref: https://arxiv.org/abs/2002.04745]\n",
        "        Here we have: Dropout -> Add -> LN -> MHA -> Dropout -> Add -> LN -> MLP, returning both\n",
        "        the hidden_states (output of the MLP) and the residual.\n",
        "        This is for performance reasons, as we can fuse the dropout, add and LayerNorm.\n",
        "        The residual needs to be provided (except for the very first block).\n",
        "        For prenorm=False, this Block has the same structure as a regular postnorm Transformer\n",
        "        block: MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add -> LN.\n",
        "        return_residual: whether each of the sub-layers (mixer and mlp) will return the residual.\n",
        "        This is for performance reason: for post-norm architecture, returning the input allows us\n",
        "        to fuse the backward of nn.Linear with the residual connection.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.prenorm = prenorm\n",
        "        self.return_residual = return_residual\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "        if self.residual_in_fp32:\n",
        "            assert self.prenorm, 'residual_in_fp32 is only compatible with prenorm=True'\n",
        "        if mixer_cls is None:\n",
        "            mixer_cls = partial(MHA, num_heads=dim // 64)\n",
        "        if mlp_cls is None:\n",
        "            mlp_cls = partial(Mlp, hidden_features=4 * dim)\n",
        "        self.mixer = mixer_cls()\n",
        "        self.dropout1 = dropout_cls(resid_dropout1)\n",
        "        self.drop_path1 = StochasticDepth(drop_path1, mode='row')\n",
        "        self.norm1 = norm_cls(dim)\n",
        "        self.mlp = mlp_cls(dim)\n",
        "        if not isinstance(self.mlp, nn.Identity):\n",
        "            self.dropout2 = dropout_cls(resid_dropout2)\n",
        "            self.drop_path2 = StochasticDepth(drop_path2, mode='row')\n",
        "            self.norm2 = norm_cls(dim)\n",
        "\n",
        "    def forward(self, hidden_states, residual = None,\n",
        "                mixer_subset=None, mixer_kwargs=None):\n",
        "        r\"\"\"Pass the input through the encoder layer.\n",
        "        Args:\n",
        "            hidden_states: the sequence to the encoder layer (required).\n",
        "            residual: if postnorm, residual=None, If prenorm, hidden_states = Attn/MLP(LN(residual))\n",
        "            mixer_subset: for cross-attention only. If not None, will take a subset of x\n",
        "                before applying the query projection. Useful for e.g., ViT where we only care\n",
        "                about the CLS token in the last layer.\n",
        "        \"\"\"\n",
        "        if self.prenorm:\n",
        "            dropped = self.drop_path1(self.dropout1(hidden_states))\n",
        "            residual = (dropped + residual) if residual is not None else dropped\n",
        "            hidden_states = self.norm1(residual.to(dtype=self.norm1.weight.dtype))\n",
        "            if self.residual_in_fp32:\n",
        "                residual = residual.to(torch.float32)\n",
        "            if mixer_kwargs is None:\n",
        "                mixer_kwargs = {}\n",
        "            if mixer_subset is not None:\n",
        "                mixer_kwargs['mixer_subset'] = mixer_subset\n",
        "            hidden_states = self.mixer(hidden_states, **mixer_kwargs)\n",
        "            if mixer_subset is not None:\n",
        "                residual = residual[:, mixer_subset]\n",
        "            if not isinstance(self.mlp, nn.Identity):\n",
        "                dropped = self.drop_path2(self.dropout2(hidden_states))\n",
        "                residual = (dropped + residual) if residual is not None else dropped\n",
        "                hidden_states = self.norm2(residual.to(dtype=self.norm2.weight.dtype))\n",
        "                if self.residual_in_fp32:\n",
        "                    residual = residual.to(torch.float32)\n",
        "\n",
        "                hidden_states = self.mlp(hidden_states)\n",
        "            return hidden_states, residual\n",
        "        else:\n",
        "            assert residual is None\n",
        "            mixer_out = self.mixer(\n",
        "                hidden_states, **(mixer_kwargs if mixer_kwargs is not None else {})\n",
        "            )\n",
        "            if self.return_residual:  # mixer out is actually a pair here\n",
        "                mixer_out, hidden_states = mixer_out\n",
        "\n",
        "            hidden_states = self.norm1((self.drop_path1(self.dropout1(mixer_out))\n",
        "                                        + hidden_states).to(dtype=self.norm1.weight.dtype))\n",
        "\n",
        "            if not isinstance(self.mlp, nn.Identity):\n",
        "                mlp_out = self.mlp(hidden_states)\n",
        "                if self.return_residual:  # mlp out is actually a pair here\n",
        "                    mlp_out, hidden_states = mlp_out\n",
        "\n",
        "                hidden_states = self.norm2((self.drop_path2(self.dropout2(mlp_out))\n",
        "                                            + hidden_states).to(dtype=self.norm2.weight.dtype))\n",
        "\n",
        "            return hidden_states\n",
        "\n",
        "def create_mixer_cls(layer=None,\n",
        "                     attn_layer_idx=None, attn_cfg=None, layer_idx=None,\n",
        "                     device=None, dtype=None):\n",
        "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "    if attn_layer_idx is not None and layer_idx in attn_layer_idx:\n",
        "        causal = True if attn_cfg is None else attn_cfg.pop('causal', True)\n",
        "\n",
        "        mha_cls = MHA\n",
        "\n",
        "        mixer_cls = partial(mha_cls, causal=causal, layer_idx=layer_idx,\n",
        "                            **(attn_cfg if attn_cfg is not None else {}),**factory_kwargs)\n",
        "    else:\n",
        "        # mixer_cls = instantiate(registry.layer, layer, partial=True, layer_idx=layer_idx, **factory_kwargs)\n",
        "\n",
        "        mixer_cls = partial(HyenaOperator, **layer)\n",
        "\n",
        "    return mixer_cls\n",
        "\n",
        "def create_mlp_cls(d_model, d_inner=None, device=None, dtype=None):\n",
        "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "    inner_dim = d_inner if d_inner is not None else 4 * d_model\n",
        "\n",
        "    mlp_cls = partial(Mlp, hidden_features=inner_dim,\n",
        "                          activation=partial(F.gelu, approximate='tanh'), **factory_kwargs)\n",
        "\n",
        "    return mlp_cls\n",
        "\n",
        "\n",
        "def create_block(d_model, d_inner=None,\n",
        "                 layer=None, attn_layer_idx=None,\n",
        "                 attn_cfg=None, layer_norm_epsilon=1e-5,\n",
        "                 resid_dropout1=0.0, resid_dropout2=0.0, residual_in_fp32=False,\n",
        "                 layer_idx=None,\n",
        "                 device=None, dtype=None):\n",
        "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "    mixer_cls = create_mixer_cls(layer=layer,\n",
        "                                 attn_layer_idx=attn_layer_idx,\n",
        "                                 attn_cfg=attn_cfg, layer_idx=layer_idx,\n",
        "                                 **factory_kwargs)\n",
        "    mlp_cls = create_mlp_cls(d_model, d_inner=d_inner,\n",
        "                             **factory_kwargs)\n",
        "    norm_cls = partial(nn.LayerNorm, eps=layer_norm_epsilon, **factory_kwargs)\n",
        "    block = Block(d_model, mixer_cls, mlp_cls, norm_cls=norm_cls,\n",
        "                  prenorm=True, resid_dropout1=resid_dropout1, resid_dropout2=resid_dropout2,residual_in_fp32=residual_in_fp32)\n",
        "    block.layer_idx = layer_idx\n",
        "    return block\n",
        "\n",
        "\n",
        "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
        "def _init_weights(module, n_layer, initializer_range=0.02, rescale_prenorm_residual=True,\n",
        "                  glu_act=False):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.normal_(module.weight, std=initializer_range)\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        nn.init.normal_(module.weight, std=initializer_range)\n",
        "\n",
        "    if rescale_prenorm_residual:\n",
        "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
        "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
        "        #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n",
        "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
        "        #\n",
        "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
        "        for name, p in module.named_parameters():\n",
        "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
        "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
        "                nn.init.normal_(p, mean=0.0, std=initializer_range / math.sqrt(2 * n_layer))\n",
        "            # If using GLU activation for now, we scale the std by 2\n",
        "            elif name in [\"output_linear.0.weight\"]:\n",
        "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
        "                if not glu_act:\n",
        "                    nn.init.normal_(p, mean=0.0, std=initializer_range / math.sqrt(2 * n_layer))\n",
        "                else:\n",
        "                    out_features = p.shape[0]\n",
        "                    # Multiplying the first half of the matrix by 2 since sigmoid scales it down by 0.5\n",
        "                    # on average.\n",
        "                    nn.init.normal_(p[:out_features // 2], mean=0.0, std=initializer_range / math.sqrt(2 * n_layer) * 2)\n",
        "\n",
        "#@title Backbone model (stack of blocks)\n",
        "\n",
        "\"\"\"\n",
        "A backbone model consists of a stack of blocks. If you use attention, then\n",
        "positional embeddings are included. When using Hyena, then the pos emb\n",
        "revert to doing nothing.\n",
        "\"\"\"\n",
        "\n",
        "class GPT2Embeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, vocab_size, max_position_embeddings, padding_idx=None,\n",
        "                 word_embed_proj_dim=None, device=None, dtype=None):\n",
        "        \"\"\"\n",
        "            If max_position_embeddings <= 0, there's no position embeddings\n",
        "            If word_embe_proj_dim is not None (e.g., OPT-350m), we embed to that dimension\n",
        "                the project up to embed_dim\n",
        "        \"\"\"\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        if word_embed_proj_dim is None:\n",
        "            self.word_embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx,\n",
        "                                                **factory_kwargs)\n",
        "            self.project_in = None\n",
        "        else:\n",
        "            self.word_embeddings = nn.Embedding(vocab_size, word_embed_proj_dim,\n",
        "                                                padding_idx=padding_idx, **factory_kwargs)\n",
        "            self.project_in = nn.Linear(word_embed_proj_dim, embed_dim, bias=False,\n",
        "                                        **factory_kwargs)\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        if self.max_position_embeddings > 0:\n",
        "            self.position_embeddings = nn.Embedding(max_position_embeddings, embed_dim,\n",
        "                                                    **factory_kwargs)\n",
        "\n",
        "    def forward(self, input_ids, position_ids=None):\n",
        "        \"\"\"\n",
        "            input_ids: (batch, seqlen)\n",
        "            position_ids: (batch, seqlen)\n",
        "        \"\"\"\n",
        "        batch_size, seqlen = input_ids.shape\n",
        "        embeddings = self.word_embeddings(input_ids)\n",
        "        if self.project_in is not None:\n",
        "            embeddings = self.project_in(embeddings)\n",
        "        if self.max_position_embeddings > 0:\n",
        "            if position_ids is None:\n",
        "                position_ids = torch.arange(seqlen, dtype=torch.long, device=input_ids.device)\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings = embeddings + position_embeddings\n",
        "        return embeddings\n",
        "\n",
        "class LMBackbone(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,\n",
        "                 process_group=None, layer=None,\n",
        "                 attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,\n",
        "                 resid_dropout: float = 0.0, embed_dropout: float = 0.1,\n",
        "                 layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,\n",
        "                 device=None, dtype=None, **kwargs) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        self.process_group = process_group\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "        # note max_position_embeddings is 0 for Hyena, and therefore isn't used\n",
        "        self.embeddings = GPT2Embeddings(d_model, vocab_size, max_position_embeddings,\n",
        "                                             **factory_kwargs)\n",
        "\n",
        "        self.layers = nn.ModuleList([create_block(\n",
        "            d_model, d_inner=d_inner,\n",
        "            layer=layer, attn_layer_idx=attn_layer_idx,\n",
        "            attn_cfg=attn_cfg, layer_norm_epsilon=layer_norm_epsilon,\n",
        "            resid_dropout1=embed_dropout if i == 0 else resid_dropout,\n",
        "            resid_dropout2=resid_dropout, residual_in_fp32=residual_in_fp32,layer_idx=i,\n",
        "            **factory_kwargs,\n",
        "        ) for i in range(n_layer)])\n",
        "\n",
        "        self.drop_f = nn.Dropout(resid_dropout)\n",
        "        self.ln_f = nn.LayerNorm(d_model, eps=layer_norm_epsilon, **factory_kwargs)\n",
        "\n",
        "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
        "                           **(initializer_cfg if initializer_cfg is not None else {})))\n",
        "\n",
        "    def forward(self, input_ids, position_ids=None):\n",
        "        hidden_states = self.embeddings(input_ids, position_ids=position_ids,)\n",
        "        residual = None\n",
        "\n",
        "        for layer in self.layers:\n",
        "            hidden_states, residual = layer(hidden_states, residual)\n",
        "\n",
        "        dropped = self.drop_f(hidden_states)\n",
        "        residual = (dropped + residual) if residual is not None else dropped\n",
        "        hidden_states = self.ln_f(residual.to(dtype=self.ln_f.weight.dtype))\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "#@title Decoder head layer\n",
        "\n",
        "\"\"\"\n",
        "A simple decoder head (using MLP) to predict a sequence level classification.\n",
        "You have the option to average across all the tokens in a sequence or using the\n",
        "\"last\" token to classify.  At least, those 2 worked best for us, but we provide\n",
        "other \"modes\" as well.\n",
        "\n",
        "We only need this for classification.  Otherwise we'll use the hidden\n",
        "states of the backbone as embeddings.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class SequenceDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, d_model, d_output=None, l_output=None, use_lengths=False, mode=\"last\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_transform = nn.Identity() if d_output is None else nn.Linear(d_model, d_output)\n",
        "\n",
        "        if l_output is None:\n",
        "            self.l_output = None\n",
        "            self.squeeze = False\n",
        "        elif l_output == 0:\n",
        "            # Equivalent to getting an output of length 1 and then squeezing\n",
        "            self.l_output = 1\n",
        "            self.squeeze = True\n",
        "        else:\n",
        "            assert l_output > 0\n",
        "            self.l_output = l_output\n",
        "            self.squeeze = False\n",
        "\n",
        "        self.use_lengths = use_lengths\n",
        "        self.mode = mode\n",
        "\n",
        "        if mode == 'ragged':\n",
        "            assert not use_lengths\n",
        "\n",
        "    def forward(self, x, state=None, lengths=None, l_output=None):\n",
        "        \"\"\"\n",
        "        x: (n_batch, l_seq, d_model)\n",
        "        Returns: (n_batch, l_output, d_output)\n",
        "        \"\"\"\n",
        "\n",
        "        if self.l_output is None:\n",
        "            if l_output is not None:\n",
        "                assert isinstance(l_output, int)  # Override by pass in\n",
        "            else:\n",
        "                # Grab entire output\n",
        "                l_output = x.size(-2)\n",
        "            squeeze = False\n",
        "        else:\n",
        "            l_output = self.l_output\n",
        "            squeeze = self.squeeze\n",
        "\n",
        "        if self.mode == \"last\":\n",
        "            restrict = lambda x: x[..., -l_output:, :]\n",
        "        elif self.mode == \"first\":\n",
        "            restrict = lambda x: x[..., :l_output, :]\n",
        "        elif self.mode == \"pool\":\n",
        "            restrict = lambda x: (\n",
        "                torch.cumsum(x, dim=-2)\n",
        "                / torch.arange(\n",
        "                    1, 1 + x.size(-2), device=x.device, dtype=x.dtype\n",
        "                ).unsqueeze(-1)\n",
        "            )[..., -l_output:, :]\n",
        "\n",
        "            def restrict(x):\n",
        "                L = x.size(-2)\n",
        "                s = x.sum(dim=-2, keepdim=True)\n",
        "                if l_output > 1:\n",
        "                    c = torch.cumsum(x[..., -(l_output - 1) :, :].flip(-2), dim=-2)\n",
        "                    c = F.pad(c, (0, 0, 1, 0))\n",
        "                    s = s - c  # (B, l_output, D)\n",
        "                    s = s.flip(-2)\n",
        "                denom = torch.arange(\n",
        "                    L - l_output + 1, L + 1, dtype=x.dtype, device=x.device\n",
        "                )\n",
        "                s = s / denom\n",
        "                return s\n",
        "\n",
        "        elif self.mode == \"sum\":\n",
        "            restrict = lambda x: torch.cumsum(x, dim=-2)[..., -l_output:, :]\n",
        "            # TODO use same restrict function as pool case\n",
        "        elif self.mode == 'ragged':\n",
        "            assert lengths is not None, \"lengths must be provided for ragged mode\"\n",
        "            # remove any additional padding (beyond max length of any sequence in the batch)\n",
        "            restrict = lambda x: x[..., : max(lengths), :]\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                \"Mode must be ['last' | 'first' | 'pool' | 'sum']\"\n",
        "            )\n",
        "\n",
        "        # Restrict to actual length of sequence\n",
        "        if self.use_lengths:\n",
        "            assert lengths is not None\n",
        "            x = torch.stack(\n",
        "                [\n",
        "                    restrict(out[..., :length, :])\n",
        "                    for out, length in zip(torch.unbind(x, dim=0), lengths)\n",
        "                ],\n",
        "                dim=0,\n",
        "            )\n",
        "        else:\n",
        "            x = restrict(x)\n",
        "\n",
        "        if squeeze:\n",
        "            assert x.size(-2) == 1\n",
        "            x = x.squeeze(-2)\n",
        "\n",
        "        x = self.output_transform(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def step(self, x, state=None):\n",
        "        # Ignore all length logic\n",
        "        return self.output_transform(x)\n",
        "\n",
        "#@title Model (backbone + head)\n",
        "\n",
        "\"\"\"\n",
        "Putting it all together, the model consists of a backbone model\n",
        "and a decoder head (you can turn off head for embeddings only too).\n",
        "\n",
        "Here we use a simple head to do multi-classification, but\n",
        "can also swap the head to do next token prediction too.  We defer to the main\n",
        "HyenaDNA for that code, since pretraining with next token prediction isn't quite\n",
        "feasible on colab.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class HyenaDNAModel(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,\n",
        "                 layer=None, attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,\n",
        "                 resid_dropout: float = 0.0, embed_dropout: float = 0.1,\n",
        "                 layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,\n",
        "                 pad_vocab_size_multiple: int = 1, use_head=False, n_classes: int = 2,\n",
        "                 device=None, dtype=None, **kwargs) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        if vocab_size % pad_vocab_size_multiple != 0:\n",
        "            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n",
        "\n",
        "        self.use_head = use_head\n",
        "\n",
        "        # check if layer (config) has d_model (HF code differs from main Safari code)\n",
        "        if 'd_model' not in layer:\n",
        "            layer['d_model'] = d_model\n",
        "\n",
        "        self.backbone = LMBackbone(\n",
        "            d_model=d_model, n_layer=n_layer, d_inner=d_inner, vocab_size=vocab_size,\n",
        "            layer=layer, attn_layer_idx=attn_layer_idx, attn_cfg=attn_cfg,\n",
        "            max_position_embeddings=max_position_embeddings,\n",
        "            resid_dropout=resid_dropout, embed_dropout=embed_dropout,\n",
        "            layer_norm_epsilon=layer_norm_epsilon,\n",
        "            initializer_cfg=initializer_cfg, residual_in_fp32=residual_in_fp32,\n",
        "            **factory_kwargs, **kwargs\n",
        "        )\n",
        "\n",
        "        # we only need a head if doing classification, otherwise we'll use the\n",
        "        # hidden states as embeddings\n",
        "        if self.use_head:\n",
        "            self.head = SequenceDecoder(d_model=d_model, d_output=n_classes, l_output=0, mode='pool')\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
        "                           **(initializer_cfg if initializer_cfg is not None else {})))\n",
        "\n",
        "        # if self.use_head:\n",
        "        #     self.tie_weights()\n",
        "\n",
        "    # def tie_weights(self):\n",
        "    #     self.head.weight = self.backbone.embeddings.word_embeddings.weight\n",
        "\n",
        "    def forward(self, input_ids, position_ids=None, state=None): # state for the repo interface\n",
        "        hidden_states = self.backbone(input_ids, position_ids=position_ids)\n",
        "\n",
        "        if self.use_head:\n",
        "            return self.head(hidden_states)\n",
        "        else:\n",
        "            return hidden_states\n",
        "\n",
        "\"\"\"# Data pipeline\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#@title Tokenizer\n",
        "\n",
        "\"\"\"\n",
        "Just a simple character level tokenizer.\n",
        "\n",
        "From: https://github.com/dariush-bahrami/character-tokenizer/blob/master/charactertokenizer/core.py\n",
        "\n",
        "CharacterTokenzier for Hugging Face Transformers.\n",
        "This is heavily inspired from CanineTokenizer in transformers package.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class CharacterTokenizer(PreTrainedTokenizer):\n",
        "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
        "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
        "        Args:\n",
        "            characters (Sequence[str]): List of desired characters. Any character which\n",
        "                is not included in this list will be replaced by a special token called\n",
        "                [UNK] with id=6. Following are list of all of the special tokens with\n",
        "                their corresponding ids:\n",
        "                    \"[CLS]\": 0\n",
        "                    \"[SEP]\": 1\n",
        "                    \"[BOS]\": 2\n",
        "                    \"[MASK]\": 3\n",
        "                    \"[PAD]\": 4\n",
        "                    \"[RESERVED]\": 5\n",
        "                    \"[UNK]\": 6\n",
        "                an id (starting at 7) will be assigned to each character.\n",
        "            model_max_length (int): Model maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.characters = characters\n",
        "        self.model_max_length = model_max_length\n",
        "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
        "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
        "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
        "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
        "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
        "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
        "\n",
        "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
        "\n",
        "        super().__init__(\n",
        "            bos_token=bos_token,\n",
        "            eos_token=sep_token,\n",
        "            sep_token=sep_token,\n",
        "            cls_token=cls_token,\n",
        "            pad_token=pad_token,\n",
        "            mask_token=mask_token,\n",
        "            unk_token=unk_token,\n",
        "            add_prefix_space=False,\n",
        "            model_max_length=model_max_length,\n",
        "            padding_side=padding_side,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        self._vocab_str_to_int = {\n",
        "            \"[CLS]\": 0,\n",
        "            \"[SEP]\": 1,\n",
        "            \"[BOS]\": 2,\n",
        "            \"[MASK]\": 3,\n",
        "            \"[PAD]\": 4,\n",
        "            \"[RESERVED]\": 5,\n",
        "            \"[UNK]\": 6,\n",
        "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
        "        }\n",
        "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return len(self._vocab_str_to_int)\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "        return list(text)\n",
        "\n",
        "    def _convert_token_to_id(self, token: str) -> int:\n",
        "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
        "\n",
        "    def _convert_id_to_token(self, index: int) -> str:\n",
        "        return self._vocab_int_to_str[index]\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        result = cls + token_ids_0 + sep\n",
        "        if token_ids_1 is not None:\n",
        "            result += token_ids_1 + sep\n",
        "        return result\n",
        "\n",
        "    def get_special_tokens_mask(\n",
        "        self,\n",
        "        token_ids_0: List[int],\n",
        "        token_ids_1: Optional[List[int]] = None,\n",
        "        already_has_special_tokens: bool = False,\n",
        "    ) -> List[int]:\n",
        "        if already_has_special_tokens:\n",
        "            return super().get_special_tokens_mask(\n",
        "                token_ids_0=token_ids_0,\n",
        "                token_ids_1=token_ids_1,\n",
        "                already_has_special_tokens=True,\n",
        "            )\n",
        "\n",
        "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
        "        if token_ids_1 is not None:\n",
        "            result += ([0] * len(token_ids_1)) + [1]\n",
        "        return result\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "\n",
        "        result = len(cls + token_ids_0 + sep) * [0]\n",
        "        if token_ids_1 is not None:\n",
        "            result += len(token_ids_1 + sep) * [1]\n",
        "        return result\n",
        "\n",
        "    def get_config(self) -> Dict:\n",
        "        return {\n",
        "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
        "            \"model_max_length\": self.model_max_length,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
        "        cfg = {}\n",
        "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
        "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
        "        return cls(**cfg)\n",
        "\n",
        "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
        "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
        "        cfg = self.get_config()\n",
        "        with open(cfg_file, \"w\") as f:\n",
        "            json.dump(cfg, f, indent=4)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
        "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
        "        with open(cfg_file) as f:\n",
        "            cfg = json.load(f)\n",
        "        return cls.from_config(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uY_NE7lCenB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import torch\n",
        "# import transformers\n",
        "from transformers import PreTrainedModel\n",
        "import re\n",
        "\n",
        "\n",
        "# helper 1\n",
        "def inject_substring(orig_str):\n",
        "    \"\"\"Hack to handle matching keys between models trained with and without\n",
        "    gradient checkpointing.\"\"\"\n",
        "\n",
        "    # modify for mixer keys\n",
        "    pattern = r\"\\.mixer\"\n",
        "    injection = \".mixer.layer\"\n",
        "\n",
        "    modified_string = re.sub(pattern, injection, orig_str)\n",
        "\n",
        "    # modify for mlp keys\n",
        "    pattern = r\"\\.mlp\"\n",
        "    injection = \".mlp.layer\"\n",
        "\n",
        "    modified_string = re.sub(pattern, injection, modified_string)\n",
        "\n",
        "    return modified_string\n",
        "\n",
        "# helper 2\n",
        "def load_weights(scratch_dict, pretrained_dict, checkpointing=False):\n",
        "    \"\"\"Loads pretrained (backbone only) weights into the scratch state dict.\"\"\"\n",
        "\n",
        "    # loop thru state dict of scratch\n",
        "    # find the corresponding weights in the loaded model, and set it\n",
        "\n",
        "    # need to do some state dict \"surgery\"\n",
        "    for key, value in scratch_dict.items():\n",
        "        if 'backbone' in key:\n",
        "            # the state dicts differ by one prefix, '.model', so we add that\n",
        "            key_loaded = 'model.' + key\n",
        "            # breakpoint()\n",
        "            # need to add an extra \".layer\" in key\n",
        "            if checkpointing:\n",
        "                key_loaded = inject_substring(key_loaded)\n",
        "            try:\n",
        "                scratch_dict[key] = pretrained_dict[key_loaded]\n",
        "            except:\n",
        "                raise Exception('key mismatch in the state dicts!')\n",
        "\n",
        "    # scratch_dict has been updated\n",
        "    return scratch_dict\n",
        "\n",
        "class HyenaDNAPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "    base_model_prefix = \"hyenadna\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        pass\n",
        "\n",
        "    def forward(self, input_ids, **kwargs):\n",
        "        return self.model(input_ids, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls,\n",
        "                        path,\n",
        "                        model_name,\n",
        "                        download=False,\n",
        "                        config=None,\n",
        "                        device='cpu',\n",
        "                        use_head=False,\n",
        "                        n_classes=2,\n",
        "                      ):\n",
        "        # first check if it is a local path\n",
        "        pretrained_model_name_or_path = os.path.join(path, model_name)\n",
        "        if os.path.isdir(pretrained_model_name_or_path) and download == False:\n",
        "            if config is None:\n",
        "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
        "        else:\n",
        "            hf_url = f'https://huggingface.co/LongSafari/{model_name}'\n",
        "\n",
        "            subprocess.run(f'rm -rf {pretrained_model_name_or_path}', shell=True)\n",
        "            command = f'mkdir -p {path} && cd {path} && git lfs install && git clone {hf_url}'\n",
        "            subprocess.run(command, shell=True)\n",
        "\n",
        "            if config is None:\n",
        "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
        "\n",
        "        scratch_model = HyenaDNAModel(**config, use_head=use_head, n_classes=n_classes)  # the new model format\n",
        "        loaded_ckpt = torch.load(\n",
        "            os.path.join(pretrained_model_name_or_path, 'weights.ckpt'),\n",
        "            map_location=torch.device(device)\n",
        "        )\n",
        "\n",
        "        # need to load weights slightly different if using gradient checkpointing\n",
        "        if config.get(\"checkpoint_mixer\", False):\n",
        "            checkpointing = config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True\n",
        "        else:\n",
        "            checkpointing = False\n",
        "\n",
        "        # grab state dict from both and load weights\n",
        "        state_dict = load_weights(scratch_model.state_dict(), loaded_ckpt['state_dict'], checkpointing=checkpointing)\n",
        "\n",
        "        # scratch model has now been updated\n",
        "        scratch_model.load_state_dict(state_dict)\n",
        "        print(\"Loaded pretrained weights ok!\")\n",
        "        return scratch_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"# Inference (450k to 1M tokens)!\n",
        "\n",
        "If all you're interested in is getting embeddings on long DNA sequences\n",
        "(inference), then we can do that right here in Colab!\n",
        "\n",
        "\n",
        "*   We provide an example how to load the weights from Huggingface.\n",
        "*   On the free tier, which uses a\n",
        "T4 GPU w/16GB of memory, we can process 450k tokens / nucleotides.\n",
        "*   For processing 1M tokens, you'll need an A100, which Colab offers as a paid tier.\n",
        "*   (Don't forget to run the entire notebook above too)\n",
        "\n",
        "--\n",
        "\n",
        "To pretrain or fine-tune the 1M long sequence model (8 layers, d_model=256),\n",
        "you'll need 8 A100s 80GB, and all that code is in the main repo!\n",
        "\"\"\"\n",
        "\n",
        "#@title Single example\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "# import transformers\n",
        "from transformers import PreTrainedModel\n",
        "\n",
        "def inference_single():\n",
        "\n",
        "    '''\n",
        "    this selects which backbone to use, and grabs weights/ config from HF\n",
        "    4 options:\n",
        "      'hyenadna-tiny-1k-seqlen'   # fine-tune on colab ok\n",
        "      'hyenadna-small-32k-seqlen'\n",
        "      'hyenadna-medium-160k-seqlen'  # inference only on colab\n",
        "      'hyenadna-medium-450k-seqlen'  # inference only on colab\n",
        "      'hyenadna-large-1m-seqlen'  # inference only on colab\n",
        "    '''\n",
        "\n",
        "    # you only need to select which model to use here, we'll do the rest!\n",
        "    pretrained_model_name = 'hyenadna-large-1m-seqlen'\n",
        "\n",
        "    max_lengths = {\n",
        "        'hyenadna-tiny-1k-seqlen': 1024,\n",
        "        'hyenadna-small-32k-seqlen': 32768,\n",
        "        'hyenadna-medium-160k-seqlen': 160000,\n",
        "        'hyenadna-medium-450k-seqlen': 450000,  # T4 up to here\n",
        "        'hyenadna-large-1m-seqlen': 1_000_000,  # only A100 (paid tier)\n",
        "    }\n",
        "\n",
        "    max_length = max_lengths[pretrained_model_name]  # auto selects\n",
        "\n",
        "    # data settings:\n",
        "    use_padding = True\n",
        "    rc_aug = False  # reverse complement augmentation\n",
        "    add_eos = False  # add end of sentence token\n",
        "\n",
        "    # we need these for the decoder head, if using\n",
        "    use_head = False\n",
        "    n_classes = 2  # not used for embeddings only\n",
        "\n",
        "    # you can override with your own backbone config here if you want,\n",
        "    # otherwise we'll load the HF one in None\n",
        "    backbone_cfg = None\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # instantiate the model (pretrained here)\n",
        "    if pretrained_model_name in ['hyenadna-tiny-1k-seqlen',\n",
        "                                 'hyenadna-small-32k-seqlen',\n",
        "                                 'hyenadna-medium-160k-seqlen',\n",
        "                                 'hyenadna-medium-450k-seqlen',\n",
        "                                 'hyenadna-large-1m-seqlen']:\n",
        "        # use the pretrained Huggingface wrapper instead\n",
        "        model = HyenaDNAPreTrainedModel.from_pretrained(\n",
        "            './checkpoints',\n",
        "            pretrained_model_name,\n",
        "            download=True,\n",
        "            config=backbone_cfg,\n",
        "            device=device,\n",
        "            use_head=use_head,\n",
        "            n_classes=n_classes,\n",
        "        )\n",
        "\n",
        "    # from scratch\n",
        "    elif pretrained_model_name is None:\n",
        "        model = HyenaDNAModel(**backbone_cfg, use_head=use_head, n_classes=n_classes)\n",
        "\n",
        "    # create tokenizer\n",
        "    tokenizer = CharacterTokenizer(\n",
        "        characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
        "        model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
        "        add_special_tokens=False,  # we handle special tokens elsewhere\n",
        "        padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
        "    )\n",
        "\n",
        "    #### Single embedding example ####\n",
        "\n",
        "    # create a sample 450k long, prepare\n",
        "    sequence = 'ACTG' * int(max_length/4)\n",
        "    tok_seq = tokenizer(sequence)\n",
        "    tok_seq = tok_seq[\"input_ids\"]  # grab ids\n",
        "\n",
        "    # place on device, convert to tensor\n",
        "    tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim\n",
        "    tok_seq = tok_seq.to(device)\n",
        "\n",
        "    # prep model and forward\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        embeddings = model(tok_seq)\n",
        "\n",
        "    print(embeddings.shape)  # embeddings here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MfrNOS8CenC"
      },
      "source": [
        "### Loading a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCuNGVJpCenD",
        "outputId": "1fcbc730-85a3-488b-881b-ea7be20c4f29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Loaded pretrained weights ok!\n"
          ]
        }
      ],
      "source": [
        "pretrained_model_name = 'hyenadna-large-1m-seqlen'\n",
        "max_lengths = {\n",
        "    'hyenadna-tiny-1k-seqlen': 1024,\n",
        "    'hyenadna-small-32k-seqlen': 32768,\n",
        "    'hyenadna-medium-160k-seqlen': 160000,\n",
        "    'hyenadna-medium-450k-seqlen': 450000,  # T4 up to here\n",
        "    'hyenadna-large-1m-seqlen': 1_000_000,  # only A100 (paid tier)\n",
        "}\n",
        "\n",
        "max_length = max_lengths[pretrained_model_name]  # auto selects\n",
        "\n",
        "# data settings:\n",
        "use_padding = True\n",
        "rc_aug = False  # reverse complement augmentation\n",
        "add_eos = False  # add end of sentence token\n",
        "\n",
        "# we need these for the decoder head, if using\n",
        "use_head = False\n",
        "n_classes = 2  # not used for embeddings only\n",
        "\n",
        "# you can override with your own backbone config here if you want,\n",
        "# otherwise we'll load the HF one in None\n",
        "backbone_cfg = None\n",
        "\n",
        "device = 'cpu'\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# use the pretrained Huggingface wrapper instead\n",
        "model = HyenaDNAPreTrainedModel.from_pretrained(\n",
        "    './checkpoints',\n",
        "    pretrained_model_name,\n",
        "    download=True,\n",
        "    config=backbone_cfg,\n",
        "    device=device,\n",
        "    use_head=use_head,\n",
        "    n_classes=n_classes,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLq4T13sCenE"
      },
      "outputs": [],
      "source": [
        "\n",
        "tok_seq = torch.LongTensor([0, 1, 2, 3]*10).unsqueeze(0)  # unsqueeze for batch dim\n",
        "tok_seq = tok_seq.to(device)\n",
        "\n",
        "# prep model and forward\n",
        "model = model.to(device)\n",
        "model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8qdlYN3CenE"
      },
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n",
        "    embeddings = model(tok_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUBSS3tWCenE"
      },
      "source": [
        "### Init from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UxjpTXXCenF",
        "outputId": "538346a0-5097-4991-a299-1886ae4dafc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "pretrained_model_name = 'hyenadna-large-1m-seqlen'\n",
        "\n",
        "max_lengths = {\n",
        "    'hyenadna-tiny-1k-seqlen': 1024,\n",
        "    'hyenadna-small-32k-seqlen': 32768,\n",
        "    'hyenadna-medium-160k-seqlen': 160000,\n",
        "    'hyenadna-medium-450k-seqlen': 450000,  # T4 up to here\n",
        "    'hyenadna-large-1m-seqlen': 1_000_000,  # only A100 (paid tier)\n",
        "}\n",
        "\n",
        "max_length = max_lengths[pretrained_model_name]  # auto selects\n",
        "\n",
        "# data settings:\n",
        "use_padding = True\n",
        "rc_aug = False  # reverse complement augmentation\n",
        "add_eos = False  # add end of sentence token\n",
        "\n",
        "# we need these for the decoder head, if using\n",
        "use_head = False\n",
        "n_classes = 2  # not used for embeddings only\n",
        "\n",
        "# you can override with your own backbone config here if you want,\n",
        "# otherwise we'll load the HF one in None\n",
        "device = 'cpu'\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "# point to hf config\n",
        "backbone_cfg = json.load(open('./checkpoints/hyenadna-large-1m-seqlen/config.json'))\n",
        "\n",
        "model = HyenaDNAModel(**backbone_cfg, use_head=use_head, n_classes=n_classes)  # the new model format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTNN__eiCenF"
      },
      "outputs": [],
      "source": [
        "# test forward\n",
        "tok_seq = torch.LongTensor([0, 1, 2, 3]*10).unsqueeze(0)  # unsqueeze for batch dim\n",
        "tok_seq = tok_seq.to(device)\n",
        "\n",
        "# prep model and forward\n",
        "model = model.to(device)\n",
        "model = model.eval()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    embeddings = model(tok_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8sgAxesD_Qm"
      },
      "outputs": [],
      "source": [
        "#@title Tokenizer\n",
        "\n",
        "\"\"\"\n",
        "Just a simple character level tokenizer.\n",
        "\n",
        "From: https://github.com/dariush-bahrami/character-tokenizer/blob/master/charactertokenizer/core.py\n",
        "\n",
        "CharacterTokenzier for Hugging Face Transformers.\n",
        "This is heavily inspired from CanineTokenizer in transformers package.\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Sequence, Union\n",
        "\n",
        "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
        "\n",
        "\n",
        "class CharacterTokenizer(PreTrainedTokenizer):\n",
        "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
        "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
        "        Args:\n",
        "            characters (Sequence[str]): List of desired characters. Any character which\n",
        "                is not included in this list will be replaced by a special token called\n",
        "                [UNK] with id=6. Following are list of all of the special tokens with\n",
        "                their corresponding ids:\n",
        "                    \"[CLS]\": 0\n",
        "                    \"[SEP]\": 1\n",
        "                    \"[BOS]\": 2\n",
        "                    \"[MASK]\": 3\n",
        "                    \"[PAD]\": 4\n",
        "                    \"[RESERVED]\": 5\n",
        "                    \"[UNK]\": 6\n",
        "                an id (starting at 7) will be assigned to each character.\n",
        "            model_max_length (int): Model maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.characters = characters\n",
        "        self.model_max_length = model_max_length\n",
        "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
        "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
        "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
        "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
        "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
        "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
        "\n",
        "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
        "\n",
        "        super().__init__(\n",
        "            bos_token=bos_token,\n",
        "            eos_token=sep_token,\n",
        "            sep_token=sep_token,\n",
        "            cls_token=cls_token,\n",
        "            pad_token=pad_token,\n",
        "            mask_token=mask_token,\n",
        "            unk_token=unk_token,\n",
        "            add_prefix_space=False,\n",
        "            model_max_length=model_max_length,\n",
        "            padding_side=padding_side,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        self._vocab_str_to_int = {\n",
        "            \"[CLS]\": 0,\n",
        "            \"[SEP]\": 1,\n",
        "            \"[BOS]\": 2,\n",
        "            \"[MASK]\": 3,\n",
        "            \"[PAD]\": 4,\n",
        "            \"[RESERVED]\": 5,\n",
        "            \"[UNK]\": 6,\n",
        "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
        "        }\n",
        "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return len(self._vocab_str_to_int)\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "        return list(text)\n",
        "\n",
        "    def _convert_token_to_id(self, token: str) -> int:\n",
        "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
        "\n",
        "    def _convert_id_to_token(self, index: int) -> str:\n",
        "        return self._vocab_int_to_str[index]\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        result = cls + token_ids_0 + sep\n",
        "        if token_ids_1 is not None:\n",
        "            result += token_ids_1 + sep\n",
        "        return result\n",
        "\n",
        "    def get_special_tokens_mask(\n",
        "        self,\n",
        "        token_ids_0: List[int],\n",
        "        token_ids_1: Optional[List[int]] = None,\n",
        "        already_has_special_tokens: bool = False,\n",
        "    ) -> List[int]:\n",
        "        if already_has_special_tokens:\n",
        "            return super().get_special_tokens_mask(\n",
        "                token_ids_0=token_ids_0,\n",
        "                token_ids_1=token_ids_1,\n",
        "                already_has_special_tokens=True,\n",
        "            )\n",
        "\n",
        "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
        "        if token_ids_1 is not None:\n",
        "            result += ([0] * len(token_ids_1)) + [1]\n",
        "        return result\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "\n",
        "        result = len(cls + token_ids_0 + sep) * [0]\n",
        "        if token_ids_1 is not None:\n",
        "            result += len(token_ids_1 + sep) * [1]\n",
        "        return result\n",
        "\n",
        "    def get_config(self) -> Dict:\n",
        "        return {\n",
        "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
        "            \"model_max_length\": self.model_max_length,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
        "        cfg = {}\n",
        "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
        "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
        "        return cls(**cfg)\n",
        "\n",
        "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
        "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
        "        cfg = self.get_config()\n",
        "        with open(cfg_file, \"w\") as f:\n",
        "            json.dump(cfg, f, indent=4)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
        "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
        "        with open(cfg_file) as f:\n",
        "            cfg = json.load(f)\n",
        "        return cls.from_config(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3mhggNjEDBG"
      },
      "outputs": [],
      "source": [
        "#@title GenomicBenchmark dataset\n",
        "\n",
        "\"\"\"\n",
        "The GenomicBenchmarks dataset will automatically download to /contents on colab.\n",
        "There are 8 datasets to choose from.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from random import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from genomic_benchmarks.loc2seq import download_dataset\n",
        "from genomic_benchmarks.data_check import is_downloaded\n",
        "\n",
        "\n",
        "# helper functions\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def coin_flip():\n",
        "    return random.random() > 0.5\n",
        "\n",
        "\n",
        "string_complement_map = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'a': 't', 'c': 'g', 'g': 'c', 't': 'a'}\n",
        "# augmentation\n",
        "def string_reverse_complement(seq):\n",
        "    rev_comp = ''\n",
        "    for base in seq[::-1]:\n",
        "        if base in string_complement_map:\n",
        "            rev_comp += string_complement_map[base]\n",
        "        # if bp not complement map, use the same bp\n",
        "        else:\n",
        "            rev_comp += base\n",
        "    return rev_comp\n",
        "\n",
        "\n",
        "class GenomicBenchmarkDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    '''\n",
        "    Loop thru bed file, retrieve (chr, start, end), query fasta file for sequence.\n",
        "    Returns a generator that retrieves the sequence.\n",
        "\n",
        "    Genomic Benchmarks Dataset, from:\n",
        "    https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks\n",
        "\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        split,\n",
        "        max_length,\n",
        "        dataset_name='human_enhancers_cohn',\n",
        "        d_output=2, # default binary classification\n",
        "        dest_path=\"/content\", # default for colab\n",
        "        tokenizer=None,\n",
        "        tokenizer_name=None,\n",
        "        use_padding=None,\n",
        "        add_eos=False,\n",
        "        rc_aug=False,\n",
        "        return_augs=False,\n",
        "    ):\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.use_padding = use_padding\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "        self.tokenizer = tokenizer\n",
        "        self.return_augs = return_augs\n",
        "        self.add_eos = add_eos\n",
        "        self.d_output = d_output  # needed for decoder to grab\n",
        "        self.rc_aug = rc_aug\n",
        "\n",
        "        if not is_downloaded(dataset_name, cache_path=dest_path):\n",
        "            print(\"downloading {} to {}\".format(dataset_name, dest_path))\n",
        "            download_dataset(dataset_name, version=0, dest_path=dest_path)\n",
        "        else:\n",
        "            print(\"already downloaded {}-{}\".format(split, dataset_name))\n",
        "\n",
        "        # use Path object\n",
        "        base_path = Path(dest_path) / dataset_name / split\n",
        "\n",
        "        self.all_paths = []\n",
        "        self.all_labels = []\n",
        "        label_mapper = {}\n",
        "\n",
        "        for i, x in enumerate(base_path.iterdir()):\n",
        "            label_mapper[x.stem] = i\n",
        "\n",
        "        for label_type in label_mapper.keys():\n",
        "            for x in (base_path / label_type).iterdir():\n",
        "                self.all_paths.append(x)\n",
        "                self.all_labels.append(label_mapper[label_type])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        txt_path = self.all_paths[idx]\n",
        "        with open(txt_path, \"r\") as f:\n",
        "            content = f.read()\n",
        "        x = content\n",
        "        y = self.all_labels[idx]\n",
        "\n",
        "        # apply rc_aug here if using\n",
        "        if self.rc_aug and coin_flip():\n",
        "            x = string_reverse_complement(x)\n",
        "\n",
        "        seq = self.tokenizer(x,\n",
        "            add_special_tokens=False,\n",
        "            padding=\"max_length\" if self.use_padding else None,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "        )  # add cls and eos token (+2)\n",
        "        seq = seq[\"input_ids\"]  # get input_ids\n",
        "\n",
        "        # need to handle eos here\n",
        "        if self.add_eos:\n",
        "            # append list seems to be faster than append tensor\n",
        "            seq.append(self.tokenizer.sep_token_id)\n",
        "\n",
        "        # convert to tensor\n",
        "        seq = torch.LongTensor(seq)\n",
        "\n",
        "        # need to wrap in list\n",
        "        target = torch.LongTensor([y])\n",
        "\n",
        "        return seq, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mey3qHwoEEBZ"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ulpk2Hb2EGB9"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\"\"\"\n",
        "We provide simple training code for the GenomicBenchmark datasets.\n",
        "\"\"\"\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, loss_fn, scheduler, log_interval=10):\n",
        "    \"\"\"Training loop.\"\"\"\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, target.squeeze())\n",
        "        loss.backward()\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "        lr = scheduler.get_last_lr()[0]\n",
        "        lr_history.append(scheduler.get_last_lr()[0])\n",
        "        print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\\tLR: {lr:.8f}')\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total += len(data)\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    train_accuracies.append(accuracy)\n",
        "    return accuracy\n",
        "\n",
        "def test(model, device, test_loader, loss_fn):\n",
        "    \"\"\"Test loop.\"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += loss_fn(output, target.squeeze()).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total += len(data)\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    test_accuracies.append(accuracy)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    return accuracy, all_preds, all_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roj3cBCXEHoN",
        "outputId": "7b555970-c4cd-4726-b462-367426f0f7a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-1.0.3-py3-none-any.whl (2.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/2.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.5.40)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install timm\n",
        "from timm.scheduler import CosineLRScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56W2LYQYEItG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
        "\n",
        "\n",
        "def gather_hyena_params(model):\n",
        "    hyena_params = []\n",
        "    other_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if any(layer in name for layer in ['filter_fn', 'HyenaOperator', 'HyenaFilter', 'pos_emb', 'modulation']):\n",
        "            hyena_params.append(param)\n",
        "        else:\n",
        "            other_params.append(param)\n",
        "\n",
        "    return hyena_params, other_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_LpiUw8EKUZ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import warnings\n",
        "import torch\n",
        "\n",
        "class CustomCosineLRScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, warmup_steps, total_steps, eta_min=0, warmup_lr_init=0, last_epoch=-1, t_in_epochs=False):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.total_steps = total_steps\n",
        "        self.eta_min = eta_min\n",
        "        self.warmup_lr_init = warmup_lr_init\n",
        "        self.t_in_epochs = t_in_epochs\n",
        "        self.cosine_annealing_steps = total_steps - warmup_steps\n",
        "        self.base_lrs = [group['lr'] for group in optimizer.param_groups]\n",
        "        super(CustomCosineLRScheduler, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.last_epoch < self.warmup_steps:\n",
        "            # Linear warmup\n",
        "            warmup_factor = self.last_epoch / self.warmup_steps\n",
        "            return [self.warmup_lr_init + warmup_factor * (base_lr - self.warmup_lr_init) for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            # Cosine annealing\n",
        "            last_epoch = self.last_epoch - self.warmup_steps\n",
        "            return [self.eta_min + (base_lr - self.eta_min) *\n",
        "                    (1 + math.cos(math.pi * last_epoch / self.cosine_annealing_steps)) / 2\n",
        "                    for base_lr in self.base_lrs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jo6CszsKefj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "seeds = [22, 42, 2222]\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cz4RW3LeELX5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import transformers\n",
        "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
        "\n",
        "def run_train(seed):\n",
        "\n",
        "    '''\n",
        "    Main entry point for training.  Select the dataset name and metadata, as\n",
        "    well as model and training args, and you're off to the genomic races!\n",
        "\n",
        "    ### GenomicBenchmarks Metadata\n",
        "    # there are 8 datasets in this suite, choose 1 at a time, with their corresponding settings\n",
        "    # name                                num_seqs        num_classes     median len    std\n",
        "    # dummy_mouse_enhancers_ensembl       1210            2               2381          984.4\n",
        "    # demo_coding_vs_intergenomic_seqs    100_000         2               200           0\n",
        "    # demo_human_or_worm                  100_000         2               200           0\n",
        "    # human_enhancers_cohn                27791           2               500           0\n",
        "    # human_enhancers_ensembl             154842          2               269           122.6\n",
        "    # human_ensembl_regulatory            289061          3               401           184.3\n",
        "    # human_nontata_promoters             36131           2               251           0\n",
        "    # human_ocr_ensembl                   174756          2               315           108.1\n",
        "\n",
        "    '''\n",
        "\n",
        "    # set seed\n",
        "    seed_everything(seed)\n",
        "\n",
        "    # Initialize accuracy lists\n",
        "    global train_accuracies\n",
        "    global test_accuracies\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    # experiment settings:\n",
        "    num_epochs = 15  # ~100 seems fine\n",
        "    max_length = 500  # max len of sequence of dataset (of what you want)\n",
        "    use_padding = True\n",
        "    dataset_name = 'human_ocr_ensembl'\n",
        "    batch_size = 256\n",
        "    learning_rate = 6e-4  # good default for Hyena\n",
        "    rc_aug = True  # reverse complement augmentation\n",
        "    add_eos = False  # add end of sentence token\n",
        "    weight_decay = 0.1\n",
        "\n",
        "    # for fine-tuning, only the 'tiny' model can fit on colab\n",
        "    pretrained_model_name = 'hyenadna-tiny-1k-seqlen'  # use None if training from scratch\n",
        "\n",
        "    # we need these for the decoder head, if using\n",
        "    use_head = True\n",
        "    n_classes = 2\n",
        "\n",
        "    # you can override with your own backbone config here if you want,\n",
        "    # otherwise we'll load the HF one by default\n",
        "    #backbone_cfg = None\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    model = HyenaDNAModel(**backbone_cfg, use_head=use_head, n_classes=n_classes)\n",
        "\n",
        "    # Print the number of parameters in the model\n",
        "    num_params = count_parameters(model)\n",
        "    print(f\"Number of trainable parameters in the model: {num_params}\")\n",
        "\n",
        "    # create tokenizer\n",
        "    tokenizer = CharacterTokenizer(\n",
        "        characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
        "        model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
        "        add_special_tokens=False,  # we handle special tokens elsewhere\n",
        "        padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
        "    )\n",
        "\n",
        "    # create datasets\n",
        "    ds_train = GenomicBenchmarkDataset(\n",
        "        max_length = max_length,\n",
        "        use_padding = use_padding,\n",
        "        split = 'train',\n",
        "        tokenizer=tokenizer,\n",
        "        dataset_name=dataset_name,\n",
        "        rc_aug=rc_aug,\n",
        "        add_eos=add_eos,\n",
        "    )\n",
        "\n",
        "    ds_test = GenomicBenchmarkDataset(\n",
        "        max_length = max_length,\n",
        "        use_padding = use_padding,\n",
        "        split = 'test',\n",
        "        tokenizer=tokenizer,\n",
        "        dataset_name=dataset_name,\n",
        "        rc_aug=rc_aug,\n",
        "        add_eos=add_eos,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # loss function\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Create optimizer parameter groups\n",
        "    hyena_params, other_params = gather_hyena_params(model)\n",
        "\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': hyena_params, 'weight_decay': 0.0},\n",
        "        {'params': other_params, 'weight_decay': weight_decay}\n",
        "    ], lr=learning_rate)\n",
        "\n",
        "    # Calculate total steps and warmup steps\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    warmup_steps = int(0.01 * total_steps)  # 1% of total steps for warmup\n",
        "    lr_min = 0.1 * learning_rate\n",
        "\n",
        "    # create learning rate scheduler\n",
        "    scheduler = CustomCosineLRScheduler(\n",
        "        optimizer,\n",
        "        warmup_steps=warmup_steps,\n",
        "        total_steps=total_steps,\n",
        "        eta_min=lr_min,\n",
        "        warmup_lr_init=1e-6,\n",
        "        t_in_epochs=False\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    global lr_history\n",
        "    lr_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train(model, device, train_loader, optimizer, epoch, loss_fn, scheduler)\n",
        "        _, all_preds, all_targets = test(model, device, test_loader, loss_fn)\n",
        "        optimizer.step()\n",
        "\n",
        "    # Plot the learning rate curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(lr_history, label='Learning Rate')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    cm_path = f'confusion_matrix_{num_epochs}_{dataset_name}_{seed}.png'\n",
        "    plt.savefig(cm_path)\n",
        "    plt.show()\n",
        "\n",
        "    # Save model\n",
        "    model_save_path = f\"trained_model_{num_epochs}_{dataset_name}_{seed}.pt\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "    # Save the train and test accuracies\n",
        "    accuracies = {\n",
        "        \"train_accuracies\": train_accuracies,\n",
        "        \"test_accuracies\": test_accuracies\n",
        "    }\n",
        "    accuracies_save_path = f\"accuracies_{num_epochs}_{dataset_name}_{seed}.json\"\n",
        "    with open(accuracies_save_path, 'w') as f:\n",
        "        json.dump(accuracies, f)\n",
        "    print(f\"Accuracies saved to {accuracies_save_path}\")\n",
        "\n",
        "    # Generate and save the confusion matrix\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot()\n",
        "    cm_path = f'confusion_matrix_{num_epochs}_{dataset_name}_{seed}.png'\n",
        "    plt.savefig(cm_path)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ASs3ptHiEy66",
        "outputId": "fae9fd5a-40b7-4300-f703-cb984ff8eb9f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Number of trainable parameters in the model: 6551042\n",
            "downloading human_enhancers_ensembl to /content\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1gZBEV_RGxJE8EON5OObdrp5Tp8JL0Fxb\n",
            "From (redirected): https://drive.google.com/uc?id=1gZBEV_RGxJE8EON5OObdrp5Tp8JL0Fxb&confirm=t&uuid=b7118c02-ef7c-495e-9af1-3ad059862217\n",
            "To: /content/human_enhancers_ensembl.zip\n",
            "100%|██████████| 51.1M/51.1M [00:01<00:00, 31.4MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train Epoch: 5 [83712/123872 (68%)]\tLoss: 0.453805\tLR: 0.00043545\n",
            "Train Epoch: 5 [83968/123872 (68%)]\tLoss: 0.510991\tLR: 0.00043534\n",
            "Train Epoch: 5 [84224/123872 (68%)]\tLoss: 0.456571\tLR: 0.00043523\n",
            "Train Epoch: 5 [84480/123872 (68%)]\tLoss: 0.374530\tLR: 0.00043512\n",
            "Train Epoch: 5 [84480/123872 (68%)]\tLoss: 0.374530\n",
            "Train Epoch: 5 [84736/123872 (68%)]\tLoss: 0.435784\tLR: 0.00043501\n",
            "Train Epoch: 5 [84992/123872 (69%)]\tLoss: 0.446197\tLR: 0.00043490\n",
            "Train Epoch: 5 [85248/123872 (69%)]\tLoss: 0.403970\tLR: 0.00043479\n",
            "Train Epoch: 5 [85504/123872 (69%)]\tLoss: 0.463622\tLR: 0.00043469\n",
            "Train Epoch: 5 [85760/123872 (69%)]\tLoss: 0.436296\tLR: 0.00043458\n",
            "Train Epoch: 5 [86016/123872 (69%)]\tLoss: 0.459713\tLR: 0.00043447\n",
            "Train Epoch: 5 [86272/123872 (70%)]\tLoss: 0.494222\tLR: 0.00043436\n",
            "Train Epoch: 5 [86528/123872 (70%)]\tLoss: 0.505489\tLR: 0.00043425\n",
            "Train Epoch: 5 [86784/123872 (70%)]\tLoss: 0.460280\tLR: 0.00043414\n",
            "Train Epoch: 5 [87040/123872 (70%)]\tLoss: 0.388698\tLR: 0.00043403\n",
            "Train Epoch: 5 [87040/123872 (70%)]\tLoss: 0.388698\n",
            "Train Epoch: 5 [87296/123872 (70%)]\tLoss: 0.410061\tLR: 0.00043392\n",
            "Train Epoch: 5 [87552/123872 (71%)]\tLoss: 0.483421\tLR: 0.00043381\n",
            "Train Epoch: 5 [87808/123872 (71%)]\tLoss: 0.499694\tLR: 0.00043371\n",
            "Train Epoch: 5 [88064/123872 (71%)]\tLoss: 0.438433\tLR: 0.00043360\n",
            "Train Epoch: 5 [88320/123872 (71%)]\tLoss: 0.386077\tLR: 0.00043349\n",
            "Train Epoch: 5 [88576/123872 (71%)]\tLoss: 0.375083\tLR: 0.00043338\n",
            "Train Epoch: 5 [88832/123872 (72%)]\tLoss: 0.461816\tLR: 0.00043327\n",
            "Train Epoch: 5 [89088/123872 (72%)]\tLoss: 0.392093\tLR: 0.00043316\n",
            "Train Epoch: 5 [89344/123872 (72%)]\tLoss: 0.424299\tLR: 0.00043305\n",
            "Train Epoch: 5 [89600/123872 (72%)]\tLoss: 0.380391\tLR: 0.00043294\n",
            "Train Epoch: 5 [89600/123872 (72%)]\tLoss: 0.380391\n",
            "Train Epoch: 5 [89856/123872 (73%)]\tLoss: 0.433571\tLR: 0.00043283\n",
            "Train Epoch: 5 [90112/123872 (73%)]\tLoss: 0.437415\tLR: 0.00043272\n",
            "Train Epoch: 5 [90368/123872 (73%)]\tLoss: 0.430114\tLR: 0.00043262\n",
            "Train Epoch: 5 [90624/123872 (73%)]\tLoss: 0.506426\tLR: 0.00043251\n",
            "Train Epoch: 5 [90880/123872 (73%)]\tLoss: 0.463050\tLR: 0.00043240\n",
            "Train Epoch: 5 [91136/123872 (74%)]\tLoss: 0.422270\tLR: 0.00043229\n",
            "Train Epoch: 5 [91392/123872 (74%)]\tLoss: 0.382958\tLR: 0.00043218\n",
            "Train Epoch: 5 [91648/123872 (74%)]\tLoss: 0.438847\tLR: 0.00043207\n",
            "Train Epoch: 5 [91904/123872 (74%)]\tLoss: 0.462123\tLR: 0.00043196\n",
            "Train Epoch: 5 [92160/123872 (74%)]\tLoss: 0.428543\tLR: 0.00043185\n",
            "Train Epoch: 5 [92160/123872 (74%)]\tLoss: 0.428543\n",
            "Train Epoch: 5 [92416/123872 (75%)]\tLoss: 0.424259\tLR: 0.00043174\n",
            "Train Epoch: 5 [92672/123872 (75%)]\tLoss: 0.430198\tLR: 0.00043163\n",
            "Train Epoch: 5 [92928/123872 (75%)]\tLoss: 0.448070\tLR: 0.00043152\n",
            "Train Epoch: 5 [93184/123872 (75%)]\tLoss: 0.428504\tLR: 0.00043141\n",
            "Train Epoch: 5 [93440/123872 (75%)]\tLoss: 0.452751\tLR: 0.00043130\n",
            "Train Epoch: 5 [93696/123872 (76%)]\tLoss: 0.420086\tLR: 0.00043119\n",
            "Train Epoch: 5 [93952/123872 (76%)]\tLoss: 0.490381\tLR: 0.00043109\n",
            "Train Epoch: 5 [94208/123872 (76%)]\tLoss: 0.415194\tLR: 0.00043098\n",
            "Train Epoch: 5 [94464/123872 (76%)]\tLoss: 0.468085\tLR: 0.00043087\n",
            "Train Epoch: 5 [94720/123872 (76%)]\tLoss: 0.423713\tLR: 0.00043076\n",
            "Train Epoch: 5 [94720/123872 (76%)]\tLoss: 0.423713\n",
            "Train Epoch: 5 [94976/123872 (77%)]\tLoss: 0.386674\tLR: 0.00043065\n",
            "Train Epoch: 5 [95232/123872 (77%)]\tLoss: 0.427315\tLR: 0.00043054\n",
            "Train Epoch: 5 [95488/123872 (77%)]\tLoss: 0.433440\tLR: 0.00043043\n",
            "Train Epoch: 5 [95744/123872 (77%)]\tLoss: 0.389193\tLR: 0.00043032\n",
            "Train Epoch: 5 [96000/123872 (77%)]\tLoss: 0.435933\tLR: 0.00043021\n",
            "Train Epoch: 5 [96256/123872 (78%)]\tLoss: 0.400437\tLR: 0.00043010\n",
            "Train Epoch: 5 [96512/123872 (78%)]\tLoss: 0.465236\tLR: 0.00042999\n",
            "Train Epoch: 5 [96768/123872 (78%)]\tLoss: 0.508068\tLR: 0.00042988\n",
            "Train Epoch: 5 [97024/123872 (78%)]\tLoss: 0.451719\tLR: 0.00042977\n",
            "Train Epoch: 5 [97280/123872 (79%)]\tLoss: 0.413627\tLR: 0.00042966\n",
            "Train Epoch: 5 [97280/123872 (79%)]\tLoss: 0.413627\n",
            "Train Epoch: 5 [97536/123872 (79%)]\tLoss: 0.427601\tLR: 0.00042955\n",
            "Train Epoch: 5 [97792/123872 (79%)]\tLoss: 0.426572\tLR: 0.00042944\n",
            "Train Epoch: 5 [98048/123872 (79%)]\tLoss: 0.480420\tLR: 0.00042933\n",
            "Train Epoch: 5 [98304/123872 (79%)]\tLoss: 0.435088\tLR: 0.00042922\n",
            "Train Epoch: 5 [98560/123872 (80%)]\tLoss: 0.500211\tLR: 0.00042911\n",
            "Train Epoch: 5 [98816/123872 (80%)]\tLoss: 0.462603\tLR: 0.00042900\n",
            "Train Epoch: 5 [99072/123872 (80%)]\tLoss: 0.463211\tLR: 0.00042889\n",
            "Train Epoch: 5 [99328/123872 (80%)]\tLoss: 0.454357\tLR: 0.00042878\n",
            "Train Epoch: 5 [99584/123872 (80%)]\tLoss: 0.466412\tLR: 0.00042867\n",
            "Train Epoch: 5 [99840/123872 (81%)]\tLoss: 0.442305\tLR: 0.00042856\n",
            "Train Epoch: 5 [99840/123872 (81%)]\tLoss: 0.442305\n",
            "Train Epoch: 5 [100096/123872 (81%)]\tLoss: 0.446171\tLR: 0.00042845\n",
            "Train Epoch: 5 [100352/123872 (81%)]\tLoss: 0.393119\tLR: 0.00042834\n",
            "Train Epoch: 5 [100608/123872 (81%)]\tLoss: 0.408346\tLR: 0.00042823\n",
            "Train Epoch: 5 [100864/123872 (81%)]\tLoss: 0.470516\tLR: 0.00042812\n",
            "Train Epoch: 5 [101120/123872 (82%)]\tLoss: 0.434933\tLR: 0.00042801\n",
            "Train Epoch: 5 [101376/123872 (82%)]\tLoss: 0.432681\tLR: 0.00042790\n",
            "Train Epoch: 5 [101632/123872 (82%)]\tLoss: 0.414629\tLR: 0.00042779\n",
            "Train Epoch: 5 [101888/123872 (82%)]\tLoss: 0.458721\tLR: 0.00042768\n",
            "Train Epoch: 5 [102144/123872 (82%)]\tLoss: 0.432611\tLR: 0.00042757\n",
            "Train Epoch: 5 [102400/123872 (83%)]\tLoss: 0.531821\tLR: 0.00042746\n",
            "Train Epoch: 5 [102400/123872 (83%)]\tLoss: 0.531821\n",
            "Train Epoch: 5 [102656/123872 (83%)]\tLoss: 0.440536\tLR: 0.00042735\n",
            "Train Epoch: 5 [102912/123872 (83%)]\tLoss: 0.454158\tLR: 0.00042724\n",
            "Train Epoch: 5 [103168/123872 (83%)]\tLoss: 0.478923\tLR: 0.00042713\n",
            "Train Epoch: 5 [103424/123872 (83%)]\tLoss: 0.442075\tLR: 0.00042702\n",
            "Train Epoch: 5 [103680/123872 (84%)]\tLoss: 0.471247\tLR: 0.00042691\n",
            "Train Epoch: 5 [103936/123872 (84%)]\tLoss: 0.426977\tLR: 0.00042680\n",
            "Train Epoch: 5 [104192/123872 (84%)]\tLoss: 0.495413\tLR: 0.00042669\n",
            "Train Epoch: 5 [104448/123872 (84%)]\tLoss: 0.509640\tLR: 0.00042658\n",
            "Train Epoch: 5 [104704/123872 (85%)]\tLoss: 0.455501\tLR: 0.00042647\n",
            "Train Epoch: 5 [104960/123872 (85%)]\tLoss: 0.438839\tLR: 0.00042636\n",
            "Train Epoch: 5 [104960/123872 (85%)]\tLoss: 0.438839\n",
            "Train Epoch: 5 [105216/123872 (85%)]\tLoss: 0.473032\tLR: 0.00042625\n",
            "Train Epoch: 5 [105472/123872 (85%)]\tLoss: 0.437661\tLR: 0.00042614\n",
            "Train Epoch: 5 [105728/123872 (85%)]\tLoss: 0.472576\tLR: 0.00042603\n",
            "Train Epoch: 5 [105984/123872 (86%)]\tLoss: 0.422030\tLR: 0.00042592\n",
            "Train Epoch: 5 [106240/123872 (86%)]\tLoss: 0.475135\tLR: 0.00042581\n",
            "Train Epoch: 5 [106496/123872 (86%)]\tLoss: 0.448966\tLR: 0.00042570\n",
            "Train Epoch: 5 [106752/123872 (86%)]\tLoss: 0.476926\tLR: 0.00042559\n",
            "Train Epoch: 5 [107008/123872 (86%)]\tLoss: 0.483721\tLR: 0.00042548\n",
            "Train Epoch: 5 [107264/123872 (87%)]\tLoss: 0.444253\tLR: 0.00042537\n",
            "Train Epoch: 5 [107520/123872 (87%)]\tLoss: 0.477850\tLR: 0.00042526\n",
            "Train Epoch: 5 [107520/123872 (87%)]\tLoss: 0.477850\n",
            "Train Epoch: 5 [107776/123872 (87%)]\tLoss: 0.482776\tLR: 0.00042515\n",
            "Train Epoch: 5 [108032/123872 (87%)]\tLoss: 0.458728\tLR: 0.00042504\n",
            "Train Epoch: 5 [108288/123872 (87%)]\tLoss: 0.430458\tLR: 0.00042493\n",
            "Train Epoch: 5 [108544/123872 (88%)]\tLoss: 0.422579\tLR: 0.00042482\n",
            "Train Epoch: 5 [108800/123872 (88%)]\tLoss: 0.421857\tLR: 0.00042471\n",
            "Train Epoch: 5 [109056/123872 (88%)]\tLoss: 0.437055\tLR: 0.00042460\n",
            "Train Epoch: 5 [109312/123872 (88%)]\tLoss: 0.406798\tLR: 0.00042449\n",
            "Train Epoch: 5 [109568/123872 (88%)]\tLoss: 0.437830\tLR: 0.00042438\n",
            "Train Epoch: 5 [109824/123872 (89%)]\tLoss: 0.401461\tLR: 0.00042426\n",
            "Train Epoch: 5 [110080/123872 (89%)]\tLoss: 0.429878\tLR: 0.00042415\n",
            "Train Epoch: 5 [110080/123872 (89%)]\tLoss: 0.429878\n",
            "Train Epoch: 5 [110336/123872 (89%)]\tLoss: 0.432578\tLR: 0.00042404\n",
            "Train Epoch: 5 [110592/123872 (89%)]\tLoss: 0.483958\tLR: 0.00042393\n",
            "Train Epoch: 5 [110848/123872 (89%)]\tLoss: 0.430752\tLR: 0.00042382\n",
            "Train Epoch: 5 [111104/123872 (90%)]\tLoss: 0.445943\tLR: 0.00042371\n",
            "Train Epoch: 5 [111360/123872 (90%)]\tLoss: 0.402454\tLR: 0.00042360\n",
            "Train Epoch: 5 [111616/123872 (90%)]\tLoss: 0.434907\tLR: 0.00042349\n",
            "Train Epoch: 5 [111872/123872 (90%)]\tLoss: 0.428056\tLR: 0.00042338\n",
            "Train Epoch: 5 [112128/123872 (90%)]\tLoss: 0.390335\tLR: 0.00042327\n",
            "Train Epoch: 5 [112384/123872 (91%)]\tLoss: 0.420039\tLR: 0.00042316\n",
            "Train Epoch: 5 [112640/123872 (91%)]\tLoss: 0.399973\tLR: 0.00042305\n",
            "Train Epoch: 5 [112640/123872 (91%)]\tLoss: 0.399973\n",
            "Train Epoch: 5 [112896/123872 (91%)]\tLoss: 0.464491\tLR: 0.00042294\n",
            "Train Epoch: 5 [113152/123872 (91%)]\tLoss: 0.427844\tLR: 0.00042283\n",
            "Train Epoch: 5 [113408/123872 (92%)]\tLoss: 0.427138\tLR: 0.00042271\n",
            "Train Epoch: 5 [113664/123872 (92%)]\tLoss: 0.437585\tLR: 0.00042260\n",
            "Train Epoch: 5 [113920/123872 (92%)]\tLoss: 0.409746\tLR: 0.00042249\n",
            "Train Epoch: 5 [114176/123872 (92%)]\tLoss: 0.398713\tLR: 0.00042238\n",
            "Train Epoch: 5 [114432/123872 (92%)]\tLoss: 0.471146\tLR: 0.00042227\n",
            "Train Epoch: 5 [114688/123872 (93%)]\tLoss: 0.440302\tLR: 0.00042216\n",
            "Train Epoch: 5 [114944/123872 (93%)]\tLoss: 0.416144\tLR: 0.00042205\n",
            "Train Epoch: 5 [115200/123872 (93%)]\tLoss: 0.436380\tLR: 0.00042194\n",
            "Train Epoch: 5 [115200/123872 (93%)]\tLoss: 0.436380\n",
            "Train Epoch: 5 [115456/123872 (93%)]\tLoss: 0.469639\tLR: 0.00042183\n",
            "Train Epoch: 5 [115712/123872 (93%)]\tLoss: 0.420736\tLR: 0.00042172\n",
            "Train Epoch: 5 [115968/123872 (94%)]\tLoss: 0.450708\tLR: 0.00042161\n",
            "Train Epoch: 5 [116224/123872 (94%)]\tLoss: 0.437898\tLR: 0.00042149\n",
            "Train Epoch: 5 [116480/123872 (94%)]\tLoss: 0.477740\tLR: 0.00042138\n",
            "Train Epoch: 5 [116736/123872 (94%)]\tLoss: 0.442240\tLR: 0.00042127\n",
            "Train Epoch: 5 [116992/123872 (94%)]\tLoss: 0.382386\tLR: 0.00042116\n",
            "Train Epoch: 5 [117248/123872 (95%)]\tLoss: 0.390051\tLR: 0.00042105\n",
            "Train Epoch: 5 [117504/123872 (95%)]\tLoss: 0.457781\tLR: 0.00042094\n",
            "Train Epoch: 5 [117760/123872 (95%)]\tLoss: 0.411066\tLR: 0.00042083\n",
            "Train Epoch: 5 [117760/123872 (95%)]\tLoss: 0.411066\n",
            "Train Epoch: 5 [118016/123872 (95%)]\tLoss: 0.435948\tLR: 0.00042072\n",
            "Train Epoch: 5 [118272/123872 (95%)]\tLoss: 0.366458\tLR: 0.00042061\n",
            "Train Epoch: 5 [118528/123872 (96%)]\tLoss: 0.427666\tLR: 0.00042049\n",
            "Train Epoch: 5 [118784/123872 (96%)]\tLoss: 0.378039\tLR: 0.00042038\n",
            "Train Epoch: 5 [119040/123872 (96%)]\tLoss: 0.464181\tLR: 0.00042027\n",
            "Train Epoch: 5 [119296/123872 (96%)]\tLoss: 0.360167\tLR: 0.00042016\n",
            "Train Epoch: 5 [119552/123872 (96%)]\tLoss: 0.392173\tLR: 0.00042005\n",
            "Train Epoch: 5 [119808/123872 (97%)]\tLoss: 0.429914\tLR: 0.00041994\n",
            "Train Epoch: 5 [120064/123872 (97%)]\tLoss: 0.464820\tLR: 0.00041983\n",
            "Train Epoch: 5 [120320/123872 (97%)]\tLoss: 0.454084\tLR: 0.00041972\n",
            "Train Epoch: 5 [120320/123872 (97%)]\tLoss: 0.454084\n",
            "Train Epoch: 5 [120576/123872 (97%)]\tLoss: 0.450916\tLR: 0.00041960\n",
            "Train Epoch: 5 [120832/123872 (98%)]\tLoss: 0.462882\tLR: 0.00041949\n",
            "Train Epoch: 5 [121088/123872 (98%)]\tLoss: 0.465322\tLR: 0.00041938\n",
            "Train Epoch: 5 [121344/123872 (98%)]\tLoss: 0.478372\tLR: 0.00041927\n",
            "Train Epoch: 5 [121600/123872 (98%)]\tLoss: 0.465685\tLR: 0.00041916\n",
            "Train Epoch: 5 [121856/123872 (98%)]\tLoss: 0.459107\tLR: 0.00041905\n",
            "Train Epoch: 5 [122112/123872 (99%)]\tLoss: 0.453405\tLR: 0.00041894\n",
            "Train Epoch: 5 [122368/123872 (99%)]\tLoss: 0.442636\tLR: 0.00041883\n",
            "Train Epoch: 5 [122624/123872 (99%)]\tLoss: 0.425195\tLR: 0.00041871\n",
            "Train Epoch: 5 [122880/123872 (99%)]\tLoss: 0.477697\tLR: 0.00041860\n",
            "Train Epoch: 5 [122880/123872 (99%)]\tLoss: 0.477697\n",
            "Train Epoch: 5 [123136/123872 (99%)]\tLoss: 0.495705\tLR: 0.00041849\n",
            "Train Epoch: 5 [123392/123872 (100%)]\tLoss: 0.471939\tLR: 0.00041838\n",
            "Train Epoch: 5 [108192/123872 (100%)]\tLoss: 0.490276\tLR: 0.00041827\n",
            "\n",
            "Test set: Average loss: 0.0018, Accuracy: 24526/30970 (79.19%)\n",
            "\n",
            "Train Epoch: 6 [0/123872 (0%)]\tLoss: 0.406727\tLR: 0.00041816\n",
            "Train Epoch: 6 [0/123872 (0%)]\tLoss: 0.406727\n",
            "Train Epoch: 6 [256/123872 (0%)]\tLoss: 0.442813\tLR: 0.00041804\n",
            "Train Epoch: 6 [512/123872 (0%)]\tLoss: 0.469073\tLR: 0.00041793\n",
            "Train Epoch: 6 [768/123872 (1%)]\tLoss: 0.409250\tLR: 0.00041782\n",
            "Train Epoch: 6 [1024/123872 (1%)]\tLoss: 0.354935\tLR: 0.00041771\n",
            "Train Epoch: 6 [1280/123872 (1%)]\tLoss: 0.417508\tLR: 0.00041760\n",
            "Train Epoch: 6 [1536/123872 (1%)]\tLoss: 0.433381\tLR: 0.00041749\n",
            "Train Epoch: 6 [1792/123872 (1%)]\tLoss: 0.386914\tLR: 0.00041738\n",
            "Train Epoch: 6 [2048/123872 (2%)]\tLoss: 0.380833\tLR: 0.00041726\n",
            "Train Epoch: 6 [2304/123872 (2%)]\tLoss: 0.397923\tLR: 0.00041715\n",
            "Train Epoch: 6 [2560/123872 (2%)]\tLoss: 0.497193\tLR: 0.00041704\n",
            "Train Epoch: 6 [2560/123872 (2%)]\tLoss: 0.497193\n",
            "Train Epoch: 6 [2816/123872 (2%)]\tLoss: 0.402615\tLR: 0.00041693\n",
            "Train Epoch: 6 [3072/123872 (2%)]\tLoss: 0.414886\tLR: 0.00041682\n",
            "Train Epoch: 6 [3328/123872 (3%)]\tLoss: 0.520185\tLR: 0.00041670\n",
            "Train Epoch: 6 [3584/123872 (3%)]\tLoss: 0.488856\tLR: 0.00041659\n",
            "Train Epoch: 6 [3840/123872 (3%)]\tLoss: 0.501568\tLR: 0.00041648\n",
            "Train Epoch: 6 [4096/123872 (3%)]\tLoss: 0.388109\tLR: 0.00041637\n",
            "Train Epoch: 6 [4352/123872 (4%)]\tLoss: 0.486495\tLR: 0.00041626\n",
            "Train Epoch: 6 [4608/123872 (4%)]\tLoss: 0.431508\tLR: 0.00041615\n",
            "Train Epoch: 6 [4864/123872 (4%)]\tLoss: 0.452523\tLR: 0.00041603\n",
            "Train Epoch: 6 [5120/123872 (4%)]\tLoss: 0.388540\tLR: 0.00041592\n",
            "Train Epoch: 6 [5120/123872 (4%)]\tLoss: 0.388540\n",
            "Train Epoch: 6 [5376/123872 (4%)]\tLoss: 0.449552\tLR: 0.00041581\n",
            "Train Epoch: 6 [5632/123872 (5%)]\tLoss: 0.414744\tLR: 0.00041570\n",
            "Train Epoch: 6 [5888/123872 (5%)]\tLoss: 0.426488\tLR: 0.00041559\n",
            "Train Epoch: 6 [6144/123872 (5%)]\tLoss: 0.410682\tLR: 0.00041547\n",
            "Train Epoch: 6 [6400/123872 (5%)]\tLoss: 0.487572\tLR: 0.00041536\n",
            "Train Epoch: 6 [6656/123872 (5%)]\tLoss: 0.460415\tLR: 0.00041525\n",
            "Train Epoch: 6 [6912/123872 (6%)]\tLoss: 0.447802\tLR: 0.00041514\n",
            "Train Epoch: 6 [7168/123872 (6%)]\tLoss: 0.496497\tLR: 0.00041503\n",
            "Train Epoch: 6 [7424/123872 (6%)]\tLoss: 0.426877\tLR: 0.00041491\n",
            "Train Epoch: 6 [7680/123872 (6%)]\tLoss: 0.437415\tLR: 0.00041480\n",
            "Train Epoch: 6 [7680/123872 (6%)]\tLoss: 0.437415\n",
            "Train Epoch: 6 [7936/123872 (6%)]\tLoss: 0.420978\tLR: 0.00041469\n",
            "Train Epoch: 6 [8192/123872 (7%)]\tLoss: 0.424524\tLR: 0.00041458\n",
            "Train Epoch: 6 [8448/123872 (7%)]\tLoss: 0.435025\tLR: 0.00041447\n",
            "Train Epoch: 6 [8704/123872 (7%)]\tLoss: 0.469205\tLR: 0.00041435\n",
            "Train Epoch: 6 [8960/123872 (7%)]\tLoss: 0.396726\tLR: 0.00041424\n",
            "Train Epoch: 6 [9216/123872 (7%)]\tLoss: 0.450338\tLR: 0.00041413\n",
            "Train Epoch: 6 [9472/123872 (8%)]\tLoss: 0.463877\tLR: 0.00041402\n",
            "Train Epoch: 6 [9728/123872 (8%)]\tLoss: 0.513804\tLR: 0.00041391\n",
            "Train Epoch: 6 [9984/123872 (8%)]\tLoss: 0.436868\tLR: 0.00041379\n",
            "Train Epoch: 6 [10240/123872 (8%)]\tLoss: 0.451768\tLR: 0.00041368\n",
            "Train Epoch: 6 [10240/123872 (8%)]\tLoss: 0.451768\n",
            "Train Epoch: 6 [10496/123872 (8%)]\tLoss: 0.494986\tLR: 0.00041357\n",
            "Train Epoch: 6 [10752/123872 (9%)]\tLoss: 0.384818\tLR: 0.00041346\n",
            "Train Epoch: 6 [11008/123872 (9%)]\tLoss: 0.450964\tLR: 0.00041334\n",
            "Train Epoch: 6 [11264/123872 (9%)]\tLoss: 0.463733\tLR: 0.00041323\n",
            "Train Epoch: 6 [11520/123872 (9%)]\tLoss: 0.414182\tLR: 0.00041312\n",
            "Train Epoch: 6 [11776/123872 (10%)]\tLoss: 0.482454\tLR: 0.00041301\n",
            "Train Epoch: 6 [12032/123872 (10%)]\tLoss: 0.469916\tLR: 0.00041290\n",
            "Train Epoch: 6 [12288/123872 (10%)]\tLoss: 0.437196\tLR: 0.00041278\n",
            "Train Epoch: 6 [12544/123872 (10%)]\tLoss: 0.426193\tLR: 0.00041267\n",
            "Train Epoch: 6 [12800/123872 (10%)]\tLoss: 0.410410\tLR: 0.00041256\n",
            "Train Epoch: 6 [12800/123872 (10%)]\tLoss: 0.410410\n",
            "Train Epoch: 6 [13056/123872 (11%)]\tLoss: 0.448261\tLR: 0.00041245\n",
            "Train Epoch: 6 [13312/123872 (11%)]\tLoss: 0.455757\tLR: 0.00041233\n",
            "Train Epoch: 6 [13568/123872 (11%)]\tLoss: 0.482391\tLR: 0.00041222\n",
            "Train Epoch: 6 [13824/123872 (11%)]\tLoss: 0.481028\tLR: 0.00041211\n",
            "Train Epoch: 6 [14080/123872 (11%)]\tLoss: 0.487359\tLR: 0.00041200\n",
            "Train Epoch: 6 [14336/123872 (12%)]\tLoss: 0.403702\tLR: 0.00041188\n",
            "Train Epoch: 6 [14592/123872 (12%)]\tLoss: 0.432532\tLR: 0.00041177\n",
            "Train Epoch: 6 [14848/123872 (12%)]\tLoss: 0.385935\tLR: 0.00041166\n",
            "Train Epoch: 6 [15104/123872 (12%)]\tLoss: 0.458884\tLR: 0.00041155\n",
            "Train Epoch: 6 [15360/123872 (12%)]\tLoss: 0.443064\tLR: 0.00041143\n",
            "Train Epoch: 6 [15360/123872 (12%)]\tLoss: 0.443064\n",
            "Train Epoch: 6 [15616/123872 (13%)]\tLoss: 0.432272\tLR: 0.00041132\n",
            "Train Epoch: 6 [15872/123872 (13%)]\tLoss: 0.436115\tLR: 0.00041121\n",
            "Train Epoch: 6 [16128/123872 (13%)]\tLoss: 0.477081\tLR: 0.00041110\n",
            "Train Epoch: 6 [16384/123872 (13%)]\tLoss: 0.446774\tLR: 0.00041098\n",
            "Train Epoch: 6 [16640/123872 (13%)]\tLoss: 0.420834\tLR: 0.00041087\n",
            "Train Epoch: 6 [16896/123872 (14%)]\tLoss: 0.473462\tLR: 0.00041076\n",
            "Train Epoch: 6 [17152/123872 (14%)]\tLoss: 0.503290\tLR: 0.00041065\n",
            "Train Epoch: 6 [17408/123872 (14%)]\tLoss: 0.428151\tLR: 0.00041053\n",
            "Train Epoch: 6 [17664/123872 (14%)]\tLoss: 0.423713\tLR: 0.00041042\n",
            "Train Epoch: 6 [17920/123872 (14%)]\tLoss: 0.463720\tLR: 0.00041031\n",
            "Train Epoch: 6 [17920/123872 (14%)]\tLoss: 0.463720\n",
            "Train Epoch: 6 [18176/123872 (15%)]\tLoss: 0.504249\tLR: 0.00041020\n",
            "Train Epoch: 6 [18432/123872 (15%)]\tLoss: 0.467281\tLR: 0.00041008\n",
            "Train Epoch: 6 [18688/123872 (15%)]\tLoss: 0.403032\tLR: 0.00040997\n",
            "Train Epoch: 6 [18944/123872 (15%)]\tLoss: 0.461445\tLR: 0.00040986\n",
            "Train Epoch: 6 [19200/123872 (15%)]\tLoss: 0.429064\tLR: 0.00040974\n",
            "Train Epoch: 6 [19456/123872 (16%)]\tLoss: 0.438024\tLR: 0.00040963\n",
            "Train Epoch: 6 [19712/123872 (16%)]\tLoss: 0.430417\tLR: 0.00040952\n",
            "Train Epoch: 6 [19968/123872 (16%)]\tLoss: 0.451652\tLR: 0.00040941\n",
            "Train Epoch: 6 [20224/123872 (16%)]\tLoss: 0.468540\tLR: 0.00040929\n",
            "Train Epoch: 6 [20480/123872 (17%)]\tLoss: 0.397620\tLR: 0.00040918\n",
            "Train Epoch: 6 [20480/123872 (17%)]\tLoss: 0.397620\n",
            "Train Epoch: 6 [20736/123872 (17%)]\tLoss: 0.439198\tLR: 0.00040907\n",
            "Train Epoch: 6 [20992/123872 (17%)]\tLoss: 0.436090\tLR: 0.00040896\n",
            "Train Epoch: 6 [21248/123872 (17%)]\tLoss: 0.387143\tLR: 0.00040884\n",
            "Train Epoch: 6 [21504/123872 (17%)]\tLoss: 0.510278\tLR: 0.00040873\n",
            "Train Epoch: 6 [21760/123872 (18%)]\tLoss: 0.483135\tLR: 0.00040862\n",
            "Train Epoch: 6 [22016/123872 (18%)]\tLoss: 0.424325\tLR: 0.00040850\n",
            "Train Epoch: 6 [22272/123872 (18%)]\tLoss: 0.411618\tLR: 0.00040839\n",
            "Train Epoch: 6 [22528/123872 (18%)]\tLoss: 0.518834\tLR: 0.00040828\n",
            "Train Epoch: 6 [22784/123872 (18%)]\tLoss: 0.432804\tLR: 0.00040817\n",
            "Train Epoch: 6 [23040/123872 (19%)]\tLoss: 0.415509\tLR: 0.00040805\n",
            "Train Epoch: 6 [23040/123872 (19%)]\tLoss: 0.415509\n",
            "Train Epoch: 6 [23296/123872 (19%)]\tLoss: 0.502687\tLR: 0.00040794\n",
            "Train Epoch: 6 [23552/123872 (19%)]\tLoss: 0.442287\tLR: 0.00040783\n",
            "Train Epoch: 6 [23808/123872 (19%)]\tLoss: 0.421001\tLR: 0.00040771\n",
            "Train Epoch: 6 [24064/123872 (19%)]\tLoss: 0.484962\tLR: 0.00040760\n",
            "Train Epoch: 6 [24320/123872 (20%)]\tLoss: 0.414261\tLR: 0.00040749\n",
            "Train Epoch: 6 [24576/123872 (20%)]\tLoss: 0.490200\tLR: 0.00040737\n",
            "Train Epoch: 6 [24832/123872 (20%)]\tLoss: 0.436997\tLR: 0.00040726\n",
            "Train Epoch: 6 [25088/123872 (20%)]\tLoss: 0.429052\tLR: 0.00040715\n",
            "Train Epoch: 6 [25344/123872 (20%)]\tLoss: 0.393506\tLR: 0.00040703\n",
            "Train Epoch: 6 [25600/123872 (21%)]\tLoss: 0.495584\tLR: 0.00040692\n",
            "Train Epoch: 6 [25600/123872 (21%)]\tLoss: 0.495584\n",
            "Train Epoch: 6 [25856/123872 (21%)]\tLoss: 0.425750\tLR: 0.00040681\n",
            "Train Epoch: 6 [26112/123872 (21%)]\tLoss: 0.407074\tLR: 0.00040670\n",
            "Train Epoch: 6 [26368/123872 (21%)]\tLoss: 0.482273\tLR: 0.00040658\n",
            "Train Epoch: 6 [26624/123872 (21%)]\tLoss: 0.435880\tLR: 0.00040647\n",
            "Train Epoch: 6 [26880/123872 (22%)]\tLoss: 0.410390\tLR: 0.00040636\n",
            "Train Epoch: 6 [27136/123872 (22%)]\tLoss: 0.436844\tLR: 0.00040624\n",
            "Train Epoch: 6 [27392/123872 (22%)]\tLoss: 0.377252\tLR: 0.00040613\n",
            "Train Epoch: 6 [27648/123872 (22%)]\tLoss: 0.453998\tLR: 0.00040602\n",
            "Train Epoch: 6 [27904/123872 (23%)]\tLoss: 0.472528\tLR: 0.00040590\n",
            "Train Epoch: 6 [28160/123872 (23%)]\tLoss: 0.440020\tLR: 0.00040579\n",
            "Train Epoch: 6 [28160/123872 (23%)]\tLoss: 0.440020\n",
            "Train Epoch: 6 [28416/123872 (23%)]\tLoss: 0.426356\tLR: 0.00040568\n",
            "Train Epoch: 6 [28672/123872 (23%)]\tLoss: 0.476065\tLR: 0.00040556\n",
            "Train Epoch: 6 [28928/123872 (23%)]\tLoss: 0.492206\tLR: 0.00040545\n",
            "Train Epoch: 6 [29184/123872 (24%)]\tLoss: 0.445009\tLR: 0.00040534\n",
            "Train Epoch: 6 [29440/123872 (24%)]\tLoss: 0.469326\tLR: 0.00040522\n",
            "Train Epoch: 6 [29696/123872 (24%)]\tLoss: 0.427292\tLR: 0.00040511\n",
            "Train Epoch: 6 [29952/123872 (24%)]\tLoss: 0.411876\tLR: 0.00040500\n",
            "Train Epoch: 6 [30208/123872 (24%)]\tLoss: 0.419986\tLR: 0.00040488\n",
            "Train Epoch: 6 [30464/123872 (25%)]\tLoss: 0.507110\tLR: 0.00040477\n",
            "Train Epoch: 6 [30720/123872 (25%)]\tLoss: 0.409237\tLR: 0.00040466\n",
            "Train Epoch: 6 [30720/123872 (25%)]\tLoss: 0.409237\n",
            "Train Epoch: 6 [30976/123872 (25%)]\tLoss: 0.414574\tLR: 0.00040454\n",
            "Train Epoch: 6 [31232/123872 (25%)]\tLoss: 0.403170\tLR: 0.00040443\n",
            "Train Epoch: 6 [31488/123872 (25%)]\tLoss: 0.457518\tLR: 0.00040432\n",
            "Train Epoch: 6 [31744/123872 (26%)]\tLoss: 0.413437\tLR: 0.00040420\n",
            "Train Epoch: 6 [32000/123872 (26%)]\tLoss: 0.391766\tLR: 0.00040409\n",
            "Train Epoch: 6 [32256/123872 (26%)]\tLoss: 0.454961\tLR: 0.00040398\n",
            "Train Epoch: 6 [32512/123872 (26%)]\tLoss: 0.466303\tLR: 0.00040386\n",
            "Train Epoch: 6 [32768/123872 (26%)]\tLoss: 0.424348\tLR: 0.00040375\n",
            "Train Epoch: 6 [33024/123872 (27%)]\tLoss: 0.394109\tLR: 0.00040364\n",
            "Train Epoch: 6 [33280/123872 (27%)]\tLoss: 0.423045\tLR: 0.00040352\n",
            "Train Epoch: 6 [33280/123872 (27%)]\tLoss: 0.423045\n",
            "Train Epoch: 6 [33536/123872 (27%)]\tLoss: 0.375728\tLR: 0.00040341\n",
            "Train Epoch: 6 [33792/123872 (27%)]\tLoss: 0.443963\tLR: 0.00040329\n",
            "Train Epoch: 6 [34048/123872 (27%)]\tLoss: 0.403421\tLR: 0.00040318\n",
            "Train Epoch: 6 [34304/123872 (28%)]\tLoss: 0.431715\tLR: 0.00040307\n",
            "Train Epoch: 6 [34560/123872 (28%)]\tLoss: 0.406257\tLR: 0.00040295\n",
            "Train Epoch: 6 [34816/123872 (28%)]\tLoss: 0.435300\tLR: 0.00040284\n",
            "Train Epoch: 6 [35072/123872 (28%)]\tLoss: 0.460220\tLR: 0.00040273\n",
            "Train Epoch: 6 [35328/123872 (29%)]\tLoss: 0.359440\tLR: 0.00040261\n",
            "Train Epoch: 6 [35584/123872 (29%)]\tLoss: 0.467218\tLR: 0.00040250\n",
            "Train Epoch: 6 [35840/123872 (29%)]\tLoss: 0.419980\tLR: 0.00040239\n",
            "Train Epoch: 6 [35840/123872 (29%)]\tLoss: 0.419980\n",
            "Train Epoch: 6 [36096/123872 (29%)]\tLoss: 0.419494\tLR: 0.00040227\n",
            "Train Epoch: 6 [36352/123872 (29%)]\tLoss: 0.390979\tLR: 0.00040216\n",
            "Train Epoch: 6 [36608/123872 (30%)]\tLoss: 0.445997\tLR: 0.00040204\n",
            "Train Epoch: 6 [36864/123872 (30%)]\tLoss: 0.442585\tLR: 0.00040193\n",
            "Train Epoch: 6 [37120/123872 (30%)]\tLoss: 0.388575\tLR: 0.00040182\n",
            "Train Epoch: 6 [37376/123872 (30%)]\tLoss: 0.453336\tLR: 0.00040170\n",
            "Train Epoch: 6 [37632/123872 (30%)]\tLoss: 0.469171\tLR: 0.00040159\n",
            "Train Epoch: 6 [37888/123872 (31%)]\tLoss: 0.374367\tLR: 0.00040148\n",
            "Train Epoch: 6 [38144/123872 (31%)]\tLoss: 0.403894\tLR: 0.00040136\n",
            "Train Epoch: 6 [38400/123872 (31%)]\tLoss: 0.450368\tLR: 0.00040125\n",
            "Train Epoch: 6 [38400/123872 (31%)]\tLoss: 0.450368\n",
            "Train Epoch: 6 [38656/123872 (31%)]\tLoss: 0.490191\tLR: 0.00040113\n",
            "Train Epoch: 6 [38912/123872 (31%)]\tLoss: 0.465566\tLR: 0.00040102\n",
            "Train Epoch: 6 [39168/123872 (32%)]\tLoss: 0.436169\tLR: 0.00040091\n",
            "Train Epoch: 6 [39424/123872 (32%)]\tLoss: 0.426782\tLR: 0.00040079\n",
            "Train Epoch: 6 [39680/123872 (32%)]\tLoss: 0.401799\tLR: 0.00040068\n",
            "Train Epoch: 6 [39936/123872 (32%)]\tLoss: 0.386315\tLR: 0.00040056\n",
            "Train Epoch: 6 [40192/123872 (32%)]\tLoss: 0.419169\tLR: 0.00040045\n",
            "Train Epoch: 6 [40448/123872 (33%)]\tLoss: 0.548127\tLR: 0.00040034\n",
            "Train Epoch: 6 [40704/123872 (33%)]\tLoss: 0.408440\tLR: 0.00040022\n",
            "Train Epoch: 6 [40960/123872 (33%)]\tLoss: 0.398481\tLR: 0.00040011\n",
            "Train Epoch: 6 [40960/123872 (33%)]\tLoss: 0.398481\n",
            "Train Epoch: 6 [41216/123872 (33%)]\tLoss: 0.348806\tLR: 0.00040000\n",
            "Train Epoch: 6 [41472/123872 (33%)]\tLoss: 0.489809\tLR: 0.00039988\n",
            "Train Epoch: 6 [41728/123872 (34%)]\tLoss: 0.432967\tLR: 0.00039977\n",
            "Train Epoch: 6 [41984/123872 (34%)]\tLoss: 0.537163\tLR: 0.00039965\n",
            "Train Epoch: 6 [42240/123872 (34%)]\tLoss: 0.427491\tLR: 0.00039954\n",
            "Train Epoch: 6 [42496/123872 (34%)]\tLoss: 0.433423\tLR: 0.00039943\n",
            "Train Epoch: 6 [42752/123872 (35%)]\tLoss: 0.401517\tLR: 0.00039931\n",
            "Train Epoch: 6 [43008/123872 (35%)]\tLoss: 0.397596\tLR: 0.00039920\n",
            "Train Epoch: 6 [43264/123872 (35%)]\tLoss: 0.435313\tLR: 0.00039908\n",
            "Train Epoch: 6 [43520/123872 (35%)]\tLoss: 0.449377\tLR: 0.00039897\n",
            "Train Epoch: 6 [43520/123872 (35%)]\tLoss: 0.449377\n",
            "Train Epoch: 6 [43776/123872 (35%)]\tLoss: 0.373284\tLR: 0.00039885\n",
            "Train Epoch: 6 [44032/123872 (36%)]\tLoss: 0.453993\tLR: 0.00039874\n",
            "Train Epoch: 6 [44288/123872 (36%)]\tLoss: 0.425051\tLR: 0.00039863\n",
            "Train Epoch: 6 [44544/123872 (36%)]\tLoss: 0.431391\tLR: 0.00039851\n",
            "Train Epoch: 6 [44800/123872 (36%)]\tLoss: 0.423274\tLR: 0.00039840\n",
            "Train Epoch: 6 [45056/123872 (36%)]\tLoss: 0.388736\tLR: 0.00039828\n",
            "Train Epoch: 6 [45312/123872 (37%)]\tLoss: 0.476193\tLR: 0.00039817\n",
            "Train Epoch: 6 [45568/123872 (37%)]\tLoss: 0.447486\tLR: 0.00039806\n",
            "Train Epoch: 6 [45824/123872 (37%)]\tLoss: 0.459501\tLR: 0.00039794\n",
            "Train Epoch: 6 [46080/123872 (37%)]\tLoss: 0.466726\tLR: 0.00039783\n",
            "Train Epoch: 6 [46080/123872 (37%)]\tLoss: 0.466726\n",
            "Train Epoch: 6 [46336/123872 (37%)]\tLoss: 0.425327\tLR: 0.00039771\n",
            "Train Epoch: 6 [46592/123872 (38%)]\tLoss: 0.399223\tLR: 0.00039760\n",
            "Train Epoch: 6 [46848/123872 (38%)]\tLoss: 0.417557\tLR: 0.00039748\n",
            "Train Epoch: 6 [47104/123872 (38%)]\tLoss: 0.463107\tLR: 0.00039737\n",
            "Train Epoch: 6 [47360/123872 (38%)]\tLoss: 0.451095\tLR: 0.00039726\n",
            "Train Epoch: 6 [47616/123872 (38%)]\tLoss: 0.405746\tLR: 0.00039714\n",
            "Train Epoch: 6 [47872/123872 (39%)]\tLoss: 0.450663\tLR: 0.00039703\n",
            "Train Epoch: 6 [48128/123872 (39%)]\tLoss: 0.496954\tLR: 0.00039691\n",
            "Train Epoch: 6 [48384/123872 (39%)]\tLoss: 0.394618\tLR: 0.00039680\n",
            "Train Epoch: 6 [48640/123872 (39%)]\tLoss: 0.450947\tLR: 0.00039668\n",
            "Train Epoch: 6 [48640/123872 (39%)]\tLoss: 0.450947\n",
            "Train Epoch: 6 [48896/123872 (39%)]\tLoss: 0.422493\tLR: 0.00039657\n",
            "Train Epoch: 6 [49152/123872 (40%)]\tLoss: 0.455090\tLR: 0.00039646\n",
            "Train Epoch: 6 [49408/123872 (40%)]\tLoss: 0.440146\tLR: 0.00039634\n",
            "Train Epoch: 6 [49664/123872 (40%)]\tLoss: 0.433183\tLR: 0.00039623\n",
            "Train Epoch: 6 [49920/123872 (40%)]\tLoss: 0.505024\tLR: 0.00039611\n",
            "Train Epoch: 6 [50176/123872 (40%)]\tLoss: 0.526900\tLR: 0.00039600\n",
            "Train Epoch: 6 [50432/123872 (41%)]\tLoss: 0.422138\tLR: 0.00039588\n",
            "Train Epoch: 6 [50688/123872 (41%)]\tLoss: 0.432957\tLR: 0.00039577\n",
            "Train Epoch: 6 [50944/123872 (41%)]\tLoss: 0.437851\tLR: 0.00039565\n",
            "Train Epoch: 6 [51200/123872 (41%)]\tLoss: 0.482355\tLR: 0.00039554\n",
            "Train Epoch: 6 [51200/123872 (41%)]\tLoss: 0.482355\n",
            "Train Epoch: 6 [51456/123872 (42%)]\tLoss: 0.455108\tLR: 0.00039543\n",
            "Train Epoch: 6 [51712/123872 (42%)]\tLoss: 0.423647\tLR: 0.00039531\n",
            "Train Epoch: 6 [51968/123872 (42%)]\tLoss: 0.427591\tLR: 0.00039520\n",
            "Train Epoch: 6 [52224/123872 (42%)]\tLoss: 0.442906\tLR: 0.00039508\n",
            "Train Epoch: 6 [52480/123872 (42%)]\tLoss: 0.377476\tLR: 0.00039497\n",
            "Train Epoch: 6 [52736/123872 (43%)]\tLoss: 0.443050\tLR: 0.00039485\n",
            "Train Epoch: 6 [52992/123872 (43%)]\tLoss: 0.431324\tLR: 0.00039474\n",
            "Train Epoch: 6 [53248/123872 (43%)]\tLoss: 0.420356\tLR: 0.00039462\n",
            "Train Epoch: 6 [53504/123872 (43%)]\tLoss: 0.424976\tLR: 0.00039451\n",
            "Train Epoch: 6 [53760/123872 (43%)]\tLoss: 0.389129\tLR: 0.00039439\n",
            "Train Epoch: 6 [53760/123872 (43%)]\tLoss: 0.389129\n",
            "Train Epoch: 6 [54016/123872 (44%)]\tLoss: 0.489407\tLR: 0.00039428\n",
            "Train Epoch: 6 [54272/123872 (44%)]\tLoss: 0.501089\tLR: 0.00039417\n",
            "Train Epoch: 6 [54528/123872 (44%)]\tLoss: 0.419985\tLR: 0.00039405\n",
            "Train Epoch: 6 [54784/123872 (44%)]\tLoss: 0.444418\tLR: 0.00039394\n",
            "Train Epoch: 6 [55040/123872 (44%)]\tLoss: 0.508691\tLR: 0.00039382\n",
            "Train Epoch: 6 [55296/123872 (45%)]\tLoss: 0.450149\tLR: 0.00039371\n",
            "Train Epoch: 6 [55552/123872 (45%)]\tLoss: 0.427272\tLR: 0.00039359\n",
            "Train Epoch: 6 [55808/123872 (45%)]\tLoss: 0.451327\tLR: 0.00039348\n",
            "Train Epoch: 6 [56064/123872 (45%)]\tLoss: 0.378320\tLR: 0.00039336\n",
            "Train Epoch: 6 [56320/123872 (45%)]\tLoss: 0.438562\tLR: 0.00039325\n",
            "Train Epoch: 6 [56320/123872 (45%)]\tLoss: 0.438562\n",
            "Train Epoch: 6 [56576/123872 (46%)]\tLoss: 0.450923\tLR: 0.00039313\n",
            "Train Epoch: 6 [56832/123872 (46%)]\tLoss: 0.456087\tLR: 0.00039302\n",
            "Train Epoch: 6 [57088/123872 (46%)]\tLoss: 0.496954\tLR: 0.00039290\n",
            "Train Epoch: 6 [57344/123872 (46%)]\tLoss: 0.394575\tLR: 0.00039279\n",
            "Train Epoch: 6 [57600/123872 (46%)]\tLoss: 0.505050\tLR: 0.00039267\n",
            "Train Epoch: 6 [57856/123872 (47%)]\tLoss: 0.446019\tLR: 0.00039256\n",
            "Train Epoch: 6 [58112/123872 (47%)]\tLoss: 0.469352\tLR: 0.00039244\n",
            "Train Epoch: 6 [58368/123872 (47%)]\tLoss: 0.435464\tLR: 0.00039233\n",
            "Train Epoch: 6 [58624/123872 (47%)]\tLoss: 0.428045\tLR: 0.00039222\n",
            "Train Epoch: 6 [58880/123872 (48%)]\tLoss: 0.429879\tLR: 0.00039210\n",
            "Train Epoch: 6 [58880/123872 (48%)]\tLoss: 0.429879\n",
            "Train Epoch: 6 [59136/123872 (48%)]\tLoss: 0.432402\tLR: 0.00039199\n",
            "Train Epoch: 6 [59392/123872 (48%)]\tLoss: 0.508774\tLR: 0.00039187\n",
            "Train Epoch: 6 [59648/123872 (48%)]\tLoss: 0.414249\tLR: 0.00039176\n",
            "Train Epoch: 6 [59904/123872 (48%)]\tLoss: 0.395585\tLR: 0.00039164\n",
            "Train Epoch: 6 [60160/123872 (49%)]\tLoss: 0.487959\tLR: 0.00039153\n",
            "Train Epoch: 6 [60416/123872 (49%)]\tLoss: 0.445404\tLR: 0.00039141\n",
            "Train Epoch: 6 [60672/123872 (49%)]\tLoss: 0.458185\tLR: 0.00039130\n",
            "Train Epoch: 6 [60928/123872 (49%)]\tLoss: 0.450494\tLR: 0.00039118\n",
            "Train Epoch: 6 [61184/123872 (49%)]\tLoss: 0.444001\tLR: 0.00039107\n",
            "Train Epoch: 6 [61440/123872 (50%)]\tLoss: 0.389922\tLR: 0.00039095\n",
            "Train Epoch: 6 [61440/123872 (50%)]\tLoss: 0.389922\n",
            "Train Epoch: 6 [61696/123872 (50%)]\tLoss: 0.437863\tLR: 0.00039084\n",
            "Train Epoch: 6 [61952/123872 (50%)]\tLoss: 0.399484\tLR: 0.00039072\n",
            "Train Epoch: 6 [62208/123872 (50%)]\tLoss: 0.398165\tLR: 0.00039061\n",
            "Train Epoch: 6 [62464/123872 (50%)]\tLoss: 0.433725\tLR: 0.00039049\n",
            "Train Epoch: 6 [62720/123872 (51%)]\tLoss: 0.417048\tLR: 0.00039038\n",
            "Train Epoch: 6 [62976/123872 (51%)]\tLoss: 0.409590\tLR: 0.00039026\n",
            "Train Epoch: 6 [63232/123872 (51%)]\tLoss: 0.505291\tLR: 0.00039015\n",
            "Train Epoch: 6 [63488/123872 (51%)]\tLoss: 0.428212\tLR: 0.00039003\n",
            "Train Epoch: 6 [63744/123872 (51%)]\tLoss: 0.406944\tLR: 0.00038992\n",
            "Train Epoch: 6 [64000/123872 (52%)]\tLoss: 0.365511\tLR: 0.00038980\n",
            "Train Epoch: 6 [64000/123872 (52%)]\tLoss: 0.365511\n",
            "Train Epoch: 6 [64256/123872 (52%)]\tLoss: 0.453564\tLR: 0.00038969\n",
            "Train Epoch: 6 [64512/123872 (52%)]\tLoss: 0.419528\tLR: 0.00038957\n",
            "Train Epoch: 6 [64768/123872 (52%)]\tLoss: 0.389642\tLR: 0.00038946\n",
            "Train Epoch: 6 [65024/123872 (52%)]\tLoss: 0.470137\tLR: 0.00038934\n",
            "Train Epoch: 6 [65280/123872 (53%)]\tLoss: 0.413851\tLR: 0.00038923\n",
            "Train Epoch: 6 [65536/123872 (53%)]\tLoss: 0.444678\tLR: 0.00038911\n",
            "Train Epoch: 6 [65792/123872 (53%)]\tLoss: 0.403356\tLR: 0.00038900\n",
            "Train Epoch: 6 [66048/123872 (53%)]\tLoss: 0.374631\tLR: 0.00038888\n",
            "Train Epoch: 6 [66304/123872 (54%)]\tLoss: 0.426237\tLR: 0.00038877\n",
            "Train Epoch: 6 [66560/123872 (54%)]\tLoss: 0.360122\tLR: 0.00038865\n",
            "Train Epoch: 6 [66560/123872 (54%)]\tLoss: 0.360122\n",
            "Train Epoch: 6 [66816/123872 (54%)]\tLoss: 0.482971\tLR: 0.00038853\n",
            "Train Epoch: 6 [67072/123872 (54%)]\tLoss: 0.444907\tLR: 0.00038842\n",
            "Train Epoch: 6 [67328/123872 (54%)]\tLoss: 0.378663\tLR: 0.00038830\n",
            "Train Epoch: 6 [67584/123872 (55%)]\tLoss: 0.419526\tLR: 0.00038819\n",
            "Train Epoch: 6 [67840/123872 (55%)]\tLoss: 0.445197\tLR: 0.00038807\n",
            "Train Epoch: 6 [68096/123872 (55%)]\tLoss: 0.456933\tLR: 0.00038796\n",
            "Train Epoch: 6 [68352/123872 (55%)]\tLoss: 0.478977\tLR: 0.00038784\n",
            "Train Epoch: 6 [68608/123872 (55%)]\tLoss: 0.434446\tLR: 0.00038773\n",
            "Train Epoch: 6 [68864/123872 (56%)]\tLoss: 0.430669\tLR: 0.00038761\n",
            "Train Epoch: 6 [69120/123872 (56%)]\tLoss: 0.516634\tLR: 0.00038750\n",
            "Train Epoch: 6 [69120/123872 (56%)]\tLoss: 0.516634\n",
            "Train Epoch: 6 [69376/123872 (56%)]\tLoss: 0.353813\tLR: 0.00038738\n",
            "Train Epoch: 6 [69632/123872 (56%)]\tLoss: 0.406649\tLR: 0.00038727\n",
            "Train Epoch: 6 [69888/123872 (56%)]\tLoss: 0.337001\tLR: 0.00038715\n",
            "Train Epoch: 6 [70144/123872 (57%)]\tLoss: 0.447617\tLR: 0.00038704\n",
            "Train Epoch: 6 [70400/123872 (57%)]\tLoss: 0.411592\tLR: 0.00038692\n",
            "Train Epoch: 6 [70656/123872 (57%)]\tLoss: 0.457321\tLR: 0.00038681\n",
            "Train Epoch: 6 [70912/123872 (57%)]\tLoss: 0.452556\tLR: 0.00038669\n",
            "Train Epoch: 6 [71168/123872 (57%)]\tLoss: 0.416261\tLR: 0.00038657\n",
            "Train Epoch: 6 [71424/123872 (58%)]\tLoss: 0.429174\tLR: 0.00038646\n",
            "Train Epoch: 6 [71680/123872 (58%)]\tLoss: 0.418469\tLR: 0.00038634\n",
            "Train Epoch: 6 [71680/123872 (58%)]\tLoss: 0.418469\n",
            "Train Epoch: 6 [71936/123872 (58%)]\tLoss: 0.503755\tLR: 0.00038623\n",
            "Train Epoch: 6 [72192/123872 (58%)]\tLoss: 0.473676\tLR: 0.00038611\n",
            "Train Epoch: 6 [72448/123872 (58%)]\tLoss: 0.391691\tLR: 0.00038600\n",
            "Train Epoch: 6 [72704/123872 (59%)]\tLoss: 0.441240\tLR: 0.00038588\n",
            "Train Epoch: 6 [72960/123872 (59%)]\tLoss: 0.513596\tLR: 0.00038577\n",
            "Train Epoch: 6 [73216/123872 (59%)]\tLoss: 0.413916\tLR: 0.00038565\n",
            "Train Epoch: 6 [73472/123872 (59%)]\tLoss: 0.442733\tLR: 0.00038554\n",
            "Train Epoch: 6 [73728/123872 (60%)]\tLoss: 0.459480\tLR: 0.00038542\n",
            "Train Epoch: 6 [73984/123872 (60%)]\tLoss: 0.492376\tLR: 0.00038530\n",
            "Train Epoch: 6 [74240/123872 (60%)]\tLoss: 0.474049\tLR: 0.00038519\n",
            "Train Epoch: 6 [74240/123872 (60%)]\tLoss: 0.474049\n",
            "Train Epoch: 6 [74496/123872 (60%)]\tLoss: 0.407766\tLR: 0.00038507\n",
            "Train Epoch: 6 [74752/123872 (60%)]\tLoss: 0.438355\tLR: 0.00038496\n",
            "Train Epoch: 6 [75008/123872 (61%)]\tLoss: 0.437344\tLR: 0.00038484\n",
            "Train Epoch: 6 [75264/123872 (61%)]\tLoss: 0.444017\tLR: 0.00038473\n",
            "Train Epoch: 6 [75520/123872 (61%)]\tLoss: 0.437837\tLR: 0.00038461\n",
            "Train Epoch: 6 [75776/123872 (61%)]\tLoss: 0.411058\tLR: 0.00038450\n",
            "Train Epoch: 6 [76032/123872 (61%)]\tLoss: 0.472787\tLR: 0.00038438\n",
            "Train Epoch: 6 [76288/123872 (62%)]\tLoss: 0.402074\tLR: 0.00038426\n",
            "Train Epoch: 6 [76544/123872 (62%)]\tLoss: 0.426279\tLR: 0.00038415\n",
            "Train Epoch: 6 [76800/123872 (62%)]\tLoss: 0.417812\tLR: 0.00038403\n",
            "Train Epoch: 6 [76800/123872 (62%)]\tLoss: 0.417812\n",
            "Train Epoch: 6 [77056/123872 (62%)]\tLoss: 0.437338\tLR: 0.00038392\n",
            "Train Epoch: 6 [77312/123872 (62%)]\tLoss: 0.455659\tLR: 0.00038380\n",
            "Train Epoch: 6 [77568/123872 (63%)]\tLoss: 0.386783\tLR: 0.00038369\n",
            "Train Epoch: 6 [77824/123872 (63%)]\tLoss: 0.532730\tLR: 0.00038357\n",
            "Train Epoch: 6 [78080/123872 (63%)]\tLoss: 0.368348\tLR: 0.00038346\n",
            "Train Epoch: 6 [78336/123872 (63%)]\tLoss: 0.462885\tLR: 0.00038334\n",
            "Train Epoch: 6 [78592/123872 (63%)]\tLoss: 0.470054\tLR: 0.00038322\n",
            "Train Epoch: 6 [78848/123872 (64%)]\tLoss: 0.490455\tLR: 0.00038311\n",
            "Train Epoch: 6 [79104/123872 (64%)]\tLoss: 0.433884\tLR: 0.00038299\n",
            "Train Epoch: 6 [79360/123872 (64%)]\tLoss: 0.430226\tLR: 0.00038288\n",
            "Train Epoch: 6 [79360/123872 (64%)]\tLoss: 0.430226\n",
            "Train Epoch: 6 [79616/123872 (64%)]\tLoss: 0.378809\tLR: 0.00038276\n",
            "Train Epoch: 6 [79872/123872 (64%)]\tLoss: 0.457311\tLR: 0.00038265\n",
            "Train Epoch: 6 [80128/123872 (65%)]\tLoss: 0.420572\tLR: 0.00038253\n",
            "Train Epoch: 6 [80384/123872 (65%)]\tLoss: 0.494879\tLR: 0.00038241\n",
            "Train Epoch: 6 [80640/123872 (65%)]\tLoss: 0.482860\tLR: 0.00038230\n",
            "Train Epoch: 6 [80896/123872 (65%)]\tLoss: 0.458772\tLR: 0.00038218\n",
            "Train Epoch: 6 [81152/123872 (65%)]\tLoss: 0.425115\tLR: 0.00038207\n",
            "Train Epoch: 6 [81408/123872 (66%)]\tLoss: 0.458043\tLR: 0.00038195\n",
            "Train Epoch: 6 [81664/123872 (66%)]\tLoss: 0.506598\tLR: 0.00038184\n",
            "Train Epoch: 6 [81920/123872 (66%)]\tLoss: 0.422890\tLR: 0.00038172\n",
            "Train Epoch: 6 [81920/123872 (66%)]\tLoss: 0.422890\n",
            "Train Epoch: 6 [82176/123872 (66%)]\tLoss: 0.401291\tLR: 0.00038160\n",
            "Train Epoch: 6 [82432/123872 (67%)]\tLoss: 0.406363\tLR: 0.00038149\n",
            "Train Epoch: 6 [82688/123872 (67%)]\tLoss: 0.369459\tLR: 0.00038137\n",
            "Train Epoch: 6 [82944/123872 (67%)]\tLoss: 0.484165\tLR: 0.00038126\n",
            "Train Epoch: 6 [83200/123872 (67%)]\tLoss: 0.472597\tLR: 0.00038114\n",
            "Train Epoch: 6 [83456/123872 (67%)]\tLoss: 0.427086\tLR: 0.00038102\n",
            "Train Epoch: 6 [83712/123872 (68%)]\tLoss: 0.439806\tLR: 0.00038091\n",
            "Train Epoch: 6 [83968/123872 (68%)]\tLoss: 0.418770\tLR: 0.00038079\n",
            "Train Epoch: 6 [84224/123872 (68%)]\tLoss: 0.416186\tLR: 0.00038068\n",
            "Train Epoch: 6 [84480/123872 (68%)]\tLoss: 0.465953\tLR: 0.00038056\n",
            "Train Epoch: 6 [84480/123872 (68%)]\tLoss: 0.465953\n",
            "Train Epoch: 6 [84736/123872 (68%)]\tLoss: 0.354593\tLR: 0.00038044\n",
            "Train Epoch: 6 [84992/123872 (69%)]\tLoss: 0.483868\tLR: 0.00038033\n",
            "Train Epoch: 6 [85248/123872 (69%)]\tLoss: 0.497191\tLR: 0.00038021\n",
            "Train Epoch: 6 [85504/123872 (69%)]\tLoss: 0.431834\tLR: 0.00038010\n",
            "Train Epoch: 6 [85760/123872 (69%)]\tLoss: 0.464064\tLR: 0.00037998\n",
            "Train Epoch: 6 [86016/123872 (69%)]\tLoss: 0.470020\tLR: 0.00037986\n",
            "Train Epoch: 6 [86272/123872 (70%)]\tLoss: 0.503620\tLR: 0.00037975\n",
            "Train Epoch: 6 [86528/123872 (70%)]\tLoss: 0.419949\tLR: 0.00037963\n",
            "Train Epoch: 6 [86784/123872 (70%)]\tLoss: 0.432607\tLR: 0.00037952\n",
            "Train Epoch: 6 [87040/123872 (70%)]\tLoss: 0.472013\tLR: 0.00037940\n",
            "Train Epoch: 6 [87040/123872 (70%)]\tLoss: 0.472013\n",
            "Train Epoch: 6 [87296/123872 (70%)]\tLoss: 0.457084\tLR: 0.00037928\n",
            "Train Epoch: 6 [87552/123872 (71%)]\tLoss: 0.464913\tLR: 0.00037917\n",
            "Train Epoch: 6 [87808/123872 (71%)]\tLoss: 0.447781\tLR: 0.00037905\n",
            "Train Epoch: 6 [88064/123872 (71%)]\tLoss: 0.439138\tLR: 0.00037894\n",
            "Train Epoch: 6 [88320/123872 (71%)]\tLoss: 0.477740\tLR: 0.00037882\n",
            "Train Epoch: 6 [88576/123872 (71%)]\tLoss: 0.357296\tLR: 0.00037870\n",
            "Train Epoch: 6 [88832/123872 (72%)]\tLoss: 0.400274\tLR: 0.00037859\n",
            "Train Epoch: 6 [89088/123872 (72%)]\tLoss: 0.430793\tLR: 0.00037847\n",
            "Train Epoch: 6 [89344/123872 (72%)]\tLoss: 0.404722\tLR: 0.00037836\n",
            "Train Epoch: 6 [89600/123872 (72%)]\tLoss: 0.442721\tLR: 0.00037824\n",
            "Train Epoch: 6 [89600/123872 (72%)]\tLoss: 0.442721\n",
            "Train Epoch: 6 [89856/123872 (73%)]\tLoss: 0.472349\tLR: 0.00037812\n",
            "Train Epoch: 6 [90112/123872 (73%)]\tLoss: 0.392049\tLR: 0.00037801\n",
            "Train Epoch: 6 [90368/123872 (73%)]\tLoss: 0.490176\tLR: 0.00037789\n",
            "Train Epoch: 6 [90624/123872 (73%)]\tLoss: 0.444641\tLR: 0.00037778\n",
            "Train Epoch: 6 [90880/123872 (73%)]\tLoss: 0.371402\tLR: 0.00037766\n",
            "Train Epoch: 6 [91136/123872 (74%)]\tLoss: 0.483336\tLR: 0.00037754\n",
            "Train Epoch: 6 [91392/123872 (74%)]\tLoss: 0.442992\tLR: 0.00037743\n",
            "Train Epoch: 6 [91648/123872 (74%)]\tLoss: 0.444696\tLR: 0.00037731\n",
            "Train Epoch: 6 [91904/123872 (74%)]\tLoss: 0.430437\tLR: 0.00037719\n",
            "Train Epoch: 6 [92160/123872 (74%)]\tLoss: 0.436084\tLR: 0.00037708\n",
            "Train Epoch: 6 [92160/123872 (74%)]\tLoss: 0.436084\n",
            "Train Epoch: 6 [92416/123872 (75%)]\tLoss: 0.440957\tLR: 0.00037696\n",
            "Train Epoch: 6 [92672/123872 (75%)]\tLoss: 0.394475\tLR: 0.00037685\n",
            "Train Epoch: 6 [92928/123872 (75%)]\tLoss: 0.412345\tLR: 0.00037673\n",
            "Train Epoch: 6 [93184/123872 (75%)]\tLoss: 0.456480\tLR: 0.00037661\n",
            "Train Epoch: 6 [93440/123872 (75%)]\tLoss: 0.446541\tLR: 0.00037650\n",
            "Train Epoch: 6 [93696/123872 (76%)]\tLoss: 0.425841\tLR: 0.00037638\n",
            "Train Epoch: 6 [93952/123872 (76%)]\tLoss: 0.395651\tLR: 0.00037627\n",
            "Train Epoch: 6 [94208/123872 (76%)]\tLoss: 0.438479\tLR: 0.00037615\n",
            "Train Epoch: 6 [94464/123872 (76%)]\tLoss: 0.459772\tLR: 0.00037603\n",
            "Train Epoch: 6 [94720/123872 (76%)]\tLoss: 0.434683\tLR: 0.00037592\n",
            "Train Epoch: 6 [94720/123872 (76%)]\tLoss: 0.434683\n",
            "Train Epoch: 6 [94976/123872 (77%)]\tLoss: 0.441641\tLR: 0.00037580\n",
            "Train Epoch: 6 [95232/123872 (77%)]\tLoss: 0.396976\tLR: 0.00037568\n",
            "Train Epoch: 6 [95488/123872 (77%)]\tLoss: 0.408541\tLR: 0.00037557\n",
            "Train Epoch: 6 [95744/123872 (77%)]\tLoss: 0.380311\tLR: 0.00037545\n",
            "Train Epoch: 6 [96000/123872 (77%)]\tLoss: 0.327084\tLR: 0.00037533\n",
            "Train Epoch: 6 [96256/123872 (78%)]\tLoss: 0.482604\tLR: 0.00037522\n",
            "Train Epoch: 6 [96512/123872 (78%)]\tLoss: 0.413130\tLR: 0.00037510\n",
            "Train Epoch: 6 [96768/123872 (78%)]\tLoss: 0.404857\tLR: 0.00037499\n",
            "Train Epoch: 6 [97024/123872 (78%)]\tLoss: 0.430760\tLR: 0.00037487\n",
            "Train Epoch: 6 [97280/123872 (79%)]\tLoss: 0.441910\tLR: 0.00037475\n",
            "Train Epoch: 6 [97280/123872 (79%)]\tLoss: 0.441910\n",
            "Train Epoch: 6 [97536/123872 (79%)]\tLoss: 0.458031\tLR: 0.00037464\n",
            "Train Epoch: 6 [97792/123872 (79%)]\tLoss: 0.403026\tLR: 0.00037452\n",
            "Train Epoch: 6 [98048/123872 (79%)]\tLoss: 0.474594\tLR: 0.00037440\n",
            "Train Epoch: 6 [98304/123872 (79%)]\tLoss: 0.402574\tLR: 0.00037429\n",
            "Train Epoch: 6 [98560/123872 (80%)]\tLoss: 0.359717\tLR: 0.00037417\n",
            "Train Epoch: 6 [98816/123872 (80%)]\tLoss: 0.421141\tLR: 0.00037405\n",
            "Train Epoch: 6 [99072/123872 (80%)]\tLoss: 0.430214\tLR: 0.00037394\n",
            "Train Epoch: 6 [99328/123872 (80%)]\tLoss: 0.433551\tLR: 0.00037382\n",
            "Train Epoch: 6 [99584/123872 (80%)]\tLoss: 0.390603\tLR: 0.00037371\n",
            "Train Epoch: 6 [99840/123872 (81%)]\tLoss: 0.412278\tLR: 0.00037359\n",
            "Train Epoch: 6 [99840/123872 (81%)]\tLoss: 0.412278\n",
            "Train Epoch: 6 [100096/123872 (81%)]\tLoss: 0.508577\tLR: 0.00037347\n",
            "Train Epoch: 6 [100352/123872 (81%)]\tLoss: 0.506083\tLR: 0.00037336\n",
            "Train Epoch: 6 [100608/123872 (81%)]\tLoss: 0.413484\tLR: 0.00037324\n",
            "Train Epoch: 6 [100864/123872 (81%)]\tLoss: 0.406409\tLR: 0.00037312\n",
            "Train Epoch: 6 [101120/123872 (82%)]\tLoss: 0.423225\tLR: 0.00037301\n",
            "Train Epoch: 6 [101376/123872 (82%)]\tLoss: 0.468559\tLR: 0.00037289\n",
            "Train Epoch: 6 [101632/123872 (82%)]\tLoss: 0.441285\tLR: 0.00037277\n",
            "Train Epoch: 6 [101888/123872 (82%)]\tLoss: 0.467539\tLR: 0.00037266\n",
            "Train Epoch: 6 [102144/123872 (82%)]\tLoss: 0.516908\tLR: 0.00037254\n",
            "Train Epoch: 6 [102400/123872 (83%)]\tLoss: 0.427376\tLR: 0.00037242\n",
            "Train Epoch: 6 [102400/123872 (83%)]\tLoss: 0.427376\n",
            "Train Epoch: 6 [102656/123872 (83%)]\tLoss: 0.448874\tLR: 0.00037231\n",
            "Train Epoch: 6 [102912/123872 (83%)]\tLoss: 0.401475\tLR: 0.00037219\n",
            "Train Epoch: 6 [103168/123872 (83%)]\tLoss: 0.412742\tLR: 0.00037207\n",
            "Train Epoch: 6 [103424/123872 (83%)]\tLoss: 0.428005\tLR: 0.00037196\n",
            "Train Epoch: 6 [103680/123872 (84%)]\tLoss: 0.403749\tLR: 0.00037184\n",
            "Train Epoch: 6 [103936/123872 (84%)]\tLoss: 0.455168\tLR: 0.00037172\n",
            "Train Epoch: 6 [104192/123872 (84%)]\tLoss: 0.416503\tLR: 0.00037161\n",
            "Train Epoch: 6 [104448/123872 (84%)]\tLoss: 0.421108\tLR: 0.00037149\n",
            "Train Epoch: 6 [104704/123872 (85%)]\tLoss: 0.480969\tLR: 0.00037137\n",
            "Train Epoch: 6 [104960/123872 (85%)]\tLoss: 0.462742\tLR: 0.00037126\n",
            "Train Epoch: 6 [104960/123872 (85%)]\tLoss: 0.462742\n",
            "Train Epoch: 6 [105216/123872 (85%)]\tLoss: 0.425570\tLR: 0.00037114\n",
            "Train Epoch: 6 [105472/123872 (85%)]\tLoss: 0.427439\tLR: 0.00037102\n",
            "Train Epoch: 6 [105728/123872 (85%)]\tLoss: 0.444950\tLR: 0.00037091\n",
            "Train Epoch: 6 [105984/123872 (86%)]\tLoss: 0.420181\tLR: 0.00037079\n",
            "Train Epoch: 6 [106240/123872 (86%)]\tLoss: 0.363470\tLR: 0.00037067\n",
            "Train Epoch: 6 [106496/123872 (86%)]\tLoss: 0.403396\tLR: 0.00037056\n",
            "Train Epoch: 6 [106752/123872 (86%)]\tLoss: 0.463057\tLR: 0.00037044\n",
            "Train Epoch: 6 [107008/123872 (86%)]\tLoss: 0.418811\tLR: 0.00037032\n",
            "Train Epoch: 6 [107264/123872 (87%)]\tLoss: 0.487078\tLR: 0.00037021\n",
            "Train Epoch: 6 [107520/123872 (87%)]\tLoss: 0.445356\tLR: 0.00037009\n",
            "Train Epoch: 6 [107520/123872 (87%)]\tLoss: 0.445356\n",
            "Train Epoch: 6 [107776/123872 (87%)]\tLoss: 0.416526\tLR: 0.00036997\n",
            "Train Epoch: 6 [108032/123872 (87%)]\tLoss: 0.395000\tLR: 0.00036986\n",
            "Train Epoch: 6 [108288/123872 (87%)]\tLoss: 0.420508\tLR: 0.00036974\n",
            "Train Epoch: 6 [108544/123872 (88%)]\tLoss: 0.490552\tLR: 0.00036962\n",
            "Train Epoch: 6 [108800/123872 (88%)]\tLoss: 0.483875\tLR: 0.00036951\n",
            "Train Epoch: 6 [109056/123872 (88%)]\tLoss: 0.383407\tLR: 0.00036939\n",
            "Train Epoch: 6 [109312/123872 (88%)]\tLoss: 0.402184\tLR: 0.00036927\n",
            "Train Epoch: 6 [109568/123872 (88%)]\tLoss: 0.394739\tLR: 0.00036916\n",
            "Train Epoch: 6 [109824/123872 (89%)]\tLoss: 0.395703\tLR: 0.00036904\n",
            "Train Epoch: 6 [110080/123872 (89%)]\tLoss: 0.398221\tLR: 0.00036892\n",
            "Train Epoch: 6 [110080/123872 (89%)]\tLoss: 0.398221\n",
            "Train Epoch: 6 [110336/123872 (89%)]\tLoss: 0.420236\tLR: 0.00036881\n",
            "Train Epoch: 6 [110592/123872 (89%)]\tLoss: 0.385256\tLR: 0.00036869\n",
            "Train Epoch: 6 [110848/123872 (89%)]\tLoss: 0.410706\tLR: 0.00036857\n",
            "Train Epoch: 6 [111104/123872 (90%)]\tLoss: 0.510595\tLR: 0.00036846\n",
            "Train Epoch: 6 [111360/123872 (90%)]\tLoss: 0.416714\tLR: 0.00036834\n",
            "Train Epoch: 6 [111616/123872 (90%)]\tLoss: 0.396325\tLR: 0.00036822\n",
            "Train Epoch: 6 [111872/123872 (90%)]\tLoss: 0.426971\tLR: 0.00036811\n",
            "Train Epoch: 6 [112128/123872 (90%)]\tLoss: 0.413838\tLR: 0.00036799\n",
            "Train Epoch: 6 [112384/123872 (91%)]\tLoss: 0.460620\tLR: 0.00036787\n",
            "Train Epoch: 6 [112640/123872 (91%)]\tLoss: 0.438078\tLR: 0.00036776\n",
            "Train Epoch: 6 [112640/123872 (91%)]\tLoss: 0.438078\n",
            "Train Epoch: 6 [112896/123872 (91%)]\tLoss: 0.403246\tLR: 0.00036764\n",
            "Train Epoch: 6 [113152/123872 (91%)]\tLoss: 0.388017\tLR: 0.00036752\n",
            "Train Epoch: 6 [113408/123872 (92%)]\tLoss: 0.416207\tLR: 0.00036741\n",
            "Train Epoch: 6 [113664/123872 (92%)]\tLoss: 0.458422\tLR: 0.00036729\n",
            "Train Epoch: 6 [113920/123872 (92%)]\tLoss: 0.438968\tLR: 0.00036717\n",
            "Train Epoch: 6 [114176/123872 (92%)]\tLoss: 0.441474\tLR: 0.00036705\n",
            "Train Epoch: 6 [114432/123872 (92%)]\tLoss: 0.462605\tLR: 0.00036694\n",
            "Train Epoch: 6 [114688/123872 (93%)]\tLoss: 0.374170\tLR: 0.00036682\n",
            "Train Epoch: 6 [114944/123872 (93%)]\tLoss: 0.428533\tLR: 0.00036670\n",
            "Train Epoch: 6 [115200/123872 (93%)]\tLoss: 0.401204\tLR: 0.00036659\n",
            "Train Epoch: 6 [115200/123872 (93%)]\tLoss: 0.401204\n",
            "Train Epoch: 6 [115456/123872 (93%)]\tLoss: 0.454943\tLR: 0.00036647\n",
            "Train Epoch: 6 [115712/123872 (93%)]\tLoss: 0.436948\tLR: 0.00036635\n",
            "Train Epoch: 6 [115968/123872 (94%)]\tLoss: 0.453195\tLR: 0.00036624\n",
            "Train Epoch: 6 [116224/123872 (94%)]\tLoss: 0.464596\tLR: 0.00036612\n",
            "Train Epoch: 6 [116480/123872 (94%)]\tLoss: 0.451134\tLR: 0.00036600\n",
            "Train Epoch: 6 [116736/123872 (94%)]\tLoss: 0.417881\tLR: 0.00036589\n",
            "Train Epoch: 6 [116992/123872 (94%)]\tLoss: 0.438886\tLR: 0.00036577\n",
            "Train Epoch: 6 [117248/123872 (95%)]\tLoss: 0.427113\tLR: 0.00036565\n",
            "Train Epoch: 6 [117504/123872 (95%)]\tLoss: 0.466309\tLR: 0.00036553\n",
            "Train Epoch: 6 [117760/123872 (95%)]\tLoss: 0.340031\tLR: 0.00036542\n",
            "Train Epoch: 6 [117760/123872 (95%)]\tLoss: 0.340031\n",
            "Train Epoch: 6 [118016/123872 (95%)]\tLoss: 0.389808\tLR: 0.00036530\n",
            "Train Epoch: 6 [118272/123872 (95%)]\tLoss: 0.508305\tLR: 0.00036518\n",
            "Train Epoch: 6 [118528/123872 (96%)]\tLoss: 0.456783\tLR: 0.00036507\n",
            "Train Epoch: 6 [118784/123872 (96%)]\tLoss: 0.446058\tLR: 0.00036495\n",
            "Train Epoch: 6 [119040/123872 (96%)]\tLoss: 0.405659\tLR: 0.00036483\n",
            "Train Epoch: 6 [119296/123872 (96%)]\tLoss: 0.463342\tLR: 0.00036472\n",
            "Train Epoch: 6 [119552/123872 (96%)]\tLoss: 0.423932\tLR: 0.00036460\n",
            "Train Epoch: 6 [119808/123872 (97%)]\tLoss: 0.436067\tLR: 0.00036448\n",
            "Train Epoch: 6 [120064/123872 (97%)]\tLoss: 0.427723\tLR: 0.00036436\n",
            "Train Epoch: 6 [120320/123872 (97%)]\tLoss: 0.372695\tLR: 0.00036425\n",
            "Train Epoch: 6 [120320/123872 (97%)]\tLoss: 0.372695\n",
            "Train Epoch: 6 [120576/123872 (97%)]\tLoss: 0.443356\tLR: 0.00036413\n",
            "Train Epoch: 6 [120832/123872 (98%)]\tLoss: 0.393277\tLR: 0.00036401\n",
            "Train Epoch: 6 [121088/123872 (98%)]\tLoss: 0.364878\tLR: 0.00036390\n",
            "Train Epoch: 6 [121344/123872 (98%)]\tLoss: 0.342773\tLR: 0.00036378\n",
            "Train Epoch: 6 [121600/123872 (98%)]\tLoss: 0.404930\tLR: 0.00036366\n",
            "Train Epoch: 6 [121856/123872 (98%)]\tLoss: 0.415277\tLR: 0.00036354\n",
            "Train Epoch: 6 [122112/123872 (99%)]\tLoss: 0.372650\tLR: 0.00036343\n",
            "Train Epoch: 6 [122368/123872 (99%)]\tLoss: 0.388684\tLR: 0.00036331\n",
            "Train Epoch: 6 [122624/123872 (99%)]\tLoss: 0.387950\tLR: 0.00036319\n",
            "Train Epoch: 6 [122880/123872 (99%)]\tLoss: 0.479718\tLR: 0.00036308\n",
            "Train Epoch: 6 [122880/123872 (99%)]\tLoss: 0.479718\n",
            "Train Epoch: 6 [123136/123872 (99%)]\tLoss: 0.421625\tLR: 0.00036296\n",
            "Train Epoch: 6 [123392/123872 (100%)]\tLoss: 0.438408\tLR: 0.00036284\n",
            "Train Epoch: 6 [108192/123872 (100%)]\tLoss: 0.428313\tLR: 0.00036273\n",
            "\n",
            "Test set: Average loss: 0.0018, Accuracy: 24566/30970 (79.32%)\n",
            "\n",
            "Train Epoch: 7 [0/123872 (0%)]\tLoss: 0.488731\tLR: 0.00036261\n",
            "Train Epoch: 7 [0/123872 (0%)]\tLoss: 0.488731\n",
            "Train Epoch: 7 [256/123872 (0%)]\tLoss: 0.462893\tLR: 0.00036249\n",
            "Train Epoch: 7 [512/123872 (0%)]\tLoss: 0.479068\tLR: 0.00036237\n",
            "Train Epoch: 7 [768/123872 (1%)]\tLoss: 0.415603\tLR: 0.00036226\n",
            "Train Epoch: 7 [1024/123872 (1%)]\tLoss: 0.359585\tLR: 0.00036214\n",
            "Train Epoch: 7 [1280/123872 (1%)]\tLoss: 0.483357\tLR: 0.00036202\n",
            "Train Epoch: 7 [1536/123872 (1%)]\tLoss: 0.401125\tLR: 0.00036191\n",
            "Train Epoch: 7 [1792/123872 (1%)]\tLoss: 0.384889\tLR: 0.00036179\n",
            "Train Epoch: 7 [2048/123872 (2%)]\tLoss: 0.445647\tLR: 0.00036167\n",
            "Train Epoch: 7 [2304/123872 (2%)]\tLoss: 0.462640\tLR: 0.00036155\n",
            "Train Epoch: 7 [2560/123872 (2%)]\tLoss: 0.413188\tLR: 0.00036144\n",
            "Train Epoch: 7 [2560/123872 (2%)]\tLoss: 0.413188\n",
            "Train Epoch: 7 [2816/123872 (2%)]\tLoss: 0.460196\tLR: 0.00036132\n",
            "Train Epoch: 7 [3072/123872 (2%)]\tLoss: 0.395451\tLR: 0.00036120\n",
            "Train Epoch: 7 [3328/123872 (3%)]\tLoss: 0.434551\tLR: 0.00036108\n",
            "Train Epoch: 7 [3584/123872 (3%)]\tLoss: 0.440946\tLR: 0.00036097\n",
            "Train Epoch: 7 [3840/123872 (3%)]\tLoss: 0.433630\tLR: 0.00036085\n",
            "Train Epoch: 7 [4096/123872 (3%)]\tLoss: 0.474231\tLR: 0.00036073\n",
            "Train Epoch: 7 [4352/123872 (4%)]\tLoss: 0.389554\tLR: 0.00036062\n",
            "Train Epoch: 7 [4608/123872 (4%)]\tLoss: 0.436551\tLR: 0.00036050\n",
            "Train Epoch: 7 [4864/123872 (4%)]\tLoss: 0.381078\tLR: 0.00036038\n",
            "Train Epoch: 7 [5120/123872 (4%)]\tLoss: 0.428421\tLR: 0.00036026\n",
            "Train Epoch: 7 [5120/123872 (4%)]\tLoss: 0.428421\n",
            "Train Epoch: 7 [5376/123872 (4%)]\tLoss: 0.412047\tLR: 0.00036015\n",
            "Train Epoch: 7 [5632/123872 (5%)]\tLoss: 0.407451\tLR: 0.00036003\n",
            "Train Epoch: 7 [5888/123872 (5%)]\tLoss: 0.407069\tLR: 0.00035991\n",
            "Train Epoch: 7 [6144/123872 (5%)]\tLoss: 0.399466\tLR: 0.00035979\n",
            "Train Epoch: 7 [6400/123872 (5%)]\tLoss: 0.495785\tLR: 0.00035968\n",
            "Train Epoch: 7 [6656/123872 (5%)]\tLoss: 0.446022\tLR: 0.00035956\n",
            "Train Epoch: 7 [6912/123872 (6%)]\tLoss: 0.393731\tLR: 0.00035944\n",
            "Train Epoch: 7 [7168/123872 (6%)]\tLoss: 0.463133\tLR: 0.00035933\n",
            "Train Epoch: 7 [7424/123872 (6%)]\tLoss: 0.357721\tLR: 0.00035921\n",
            "Train Epoch: 7 [7680/123872 (6%)]\tLoss: 0.433620\tLR: 0.00035909\n",
            "Train Epoch: 7 [7680/123872 (6%)]\tLoss: 0.433620\n",
            "Train Epoch: 7 [7936/123872 (6%)]\tLoss: 0.407605\tLR: 0.00035897\n",
            "Train Epoch: 7 [8192/123872 (7%)]\tLoss: 0.367761\tLR: 0.00035886\n",
            "Train Epoch: 7 [8448/123872 (7%)]\tLoss: 0.445009\tLR: 0.00035874\n",
            "Train Epoch: 7 [8704/123872 (7%)]\tLoss: 0.382176\tLR: 0.00035862\n",
            "Train Epoch: 7 [8960/123872 (7%)]\tLoss: 0.412647\tLR: 0.00035850\n",
            "Train Epoch: 7 [9216/123872 (7%)]\tLoss: 0.417107\tLR: 0.00035839\n",
            "Train Epoch: 7 [9472/123872 (8%)]\tLoss: 0.463119\tLR: 0.00035827\n",
            "Train Epoch: 7 [9728/123872 (8%)]\tLoss: 0.380501\tLR: 0.00035815\n",
            "Train Epoch: 7 [9984/123872 (8%)]\tLoss: 0.530445\tLR: 0.00035803\n",
            "Train Epoch: 7 [10240/123872 (8%)]\tLoss: 0.380907\tLR: 0.00035792\n",
            "Train Epoch: 7 [10240/123872 (8%)]\tLoss: 0.380907\n",
            "Train Epoch: 7 [10496/123872 (8%)]\tLoss: 0.402563\tLR: 0.00035780\n",
            "Train Epoch: 7 [10752/123872 (9%)]\tLoss: 0.417601\tLR: 0.00035768\n",
            "Train Epoch: 7 [11008/123872 (9%)]\tLoss: 0.382836\tLR: 0.00035757\n",
            "Train Epoch: 7 [11264/123872 (9%)]\tLoss: 0.446844\tLR: 0.00035745\n",
            "Train Epoch: 7 [11520/123872 (9%)]\tLoss: 0.433904\tLR: 0.00035733\n",
            "Train Epoch: 7 [11776/123872 (10%)]\tLoss: 0.383707\tLR: 0.00035721\n",
            "Train Epoch: 7 [12032/123872 (10%)]\tLoss: 0.424581\tLR: 0.00035710\n",
            "Train Epoch: 7 [12288/123872 (10%)]\tLoss: 0.421687\tLR: 0.00035698\n",
            "Train Epoch: 7 [12544/123872 (10%)]\tLoss: 0.467835\tLR: 0.00035686\n",
            "Train Epoch: 7 [12800/123872 (10%)]\tLoss: 0.408349\tLR: 0.00035674\n",
            "Train Epoch: 7 [12800/123872 (10%)]\tLoss: 0.408349\n",
            "Train Epoch: 7 [13056/123872 (11%)]\tLoss: 0.430531\tLR: 0.00035663\n",
            "Train Epoch: 7 [13312/123872 (11%)]\tLoss: 0.452942\tLR: 0.00035651\n",
            "Train Epoch: 7 [13568/123872 (11%)]\tLoss: 0.472580\tLR: 0.00035639\n",
            "Train Epoch: 7 [13824/123872 (11%)]\tLoss: 0.444564\tLR: 0.00035627\n",
            "Train Epoch: 7 [14080/123872 (11%)]\tLoss: 0.493191\tLR: 0.00035616\n",
            "Train Epoch: 7 [14336/123872 (12%)]\tLoss: 0.418090\tLR: 0.00035604\n",
            "Train Epoch: 7 [14592/123872 (12%)]\tLoss: 0.464124\tLR: 0.00035592\n",
            "Train Epoch: 7 [14848/123872 (12%)]\tLoss: 0.410329\tLR: 0.00035580\n",
            "Train Epoch: 7 [15104/123872 (12%)]\tLoss: 0.444258\tLR: 0.00035569\n",
            "Train Epoch: 7 [15360/123872 (12%)]\tLoss: 0.407815\tLR: 0.00035557\n",
            "Train Epoch: 7 [15360/123872 (12%)]\tLoss: 0.407815\n",
            "Train Epoch: 7 [15616/123872 (13%)]\tLoss: 0.398719\tLR: 0.00035545\n",
            "Train Epoch: 7 [15872/123872 (13%)]\tLoss: 0.456151\tLR: 0.00035533\n",
            "Train Epoch: 7 [16128/123872 (13%)]\tLoss: 0.421960\tLR: 0.00035522\n",
            "Train Epoch: 7 [16384/123872 (13%)]\tLoss: 0.441663\tLR: 0.00035510\n",
            "Train Epoch: 7 [16640/123872 (13%)]\tLoss: 0.406520\tLR: 0.00035498\n",
            "Train Epoch: 7 [16896/123872 (14%)]\tLoss: 0.398655\tLR: 0.00035486\n",
            "Train Epoch: 7 [17152/123872 (14%)]\tLoss: 0.404985\tLR: 0.00035475\n",
            "Train Epoch: 7 [17408/123872 (14%)]\tLoss: 0.436783\tLR: 0.00035463\n",
            "Train Epoch: 7 [17664/123872 (14%)]\tLoss: 0.443710\tLR: 0.00035451\n",
            "Train Epoch: 7 [17920/123872 (14%)]\tLoss: 0.396187\tLR: 0.00035439\n",
            "Train Epoch: 7 [17920/123872 (14%)]\tLoss: 0.396187\n",
            "Train Epoch: 7 [18176/123872 (15%)]\tLoss: 0.516850\tLR: 0.00035428\n",
            "Train Epoch: 7 [18432/123872 (15%)]\tLoss: 0.373257\tLR: 0.00035416\n",
            "Train Epoch: 7 [18688/123872 (15%)]\tLoss: 0.384241\tLR: 0.00035404\n",
            "Train Epoch: 7 [18944/123872 (15%)]\tLoss: 0.397459\tLR: 0.00035392\n",
            "Train Epoch: 7 [19200/123872 (15%)]\tLoss: 0.401779\tLR: 0.00035381\n",
            "Train Epoch: 7 [19456/123872 (16%)]\tLoss: 0.435483\tLR: 0.00035369\n",
            "Train Epoch: 7 [19712/123872 (16%)]\tLoss: 0.402960\tLR: 0.00035357\n",
            "Train Epoch: 7 [19968/123872 (16%)]\tLoss: 0.415117\tLR: 0.00035345\n",
            "Train Epoch: 7 [20224/123872 (16%)]\tLoss: 0.351651\tLR: 0.00035334\n",
            "Train Epoch: 7 [20480/123872 (17%)]\tLoss: 0.469562\tLR: 0.00035322\n",
            "Train Epoch: 7 [20480/123872 (17%)]\tLoss: 0.469562\n",
            "Train Epoch: 7 [20736/123872 (17%)]\tLoss: 0.458487\tLR: 0.00035310\n",
            "Train Epoch: 7 [20992/123872 (17%)]\tLoss: 0.432868\tLR: 0.00035298\n",
            "Train Epoch: 7 [21248/123872 (17%)]\tLoss: 0.451851\tLR: 0.00035287\n",
            "Train Epoch: 7 [21504/123872 (17%)]\tLoss: 0.468023\tLR: 0.00035275\n",
            "Train Epoch: 7 [21760/123872 (18%)]\tLoss: 0.408928\tLR: 0.00035263\n",
            "Train Epoch: 7 [22016/123872 (18%)]\tLoss: 0.431570\tLR: 0.00035251\n",
            "Train Epoch: 7 [22272/123872 (18%)]\tLoss: 0.410759\tLR: 0.00035240\n",
            "Train Epoch: 7 [22528/123872 (18%)]\tLoss: 0.494176\tLR: 0.00035228\n",
            "Train Epoch: 7 [22784/123872 (18%)]\tLoss: 0.451367\tLR: 0.00035216\n",
            "Train Epoch: 7 [23040/123872 (19%)]\tLoss: 0.430018\tLR: 0.00035204\n",
            "Train Epoch: 7 [23040/123872 (19%)]\tLoss: 0.430018\n",
            "Train Epoch: 7 [23296/123872 (19%)]\tLoss: 0.404777\tLR: 0.00035193\n",
            "Train Epoch: 7 [23552/123872 (19%)]\tLoss: 0.482800\tLR: 0.00035181\n",
            "Train Epoch: 7 [23808/123872 (19%)]\tLoss: 0.421340\tLR: 0.00035169\n",
            "Train Epoch: 7 [24064/123872 (19%)]\tLoss: 0.431050\tLR: 0.00035157\n",
            "Train Epoch: 7 [24320/123872 (20%)]\tLoss: 0.463613\tLR: 0.00035145\n",
            "Train Epoch: 7 [24576/123872 (20%)]\tLoss: 0.378667\tLR: 0.00035134\n",
            "Train Epoch: 7 [24832/123872 (20%)]\tLoss: 0.472328\tLR: 0.00035122\n",
            "Train Epoch: 7 [25088/123872 (20%)]\tLoss: 0.440629\tLR: 0.00035110\n",
            "Train Epoch: 7 [25344/123872 (20%)]\tLoss: 0.389033\tLR: 0.00035098\n",
            "Train Epoch: 7 [25600/123872 (21%)]\tLoss: 0.436320\tLR: 0.00035087\n",
            "Train Epoch: 7 [25600/123872 (21%)]\tLoss: 0.436320\n",
            "Train Epoch: 7 [25856/123872 (21%)]\tLoss: 0.455214\tLR: 0.00035075\n",
            "Train Epoch: 7 [26112/123872 (21%)]\tLoss: 0.432435\tLR: 0.00035063\n",
            "Train Epoch: 7 [26368/123872 (21%)]\tLoss: 0.440913\tLR: 0.00035051\n",
            "Train Epoch: 7 [26624/123872 (21%)]\tLoss: 0.464415\tLR: 0.00035040\n",
            "Train Epoch: 7 [26880/123872 (22%)]\tLoss: 0.420389\tLR: 0.00035028\n",
            "Train Epoch: 7 [27136/123872 (22%)]\tLoss: 0.465861\tLR: 0.00035016\n",
            "Train Epoch: 7 [27392/123872 (22%)]\tLoss: 0.464522\tLR: 0.00035004\n",
            "Train Epoch: 7 [27648/123872 (22%)]\tLoss: 0.425086\tLR: 0.00034992\n",
            "Train Epoch: 7 [27904/123872 (23%)]\tLoss: 0.516149\tLR: 0.00034981\n",
            "Train Epoch: 7 [28160/123872 (23%)]\tLoss: 0.420255\tLR: 0.00034969\n",
            "Train Epoch: 7 [28160/123872 (23%)]\tLoss: 0.420255\n",
            "Train Epoch: 7 [28416/123872 (23%)]\tLoss: 0.421270\tLR: 0.00034957\n",
            "Train Epoch: 7 [28672/123872 (23%)]\tLoss: 0.393529\tLR: 0.00034945\n",
            "Train Epoch: 7 [28928/123872 (23%)]\tLoss: 0.341963\tLR: 0.00034934\n",
            "Train Epoch: 7 [29184/123872 (24%)]\tLoss: 0.409084\tLR: 0.00034922\n",
            "Train Epoch: 7 [29440/123872 (24%)]\tLoss: 0.390559\tLR: 0.00034910\n",
            "Train Epoch: 7 [29696/123872 (24%)]\tLoss: 0.422520\tLR: 0.00034898\n",
            "Train Epoch: 7 [29952/123872 (24%)]\tLoss: 0.460954\tLR: 0.00034887\n",
            "Train Epoch: 7 [30208/123872 (24%)]\tLoss: 0.432639\tLR: 0.00034875\n",
            "Train Epoch: 7 [30464/123872 (25%)]\tLoss: 0.499683\tLR: 0.00034863\n",
            "Train Epoch: 7 [30720/123872 (25%)]\tLoss: 0.392985\tLR: 0.00034851\n",
            "Train Epoch: 7 [30720/123872 (25%)]\tLoss: 0.392985\n",
            "Train Epoch: 7 [30976/123872 (25%)]\tLoss: 0.411949\tLR: 0.00034839\n",
            "Train Epoch: 7 [31232/123872 (25%)]\tLoss: 0.438358\tLR: 0.00034828\n",
            "Train Epoch: 7 [31488/123872 (25%)]\tLoss: 0.406880\tLR: 0.00034816\n",
            "Train Epoch: 7 [31744/123872 (26%)]\tLoss: 0.487832\tLR: 0.00034804\n",
            "Train Epoch: 7 [32000/123872 (26%)]\tLoss: 0.417484\tLR: 0.00034792\n",
            "Train Epoch: 7 [32256/123872 (26%)]\tLoss: 0.434400\tLR: 0.00034781\n",
            "Train Epoch: 7 [32512/123872 (26%)]\tLoss: 0.408251\tLR: 0.00034769\n",
            "Train Epoch: 7 [32768/123872 (26%)]\tLoss: 0.502068\tLR: 0.00034757\n",
            "Train Epoch: 7 [33024/123872 (27%)]\tLoss: 0.441633\tLR: 0.00034745\n",
            "Train Epoch: 7 [33280/123872 (27%)]\tLoss: 0.459979\tLR: 0.00034734\n",
            "Train Epoch: 7 [33280/123872 (27%)]\tLoss: 0.459979\n",
            "Train Epoch: 7 [33536/123872 (27%)]\tLoss: 0.436768\tLR: 0.00034722\n",
            "Train Epoch: 7 [33792/123872 (27%)]\tLoss: 0.430852\tLR: 0.00034710\n",
            "Train Epoch: 7 [34048/123872 (27%)]\tLoss: 0.377110\tLR: 0.00034698\n",
            "Train Epoch: 7 [34304/123872 (28%)]\tLoss: 0.401238\tLR: 0.00034686\n",
            "Train Epoch: 7 [34560/123872 (28%)]\tLoss: 0.419153\tLR: 0.00034675\n",
            "Train Epoch: 7 [34816/123872 (28%)]\tLoss: 0.423923\tLR: 0.00034663\n",
            "Train Epoch: 7 [35072/123872 (28%)]\tLoss: 0.421950\tLR: 0.00034651\n",
            "Train Epoch: 7 [35328/123872 (29%)]\tLoss: 0.480133\tLR: 0.00034639\n",
            "Train Epoch: 7 [35584/123872 (29%)]\tLoss: 0.403368\tLR: 0.00034628\n",
            "Train Epoch: 7 [35840/123872 (29%)]\tLoss: 0.397970\tLR: 0.00034616\n",
            "Train Epoch: 7 [35840/123872 (29%)]\tLoss: 0.397970\n",
            "Train Epoch: 7 [36096/123872 (29%)]\tLoss: 0.423778\tLR: 0.00034604\n",
            "Train Epoch: 7 [36352/123872 (29%)]\tLoss: 0.418398\tLR: 0.00034592\n",
            "Train Epoch: 7 [36608/123872 (30%)]\tLoss: 0.360531\tLR: 0.00034580\n",
            "Train Epoch: 7 [36864/123872 (30%)]\tLoss: 0.467989\tLR: 0.00034569\n",
            "Train Epoch: 7 [37120/123872 (30%)]\tLoss: 0.405187\tLR: 0.00034557\n",
            "Train Epoch: 7 [37376/123872 (30%)]\tLoss: 0.448495\tLR: 0.00034545\n",
            "Train Epoch: 7 [37632/123872 (30%)]\tLoss: 0.457036\tLR: 0.00034533\n",
            "Train Epoch: 7 [37888/123872 (31%)]\tLoss: 0.439289\tLR: 0.00034521\n",
            "Train Epoch: 7 [38144/123872 (31%)]\tLoss: 0.411167\tLR: 0.00034510\n",
            "Train Epoch: 7 [38400/123872 (31%)]\tLoss: 0.427465\tLR: 0.00034498\n",
            "Train Epoch: 7 [38400/123872 (31%)]\tLoss: 0.427465\n",
            "Train Epoch: 7 [38656/123872 (31%)]\tLoss: 0.411227\tLR: 0.00034486\n",
            "Train Epoch: 7 [38912/123872 (31%)]\tLoss: 0.519122\tLR: 0.00034474\n",
            "Train Epoch: 7 [39168/123872 (32%)]\tLoss: 0.426236\tLR: 0.00034463\n",
            "Train Epoch: 7 [39424/123872 (32%)]\tLoss: 0.439564\tLR: 0.00034451\n",
            "Train Epoch: 7 [39680/123872 (32%)]\tLoss: 0.438968\tLR: 0.00034439\n",
            "Train Epoch: 7 [39936/123872 (32%)]\tLoss: 0.434372\tLR: 0.00034427\n",
            "Train Epoch: 7 [40192/123872 (32%)]\tLoss: 0.447508\tLR: 0.00034415\n",
            "Train Epoch: 7 [40448/123872 (33%)]\tLoss: 0.407287\tLR: 0.00034404\n",
            "Train Epoch: 7 [40704/123872 (33%)]\tLoss: 0.380050\tLR: 0.00034392\n",
            "Train Epoch: 7 [40960/123872 (33%)]\tLoss: 0.369923\tLR: 0.00034380\n",
            "Train Epoch: 7 [40960/123872 (33%)]\tLoss: 0.369923\n",
            "Train Epoch: 7 [41216/123872 (33%)]\tLoss: 0.416202\tLR: 0.00034368\n",
            "Train Epoch: 7 [41472/123872 (33%)]\tLoss: 0.418022\tLR: 0.00034357\n",
            "Train Epoch: 7 [41728/123872 (34%)]\tLoss: 0.414318\tLR: 0.00034345\n",
            "Train Epoch: 7 [41984/123872 (34%)]\tLoss: 0.381356\tLR: 0.00034333\n",
            "Train Epoch: 7 [42240/123872 (34%)]\tLoss: 0.434467\tLR: 0.00034321\n",
            "Train Epoch: 7 [42496/123872 (34%)]\tLoss: 0.409014\tLR: 0.00034309\n",
            "Train Epoch: 7 [42752/123872 (35%)]\tLoss: 0.408398\tLR: 0.00034298\n",
            "Train Epoch: 7 [43008/123872 (35%)]\tLoss: 0.366811\tLR: 0.00034286\n",
            "Train Epoch: 7 [43264/123872 (35%)]\tLoss: 0.386420\tLR: 0.00034274\n",
            "Train Epoch: 7 [43520/123872 (35%)]\tLoss: 0.465217\tLR: 0.00034262\n",
            "Train Epoch: 7 [43520/123872 (35%)]\tLoss: 0.465217\n",
            "Train Epoch: 7 [43776/123872 (35%)]\tLoss: 0.405141\tLR: 0.00034250\n",
            "Train Epoch: 7 [44032/123872 (36%)]\tLoss: 0.414796\tLR: 0.00034239\n",
            "Train Epoch: 7 [44288/123872 (36%)]\tLoss: 0.411064\tLR: 0.00034227\n",
            "Train Epoch: 7 [44544/123872 (36%)]\tLoss: 0.417613\tLR: 0.00034215\n",
            "Train Epoch: 7 [44800/123872 (36%)]\tLoss: 0.405589\tLR: 0.00034203\n",
            "Train Epoch: 7 [45056/123872 (36%)]\tLoss: 0.349843\tLR: 0.00034191\n",
            "Train Epoch: 7 [45312/123872 (37%)]\tLoss: 0.425804\tLR: 0.00034180\n",
            "Train Epoch: 7 [45568/123872 (37%)]\tLoss: 0.439899\tLR: 0.00034168\n",
            "Train Epoch: 7 [45824/123872 (37%)]\tLoss: 0.458734\tLR: 0.00034156\n",
            "Train Epoch: 7 [46080/123872 (37%)]\tLoss: 0.437883\tLR: 0.00034144\n",
            "Train Epoch: 7 [46080/123872 (37%)]\tLoss: 0.437883\n",
            "Train Epoch: 7 [46336/123872 (37%)]\tLoss: 0.465808\tLR: 0.00034133\n",
            "Train Epoch: 7 [46592/123872 (38%)]\tLoss: 0.414299\tLR: 0.00034121\n",
            "Train Epoch: 7 [46848/123872 (38%)]\tLoss: 0.455640\tLR: 0.00034109\n",
            "Train Epoch: 7 [47104/123872 (38%)]\tLoss: 0.455793\tLR: 0.00034097\n",
            "Train Epoch: 7 [47360/123872 (38%)]\tLoss: 0.390152\tLR: 0.00034085\n",
            "Train Epoch: 7 [47616/123872 (38%)]\tLoss: 0.438315\tLR: 0.00034074\n",
            "Train Epoch: 7 [47872/123872 (39%)]\tLoss: 0.435958\tLR: 0.00034062\n",
            "Train Epoch: 7 [48128/123872 (39%)]\tLoss: 0.470732\tLR: 0.00034050\n",
            "Train Epoch: 7 [48384/123872 (39%)]\tLoss: 0.433307\tLR: 0.00034038\n",
            "Train Epoch: 7 [48640/123872 (39%)]\tLoss: 0.388532\tLR: 0.00034026\n",
            "Train Epoch: 7 [48640/123872 (39%)]\tLoss: 0.388532\n",
            "Train Epoch: 7 [48896/123872 (39%)]\tLoss: 0.495137\tLR: 0.00034015\n",
            "Train Epoch: 7 [49152/123872 (40%)]\tLoss: 0.478764\tLR: 0.00034003\n",
            "Train Epoch: 7 [49408/123872 (40%)]\tLoss: 0.426817\tLR: 0.00033991\n",
            "Train Epoch: 7 [49664/123872 (40%)]\tLoss: 0.377878\tLR: 0.00033979\n",
            "Train Epoch: 7 [49920/123872 (40%)]\tLoss: 0.402714\tLR: 0.00033967\n",
            "Train Epoch: 7 [50176/123872 (40%)]\tLoss: 0.482576\tLR: 0.00033956\n",
            "Train Epoch: 7 [50432/123872 (41%)]\tLoss: 0.408534\tLR: 0.00033944\n",
            "Train Epoch: 7 [50688/123872 (41%)]\tLoss: 0.388223\tLR: 0.00033932\n",
            "Train Epoch: 7 [50944/123872 (41%)]\tLoss: 0.479065\tLR: 0.00033920\n",
            "Train Epoch: 7 [51200/123872 (41%)]\tLoss: 0.415086\tLR: 0.00033908\n",
            "Train Epoch: 7 [51200/123872 (41%)]\tLoss: 0.415086\n",
            "Train Epoch: 7 [51456/123872 (42%)]\tLoss: 0.448531\tLR: 0.00033897\n",
            "Train Epoch: 7 [51712/123872 (42%)]\tLoss: 0.415855\tLR: 0.00033885\n",
            "Train Epoch: 7 [51968/123872 (42%)]\tLoss: 0.475052\tLR: 0.00033873\n",
            "Train Epoch: 7 [52224/123872 (42%)]\tLoss: 0.456245\tLR: 0.00033861\n",
            "Train Epoch: 7 [52480/123872 (42%)]\tLoss: 0.390204\tLR: 0.00033850\n",
            "Train Epoch: 7 [52736/123872 (43%)]\tLoss: 0.402319\tLR: 0.00033838\n",
            "Train Epoch: 7 [52992/123872 (43%)]\tLoss: 0.485808\tLR: 0.00033826\n",
            "Train Epoch: 7 [53248/123872 (43%)]\tLoss: 0.418650\tLR: 0.00033814\n",
            "Train Epoch: 7 [53504/123872 (43%)]\tLoss: 0.421770\tLR: 0.00033802\n",
            "Train Epoch: 7 [53760/123872 (43%)]\tLoss: 0.459150\tLR: 0.00033791\n",
            "Train Epoch: 7 [53760/123872 (43%)]\tLoss: 0.459150\n",
            "Train Epoch: 7 [54016/123872 (44%)]\tLoss: 0.438241\tLR: 0.00033779\n",
            "Train Epoch: 7 [54272/123872 (44%)]\tLoss: 0.422207\tLR: 0.00033767\n",
            "Train Epoch: 7 [54528/123872 (44%)]\tLoss: 0.398126\tLR: 0.00033755\n",
            "Train Epoch: 7 [54784/123872 (44%)]\tLoss: 0.449499\tLR: 0.00033743\n",
            "Train Epoch: 7 [55040/123872 (44%)]\tLoss: 0.425644\tLR: 0.00033732\n",
            "Train Epoch: 7 [55296/123872 (45%)]\tLoss: 0.410308\tLR: 0.00033720\n",
            "Train Epoch: 7 [55552/123872 (45%)]\tLoss: 0.436115\tLR: 0.00033708\n",
            "Train Epoch: 7 [55808/123872 (45%)]\tLoss: 0.363103\tLR: 0.00033696\n",
            "Train Epoch: 7 [56064/123872 (45%)]\tLoss: 0.479225\tLR: 0.00033684\n",
            "Train Epoch: 7 [56320/123872 (45%)]\tLoss: 0.442068\tLR: 0.00033673\n",
            "Train Epoch: 7 [56320/123872 (45%)]\tLoss: 0.442068\n",
            "Train Epoch: 7 [56576/123872 (46%)]\tLoss: 0.442122\tLR: 0.00033661\n",
            "Train Epoch: 7 [56832/123872 (46%)]\tLoss: 0.461535\tLR: 0.00033649\n",
            "Train Epoch: 7 [57088/123872 (46%)]\tLoss: 0.448351\tLR: 0.00033637\n",
            "Train Epoch: 7 [57344/123872 (46%)]\tLoss: 0.442483\tLR: 0.00033625\n",
            "Train Epoch: 7 [57600/123872 (46%)]\tLoss: 0.424098\tLR: 0.00033614\n",
            "Train Epoch: 7 [57856/123872 (47%)]\tLoss: 0.430300\tLR: 0.00033602\n",
            "Train Epoch: 7 [58112/123872 (47%)]\tLoss: 0.415478\tLR: 0.00033590\n",
            "Train Epoch: 7 [58368/123872 (47%)]\tLoss: 0.461033\tLR: 0.00033578\n",
            "Train Epoch: 7 [58624/123872 (47%)]\tLoss: 0.443423\tLR: 0.00033566\n",
            "Train Epoch: 7 [58880/123872 (48%)]\tLoss: 0.467302\tLR: 0.00033555\n",
            "Train Epoch: 7 [58880/123872 (48%)]\tLoss: 0.467302\n",
            "Train Epoch: 7 [59136/123872 (48%)]\tLoss: 0.429844\tLR: 0.00033543\n",
            "Train Epoch: 7 [59392/123872 (48%)]\tLoss: 0.441446\tLR: 0.00033531\n",
            "Train Epoch: 7 [59648/123872 (48%)]\tLoss: 0.397744\tLR: 0.00033519\n",
            "Train Epoch: 7 [59904/123872 (48%)]\tLoss: 0.429397\tLR: 0.00033507\n",
            "Train Epoch: 7 [60160/123872 (49%)]\tLoss: 0.413333\tLR: 0.00033496\n",
            "Train Epoch: 7 [60416/123872 (49%)]\tLoss: 0.462722\tLR: 0.00033484\n",
            "Train Epoch: 7 [60672/123872 (49%)]\tLoss: 0.441832\tLR: 0.00033472\n",
            "Train Epoch: 7 [60928/123872 (49%)]\tLoss: 0.433658\tLR: 0.00033460\n",
            "Train Epoch: 7 [61184/123872 (49%)]\tLoss: 0.467237\tLR: 0.00033448\n",
            "Train Epoch: 7 [61440/123872 (50%)]\tLoss: 0.410699\tLR: 0.00033437\n",
            "Train Epoch: 7 [61440/123872 (50%)]\tLoss: 0.410699\n",
            "Train Epoch: 7 [61696/123872 (50%)]\tLoss: 0.435510\tLR: 0.00033425\n",
            "Train Epoch: 7 [61952/123872 (50%)]\tLoss: 0.418275\tLR: 0.00033413\n",
            "Train Epoch: 7 [62208/123872 (50%)]\tLoss: 0.418271\tLR: 0.00033401\n",
            "Train Epoch: 7 [62464/123872 (50%)]\tLoss: 0.495066\tLR: 0.00033389\n",
            "Train Epoch: 7 [62720/123872 (51%)]\tLoss: 0.415957\tLR: 0.00033378\n",
            "Train Epoch: 7 [62976/123872 (51%)]\tLoss: 0.432671\tLR: 0.00033366\n",
            "Train Epoch: 7 [63232/123872 (51%)]\tLoss: 0.466437\tLR: 0.00033354\n",
            "Train Epoch: 7 [63488/123872 (51%)]\tLoss: 0.418444\tLR: 0.00033342\n",
            "Train Epoch: 7 [63744/123872 (51%)]\tLoss: 0.366315\tLR: 0.00033330\n",
            "Train Epoch: 7 [64000/123872 (52%)]\tLoss: 0.430294\tLR: 0.00033319\n",
            "Train Epoch: 7 [64000/123872 (52%)]\tLoss: 0.430294\n",
            "Train Epoch: 7 [64256/123872 (52%)]\tLoss: 0.458972\tLR: 0.00033307\n",
            "Train Epoch: 7 [64512/123872 (52%)]\tLoss: 0.399094\tLR: 0.00033295\n",
            "Train Epoch: 7 [64768/123872 (52%)]\tLoss: 0.422075\tLR: 0.00033283\n",
            "Train Epoch: 7 [65024/123872 (52%)]\tLoss: 0.507335\tLR: 0.00033271\n",
            "Train Epoch: 7 [65280/123872 (53%)]\tLoss: 0.412369\tLR: 0.00033260\n",
            "Train Epoch: 7 [65536/123872 (53%)]\tLoss: 0.423326\tLR: 0.00033248\n",
            "Train Epoch: 7 [65792/123872 (53%)]\tLoss: 0.424044\tLR: 0.00033236\n",
            "Train Epoch: 7 [66048/123872 (53%)]\tLoss: 0.454708\tLR: 0.00033224\n",
            "Train Epoch: 7 [66304/123872 (54%)]\tLoss: 0.469294\tLR: 0.00033212\n",
            "Train Epoch: 7 [66560/123872 (54%)]\tLoss: 0.464285\tLR: 0.00033201\n",
            "Train Epoch: 7 [66560/123872 (54%)]\tLoss: 0.464285\n",
            "Train Epoch: 7 [66816/123872 (54%)]\tLoss: 0.423701\tLR: 0.00033189\n",
            "Train Epoch: 7 [67072/123872 (54%)]\tLoss: 0.409858\tLR: 0.00033177\n",
            "Train Epoch: 7 [67328/123872 (54%)]\tLoss: 0.440090\tLR: 0.00033165\n",
            "Train Epoch: 7 [67584/123872 (55%)]\tLoss: 0.350843\tLR: 0.00033153\n",
            "Train Epoch: 7 [67840/123872 (55%)]\tLoss: 0.401854\tLR: 0.00033142\n",
            "Train Epoch: 7 [68096/123872 (55%)]\tLoss: 0.445070\tLR: 0.00033130\n",
            "Train Epoch: 7 [68352/123872 (55%)]\tLoss: 0.380937\tLR: 0.00033118\n",
            "Train Epoch: 7 [68608/123872 (55%)]\tLoss: 0.453918\tLR: 0.00033106\n",
            "Train Epoch: 7 [68864/123872 (56%)]\tLoss: 0.431560\tLR: 0.00033094\n",
            "Train Epoch: 7 [69120/123872 (56%)]\tLoss: 0.411986\tLR: 0.00033083\n",
            "Train Epoch: 7 [69120/123872 (56%)]\tLoss: 0.411986\n",
            "Train Epoch: 7 [69376/123872 (56%)]\tLoss: 0.392106\tLR: 0.00033071\n",
            "Train Epoch: 7 [69632/123872 (56%)]\tLoss: 0.368434\tLR: 0.00033059\n",
            "Train Epoch: 7 [69888/123872 (56%)]\tLoss: 0.382722\tLR: 0.00033047\n",
            "Train Epoch: 7 [70144/123872 (57%)]\tLoss: 0.361719\tLR: 0.00033035\n",
            "Train Epoch: 7 [70400/123872 (57%)]\tLoss: 0.366672\tLR: 0.00033024\n",
            "Train Epoch: 7 [70656/123872 (57%)]\tLoss: 0.416290\tLR: 0.00033012\n",
            "Train Epoch: 7 [70912/123872 (57%)]\tLoss: 0.433113\tLR: 0.00033000\n",
            "Train Epoch: 7 [71168/123872 (57%)]\tLoss: 0.381203\tLR: 0.00032988\n",
            "Train Epoch: 7 [71424/123872 (58%)]\tLoss: 0.423607\tLR: 0.00032976\n",
            "Train Epoch: 7 [71680/123872 (58%)]\tLoss: 0.465501\tLR: 0.00032965\n",
            "Train Epoch: 7 [71680/123872 (58%)]\tLoss: 0.465501\n",
            "Train Epoch: 7 [71936/123872 (58%)]\tLoss: 0.401655\tLR: 0.00032953\n",
            "Train Epoch: 7 [72192/123872 (58%)]\tLoss: 0.441745\tLR: 0.00032941\n",
            "Train Epoch: 7 [72448/123872 (58%)]\tLoss: 0.420560\tLR: 0.00032929\n",
            "Train Epoch: 7 [72704/123872 (59%)]\tLoss: 0.444670\tLR: 0.00032917\n",
            "Train Epoch: 7 [72960/123872 (59%)]\tLoss: 0.413647\tLR: 0.00032906\n",
            "Train Epoch: 7 [73216/123872 (59%)]\tLoss: 0.438155\tLR: 0.00032894\n",
            "Train Epoch: 7 [73472/123872 (59%)]\tLoss: 0.439403\tLR: 0.00032882\n",
            "Train Epoch: 7 [73728/123872 (60%)]\tLoss: 0.393359\tLR: 0.00032870\n",
            "Train Epoch: 7 [73984/123872 (60%)]\tLoss: 0.406599\tLR: 0.00032858\n",
            "Train Epoch: 7 [74240/123872 (60%)]\tLoss: 0.458089\tLR: 0.00032847\n",
            "Train Epoch: 7 [74240/123872 (60%)]\tLoss: 0.458089\n",
            "Train Epoch: 7 [74496/123872 (60%)]\tLoss: 0.485858\tLR: 0.00032835\n",
            "Train Epoch: 7 [74752/123872 (60%)]\tLoss: 0.454770\tLR: 0.00032823\n",
            "Train Epoch: 7 [75008/123872 (61%)]\tLoss: 0.460849\tLR: 0.00032811\n",
            "Train Epoch: 7 [75264/123872 (61%)]\tLoss: 0.406340\tLR: 0.00032799\n",
            "Train Epoch: 7 [75520/123872 (61%)]\tLoss: 0.386774\tLR: 0.00032788\n",
            "Train Epoch: 7 [75776/123872 (61%)]\tLoss: 0.382445\tLR: 0.00032776\n",
            "Train Epoch: 7 [76032/123872 (61%)]\tLoss: 0.447082\tLR: 0.00032764\n",
            "Train Epoch: 7 [76288/123872 (62%)]\tLoss: 0.445929\tLR: 0.00032752\n",
            "Train Epoch: 7 [76544/123872 (62%)]\tLoss: 0.405719\tLR: 0.00032740\n",
            "Train Epoch: 7 [76800/123872 (62%)]\tLoss: 0.392563\tLR: 0.00032729\n",
            "Train Epoch: 7 [76800/123872 (62%)]\tLoss: 0.392563\n",
            "Train Epoch: 7 [77056/123872 (62%)]\tLoss: 0.423605\tLR: 0.00032717\n",
            "Train Epoch: 7 [77312/123872 (62%)]\tLoss: 0.394408\tLR: 0.00032705\n",
            "Train Epoch: 7 [77568/123872 (63%)]\tLoss: 0.445729\tLR: 0.00032693\n",
            "Train Epoch: 7 [77824/123872 (63%)]\tLoss: 0.381740\tLR: 0.00032681\n",
            "Train Epoch: 7 [78080/123872 (63%)]\tLoss: 0.401012\tLR: 0.00032670\n",
            "Train Epoch: 7 [78336/123872 (63%)]\tLoss: 0.471834\tLR: 0.00032658\n",
            "Train Epoch: 7 [78592/123872 (63%)]\tLoss: 0.415043\tLR: 0.00032646\n",
            "Train Epoch: 7 [78848/123872 (64%)]\tLoss: 0.385785\tLR: 0.00032634\n",
            "Train Epoch: 7 [79104/123872 (64%)]\tLoss: 0.444983\tLR: 0.00032622\n",
            "Train Epoch: 7 [79360/123872 (64%)]\tLoss: 0.384420\tLR: 0.00032611\n",
            "Train Epoch: 7 [79360/123872 (64%)]\tLoss: 0.384420\n",
            "Train Epoch: 7 [79616/123872 (64%)]\tLoss: 0.397690\tLR: 0.00032599\n",
            "Train Epoch: 7 [79872/123872 (64%)]\tLoss: 0.446236\tLR: 0.00032587\n",
            "Train Epoch: 7 [80128/123872 (65%)]\tLoss: 0.433617\tLR: 0.00032575\n",
            "Train Epoch: 7 [80384/123872 (65%)]\tLoss: 0.370766\tLR: 0.00032563\n",
            "Train Epoch: 7 [80640/123872 (65%)]\tLoss: 0.396041\tLR: 0.00032552\n",
            "Train Epoch: 7 [80896/123872 (65%)]\tLoss: 0.457588\tLR: 0.00032540\n",
            "Train Epoch: 7 [81152/123872 (65%)]\tLoss: 0.408170\tLR: 0.00032528\n",
            "Train Epoch: 7 [81408/123872 (66%)]\tLoss: 0.447077\tLR: 0.00032516\n",
            "Train Epoch: 7 [81664/123872 (66%)]\tLoss: 0.404003\tLR: 0.00032504\n",
            "Train Epoch: 7 [81920/123872 (66%)]\tLoss: 0.336978\tLR: 0.00032493\n",
            "Train Epoch: 7 [81920/123872 (66%)]\tLoss: 0.336978\n",
            "Train Epoch: 7 [82176/123872 (66%)]\tLoss: 0.416734\tLR: 0.00032481\n",
            "Train Epoch: 7 [82432/123872 (67%)]\tLoss: 0.425565\tLR: 0.00032469\n",
            "Train Epoch: 7 [82688/123872 (67%)]\tLoss: 0.399462\tLR: 0.00032457\n",
            "Train Epoch: 7 [82944/123872 (67%)]\tLoss: 0.406980\tLR: 0.00032445\n",
            "Train Epoch: 7 [83200/123872 (67%)]\tLoss: 0.376636\tLR: 0.00032434\n",
            "Train Epoch: 7 [83456/123872 (67%)]\tLoss: 0.419018\tLR: 0.00032422\n",
            "Train Epoch: 7 [83712/123872 (68%)]\tLoss: 0.410689\tLR: 0.00032410\n",
            "Train Epoch: 7 [83968/123872 (68%)]\tLoss: 0.409752\tLR: 0.00032398\n",
            "Train Epoch: 7 [84224/123872 (68%)]\tLoss: 0.418179\tLR: 0.00032386\n",
            "Train Epoch: 7 [84480/123872 (68%)]\tLoss: 0.417961\tLR: 0.00032375\n",
            "Train Epoch: 7 [84480/123872 (68%)]\tLoss: 0.417961\n",
            "Train Epoch: 7 [84736/123872 (68%)]\tLoss: 0.386296\tLR: 0.00032363\n",
            "Train Epoch: 7 [84992/123872 (69%)]\tLoss: 0.374863\tLR: 0.00032351\n",
            "Train Epoch: 7 [85248/123872 (69%)]\tLoss: 0.430866\tLR: 0.00032339\n",
            "Train Epoch: 7 [85504/123872 (69%)]\tLoss: 0.399577\tLR: 0.00032327\n",
            "Train Epoch: 7 [85760/123872 (69%)]\tLoss: 0.388042\tLR: 0.00032316\n",
            "Train Epoch: 7 [86016/123872 (69%)]\tLoss: 0.460892\tLR: 0.00032304\n",
            "Train Epoch: 7 [86272/123872 (70%)]\tLoss: 0.459360\tLR: 0.00032292\n",
            "Train Epoch: 7 [86528/123872 (70%)]\tLoss: 0.393336\tLR: 0.00032280\n",
            "Train Epoch: 7 [86784/123872 (70%)]\tLoss: 0.420828\tLR: 0.00032268\n",
            "Train Epoch: 7 [87040/123872 (70%)]\tLoss: 0.432318\tLR: 0.00032257\n",
            "Train Epoch: 7 [87040/123872 (70%)]\tLoss: 0.432318\n",
            "Train Epoch: 7 [87296/123872 (70%)]\tLoss: 0.380189\tLR: 0.00032245\n",
            "Train Epoch: 7 [87552/123872 (71%)]\tLoss: 0.370949\tLR: 0.00032233\n",
            "Train Epoch: 7 [87808/123872 (71%)]\tLoss: 0.391296\tLR: 0.00032221\n",
            "Train Epoch: 7 [88064/123872 (71%)]\tLoss: 0.430327\tLR: 0.00032209\n",
            "Train Epoch: 7 [88320/123872 (71%)]\tLoss: 0.457521\tLR: 0.00032198\n",
            "Train Epoch: 7 [88576/123872 (71%)]\tLoss: 0.461832\tLR: 0.00032186\n",
            "Train Epoch: 7 [88832/123872 (72%)]\tLoss: 0.490883\tLR: 0.00032174\n",
            "Train Epoch: 7 [89088/123872 (72%)]\tLoss: 0.424771\tLR: 0.00032162\n",
            "Train Epoch: 7 [89344/123872 (72%)]\tLoss: 0.458987\tLR: 0.00032150\n",
            "Train Epoch: 7 [89600/123872 (72%)]\tLoss: 0.472023\tLR: 0.00032139\n",
            "Train Epoch: 7 [89600/123872 (72%)]\tLoss: 0.472023\n",
            "Train Epoch: 7 [89856/123872 (73%)]\tLoss: 0.445226\tLR: 0.00032127\n",
            "Train Epoch: 7 [90112/123872 (73%)]\tLoss: 0.422483\tLR: 0.00032115\n",
            "Train Epoch: 7 [90368/123872 (73%)]\tLoss: 0.414487\tLR: 0.00032103\n",
            "Train Epoch: 7 [90624/123872 (73%)]\tLoss: 0.418509\tLR: 0.00032092\n",
            "Train Epoch: 7 [90880/123872 (73%)]\tLoss: 0.396651\tLR: 0.00032080\n",
            "Train Epoch: 7 [91136/123872 (74%)]\tLoss: 0.433691\tLR: 0.00032068\n",
            "Train Epoch: 7 [91392/123872 (74%)]\tLoss: 0.439231\tLR: 0.00032056\n",
            "Train Epoch: 7 [91648/123872 (74%)]\tLoss: 0.433447\tLR: 0.00032044\n",
            "Train Epoch: 7 [91904/123872 (74%)]\tLoss: 0.433008\tLR: 0.00032033\n",
            "Train Epoch: 7 [92160/123872 (74%)]\tLoss: 0.414118\tLR: 0.00032021\n",
            "Train Epoch: 7 [92160/123872 (74%)]\tLoss: 0.414118\n",
            "Train Epoch: 7 [92416/123872 (75%)]\tLoss: 0.462512\tLR: 0.00032009\n",
            "Train Epoch: 7 [92672/123872 (75%)]\tLoss: 0.423231\tLR: 0.00031997\n",
            "Train Epoch: 7 [92928/123872 (75%)]\tLoss: 0.421430\tLR: 0.00031985\n",
            "Train Epoch: 7 [93184/123872 (75%)]\tLoss: 0.438186\tLR: 0.00031974\n",
            "Train Epoch: 7 [93440/123872 (75%)]\tLoss: 0.378664\tLR: 0.00031962\n",
            "Train Epoch: 7 [93696/123872 (76%)]\tLoss: 0.475095\tLR: 0.00031950\n",
            "Train Epoch: 7 [93952/123872 (76%)]\tLoss: 0.423322\tLR: 0.00031938\n",
            "Train Epoch: 7 [94208/123872 (76%)]\tLoss: 0.384783\tLR: 0.00031926\n",
            "Train Epoch: 7 [94464/123872 (76%)]\tLoss: 0.429431\tLR: 0.00031915\n",
            "Train Epoch: 7 [94720/123872 (76%)]\tLoss: 0.445363\tLR: 0.00031903\n",
            "Train Epoch: 7 [94720/123872 (76%)]\tLoss: 0.445363\n",
            "Train Epoch: 7 [94976/123872 (77%)]\tLoss: 0.402406\tLR: 0.00031891\n",
            "Train Epoch: 7 [95232/123872 (77%)]\tLoss: 0.419287\tLR: 0.00031879\n",
            "Train Epoch: 7 [95488/123872 (77%)]\tLoss: 0.383353\tLR: 0.00031867\n",
            "Train Epoch: 7 [95744/123872 (77%)]\tLoss: 0.460010\tLR: 0.00031856\n",
            "Train Epoch: 7 [96000/123872 (77%)]\tLoss: 0.420349\tLR: 0.00031844\n",
            "Train Epoch: 7 [96256/123872 (78%)]\tLoss: 0.445645\tLR: 0.00031832\n",
            "Train Epoch: 7 [96512/123872 (78%)]\tLoss: 0.442268\tLR: 0.00031820\n",
            "Train Epoch: 7 [96768/123872 (78%)]\tLoss: 0.420954\tLR: 0.00031809\n",
            "Train Epoch: 7 [97024/123872 (78%)]\tLoss: 0.425342\tLR: 0.00031797\n",
            "Train Epoch: 7 [97280/123872 (79%)]\tLoss: 0.406603\tLR: 0.00031785\n",
            "Train Epoch: 7 [97280/123872 (79%)]\tLoss: 0.406603\n",
            "Train Epoch: 7 [97536/123872 (79%)]\tLoss: 0.477347\tLR: 0.00031773\n",
            "Train Epoch: 7 [97792/123872 (79%)]\tLoss: 0.369850\tLR: 0.00031761\n",
            "Train Epoch: 7 [98048/123872 (79%)]\tLoss: 0.490632\tLR: 0.00031750\n",
            "Train Epoch: 7 [98304/123872 (79%)]\tLoss: 0.408707\tLR: 0.00031738\n",
            "Train Epoch: 7 [98560/123872 (80%)]\tLoss: 0.386565\tLR: 0.00031726\n",
            "Train Epoch: 7 [98816/123872 (80%)]\tLoss: 0.391125\tLR: 0.00031714\n",
            "Train Epoch: 7 [99072/123872 (80%)]\tLoss: 0.371377\tLR: 0.00031702\n",
            "Train Epoch: 7 [99328/123872 (80%)]\tLoss: 0.345441\tLR: 0.00031691\n",
            "Train Epoch: 7 [99584/123872 (80%)]\tLoss: 0.413604\tLR: 0.00031679\n",
            "Train Epoch: 7 [99840/123872 (81%)]\tLoss: 0.332775\tLR: 0.00031667\n",
            "Train Epoch: 7 [99840/123872 (81%)]\tLoss: 0.332775\n",
            "Train Epoch: 7 [100096/123872 (81%)]\tLoss: 0.434395\tLR: 0.00031655\n",
            "Train Epoch: 7 [100352/123872 (81%)]\tLoss: 0.460467\tLR: 0.00031643\n",
            "Train Epoch: 7 [100608/123872 (81%)]\tLoss: 0.430047\tLR: 0.00031632\n",
            "Train Epoch: 7 [100864/123872 (81%)]\tLoss: 0.377986\tLR: 0.00031620\n",
            "Train Epoch: 7 [101120/123872 (82%)]\tLoss: 0.374193\tLR: 0.00031608\n",
            "Train Epoch: 7 [101376/123872 (82%)]\tLoss: 0.400103\tLR: 0.00031596\n",
            "Train Epoch: 7 [101632/123872 (82%)]\tLoss: 0.439285\tLR: 0.00031585\n",
            "Train Epoch: 7 [101888/123872 (82%)]\tLoss: 0.403566\tLR: 0.00031573\n",
            "Train Epoch: 7 [102144/123872 (82%)]\tLoss: 0.469555\tLR: 0.00031561\n",
            "Train Epoch: 7 [102400/123872 (83%)]\tLoss: 0.398442\tLR: 0.00031549\n",
            "Train Epoch: 7 [102400/123872 (83%)]\tLoss: 0.398442\n",
            "Train Epoch: 7 [102656/123872 (83%)]\tLoss: 0.462226\tLR: 0.00031537\n",
            "Train Epoch: 7 [102912/123872 (83%)]\tLoss: 0.357448\tLR: 0.00031526\n",
            "Train Epoch: 7 [103168/123872 (83%)]\tLoss: 0.411454\tLR: 0.00031514\n",
            "Train Epoch: 7 [103424/123872 (83%)]\tLoss: 0.406850\tLR: 0.00031502\n",
            "Train Epoch: 7 [103680/123872 (84%)]\tLoss: 0.415203\tLR: 0.00031490\n",
            "Train Epoch: 7 [103936/123872 (84%)]\tLoss: 0.439391\tLR: 0.00031479\n",
            "Train Epoch: 7 [104192/123872 (84%)]\tLoss: 0.391585\tLR: 0.00031467\n",
            "Train Epoch: 7 [104448/123872 (84%)]\tLoss: 0.491796\tLR: 0.00031455\n",
            "Train Epoch: 7 [104704/123872 (85%)]\tLoss: 0.343303\tLR: 0.00031443\n",
            "Train Epoch: 7 [104960/123872 (85%)]\tLoss: 0.414088\tLR: 0.00031431\n",
            "Train Epoch: 7 [104960/123872 (85%)]\tLoss: 0.414088\n",
            "Train Epoch: 7 [105216/123872 (85%)]\tLoss: 0.445908\tLR: 0.00031420\n",
            "Train Epoch: 7 [105472/123872 (85%)]\tLoss: 0.477435\tLR: 0.00031408\n",
            "Train Epoch: 7 [105728/123872 (85%)]\tLoss: 0.402990\tLR: 0.00031396\n",
            "Train Epoch: 7 [105984/123872 (86%)]\tLoss: 0.487479\tLR: 0.00031384\n",
            "Train Epoch: 7 [106240/123872 (86%)]\tLoss: 0.413846\tLR: 0.00031372\n",
            "Train Epoch: 7 [106496/123872 (86%)]\tLoss: 0.407695\tLR: 0.00031361\n",
            "Train Epoch: 7 [106752/123872 (86%)]\tLoss: 0.454913\tLR: 0.00031349\n",
            "Train Epoch: 7 [107008/123872 (86%)]\tLoss: 0.423822\tLR: 0.00031337\n",
            "Train Epoch: 7 [107264/123872 (87%)]\tLoss: 0.389058\tLR: 0.00031325\n",
            "Train Epoch: 7 [107520/123872 (87%)]\tLoss: 0.523501\tLR: 0.00031314\n",
            "Train Epoch: 7 [107520/123872 (87%)]\tLoss: 0.523501\n",
            "Train Epoch: 7 [107776/123872 (87%)]\tLoss: 0.401580\tLR: 0.00031302\n",
            "Train Epoch: 7 [108032/123872 (87%)]\tLoss: 0.329425\tLR: 0.00031290\n",
            "Train Epoch: 7 [108288/123872 (87%)]\tLoss: 0.354799\tLR: 0.00031278\n",
            "Train Epoch: 7 [108544/123872 (88%)]\tLoss: 0.429264\tLR: 0.00031266\n",
            "Train Epoch: 7 [108800/123872 (88%)]\tLoss: 0.377188\tLR: 0.00031255\n",
            "Train Epoch: 7 [109056/123872 (88%)]\tLoss: 0.420794\tLR: 0.00031243\n",
            "Train Epoch: 7 [109312/123872 (88%)]\tLoss: 0.516980\tLR: 0.00031231\n",
            "Train Epoch: 7 [109568/123872 (88%)]\tLoss: 0.456481\tLR: 0.00031219\n",
            "Train Epoch: 7 [109824/123872 (89%)]\tLoss: 0.419513\tLR: 0.00031208\n",
            "Train Epoch: 7 [110080/123872 (89%)]\tLoss: 0.374913\tLR: 0.00031196\n",
            "Train Epoch: 7 [110080/123872 (89%)]\tLoss: 0.374913\n",
            "Train Epoch: 7 [110336/123872 (89%)]\tLoss: 0.401390\tLR: 0.00031184\n",
            "Train Epoch: 7 [110592/123872 (89%)]\tLoss: 0.363712\tLR: 0.00031172\n",
            "Train Epoch: 7 [110848/123872 (89%)]\tLoss: 0.390830\tLR: 0.00031161\n",
            "Train Epoch: 7 [111104/123872 (90%)]\tLoss: 0.406125\tLR: 0.00031149\n",
            "Train Epoch: 7 [111360/123872 (90%)]\tLoss: 0.505476\tLR: 0.00031137\n",
            "Train Epoch: 7 [111616/123872 (90%)]\tLoss: 0.367753\tLR: 0.00031125\n",
            "Train Epoch: 7 [111872/123872 (90%)]\tLoss: 0.446441\tLR: 0.00031113\n",
            "Train Epoch: 7 [112128/123872 (90%)]\tLoss: 0.440529\tLR: 0.00031102\n",
            "Train Epoch: 7 [112384/123872 (91%)]\tLoss: 0.456660\tLR: 0.00031090\n",
            "Train Epoch: 7 [112640/123872 (91%)]\tLoss: 0.465498\tLR: 0.00031078\n",
            "Train Epoch: 7 [112640/123872 (91%)]\tLoss: 0.465498\n",
            "Train Epoch: 7 [112896/123872 (91%)]\tLoss: 0.398228\tLR: 0.00031066\n",
            "Train Epoch: 7 [113152/123872 (91%)]\tLoss: 0.410750\tLR: 0.00031055\n",
            "Train Epoch: 7 [113408/123872 (92%)]\tLoss: 0.482568\tLR: 0.00031043\n",
            "Train Epoch: 7 [113664/123872 (92%)]\tLoss: 0.436970\tLR: 0.00031031\n",
            "Train Epoch: 7 [113920/123872 (92%)]\tLoss: 0.402149\tLR: 0.00031019\n",
            "Train Epoch: 7 [114176/123872 (92%)]\tLoss: 0.419867\tLR: 0.00031008\n",
            "Train Epoch: 7 [114432/123872 (92%)]\tLoss: 0.451661\tLR: 0.00030996\n",
            "Train Epoch: 7 [114688/123872 (93%)]\tLoss: 0.352729\tLR: 0.00030984\n",
            "Train Epoch: 7 [114944/123872 (93%)]\tLoss: 0.404935\tLR: 0.00030972\n",
            "Train Epoch: 7 [115200/123872 (93%)]\tLoss: 0.447424\tLR: 0.00030960\n",
            "Train Epoch: 7 [115200/123872 (93%)]\tLoss: 0.447424\n",
            "Train Epoch: 7 [115456/123872 (93%)]\tLoss: 0.443887\tLR: 0.00030949\n",
            "Train Epoch: 7 [115712/123872 (93%)]\tLoss: 0.418994\tLR: 0.00030937\n",
            "Train Epoch: 7 [115968/123872 (94%)]\tLoss: 0.388425\tLR: 0.00030925\n",
            "Train Epoch: 7 [116224/123872 (94%)]\tLoss: 0.419565\tLR: 0.00030913\n",
            "Train Epoch: 7 [116480/123872 (94%)]\tLoss: 0.448919\tLR: 0.00030902\n",
            "Train Epoch: 7 [116736/123872 (94%)]\tLoss: 0.422246\tLR: 0.00030890\n",
            "Train Epoch: 7 [116992/123872 (94%)]\tLoss: 0.357017\tLR: 0.00030878\n",
            "Train Epoch: 7 [117248/123872 (95%)]\tLoss: 0.401228\tLR: 0.00030866\n",
            "Train Epoch: 7 [117504/123872 (95%)]\tLoss: 0.454877\tLR: 0.00030855\n",
            "Train Epoch: 7 [117760/123872 (95%)]\tLoss: 0.471823\tLR: 0.00030843\n",
            "Train Epoch: 7 [117760/123872 (95%)]\tLoss: 0.471823\n",
            "Train Epoch: 7 [118016/123872 (95%)]\tLoss: 0.396989\tLR: 0.00030831\n",
            "Train Epoch: 7 [118272/123872 (95%)]\tLoss: 0.380485\tLR: 0.00030819\n",
            "Train Epoch: 7 [118528/123872 (96%)]\tLoss: 0.422827\tLR: 0.00030807\n",
            "Train Epoch: 7 [118784/123872 (96%)]\tLoss: 0.459411\tLR: 0.00030796\n",
            "Train Epoch: 7 [119040/123872 (96%)]\tLoss: 0.413640\tLR: 0.00030784\n",
            "Train Epoch: 7 [119296/123872 (96%)]\tLoss: 0.444712\tLR: 0.00030772\n",
            "Train Epoch: 7 [119552/123872 (96%)]\tLoss: 0.420747\tLR: 0.00030760\n",
            "Train Epoch: 7 [119808/123872 (97%)]\tLoss: 0.479705\tLR: 0.00030749\n",
            "Train Epoch: 7 [120064/123872 (97%)]\tLoss: 0.446980\tLR: 0.00030737\n",
            "Train Epoch: 7 [120320/123872 (97%)]\tLoss: 0.434676\tLR: 0.00030725\n",
            "Train Epoch: 7 [120320/123872 (97%)]\tLoss: 0.434676\n",
            "Train Epoch: 7 [120576/123872 (97%)]\tLoss: 0.389304\tLR: 0.00030713\n",
            "Train Epoch: 7 [120832/123872 (98%)]\tLoss: 0.482687\tLR: 0.00030702\n",
            "Train Epoch: 7 [121088/123872 (98%)]\tLoss: 0.405592\tLR: 0.00030690\n",
            "Train Epoch: 7 [121344/123872 (98%)]\tLoss: 0.412891\tLR: 0.00030678\n",
            "Train Epoch: 7 [121600/123872 (98%)]\tLoss: 0.438609\tLR: 0.00030666\n",
            "Train Epoch: 7 [121856/123872 (98%)]\tLoss: 0.416987\tLR: 0.00030655\n",
            "Train Epoch: 7 [122112/123872 (99%)]\tLoss: 0.388201\tLR: 0.00030643\n",
            "Train Epoch: 7 [122368/123872 (99%)]\tLoss: 0.464683\tLR: 0.00030631\n",
            "Train Epoch: 7 [122624/123872 (99%)]\tLoss: 0.405489\tLR: 0.00030619\n",
            "Train Epoch: 7 [122880/123872 (99%)]\tLoss: 0.468841\tLR: 0.00030608\n",
            "Train Epoch: 7 [122880/123872 (99%)]\tLoss: 0.468841\n",
            "Train Epoch: 7 [123136/123872 (99%)]\tLoss: 0.396813\tLR: 0.00030596\n",
            "Train Epoch: 7 [123392/123872 (100%)]\tLoss: 0.414487\tLR: 0.00030584\n",
            "Train Epoch: 7 [108192/123872 (100%)]\tLoss: 0.385917\tLR: 0.00030572\n",
            "\n",
            "Test set: Average loss: 0.0017, Accuracy: 24716/30970 (79.81%)\n",
            "\n",
            "Train Epoch: 8 [0/123872 (0%)]\tLoss: 0.433459\tLR: 0.00030561\n",
            "Train Epoch: 8 [0/123872 (0%)]\tLoss: 0.433459\n",
            "Train Epoch: 8 [256/123872 (0%)]\tLoss: 0.467408\tLR: 0.00030549\n",
            "Train Epoch: 8 [512/123872 (0%)]\tLoss: 0.486640\tLR: 0.00030537\n",
            "Train Epoch: 8 [768/123872 (1%)]\tLoss: 0.414864\tLR: 0.00030525\n",
            "Train Epoch: 8 [1024/123872 (1%)]\tLoss: 0.455802\tLR: 0.00030514\n",
            "Train Epoch: 8 [1280/123872 (1%)]\tLoss: 0.443596\tLR: 0.00030502\n",
            "Train Epoch: 8 [1536/123872 (1%)]\tLoss: 0.438103\tLR: 0.00030490\n",
            "Train Epoch: 8 [1792/123872 (1%)]\tLoss: 0.390811\tLR: 0.00030478\n",
            "Train Epoch: 8 [2048/123872 (2%)]\tLoss: 0.420842\tLR: 0.00030467\n",
            "Train Epoch: 8 [2304/123872 (2%)]\tLoss: 0.442446\tLR: 0.00030455\n",
            "Train Epoch: 8 [2560/123872 (2%)]\tLoss: 0.414777\tLR: 0.00030443\n",
            "Train Epoch: 8 [2560/123872 (2%)]\tLoss: 0.414777\n",
            "Train Epoch: 8 [2816/123872 (2%)]\tLoss: 0.385831\tLR: 0.00030431\n",
            "Train Epoch: 8 [3072/123872 (2%)]\tLoss: 0.414440\tLR: 0.00030420\n",
            "Train Epoch: 8 [3328/123872 (3%)]\tLoss: 0.383721\tLR: 0.00030408\n",
            "Train Epoch: 8 [3584/123872 (3%)]\tLoss: 0.416503\tLR: 0.00030396\n",
            "Train Epoch: 8 [3840/123872 (3%)]\tLoss: 0.376013\tLR: 0.00030384\n",
            "Train Epoch: 8 [4096/123872 (3%)]\tLoss: 0.373962\tLR: 0.00030373\n",
            "Train Epoch: 8 [4352/123872 (4%)]\tLoss: 0.400131\tLR: 0.00030361\n",
            "Train Epoch: 8 [4608/123872 (4%)]\tLoss: 0.432956\tLR: 0.00030349\n",
            "Train Epoch: 8 [4864/123872 (4%)]\tLoss: 0.430521\tLR: 0.00030337\n",
            "Train Epoch: 8 [5120/123872 (4%)]\tLoss: 0.499008\tLR: 0.00030326\n",
            "Train Epoch: 8 [5120/123872 (4%)]\tLoss: 0.499008\n",
            "Train Epoch: 8 [5376/123872 (4%)]\tLoss: 0.390398\tLR: 0.00030314\n",
            "Train Epoch: 8 [5632/123872 (5%)]\tLoss: 0.377922\tLR: 0.00030302\n",
            "Train Epoch: 8 [5888/123872 (5%)]\tLoss: 0.457249\tLR: 0.00030290\n",
            "Train Epoch: 8 [6144/123872 (5%)]\tLoss: 0.395527\tLR: 0.00030279\n",
            "Train Epoch: 8 [6400/123872 (5%)]\tLoss: 0.458724\tLR: 0.00030267\n",
            "Train Epoch: 8 [6656/123872 (5%)]\tLoss: 0.393154\tLR: 0.00030255\n",
            "Train Epoch: 8 [6912/123872 (6%)]\tLoss: 0.408916\tLR: 0.00030243\n",
            "Train Epoch: 8 [7168/123872 (6%)]\tLoss: 0.465746\tLR: 0.00030232\n",
            "Train Epoch: 8 [7424/123872 (6%)]\tLoss: 0.389026\tLR: 0.00030220\n",
            "Train Epoch: 8 [7680/123872 (6%)]\tLoss: 0.455626\tLR: 0.00030208\n",
            "Train Epoch: 8 [7680/123872 (6%)]\tLoss: 0.455626\n",
            "Train Epoch: 8 [7936/123872 (6%)]\tLoss: 0.386167\tLR: 0.00030197\n",
            "Train Epoch: 8 [8192/123872 (7%)]\tLoss: 0.437692\tLR: 0.00030185\n",
            "Train Epoch: 8 [8448/123872 (7%)]\tLoss: 0.409512\tLR: 0.00030173\n",
            "Train Epoch: 8 [8704/123872 (7%)]\tLoss: 0.398377\tLR: 0.00030161\n",
            "Train Epoch: 8 [8960/123872 (7%)]\tLoss: 0.374844\tLR: 0.00030150\n",
            "Train Epoch: 8 [9216/123872 (7%)]\tLoss: 0.417353\tLR: 0.00030138\n",
            "Train Epoch: 8 [9472/123872 (8%)]\tLoss: 0.389063\tLR: 0.00030126\n",
            "Train Epoch: 8 [9728/123872 (8%)]\tLoss: 0.424160\tLR: 0.00030114\n",
            "Train Epoch: 8 [9984/123872 (8%)]\tLoss: 0.411438\tLR: 0.00030103\n",
            "Train Epoch: 8 [10240/123872 (8%)]\tLoss: 0.426749\tLR: 0.00030091\n",
            "Train Epoch: 8 [10240/123872 (8%)]\tLoss: 0.426749\n",
            "Train Epoch: 8 [10496/123872 (8%)]\tLoss: 0.442798\tLR: 0.00030079\n",
            "Train Epoch: 8 [10752/123872 (9%)]\tLoss: 0.406712\tLR: 0.00030067\n",
            "Train Epoch: 8 [11008/123872 (9%)]\tLoss: 0.372730\tLR: 0.00030056\n",
            "Train Epoch: 8 [11264/123872 (9%)]\tLoss: 0.393859\tLR: 0.00030044\n",
            "Train Epoch: 8 [11520/123872 (9%)]\tLoss: 0.389214\tLR: 0.00030032\n",
            "Train Epoch: 8 [11776/123872 (10%)]\tLoss: 0.420718\tLR: 0.00030021\n",
            "Train Epoch: 8 [12032/123872 (10%)]\tLoss: 0.416396\tLR: 0.00030009\n",
            "Train Epoch: 8 [12288/123872 (10%)]\tLoss: 0.386975\tLR: 0.00029997\n",
            "Train Epoch: 8 [12544/123872 (10%)]\tLoss: 0.486460\tLR: 0.00029985\n",
            "Train Epoch: 8 [12800/123872 (10%)]\tLoss: 0.436000\tLR: 0.00029974\n",
            "Train Epoch: 8 [12800/123872 (10%)]\tLoss: 0.436000\n",
            "Train Epoch: 8 [13056/123872 (11%)]\tLoss: 0.418826\tLR: 0.00029962\n",
            "Train Epoch: 8 [13312/123872 (11%)]\tLoss: 0.448236\tLR: 0.00029950\n",
            "Train Epoch: 8 [13568/123872 (11%)]\tLoss: 0.417986\tLR: 0.00029938\n",
            "Train Epoch: 8 [13824/123872 (11%)]\tLoss: 0.419688\tLR: 0.00029927\n",
            "Train Epoch: 8 [14080/123872 (11%)]\tLoss: 0.432428\tLR: 0.00029915\n",
            "Train Epoch: 8 [14336/123872 (12%)]\tLoss: 0.364775\tLR: 0.00029903\n",
            "Train Epoch: 8 [14592/123872 (12%)]\tLoss: 0.415141\tLR: 0.00029892\n",
            "Train Epoch: 8 [14848/123872 (12%)]\tLoss: 0.477423\tLR: 0.00029880\n",
            "Train Epoch: 8 [15104/123872 (12%)]\tLoss: 0.469438\tLR: 0.00029868\n",
            "Train Epoch: 8 [15360/123872 (12%)]\tLoss: 0.485993\tLR: 0.00029856\n",
            "Train Epoch: 8 [15360/123872 (12%)]\tLoss: 0.485993\n",
            "Train Epoch: 8 [15616/123872 (13%)]\tLoss: 0.401651\tLR: 0.00029845\n",
            "Train Epoch: 8 [15872/123872 (13%)]\tLoss: 0.409273\tLR: 0.00029833\n",
            "Train Epoch: 8 [16128/123872 (13%)]\tLoss: 0.435222\tLR: 0.00029821\n",
            "Train Epoch: 8 [16384/123872 (13%)]\tLoss: 0.399790\tLR: 0.00029809\n",
            "Train Epoch: 8 [16640/123872 (13%)]\tLoss: 0.422753\tLR: 0.00029798\n",
            "Train Epoch: 8 [16896/123872 (14%)]\tLoss: 0.400377\tLR: 0.00029786\n",
            "Train Epoch: 8 [17152/123872 (14%)]\tLoss: 0.441281\tLR: 0.00029774\n",
            "Train Epoch: 8 [17408/123872 (14%)]\tLoss: 0.457612\tLR: 0.00029763\n",
            "Train Epoch: 8 [17664/123872 (14%)]\tLoss: 0.396078\tLR: 0.00029751\n",
            "Train Epoch: 8 [17920/123872 (14%)]\tLoss: 0.408566\tLR: 0.00029739\n",
            "Train Epoch: 8 [17920/123872 (14%)]\tLoss: 0.408566\n",
            "Train Epoch: 8 [18176/123872 (15%)]\tLoss: 0.342118\tLR: 0.00029727\n",
            "Train Epoch: 8 [18432/123872 (15%)]\tLoss: 0.361582\tLR: 0.00029716\n",
            "Train Epoch: 8 [18688/123872 (15%)]\tLoss: 0.403647\tLR: 0.00029704\n",
            "Train Epoch: 8 [18944/123872 (15%)]\tLoss: 0.430598\tLR: 0.00029692\n",
            "Train Epoch: 8 [19200/123872 (15%)]\tLoss: 0.383071\tLR: 0.00029681\n",
            "Train Epoch: 8 [19456/123872 (16%)]\tLoss: 0.399835\tLR: 0.00029669\n",
            "Train Epoch: 8 [19712/123872 (16%)]\tLoss: 0.457833\tLR: 0.00029657\n",
            "Train Epoch: 8 [19968/123872 (16%)]\tLoss: 0.386716\tLR: 0.00029646\n",
            "Train Epoch: 8 [20224/123872 (16%)]\tLoss: 0.412993\tLR: 0.00029634\n",
            "Train Epoch: 8 [20480/123872 (17%)]\tLoss: 0.421067\tLR: 0.00029622\n",
            "Train Epoch: 8 [20480/123872 (17%)]\tLoss: 0.421067\n",
            "Train Epoch: 8 [20736/123872 (17%)]\tLoss: 0.408564\tLR: 0.00029610\n",
            "Train Epoch: 8 [20992/123872 (17%)]\tLoss: 0.446507\tLR: 0.00029599\n",
            "Train Epoch: 8 [21248/123872 (17%)]\tLoss: 0.435537\tLR: 0.00029587\n",
            "Train Epoch: 8 [21504/123872 (17%)]\tLoss: 0.376625\tLR: 0.00029575\n",
            "Train Epoch: 8 [21760/123872 (18%)]\tLoss: 0.414991\tLR: 0.00029564\n",
            "Train Epoch: 8 [22016/123872 (18%)]\tLoss: 0.404243\tLR: 0.00029552\n",
            "Train Epoch: 8 [22272/123872 (18%)]\tLoss: 0.441273\tLR: 0.00029540\n",
            "Train Epoch: 8 [22528/123872 (18%)]\tLoss: 0.412930\tLR: 0.00029528\n",
            "Train Epoch: 8 [22784/123872 (18%)]\tLoss: 0.409263\tLR: 0.00029517\n",
            "Train Epoch: 8 [23040/123872 (19%)]\tLoss: 0.454548\tLR: 0.00029505\n",
            "Train Epoch: 8 [23040/123872 (19%)]\tLoss: 0.454548\n",
            "Train Epoch: 8 [23296/123872 (19%)]\tLoss: 0.375049\tLR: 0.00029493\n",
            "Train Epoch: 8 [23552/123872 (19%)]\tLoss: 0.445639\tLR: 0.00029482\n",
            "Train Epoch: 8 [23808/123872 (19%)]\tLoss: 0.462873\tLR: 0.00029470\n",
            "Train Epoch: 8 [24064/123872 (19%)]\tLoss: 0.409284\tLR: 0.00029458\n",
            "Train Epoch: 8 [24320/123872 (20%)]\tLoss: 0.435322\tLR: 0.00029447\n",
            "Train Epoch: 8 [24576/123872 (20%)]\tLoss: 0.454441\tLR: 0.00029435\n",
            "Train Epoch: 8 [24832/123872 (20%)]\tLoss: 0.411196\tLR: 0.00029423\n",
            "Train Epoch: 8 [25088/123872 (20%)]\tLoss: 0.462212\tLR: 0.00029411\n",
            "Train Epoch: 8 [25344/123872 (20%)]\tLoss: 0.421585\tLR: 0.00029400\n",
            "Train Epoch: 8 [25600/123872 (21%)]\tLoss: 0.467893\tLR: 0.00029388\n",
            "Train Epoch: 8 [25600/123872 (21%)]\tLoss: 0.467893\n",
            "Train Epoch: 8 [25856/123872 (21%)]\tLoss: 0.424055\tLR: 0.00029376\n",
            "Train Epoch: 8 [26112/123872 (21%)]\tLoss: 0.499190\tLR: 0.00029365\n",
            "Train Epoch: 8 [26368/123872 (21%)]\tLoss: 0.419784\tLR: 0.00029353\n",
            "Train Epoch: 8 [26624/123872 (21%)]\tLoss: 0.405252\tLR: 0.00029341\n",
            "Train Epoch: 8 [26880/123872 (22%)]\tLoss: 0.456423\tLR: 0.00029330\n",
            "Train Epoch: 8 [27136/123872 (22%)]\tLoss: 0.413883\tLR: 0.00029318\n",
            "Train Epoch: 8 [27392/123872 (22%)]\tLoss: 0.400422\tLR: 0.00029306\n",
            "Train Epoch: 8 [27648/123872 (22%)]\tLoss: 0.436432\tLR: 0.00029295\n",
            "Train Epoch: 8 [27904/123872 (23%)]\tLoss: 0.419098\tLR: 0.00029283\n",
            "Train Epoch: 8 [28160/123872 (23%)]\tLoss: 0.362093\tLR: 0.00029271\n",
            "Train Epoch: 8 [28160/123872 (23%)]\tLoss: 0.362093\n",
            "Train Epoch: 8 [28416/123872 (23%)]\tLoss: 0.447187\tLR: 0.00029259\n",
            "Train Epoch: 8 [28672/123872 (23%)]\tLoss: 0.398836\tLR: 0.00029248\n",
            "Train Epoch: 8 [28928/123872 (23%)]\tLoss: 0.427180\tLR: 0.00029236\n",
            "Train Epoch: 8 [29184/123872 (24%)]\tLoss: 0.373970\tLR: 0.00029224\n",
            "Train Epoch: 8 [29440/123872 (24%)]\tLoss: 0.395485\tLR: 0.00029213\n",
            "Train Epoch: 8 [29696/123872 (24%)]\tLoss: 0.460023\tLR: 0.00029201\n",
            "Train Epoch: 8 [29952/123872 (24%)]\tLoss: 0.475553\tLR: 0.00029189\n",
            "Train Epoch: 8 [30208/123872 (24%)]\tLoss: 0.439136\tLR: 0.00029178\n",
            "Train Epoch: 8 [30464/123872 (25%)]\tLoss: 0.402210\tLR: 0.00029166\n",
            "Train Epoch: 8 [30720/123872 (25%)]\tLoss: 0.429247\tLR: 0.00029154\n",
            "Train Epoch: 8 [30720/123872 (25%)]\tLoss: 0.429247\n",
            "Train Epoch: 8 [30976/123872 (25%)]\tLoss: 0.449904\tLR: 0.00029143\n",
            "Train Epoch: 8 [31232/123872 (25%)]\tLoss: 0.379841\tLR: 0.00029131\n",
            "Train Epoch: 8 [31488/123872 (25%)]\tLoss: 0.443213\tLR: 0.00029119\n",
            "Train Epoch: 8 [31744/123872 (26%)]\tLoss: 0.407929\tLR: 0.00029108\n",
            "Train Epoch: 8 [32000/123872 (26%)]\tLoss: 0.423750\tLR: 0.00029096\n",
            "Train Epoch: 8 [32256/123872 (26%)]\tLoss: 0.429685\tLR: 0.00029084\n",
            "Train Epoch: 8 [32512/123872 (26%)]\tLoss: 0.403651\tLR: 0.00029073\n",
            "Train Epoch: 8 [32768/123872 (26%)]\tLoss: 0.450377\tLR: 0.00029061\n",
            "Train Epoch: 8 [33024/123872 (27%)]\tLoss: 0.396028\tLR: 0.00029049\n",
            "Train Epoch: 8 [33280/123872 (27%)]\tLoss: 0.368994\tLR: 0.00029038\n",
            "Train Epoch: 8 [33280/123872 (27%)]\tLoss: 0.368994\n",
            "Train Epoch: 8 [33536/123872 (27%)]\tLoss: 0.382769\tLR: 0.00029026\n",
            "Train Epoch: 8 [33792/123872 (27%)]\tLoss: 0.391758\tLR: 0.00029014\n",
            "Train Epoch: 8 [34048/123872 (27%)]\tLoss: 0.428171\tLR: 0.00029003\n",
            "Train Epoch: 8 [34304/123872 (28%)]\tLoss: 0.435643\tLR: 0.00028991\n",
            "Train Epoch: 8 [34560/123872 (28%)]\tLoss: 0.405144\tLR: 0.00028979\n",
            "Train Epoch: 8 [34816/123872 (28%)]\tLoss: 0.499478\tLR: 0.00028968\n",
            "Train Epoch: 8 [35072/123872 (28%)]\tLoss: 0.383633\tLR: 0.00028956\n",
            "Train Epoch: 8 [35328/123872 (29%)]\tLoss: 0.374004\tLR: 0.00028944\n",
            "Train Epoch: 8 [35584/123872 (29%)]\tLoss: 0.398595\tLR: 0.00028933\n",
            "Train Epoch: 8 [35840/123872 (29%)]\tLoss: 0.387433\tLR: 0.00028921\n",
            "Train Epoch: 8 [35840/123872 (29%)]\tLoss: 0.387433\n",
            "Train Epoch: 8 [36096/123872 (29%)]\tLoss: 0.327799\tLR: 0.00028909\n",
            "Train Epoch: 8 [36352/123872 (29%)]\tLoss: 0.526978\tLR: 0.00028898\n",
            "Train Epoch: 8 [36608/123872 (30%)]\tLoss: 0.420398\tLR: 0.00028886\n",
            "Train Epoch: 8 [36864/123872 (30%)]\tLoss: 0.311894\tLR: 0.00028874\n",
            "Train Epoch: 8 [37120/123872 (30%)]\tLoss: 0.457529\tLR: 0.00028863\n",
            "Train Epoch: 8 [37376/123872 (30%)]\tLoss: 0.382818\tLR: 0.00028851\n",
            "Train Epoch: 8 [37632/123872 (30%)]\tLoss: 0.404676\tLR: 0.00028839\n",
            "Train Epoch: 8 [37888/123872 (31%)]\tLoss: 0.437683\tLR: 0.00028828\n",
            "Train Epoch: 8 [38144/123872 (31%)]\tLoss: 0.451742\tLR: 0.00028816\n",
            "Train Epoch: 8 [38400/123872 (31%)]\tLoss: 0.399913\tLR: 0.00028804\n",
            "Train Epoch: 8 [38400/123872 (31%)]\tLoss: 0.399913\n",
            "Train Epoch: 8 [38656/123872 (31%)]\tLoss: 0.366864\tLR: 0.00028793\n",
            "Train Epoch: 8 [38912/123872 (31%)]\tLoss: 0.432007\tLR: 0.00028781\n",
            "Train Epoch: 8 [39168/123872 (32%)]\tLoss: 0.442506\tLR: 0.00028769\n",
            "Train Epoch: 8 [39424/123872 (32%)]\tLoss: 0.402815\tLR: 0.00028758\n",
            "Train Epoch: 8 [39680/123872 (32%)]\tLoss: 0.427724\tLR: 0.00028746\n",
            "Train Epoch: 8 [39936/123872 (32%)]\tLoss: 0.474810\tLR: 0.00028734\n",
            "Train Epoch: 8 [40192/123872 (32%)]\tLoss: 0.363477\tLR: 0.00028723\n",
            "Train Epoch: 8 [40448/123872 (33%)]\tLoss: 0.442502\tLR: 0.00028711\n",
            "Train Epoch: 8 [40704/123872 (33%)]\tLoss: 0.482327\tLR: 0.00028699\n",
            "Train Epoch: 8 [40960/123872 (33%)]\tLoss: 0.414419\tLR: 0.00028688\n",
            "Train Epoch: 8 [40960/123872 (33%)]\tLoss: 0.414419\n",
            "Train Epoch: 8 [41216/123872 (33%)]\tLoss: 0.444268\tLR: 0.00028676\n",
            "Train Epoch: 8 [41472/123872 (33%)]\tLoss: 0.378394\tLR: 0.00028664\n",
            "Train Epoch: 8 [41728/123872 (34%)]\tLoss: 0.402501\tLR: 0.00028653\n",
            "Train Epoch: 8 [41984/123872 (34%)]\tLoss: 0.450083\tLR: 0.00028641\n",
            "Train Epoch: 8 [42240/123872 (34%)]\tLoss: 0.428335\tLR: 0.00028629\n",
            "Train Epoch: 8 [42496/123872 (34%)]\tLoss: 0.487360\tLR: 0.00028618\n",
            "Train Epoch: 8 [42752/123872 (35%)]\tLoss: 0.387680\tLR: 0.00028606\n",
            "Train Epoch: 8 [43008/123872 (35%)]\tLoss: 0.410649\tLR: 0.00028595\n",
            "Train Epoch: 8 [43264/123872 (35%)]\tLoss: 0.372039\tLR: 0.00028583\n",
            "Train Epoch: 8 [43520/123872 (35%)]\tLoss: 0.395155\tLR: 0.00028571\n",
            "Train Epoch: 8 [43520/123872 (35%)]\tLoss: 0.395155\n",
            "Train Epoch: 8 [43776/123872 (35%)]\tLoss: 0.479846\tLR: 0.00028560\n",
            "Train Epoch: 8 [44032/123872 (36%)]\tLoss: 0.359503\tLR: 0.00028548\n",
            "Train Epoch: 8 [44288/123872 (36%)]\tLoss: 0.395813\tLR: 0.00028536\n",
            "Train Epoch: 8 [44544/123872 (36%)]\tLoss: 0.379235\tLR: 0.00028525\n",
            "Train Epoch: 8 [44800/123872 (36%)]\tLoss: 0.476907\tLR: 0.00028513\n",
            "Train Epoch: 8 [45056/123872 (36%)]\tLoss: 0.381036\tLR: 0.00028501\n",
            "Train Epoch: 8 [45312/123872 (37%)]\tLoss: 0.430775\tLR: 0.00028490\n",
            "Train Epoch: 8 [45568/123872 (37%)]\tLoss: 0.448313\tLR: 0.00028478\n",
            "Train Epoch: 8 [45824/123872 (37%)]\tLoss: 0.418343\tLR: 0.00028467\n",
            "Train Epoch: 8 [46080/123872 (37%)]\tLoss: 0.373651\tLR: 0.00028455\n",
            "Train Epoch: 8 [46080/123872 (37%)]\tLoss: 0.373651\n",
            "Train Epoch: 8 [46336/123872 (37%)]\tLoss: 0.390705\tLR: 0.00028443\n",
            "Train Epoch: 8 [46592/123872 (38%)]\tLoss: 0.391877\tLR: 0.00028432\n",
            "Train Epoch: 8 [46848/123872 (38%)]\tLoss: 0.426968\tLR: 0.00028420\n",
            "Train Epoch: 8 [47104/123872 (38%)]\tLoss: 0.354452\tLR: 0.00028408\n",
            "Train Epoch: 8 [47360/123872 (38%)]\tLoss: 0.424077\tLR: 0.00028397\n",
            "Train Epoch: 8 [47616/123872 (38%)]\tLoss: 0.411448\tLR: 0.00028385\n",
            "Train Epoch: 8 [47872/123872 (39%)]\tLoss: 0.469237\tLR: 0.00028373\n",
            "Train Epoch: 8 [48128/123872 (39%)]\tLoss: 0.396812\tLR: 0.00028362\n",
            "Train Epoch: 8 [48384/123872 (39%)]\tLoss: 0.430253\tLR: 0.00028350\n",
            "Train Epoch: 8 [48640/123872 (39%)]\tLoss: 0.381781\tLR: 0.00028339\n",
            "Train Epoch: 8 [48640/123872 (39%)]\tLoss: 0.381781\n",
            "Train Epoch: 8 [48896/123872 (39%)]\tLoss: 0.515233\tLR: 0.00028327\n",
            "Train Epoch: 8 [49152/123872 (40%)]\tLoss: 0.433725\tLR: 0.00028315\n",
            "Train Epoch: 8 [49408/123872 (40%)]\tLoss: 0.409759\tLR: 0.00028304\n",
            "Train Epoch: 8 [49664/123872 (40%)]\tLoss: 0.455724\tLR: 0.00028292\n",
            "Train Epoch: 8 [49920/123872 (40%)]\tLoss: 0.410030\tLR: 0.00028281\n",
            "Train Epoch: 8 [50176/123872 (40%)]\tLoss: 0.368752\tLR: 0.00028269\n",
            "Train Epoch: 8 [50432/123872 (41%)]\tLoss: 0.409324\tLR: 0.00028257\n",
            "Train Epoch: 8 [50688/123872 (41%)]\tLoss: 0.403959\tLR: 0.00028246\n",
            "Train Epoch: 8 [50944/123872 (41%)]\tLoss: 0.389928\tLR: 0.00028234\n",
            "Train Epoch: 8 [51200/123872 (41%)]\tLoss: 0.355507\tLR: 0.00028222\n",
            "Train Epoch: 8 [51200/123872 (41%)]\tLoss: 0.355507\n",
            "Train Epoch: 8 [51456/123872 (42%)]\tLoss: 0.467996\tLR: 0.00028211\n",
            "Train Epoch: 8 [51712/123872 (42%)]\tLoss: 0.444345\tLR: 0.00028199\n",
            "Train Epoch: 8 [51968/123872 (42%)]\tLoss: 0.384151\tLR: 0.00028188\n",
            "Train Epoch: 8 [52224/123872 (42%)]\tLoss: 0.446197\tLR: 0.00028176\n",
            "Train Epoch: 8 [52480/123872 (42%)]\tLoss: 0.440501\tLR: 0.00028164\n",
            "Train Epoch: 8 [52736/123872 (43%)]\tLoss: 0.366574\tLR: 0.00028153\n",
            "Train Epoch: 8 [52992/123872 (43%)]\tLoss: 0.442593\tLR: 0.00028141\n",
            "Train Epoch: 8 [53248/123872 (43%)]\tLoss: 0.415158\tLR: 0.00028130\n",
            "Train Epoch: 8 [53504/123872 (43%)]\tLoss: 0.445036\tLR: 0.00028118\n",
            "Train Epoch: 8 [53760/123872 (43%)]\tLoss: 0.376675\tLR: 0.00028106\n",
            "Train Epoch: 8 [53760/123872 (43%)]\tLoss: 0.376675\n",
            "Train Epoch: 8 [54016/123872 (44%)]\tLoss: 0.405431\tLR: 0.00028095\n",
            "Train Epoch: 8 [54272/123872 (44%)]\tLoss: 0.380172\tLR: 0.00028083\n",
            "Train Epoch: 8 [54528/123872 (44%)]\tLoss: 0.381305\tLR: 0.00028072\n",
            "Train Epoch: 8 [54784/123872 (44%)]\tLoss: 0.399026\tLR: 0.00028060\n",
            "Train Epoch: 8 [55040/123872 (44%)]\tLoss: 0.479892\tLR: 0.00028048\n",
            "Train Epoch: 8 [55296/123872 (45%)]\tLoss: 0.418250\tLR: 0.00028037\n",
            "Train Epoch: 8 [55552/123872 (45%)]\tLoss: 0.376626\tLR: 0.00028025\n",
            "Train Epoch: 8 [55808/123872 (45%)]\tLoss: 0.393146\tLR: 0.00028014\n",
            "Train Epoch: 8 [56064/123872 (45%)]\tLoss: 0.450642\tLR: 0.00028002\n",
            "Train Epoch: 8 [56320/123872 (45%)]\tLoss: 0.380657\tLR: 0.00027990\n",
            "Train Epoch: 8 [56320/123872 (45%)]\tLoss: 0.380657\n",
            "Train Epoch: 8 [56576/123872 (46%)]\tLoss: 0.413335\tLR: 0.00027979\n",
            "Train Epoch: 8 [56832/123872 (46%)]\tLoss: 0.401010\tLR: 0.00027967\n",
            "Train Epoch: 8 [57088/123872 (46%)]\tLoss: 0.388798\tLR: 0.00027956\n",
            "Train Epoch: 8 [57344/123872 (46%)]\tLoss: 0.374525\tLR: 0.00027944\n",
            "Train Epoch: 8 [57600/123872 (46%)]\tLoss: 0.406962\tLR: 0.00027932\n",
            "Train Epoch: 8 [57856/123872 (47%)]\tLoss: 0.387421\tLR: 0.00027921\n",
            "Train Epoch: 8 [58112/123872 (47%)]\tLoss: 0.398194\tLR: 0.00027909\n",
            "Train Epoch: 8 [58368/123872 (47%)]\tLoss: 0.412900\tLR: 0.00027898\n",
            "Train Epoch: 8 [58624/123872 (47%)]\tLoss: 0.457749\tLR: 0.00027886\n",
            "Train Epoch: 8 [58880/123872 (48%)]\tLoss: 0.471983\tLR: 0.00027874\n",
            "Train Epoch: 8 [58880/123872 (48%)]\tLoss: 0.471983\n",
            "Train Epoch: 8 [59136/123872 (48%)]\tLoss: 0.387252\tLR: 0.00027863\n",
            "Train Epoch: 8 [59392/123872 (48%)]\tLoss: 0.468923\tLR: 0.00027851\n",
            "Train Epoch: 8 [59648/123872 (48%)]\tLoss: 0.395296\tLR: 0.00027840\n",
            "Train Epoch: 8 [59904/123872 (48%)]\tLoss: 0.414541\tLR: 0.00027828\n",
            "Train Epoch: 8 [60160/123872 (49%)]\tLoss: 0.355364\tLR: 0.00027816\n",
            "Train Epoch: 8 [60416/123872 (49%)]\tLoss: 0.393369\tLR: 0.00027805\n",
            "Train Epoch: 8 [60672/123872 (49%)]\tLoss: 0.448653\tLR: 0.00027793\n",
            "Train Epoch: 8 [60928/123872 (49%)]\tLoss: 0.461427\tLR: 0.00027782\n",
            "Train Epoch: 8 [61184/123872 (49%)]\tLoss: 0.479831\tLR: 0.00027770\n",
            "Train Epoch: 8 [61440/123872 (50%)]\tLoss: 0.421087\tLR: 0.00027759\n",
            "Train Epoch: 8 [61440/123872 (50%)]\tLoss: 0.421087\n",
            "Train Epoch: 8 [61696/123872 (50%)]\tLoss: 0.441552\tLR: 0.00027747\n",
            "Train Epoch: 8 [61952/123872 (50%)]\tLoss: 0.371087\tLR: 0.00027735\n",
            "Train Epoch: 8 [62208/123872 (50%)]\tLoss: 0.435024\tLR: 0.00027724\n",
            "Train Epoch: 8 [62464/123872 (50%)]\tLoss: 0.360181\tLR: 0.00027712\n",
            "Train Epoch: 8 [62720/123872 (51%)]\tLoss: 0.403998\tLR: 0.00027701\n",
            "Train Epoch: 8 [62976/123872 (51%)]\tLoss: 0.419553\tLR: 0.00027689\n",
            "Train Epoch: 8 [63232/123872 (51%)]\tLoss: 0.414684\tLR: 0.00027678\n",
            "Train Epoch: 8 [63488/123872 (51%)]\tLoss: 0.412227\tLR: 0.00027666\n",
            "Train Epoch: 8 [63744/123872 (51%)]\tLoss: 0.418643\tLR: 0.00027654\n",
            "Train Epoch: 8 [64000/123872 (52%)]\tLoss: 0.419646\tLR: 0.00027643\n",
            "Train Epoch: 8 [64000/123872 (52%)]\tLoss: 0.419646\n",
            "Train Epoch: 8 [64256/123872 (52%)]\tLoss: 0.477495\tLR: 0.00027631\n",
            "Train Epoch: 8 [64512/123872 (52%)]\tLoss: 0.448156\tLR: 0.00027620\n",
            "Train Epoch: 8 [64768/123872 (52%)]\tLoss: 0.393861\tLR: 0.00027608\n",
            "Train Epoch: 8 [65024/123872 (52%)]\tLoss: 0.435122\tLR: 0.00027597\n",
            "Train Epoch: 8 [65280/123872 (53%)]\tLoss: 0.421226\tLR: 0.00027585\n",
            "Train Epoch: 8 [65536/123872 (53%)]\tLoss: 0.410590\tLR: 0.00027574\n",
            "Train Epoch: 8 [65792/123872 (53%)]\tLoss: 0.391228\tLR: 0.00027562\n",
            "Train Epoch: 8 [66048/123872 (53%)]\tLoss: 0.465094\tLR: 0.00027550\n",
            "Train Epoch: 8 [66304/123872 (54%)]\tLoss: 0.402877\tLR: 0.00027539\n",
            "Train Epoch: 8 [66560/123872 (54%)]\tLoss: 0.417556\tLR: 0.00027527\n",
            "Train Epoch: 8 [66560/123872 (54%)]\tLoss: 0.417556\n",
            "Train Epoch: 8 [66816/123872 (54%)]\tLoss: 0.426684\tLR: 0.00027516\n",
            "Train Epoch: 8 [67072/123872 (54%)]\tLoss: 0.390121\tLR: 0.00027504\n",
            "Train Epoch: 8 [67328/123872 (54%)]\tLoss: 0.394634\tLR: 0.00027493\n",
            "Train Epoch: 8 [67584/123872 (55%)]\tLoss: 0.384464\tLR: 0.00027481\n",
            "Train Epoch: 8 [67840/123872 (55%)]\tLoss: 0.387707\tLR: 0.00027470\n",
            "Train Epoch: 8 [68096/123872 (55%)]\tLoss: 0.397685\tLR: 0.00027458\n",
            "Train Epoch: 8 [68352/123872 (55%)]\tLoss: 0.438334\tLR: 0.00027446\n",
            "Train Epoch: 8 [68608/123872 (55%)]\tLoss: 0.450801\tLR: 0.00027435\n",
            "Train Epoch: 8 [68864/123872 (56%)]\tLoss: 0.438765\tLR: 0.00027423\n",
            "Train Epoch: 8 [69120/123872 (56%)]\tLoss: 0.443464\tLR: 0.00027412\n",
            "Train Epoch: 8 [69120/123872 (56%)]\tLoss: 0.443464\n",
            "Train Epoch: 8 [69376/123872 (56%)]\tLoss: 0.408895\tLR: 0.00027400\n",
            "Train Epoch: 8 [69632/123872 (56%)]\tLoss: 0.391736\tLR: 0.00027389\n",
            "Train Epoch: 8 [69888/123872 (56%)]\tLoss: 0.410050\tLR: 0.00027377\n",
            "Train Epoch: 8 [70144/123872 (57%)]\tLoss: 0.372309\tLR: 0.00027366\n",
            "Train Epoch: 8 [70400/123872 (57%)]\tLoss: 0.434018\tLR: 0.00027354\n",
            "Train Epoch: 8 [70656/123872 (57%)]\tLoss: 0.448078\tLR: 0.00027343\n",
            "Train Epoch: 8 [70912/123872 (57%)]\tLoss: 0.384089\tLR: 0.00027331\n",
            "Train Epoch: 8 [71168/123872 (57%)]\tLoss: 0.465305\tLR: 0.00027319\n",
            "Train Epoch: 8 [71424/123872 (58%)]\tLoss: 0.440114\tLR: 0.00027308\n",
            "Train Epoch: 8 [71680/123872 (58%)]\tLoss: 0.391777\tLR: 0.00027296\n",
            "Train Epoch: 8 [71680/123872 (58%)]\tLoss: 0.391777\n",
            "Train Epoch: 8 [71936/123872 (58%)]\tLoss: 0.356632\tLR: 0.00027285\n",
            "Train Epoch: 8 [72192/123872 (58%)]\tLoss: 0.381682\tLR: 0.00027273\n",
            "Train Epoch: 8 [72448/123872 (58%)]\tLoss: 0.395668\tLR: 0.00027262\n",
            "Train Epoch: 8 [72704/123872 (59%)]\tLoss: 0.346394\tLR: 0.00027250\n",
            "Train Epoch: 8 [72960/123872 (59%)]\tLoss: 0.399524\tLR: 0.00027239\n",
            "Train Epoch: 8 [73216/123872 (59%)]\tLoss: 0.392537\tLR: 0.00027227\n",
            "Train Epoch: 8 [73472/123872 (59%)]\tLoss: 0.404320\tLR: 0.00027216\n",
            "Train Epoch: 8 [73728/123872 (60%)]\tLoss: 0.444651\tLR: 0.00027204\n",
            "Train Epoch: 8 [73984/123872 (60%)]\tLoss: 0.385502\tLR: 0.00027193\n",
            "Train Epoch: 8 [74240/123872 (60%)]\tLoss: 0.398342\tLR: 0.00027181\n",
            "Train Epoch: 8 [74240/123872 (60%)]\tLoss: 0.398342\n",
            "Train Epoch: 8 [74496/123872 (60%)]\tLoss: 0.412162\tLR: 0.00027170\n",
            "Train Epoch: 8 [74752/123872 (60%)]\tLoss: 0.460477\tLR: 0.00027158\n",
            "Train Epoch: 8 [75008/123872 (61%)]\tLoss: 0.369026\tLR: 0.00027147\n",
            "Train Epoch: 8 [75264/123872 (61%)]\tLoss: 0.459567\tLR: 0.00027135\n",
            "Train Epoch: 8 [75520/123872 (61%)]\tLoss: 0.397699\tLR: 0.00027123\n",
            "Train Epoch: 8 [75776/123872 (61%)]\tLoss: 0.413636\tLR: 0.00027112\n",
            "Train Epoch: 8 [76032/123872 (61%)]\tLoss: 0.372316\tLR: 0.00027100\n",
            "Train Epoch: 8 [76288/123872 (62%)]\tLoss: 0.468617\tLR: 0.00027089\n",
            "Train Epoch: 8 [76544/123872 (62%)]\tLoss: 0.381941\tLR: 0.00027077\n",
            "Train Epoch: 8 [76800/123872 (62%)]\tLoss: 0.390627\tLR: 0.00027066\n",
            "Train Epoch: 8 [76800/123872 (62%)]\tLoss: 0.390627\n",
            "Train Epoch: 8 [77056/123872 (62%)]\tLoss: 0.419013\tLR: 0.00027054\n",
            "Train Epoch: 8 [77312/123872 (62%)]\tLoss: 0.415798\tLR: 0.00027043\n",
            "Train Epoch: 8 [77568/123872 (63%)]\tLoss: 0.399732\tLR: 0.00027031\n",
            "Train Epoch: 8 [77824/123872 (63%)]\tLoss: 0.378225\tLR: 0.00027020\n",
            "Train Epoch: 8 [78080/123872 (63%)]\tLoss: 0.442239\tLR: 0.00027008\n",
            "Train Epoch: 8 [78336/123872 (63%)]\tLoss: 0.414455\tLR: 0.00026997\n",
            "Train Epoch: 8 [78592/123872 (63%)]\tLoss: 0.383941\tLR: 0.00026985\n",
            "Train Epoch: 8 [78848/123872 (64%)]\tLoss: 0.395568\tLR: 0.00026974\n",
            "Train Epoch: 8 [79104/123872 (64%)]\tLoss: 0.396415\tLR: 0.00026962\n",
            "Train Epoch: 8 [79360/123872 (64%)]\tLoss: 0.391299\tLR: 0.00026951\n",
            "Train Epoch: 8 [79360/123872 (64%)]\tLoss: 0.391299\n",
            "Train Epoch: 8 [79616/123872 (64%)]\tLoss: 0.420269\tLR: 0.00026939\n",
            "Train Epoch: 8 [79872/123872 (64%)]\tLoss: 0.396716\tLR: 0.00026928\n",
            "Train Epoch: 8 [80128/123872 (65%)]\tLoss: 0.359756\tLR: 0.00026916\n",
            "Train Epoch: 8 [80384/123872 (65%)]\tLoss: 0.343099\tLR: 0.00026905\n",
            "Train Epoch: 8 [80640/123872 (65%)]\tLoss: 0.425671\tLR: 0.00026893\n",
            "Train Epoch: 8 [80896/123872 (65%)]\tLoss: 0.416663\tLR: 0.00026882\n",
            "Train Epoch: 8 [81152/123872 (65%)]\tLoss: 0.403200\tLR: 0.00026870\n",
            "Train Epoch: 8 [81408/123872 (66%)]\tLoss: 0.395613\tLR: 0.00026859\n",
            "Train Epoch: 8 [81664/123872 (66%)]\tLoss: 0.463977\tLR: 0.00026847\n",
            "Train Epoch: 8 [81920/123872 (66%)]\tLoss: 0.440502\tLR: 0.00026836\n",
            "Train Epoch: 8 [81920/123872 (66%)]\tLoss: 0.440502\n",
            "Train Epoch: 8 [82176/123872 (66%)]\tLoss: 0.423771\tLR: 0.00026824\n",
            "Train Epoch: 8 [82432/123872 (67%)]\tLoss: 0.471615\tLR: 0.00026813\n",
            "Train Epoch: 8 [82688/123872 (67%)]\tLoss: 0.411820\tLR: 0.00026801\n",
            "Train Epoch: 8 [82944/123872 (67%)]\tLoss: 0.449297\tLR: 0.00026790\n",
            "Train Epoch: 8 [83200/123872 (67%)]\tLoss: 0.357742\tLR: 0.00026778\n",
            "Train Epoch: 8 [83456/123872 (67%)]\tLoss: 0.422056\tLR: 0.00026767\n",
            "Train Epoch: 8 [83712/123872 (68%)]\tLoss: 0.410022\tLR: 0.00026756\n",
            "Train Epoch: 8 [83968/123872 (68%)]\tLoss: 0.414760\tLR: 0.00026744\n",
            "Train Epoch: 8 [84224/123872 (68%)]\tLoss: 0.438016\tLR: 0.00026733\n",
            "Train Epoch: 8 [84480/123872 (68%)]\tLoss: 0.398450\tLR: 0.00026721\n",
            "Train Epoch: 8 [84480/123872 (68%)]\tLoss: 0.398450\n",
            "Train Epoch: 8 [84736/123872 (68%)]\tLoss: 0.364134\tLR: 0.00026710\n",
            "Train Epoch: 8 [84992/123872 (69%)]\tLoss: 0.373250\tLR: 0.00026698\n",
            "Train Epoch: 8 [85248/123872 (69%)]\tLoss: 0.409790\tLR: 0.00026687\n",
            "Train Epoch: 8 [85504/123872 (69%)]\tLoss: 0.372358\tLR: 0.00026675\n",
            "Train Epoch: 8 [85760/123872 (69%)]\tLoss: 0.424148\tLR: 0.00026664\n",
            "Train Epoch: 8 [86016/123872 (69%)]\tLoss: 0.453044\tLR: 0.00026652\n",
            "Train Epoch: 8 [86272/123872 (70%)]\tLoss: 0.412462\tLR: 0.00026641\n",
            "Train Epoch: 8 [86528/123872 (70%)]\tLoss: 0.427055\tLR: 0.00026629\n",
            "Train Epoch: 8 [86784/123872 (70%)]\tLoss: 0.409197\tLR: 0.00026618\n",
            "Train Epoch: 8 [87040/123872 (70%)]\tLoss: 0.443487\tLR: 0.00026606\n",
            "Train Epoch: 8 [87040/123872 (70%)]\tLoss: 0.443487\n",
            "Train Epoch: 8 [87296/123872 (70%)]\tLoss: 0.387118\tLR: 0.00026595\n",
            "Train Epoch: 8 [87552/123872 (71%)]\tLoss: 0.404479\tLR: 0.00026583\n",
            "Train Epoch: 8 [87808/123872 (71%)]\tLoss: 0.391601\tLR: 0.00026572\n",
            "Train Epoch: 8 [88064/123872 (71%)]\tLoss: 0.366470\tLR: 0.00026561\n",
            "Train Epoch: 8 [88320/123872 (71%)]\tLoss: 0.423025\tLR: 0.00026549\n",
            "Train Epoch: 8 [88576/123872 (71%)]\tLoss: 0.408652\tLR: 0.00026538\n",
            "Train Epoch: 8 [88832/123872 (72%)]\tLoss: 0.366518\tLR: 0.00026526\n",
            "Train Epoch: 8 [89088/123872 (72%)]\tLoss: 0.417485\tLR: 0.00026515\n",
            "Train Epoch: 8 [89344/123872 (72%)]\tLoss: 0.353774\tLR: 0.00026503\n",
            "Train Epoch: 8 [89600/123872 (72%)]\tLoss: 0.423319\tLR: 0.00026492\n",
            "Train Epoch: 8 [89600/123872 (72%)]\tLoss: 0.423319\n",
            "Train Epoch: 8 [89856/123872 (73%)]\tLoss: 0.412042\tLR: 0.00026480\n",
            "Train Epoch: 8 [90112/123872 (73%)]\tLoss: 0.399933\tLR: 0.00026469\n",
            "Train Epoch: 8 [90368/123872 (73%)]\tLoss: 0.409038\tLR: 0.00026457\n",
            "Train Epoch: 8 [90624/123872 (73%)]\tLoss: 0.367105\tLR: 0.00026446\n",
            "Train Epoch: 8 [90880/123872 (73%)]\tLoss: 0.489726\tLR: 0.00026435\n",
            "Train Epoch: 8 [91136/123872 (74%)]\tLoss: 0.413104\tLR: 0.00026423\n",
            "Train Epoch: 8 [91392/123872 (74%)]\tLoss: 0.436395\tLR: 0.00026412\n",
            "Train Epoch: 8 [91648/123872 (74%)]\tLoss: 0.376312\tLR: 0.00026400\n",
            "Train Epoch: 8 [91904/123872 (74%)]\tLoss: 0.427958\tLR: 0.00026389\n",
            "Train Epoch: 8 [92160/123872 (74%)]\tLoss: 0.442175\tLR: 0.00026377\n",
            "Train Epoch: 8 [92160/123872 (74%)]\tLoss: 0.442175\n",
            "Train Epoch: 8 [92416/123872 (75%)]\tLoss: 0.413783\tLR: 0.00026366\n",
            "Train Epoch: 8 [92672/123872 (75%)]\tLoss: 0.402980\tLR: 0.00026354\n",
            "Train Epoch: 8 [92928/123872 (75%)]\tLoss: 0.356503\tLR: 0.00026343\n",
            "Train Epoch: 8 [93184/123872 (75%)]\tLoss: 0.455171\tLR: 0.00026332\n",
            "Train Epoch: 8 [93440/123872 (75%)]\tLoss: 0.389781\tLR: 0.00026320\n",
            "Train Epoch: 8 [93696/123872 (76%)]\tLoss: 0.409901\tLR: 0.00026309\n",
            "Train Epoch: 8 [93952/123872 (76%)]\tLoss: 0.402787\tLR: 0.00026297\n",
            "Train Epoch: 8 [94208/123872 (76%)]\tLoss: 0.402543\tLR: 0.00026286\n",
            "Train Epoch: 8 [94464/123872 (76%)]\tLoss: 0.357065\tLR: 0.00026274\n",
            "Train Epoch: 8 [94720/123872 (76%)]\tLoss: 0.392403\tLR: 0.00026263\n",
            "Train Epoch: 8 [94720/123872 (76%)]\tLoss: 0.392403\n",
            "Train Epoch: 8 [94976/123872 (77%)]\tLoss: 0.379537\tLR: 0.00026252\n",
            "Train Epoch: 8 [95232/123872 (77%)]\tLoss: 0.422644\tLR: 0.00026240\n",
            "Train Epoch: 8 [95488/123872 (77%)]\tLoss: 0.328057\tLR: 0.00026229\n",
            "Train Epoch: 8 [95744/123872 (77%)]\tLoss: 0.402517\tLR: 0.00026217\n",
            "Train Epoch: 8 [96000/123872 (77%)]\tLoss: 0.416162\tLR: 0.00026206\n",
            "Train Epoch: 8 [96256/123872 (78%)]\tLoss: 0.379566\tLR: 0.00026194\n",
            "Train Epoch: 8 [96512/123872 (78%)]\tLoss: 0.405286\tLR: 0.00026183\n",
            "Train Epoch: 8 [96768/123872 (78%)]\tLoss: 0.370673\tLR: 0.00026172\n",
            "Train Epoch: 8 [97024/123872 (78%)]\tLoss: 0.471149\tLR: 0.00026160\n",
            "Train Epoch: 8 [97280/123872 (79%)]\tLoss: 0.399870\tLR: 0.00026149\n",
            "Train Epoch: 8 [97280/123872 (79%)]\tLoss: 0.399870\n",
            "Train Epoch: 8 [97536/123872 (79%)]\tLoss: 0.370170\tLR: 0.00026137\n",
            "Train Epoch: 8 [97792/123872 (79%)]\tLoss: 0.427403\tLR: 0.00026126\n",
            "Train Epoch: 8 [98048/123872 (79%)]\tLoss: 0.443256\tLR: 0.00026115\n",
            "Train Epoch: 8 [98304/123872 (79%)]\tLoss: 0.380263\tLR: 0.00026103\n",
            "Train Epoch: 8 [98560/123872 (80%)]\tLoss: 0.414360\tLR: 0.00026092\n",
            "Train Epoch: 8 [98816/123872 (80%)]\tLoss: 0.339797\tLR: 0.00026080\n",
            "Train Epoch: 8 [99072/123872 (80%)]\tLoss: 0.472451\tLR: 0.00026069\n",
            "Train Epoch: 8 [99328/123872 (80%)]\tLoss: 0.429600\tLR: 0.00026057\n",
            "Train Epoch: 8 [99584/123872 (80%)]\tLoss: 0.417539\tLR: 0.00026046\n",
            "Train Epoch: 8 [99840/123872 (81%)]\tLoss: 0.465095\tLR: 0.00026035\n",
            "Train Epoch: 8 [99840/123872 (81%)]\tLoss: 0.465095\n",
            "Train Epoch: 8 [100096/123872 (81%)]\tLoss: 0.407297\tLR: 0.00026023\n",
            "Train Epoch: 8 [100352/123872 (81%)]\tLoss: 0.426363\tLR: 0.00026012\n",
            "Train Epoch: 8 [100608/123872 (81%)]\tLoss: 0.402021\tLR: 0.00026000\n",
            "Train Epoch: 8 [100864/123872 (81%)]\tLoss: 0.440871\tLR: 0.00025989\n",
            "Train Epoch: 8 [101120/123872 (82%)]\tLoss: 0.438844\tLR: 0.00025978\n",
            "Train Epoch: 8 [101376/123872 (82%)]\tLoss: 0.343009\tLR: 0.00025966\n",
            "Train Epoch: 8 [101632/123872 (82%)]\tLoss: 0.406590\tLR: 0.00025955\n",
            "Train Epoch: 8 [101888/123872 (82%)]\tLoss: 0.384275\tLR: 0.00025944\n",
            "Train Epoch: 8 [102144/123872 (82%)]\tLoss: 0.383742\tLR: 0.00025932\n",
            "Train Epoch: 8 [102400/123872 (83%)]\tLoss: 0.333952\tLR: 0.00025921\n",
            "Train Epoch: 8 [102400/123872 (83%)]\tLoss: 0.333952\n",
            "Train Epoch: 8 [102656/123872 (83%)]\tLoss: 0.418301\tLR: 0.00025909\n",
            "Train Epoch: 8 [102912/123872 (83%)]\tLoss: 0.400761\tLR: 0.00025898\n",
            "Train Epoch: 8 [103168/123872 (83%)]\tLoss: 0.445699\tLR: 0.00025887\n",
            "Train Epoch: 8 [103424/123872 (83%)]\tLoss: 0.366651\tLR: 0.00025875\n",
            "Train Epoch: 8 [103680/123872 (84%)]\tLoss: 0.376367\tLR: 0.00025864\n",
            "Train Epoch: 8 [103936/123872 (84%)]\tLoss: 0.415653\tLR: 0.00025852\n",
            "Train Epoch: 8 [104192/123872 (84%)]\tLoss: 0.401187\tLR: 0.00025841\n",
            "Train Epoch: 8 [104448/123872 (84%)]\tLoss: 0.415855\tLR: 0.00025830\n",
            "Train Epoch: 8 [104704/123872 (85%)]\tLoss: 0.388936\tLR: 0.00025818\n",
            "Train Epoch: 8 [104960/123872 (85%)]\tLoss: 0.325668\tLR: 0.00025807\n",
            "Train Epoch: 8 [104960/123872 (85%)]\tLoss: 0.325668\n",
            "Train Epoch: 8 [105216/123872 (85%)]\tLoss: 0.438352\tLR: 0.00025796\n",
            "Train Epoch: 8 [105472/123872 (85%)]\tLoss: 0.438423\tLR: 0.00025784\n",
            "Train Epoch: 8 [105728/123872 (85%)]\tLoss: 0.455152\tLR: 0.00025773\n",
            "Train Epoch: 8 [105984/123872 (86%)]\tLoss: 0.423277\tLR: 0.00025761\n",
            "Train Epoch: 8 [106240/123872 (86%)]\tLoss: 0.426882\tLR: 0.00025750\n",
            "Train Epoch: 8 [106496/123872 (86%)]\tLoss: 0.367694\tLR: 0.00025739\n",
            "Train Epoch: 8 [106752/123872 (86%)]\tLoss: 0.346129\tLR: 0.00025727\n",
            "Train Epoch: 8 [107008/123872 (86%)]\tLoss: 0.517687\tLR: 0.00025716\n",
            "Train Epoch: 8 [107264/123872 (87%)]\tLoss: 0.372111\tLR: 0.00025705\n",
            "Train Epoch: 8 [107520/123872 (87%)]\tLoss: 0.451037\tLR: 0.00025693\n",
            "Train Epoch: 8 [107520/123872 (87%)]\tLoss: 0.451037\n",
            "Train Epoch: 8 [107776/123872 (87%)]\tLoss: 0.441440\tLR: 0.00025682\n",
            "Train Epoch: 8 [108032/123872 (87%)]\tLoss: 0.411893\tLR: 0.00025671\n",
            "Train Epoch: 8 [108288/123872 (87%)]\tLoss: 0.399965\tLR: 0.00025659\n",
            "Train Epoch: 8 [108544/123872 (88%)]\tLoss: 0.415143\tLR: 0.00025648\n",
            "Train Epoch: 8 [108800/123872 (88%)]\tLoss: 0.444891\tLR: 0.00025636\n",
            "Train Epoch: 8 [109056/123872 (88%)]\tLoss: 0.428462\tLR: 0.00025625\n",
            "Train Epoch: 8 [109312/123872 (88%)]\tLoss: 0.400451\tLR: 0.00025614\n",
            "Train Epoch: 8 [109568/123872 (88%)]\tLoss: 0.380057\tLR: 0.00025602\n",
            "Train Epoch: 8 [109824/123872 (89%)]\tLoss: 0.405480\tLR: 0.00025591\n",
            "Train Epoch: 8 [110080/123872 (89%)]\tLoss: 0.378437\tLR: 0.00025580\n",
            "Train Epoch: 8 [110080/123872 (89%)]\tLoss: 0.378437\n",
            "Train Epoch: 8 [110336/123872 (89%)]\tLoss: 0.387988\tLR: 0.00025568\n",
            "Train Epoch: 8 [110592/123872 (89%)]\tLoss: 0.412601\tLR: 0.00025557\n",
            "Train Epoch: 8 [110848/123872 (89%)]\tLoss: 0.387827\tLR: 0.00025546\n",
            "Train Epoch: 8 [111104/123872 (90%)]\tLoss: 0.398726\tLR: 0.00025534\n",
            "Train Epoch: 8 [111360/123872 (90%)]\tLoss: 0.454820\tLR: 0.00025523\n",
            "Train Epoch: 8 [111616/123872 (90%)]\tLoss: 0.423576\tLR: 0.00025512\n",
            "Train Epoch: 8 [111872/123872 (90%)]\tLoss: 0.431144\tLR: 0.00025500\n",
            "Train Epoch: 8 [112128/123872 (90%)]\tLoss: 0.417957\tLR: 0.00025489\n",
            "Train Epoch: 8 [112384/123872 (91%)]\tLoss: 0.385162\tLR: 0.00025478\n",
            "Train Epoch: 8 [112640/123872 (91%)]\tLoss: 0.368526\tLR: 0.00025466\n",
            "Train Epoch: 8 [112640/123872 (91%)]\tLoss: 0.368526\n",
            "Train Epoch: 8 [112896/123872 (91%)]\tLoss: 0.432113\tLR: 0.00025455\n",
            "Train Epoch: 8 [113152/123872 (91%)]\tLoss: 0.410287\tLR: 0.00025444\n",
            "Train Epoch: 8 [113408/123872 (92%)]\tLoss: 0.500057\tLR: 0.00025432\n",
            "Train Epoch: 8 [113664/123872 (92%)]\tLoss: 0.450245\tLR: 0.00025421\n",
            "Train Epoch: 8 [113920/123872 (92%)]\tLoss: 0.416081\tLR: 0.00025410\n",
            "Train Epoch: 8 [114176/123872 (92%)]\tLoss: 0.409148\tLR: 0.00025398\n",
            "Train Epoch: 8 [114432/123872 (92%)]\tLoss: 0.414211\tLR: 0.00025387\n",
            "Train Epoch: 8 [114688/123872 (93%)]\tLoss: 0.410734\tLR: 0.00025376\n",
            "Train Epoch: 8 [114944/123872 (93%)]\tLoss: 0.466398\tLR: 0.00025364\n",
            "Train Epoch: 8 [115200/123872 (93%)]\tLoss: 0.388544\tLR: 0.00025353\n",
            "Train Epoch: 8 [115200/123872 (93%)]\tLoss: 0.388544\n",
            "Train Epoch: 8 [115456/123872 (93%)]\tLoss: 0.429166\tLR: 0.00025342\n",
            "Train Epoch: 8 [115712/123872 (93%)]\tLoss: 0.434224\tLR: 0.00025330\n",
            "Train Epoch: 8 [115968/123872 (94%)]\tLoss: 0.430907\tLR: 0.00025319\n",
            "Train Epoch: 8 [116224/123872 (94%)]\tLoss: 0.374250\tLR: 0.00025308\n",
            "Train Epoch: 8 [116480/123872 (94%)]\tLoss: 0.425074\tLR: 0.00025297\n",
            "Train Epoch: 8 [116736/123872 (94%)]\tLoss: 0.422031\tLR: 0.00025285\n",
            "Train Epoch: 8 [116992/123872 (94%)]\tLoss: 0.394200\tLR: 0.00025274\n",
            "Train Epoch: 8 [117248/123872 (95%)]\tLoss: 0.420052\tLR: 0.00025263\n",
            "Train Epoch: 8 [117504/123872 (95%)]\tLoss: 0.437887\tLR: 0.00025251\n",
            "Train Epoch: 8 [117760/123872 (95%)]\tLoss: 0.437218\tLR: 0.00025240\n",
            "Train Epoch: 8 [117760/123872 (95%)]\tLoss: 0.437218\n",
            "Train Epoch: 8 [118016/123872 (95%)]\tLoss: 0.429476\tLR: 0.00025229\n",
            "Train Epoch: 8 [118272/123872 (95%)]\tLoss: 0.440757\tLR: 0.00025217\n",
            "Train Epoch: 8 [118528/123872 (96%)]\tLoss: 0.379396\tLR: 0.00025206\n",
            "Train Epoch: 8 [118784/123872 (96%)]\tLoss: 0.424063\tLR: 0.00025195\n",
            "Train Epoch: 8 [119040/123872 (96%)]\tLoss: 0.426599\tLR: 0.00025183\n",
            "Train Epoch: 8 [119296/123872 (96%)]\tLoss: 0.384972\tLR: 0.00025172\n",
            "Train Epoch: 8 [119552/123872 (96%)]\tLoss: 0.358329\tLR: 0.00025161\n",
            "Train Epoch: 8 [119808/123872 (97%)]\tLoss: 0.479253\tLR: 0.00025150\n",
            "Train Epoch: 8 [120064/123872 (97%)]\tLoss: 0.390771\tLR: 0.00025138\n",
            "Train Epoch: 8 [120320/123872 (97%)]\tLoss: 0.423268\tLR: 0.00025127\n",
            "Train Epoch: 8 [120320/123872 (97%)]\tLoss: 0.423268\n",
            "Train Epoch: 8 [120576/123872 (97%)]\tLoss: 0.391744\tLR: 0.00025116\n",
            "Train Epoch: 8 [120832/123872 (98%)]\tLoss: 0.459091\tLR: 0.00025104\n",
            "Train Epoch: 8 [121088/123872 (98%)]\tLoss: 0.391254\tLR: 0.00025093\n",
            "Train Epoch: 8 [121344/123872 (98%)]\tLoss: 0.350199\tLR: 0.00025082\n",
            "Train Epoch: 8 [121600/123872 (98%)]\tLoss: 0.429523\tLR: 0.00025071\n",
            "Train Epoch: 8 [121856/123872 (98%)]\tLoss: 0.377780\tLR: 0.00025059\n",
            "Train Epoch: 8 [122112/123872 (99%)]\tLoss: 0.382767\tLR: 0.00025048\n",
            "Train Epoch: 8 [122368/123872 (99%)]\tLoss: 0.380657\tLR: 0.00025037\n",
            "Train Epoch: 8 [122624/123872 (99%)]\tLoss: 0.477427\tLR: 0.00025026\n",
            "Train Epoch: 8 [122880/123872 (99%)]\tLoss: 0.426159\tLR: 0.00025014\n",
            "Train Epoch: 8 [122880/123872 (99%)]\tLoss: 0.426159\n",
            "Train Epoch: 8 [123136/123872 (99%)]\tLoss: 0.398261\tLR: 0.00025003\n",
            "Train Epoch: 8 [123392/123872 (100%)]\tLoss: 0.395736\tLR: 0.00024992\n",
            "Train Epoch: 8 [108192/123872 (100%)]\tLoss: 0.426167\tLR: 0.00024980\n",
            "\n",
            "Test set: Average loss: 0.0016, Accuracy: 25036/30970 (80.84%)\n",
            "\n",
            "Train Epoch: 9 [0/123872 (0%)]\tLoss: 0.365735\tLR: 0.00024969\n",
            "Train Epoch: 9 [0/123872 (0%)]\tLoss: 0.365735\n",
            "Train Epoch: 9 [256/123872 (0%)]\tLoss: 0.370344\tLR: 0.00024958\n",
            "Train Epoch: 9 [512/123872 (0%)]\tLoss: 0.382171\tLR: 0.00024947\n",
            "Train Epoch: 9 [768/123872 (1%)]\tLoss: 0.433058\tLR: 0.00024935\n",
            "Train Epoch: 9 [1024/123872 (1%)]\tLoss: 0.352097\tLR: 0.00024924\n",
            "Train Epoch: 9 [1280/123872 (1%)]\tLoss: 0.408092\tLR: 0.00024913\n",
            "Train Epoch: 9 [1536/123872 (1%)]\tLoss: 0.434729\tLR: 0.00024902\n",
            "Train Epoch: 9 [1792/123872 (1%)]\tLoss: 0.373891\tLR: 0.00024890\n",
            "Train Epoch: 9 [2048/123872 (2%)]\tLoss: 0.380044\tLR: 0.00024879\n",
            "Train Epoch: 9 [2304/123872 (2%)]\tLoss: 0.381550\tLR: 0.00024868\n",
            "Train Epoch: 9 [2560/123872 (2%)]\tLoss: 0.376716\tLR: 0.00024857\n",
            "Train Epoch: 9 [2560/123872 (2%)]\tLoss: 0.376716\n",
            "Train Epoch: 9 [2816/123872 (2%)]\tLoss: 0.438366\tLR: 0.00024845\n",
            "Train Epoch: 9 [3072/123872 (2%)]\tLoss: 0.389950\tLR: 0.00024834\n",
            "Train Epoch: 9 [3328/123872 (3%)]\tLoss: 0.440361\tLR: 0.00024823\n",
            "Train Epoch: 9 [3584/123872 (3%)]\tLoss: 0.359041\tLR: 0.00024812\n",
            "Train Epoch: 9 [3840/123872 (3%)]\tLoss: 0.380878\tLR: 0.00024800\n",
            "Train Epoch: 9 [4096/123872 (3%)]\tLoss: 0.413597\tLR: 0.00024789\n",
            "Train Epoch: 9 [4352/123872 (4%)]\tLoss: 0.384631\tLR: 0.00024778\n",
            "Train Epoch: 9 [4608/123872 (4%)]\tLoss: 0.399352\tLR: 0.00024767\n",
            "Train Epoch: 9 [4864/123872 (4%)]\tLoss: 0.421682\tLR: 0.00024755\n",
            "Train Epoch: 9 [5120/123872 (4%)]\tLoss: 0.419551\tLR: 0.00024744\n",
            "Train Epoch: 9 [5120/123872 (4%)]\tLoss: 0.419551\n",
            "Train Epoch: 9 [5376/123872 (4%)]\tLoss: 0.359954\tLR: 0.00024733\n",
            "Train Epoch: 9 [5632/123872 (5%)]\tLoss: 0.383940\tLR: 0.00024722\n",
            "Train Epoch: 9 [5888/123872 (5%)]\tLoss: 0.447680\tLR: 0.00024710\n",
            "Train Epoch: 9 [6144/123872 (5%)]\tLoss: 0.425657\tLR: 0.00024699\n",
            "Train Epoch: 9 [6400/123872 (5%)]\tLoss: 0.415199\tLR: 0.00024688\n",
            "Train Epoch: 9 [6656/123872 (5%)]\tLoss: 0.429490\tLR: 0.00024677\n",
            "Train Epoch: 9 [6912/123872 (6%)]\tLoss: 0.393458\tLR: 0.00024666\n",
            "Train Epoch: 9 [7168/123872 (6%)]\tLoss: 0.400448\tLR: 0.00024654\n",
            "Train Epoch: 9 [7424/123872 (6%)]\tLoss: 0.366951\tLR: 0.00024643\n",
            "Train Epoch: 9 [7680/123872 (6%)]\tLoss: 0.372324\tLR: 0.00024632\n",
            "Train Epoch: 9 [7680/123872 (6%)]\tLoss: 0.372324\n",
            "Train Epoch: 9 [7936/123872 (6%)]\tLoss: 0.410907\tLR: 0.00024621\n",
            "Train Epoch: 9 [8192/123872 (7%)]\tLoss: 0.422414\tLR: 0.00024609\n",
            "Train Epoch: 9 [8448/123872 (7%)]\tLoss: 0.445248\tLR: 0.00024598\n",
            "Train Epoch: 9 [8704/123872 (7%)]\tLoss: 0.401333\tLR: 0.00024587\n",
            "Train Epoch: 9 [8960/123872 (7%)]\tLoss: 0.450097\tLR: 0.00024576\n",
            "Train Epoch: 9 [9216/123872 (7%)]\tLoss: 0.382046\tLR: 0.00024565\n",
            "Train Epoch: 9 [9472/123872 (8%)]\tLoss: 0.483736\tLR: 0.00024553\n",
            "Train Epoch: 9 [9728/123872 (8%)]\tLoss: 0.444263\tLR: 0.00024542\n",
            "Train Epoch: 9 [9984/123872 (8%)]\tLoss: 0.416898\tLR: 0.00024531\n",
            "Train Epoch: 9 [10240/123872 (8%)]\tLoss: 0.420146\tLR: 0.00024520\n",
            "Train Epoch: 9 [10240/123872 (8%)]\tLoss: 0.420146\n",
            "Train Epoch: 9 [10496/123872 (8%)]\tLoss: 0.346522\tLR: 0.00024509\n",
            "Train Epoch: 9 [10752/123872 (9%)]\tLoss: 0.416233\tLR: 0.00024497\n",
            "Train Epoch: 9 [11008/123872 (9%)]\tLoss: 0.351421\tLR: 0.00024486\n",
            "Train Epoch: 9 [11264/123872 (9%)]\tLoss: 0.350323\tLR: 0.00024475\n",
            "Train Epoch: 9 [11520/123872 (9%)]\tLoss: 0.347796\tLR: 0.00024464\n",
            "Train Epoch: 9 [11776/123872 (10%)]\tLoss: 0.447983\tLR: 0.00024453\n",
            "Train Epoch: 9 [12032/123872 (10%)]\tLoss: 0.398632\tLR: 0.00024441\n",
            "Train Epoch: 9 [12288/123872 (10%)]\tLoss: 0.399600\tLR: 0.00024430\n",
            "Train Epoch: 9 [12544/123872 (10%)]\tLoss: 0.365758\tLR: 0.00024419\n",
            "Train Epoch: 9 [12800/123872 (10%)]\tLoss: 0.406765\tLR: 0.00024408\n",
            "Train Epoch: 9 [12800/123872 (10%)]\tLoss: 0.406765\n",
            "Train Epoch: 9 [13056/123872 (11%)]\tLoss: 0.504175\tLR: 0.00024397\n",
            "Train Epoch: 9 [13312/123872 (11%)]\tLoss: 0.360827\tLR: 0.00024385\n",
            "Train Epoch: 9 [13568/123872 (11%)]\tLoss: 0.413652\tLR: 0.00024374\n",
            "Train Epoch: 9 [13824/123872 (11%)]\tLoss: 0.380297\tLR: 0.00024363\n",
            "Train Epoch: 9 [14080/123872 (11%)]\tLoss: 0.472823\tLR: 0.00024352\n",
            "Train Epoch: 9 [14336/123872 (12%)]\tLoss: 0.447789\tLR: 0.00024341\n",
            "Train Epoch: 9 [14592/123872 (12%)]\tLoss: 0.410093\tLR: 0.00024330\n",
            "Train Epoch: 9 [14848/123872 (12%)]\tLoss: 0.362946\tLR: 0.00024318\n",
            "Train Epoch: 9 [15104/123872 (12%)]\tLoss: 0.414524\tLR: 0.00024307\n",
            "Train Epoch: 9 [15360/123872 (12%)]\tLoss: 0.428577\tLR: 0.00024296\n",
            "Train Epoch: 9 [15360/123872 (12%)]\tLoss: 0.428577\n",
            "Train Epoch: 9 [15616/123872 (13%)]\tLoss: 0.388645\tLR: 0.00024285\n",
            "Train Epoch: 9 [15872/123872 (13%)]\tLoss: 0.433013\tLR: 0.00024274\n",
            "Train Epoch: 9 [16128/123872 (13%)]\tLoss: 0.411116\tLR: 0.00024262\n",
            "Train Epoch: 9 [16384/123872 (13%)]\tLoss: 0.376958\tLR: 0.00024251\n",
            "Train Epoch: 9 [16640/123872 (13%)]\tLoss: 0.390730\tLR: 0.00024240\n",
            "Train Epoch: 9 [16896/123872 (14%)]\tLoss: 0.398656\tLR: 0.00024229\n",
            "Train Epoch: 9 [17152/123872 (14%)]\tLoss: 0.420681\tLR: 0.00024218\n",
            "Train Epoch: 9 [17408/123872 (14%)]\tLoss: 0.442896\tLR: 0.00024207\n",
            "Train Epoch: 9 [17664/123872 (14%)]\tLoss: 0.383465\tLR: 0.00024196\n",
            "Train Epoch: 9 [17920/123872 (14%)]\tLoss: 0.378092\tLR: 0.00024184\n",
            "Train Epoch: 9 [17920/123872 (14%)]\tLoss: 0.378092\n",
            "Train Epoch: 9 [18176/123872 (15%)]\tLoss: 0.379858\tLR: 0.00024173\n",
            "Train Epoch: 9 [18432/123872 (15%)]\tLoss: 0.441594\tLR: 0.00024162\n",
            "Train Epoch: 9 [18688/123872 (15%)]\tLoss: 0.431068\tLR: 0.00024151\n",
            "Train Epoch: 9 [18944/123872 (15%)]\tLoss: 0.421655\tLR: 0.00024140\n",
            "Train Epoch: 9 [19200/123872 (15%)]\tLoss: 0.446754\tLR: 0.00024129\n",
            "Train Epoch: 9 [19456/123872 (16%)]\tLoss: 0.388055\tLR: 0.00024117\n",
            "Train Epoch: 9 [19712/123872 (16%)]\tLoss: 0.450544\tLR: 0.00024106\n",
            "Train Epoch: 9 [19968/123872 (16%)]\tLoss: 0.410561\tLR: 0.00024095\n",
            "Train Epoch: 9 [20224/123872 (16%)]\tLoss: 0.405623\tLR: 0.00024084\n",
            "Train Epoch: 9 [20480/123872 (17%)]\tLoss: 0.517587\tLR: 0.00024073\n",
            "Train Epoch: 9 [20480/123872 (17%)]\tLoss: 0.517587\n",
            "Train Epoch: 9 [20736/123872 (17%)]\tLoss: 0.387291\tLR: 0.00024062\n",
            "Train Epoch: 9 [20992/123872 (17%)]\tLoss: 0.433227\tLR: 0.00024051\n",
            "Train Epoch: 9 [21248/123872 (17%)]\tLoss: 0.433960\tLR: 0.00024040\n",
            "Train Epoch: 9 [21504/123872 (17%)]\tLoss: 0.433582\tLR: 0.00024028\n",
            "Train Epoch: 9 [21760/123872 (18%)]\tLoss: 0.445837\tLR: 0.00024017\n",
            "Train Epoch: 9 [22016/123872 (18%)]\tLoss: 0.358630\tLR: 0.00024006\n",
            "Train Epoch: 9 [22272/123872 (18%)]\tLoss: 0.367374\tLR: 0.00023995\n",
            "Train Epoch: 9 [22528/123872 (18%)]\tLoss: 0.490090\tLR: 0.00023984\n",
            "Train Epoch: 9 [22784/123872 (18%)]\tLoss: 0.390446\tLR: 0.00023973\n",
            "Train Epoch: 9 [23040/123872 (19%)]\tLoss: 0.420552\tLR: 0.00023962\n",
            "Train Epoch: 9 [23040/123872 (19%)]\tLoss: 0.420552\n",
            "Train Epoch: 9 [23296/123872 (19%)]\tLoss: 0.432508\tLR: 0.00023951\n",
            "Train Epoch: 9 [23552/123872 (19%)]\tLoss: 0.435501\tLR: 0.00023939\n",
            "Train Epoch: 9 [23808/123872 (19%)]\tLoss: 0.389644\tLR: 0.00023928\n",
            "Train Epoch: 9 [24064/123872 (19%)]\tLoss: 0.439429\tLR: 0.00023917\n",
            "Train Epoch: 9 [24320/123872 (20%)]\tLoss: 0.445396\tLR: 0.00023906\n",
            "Train Epoch: 9 [24576/123872 (20%)]\tLoss: 0.443311\tLR: 0.00023895\n",
            "Train Epoch: 9 [24832/123872 (20%)]\tLoss: 0.336820\tLR: 0.00023884\n",
            "Train Epoch: 9 [25088/123872 (20%)]\tLoss: 0.381545\tLR: 0.00023873\n",
            "Train Epoch: 9 [25344/123872 (20%)]\tLoss: 0.488266\tLR: 0.00023862\n",
            "Train Epoch: 9 [25600/123872 (21%)]\tLoss: 0.416921\tLR: 0.00023851\n",
            "Train Epoch: 9 [25600/123872 (21%)]\tLoss: 0.416921\n",
            "Train Epoch: 9 [25856/123872 (21%)]\tLoss: 0.391268\tLR: 0.00023839\n",
            "Train Epoch: 9 [26112/123872 (21%)]\tLoss: 0.373987\tLR: 0.00023828\n",
            "Train Epoch: 9 [26368/123872 (21%)]\tLoss: 0.450386\tLR: 0.00023817\n",
            "Train Epoch: 9 [26624/123872 (21%)]\tLoss: 0.412798\tLR: 0.00023806\n",
            "Train Epoch: 9 [26880/123872 (22%)]\tLoss: 0.422355\tLR: 0.00023795\n",
            "Train Epoch: 9 [27136/123872 (22%)]\tLoss: 0.425782\tLR: 0.00023784\n",
            "Train Epoch: 9 [27392/123872 (22%)]\tLoss: 0.380863\tLR: 0.00023773\n",
            "Train Epoch: 9 [27648/123872 (22%)]\tLoss: 0.419449\tLR: 0.00023762\n",
            "Train Epoch: 9 [27904/123872 (23%)]\tLoss: 0.414758\tLR: 0.00023751\n",
            "Train Epoch: 9 [28160/123872 (23%)]\tLoss: 0.412986\tLR: 0.00023740\n",
            "Train Epoch: 9 [28160/123872 (23%)]\tLoss: 0.412986\n",
            "Train Epoch: 9 [28416/123872 (23%)]\tLoss: 0.441446\tLR: 0.00023729\n",
            "Train Epoch: 9 [28672/123872 (23%)]\tLoss: 0.399768\tLR: 0.00023717\n",
            "Train Epoch: 9 [28928/123872 (23%)]\tLoss: 0.412114\tLR: 0.00023706\n",
            "Train Epoch: 9 [29184/123872 (24%)]\tLoss: 0.407146\tLR: 0.00023695\n",
            "Train Epoch: 9 [29440/123872 (24%)]\tLoss: 0.395848\tLR: 0.00023684\n",
            "Train Epoch: 9 [29696/123872 (24%)]\tLoss: 0.392763\tLR: 0.00023673\n",
            "Train Epoch: 9 [29952/123872 (24%)]\tLoss: 0.408311\tLR: 0.00023662\n",
            "Train Epoch: 9 [30208/123872 (24%)]\tLoss: 0.355429\tLR: 0.00023651\n",
            "Train Epoch: 9 [30464/123872 (25%)]\tLoss: 0.382153\tLR: 0.00023640\n",
            "Train Epoch: 9 [30720/123872 (25%)]\tLoss: 0.357437\tLR: 0.00023629\n",
            "Train Epoch: 9 [30720/123872 (25%)]\tLoss: 0.357437\n",
            "Train Epoch: 9 [30976/123872 (25%)]\tLoss: 0.450146\tLR: 0.00023618\n",
            "Train Epoch: 9 [31232/123872 (25%)]\tLoss: 0.409698\tLR: 0.00023607\n",
            "Train Epoch: 9 [31488/123872 (25%)]\tLoss: 0.427492\tLR: 0.00023596\n",
            "Train Epoch: 9 [31744/123872 (26%)]\tLoss: 0.343561\tLR: 0.00023585\n",
            "Train Epoch: 9 [32000/123872 (26%)]\tLoss: 0.449054\tLR: 0.00023574\n",
            "Train Epoch: 9 [32256/123872 (26%)]\tLoss: 0.382470\tLR: 0.00023562\n",
            "Train Epoch: 9 [32512/123872 (26%)]\tLoss: 0.358264\tLR: 0.00023551\n",
            "Train Epoch: 9 [32768/123872 (26%)]\tLoss: 0.369457\tLR: 0.00023540\n",
            "Train Epoch: 9 [33024/123872 (27%)]\tLoss: 0.424044\tLR: 0.00023529\n",
            "Train Epoch: 9 [33280/123872 (27%)]\tLoss: 0.413144\tLR: 0.00023518\n",
            "Train Epoch: 9 [33280/123872 (27%)]\tLoss: 0.413144\n",
            "Train Epoch: 9 [33536/123872 (27%)]\tLoss: 0.346725\tLR: 0.00023507\n",
            "Train Epoch: 9 [33792/123872 (27%)]\tLoss: 0.483419\tLR: 0.00023496\n",
            "Train Epoch: 9 [34048/123872 (27%)]\tLoss: 0.424539\tLR: 0.00023485\n",
            "Train Epoch: 9 [34304/123872 (28%)]\tLoss: 0.394839\tLR: 0.00023474\n",
            "Train Epoch: 9 [34560/123872 (28%)]\tLoss: 0.450046\tLR: 0.00023463\n",
            "Train Epoch: 9 [34816/123872 (28%)]\tLoss: 0.457220\tLR: 0.00023452\n",
            "Train Epoch: 9 [35072/123872 (28%)]\tLoss: 0.375197\tLR: 0.00023441\n",
            "Train Epoch: 9 [35328/123872 (29%)]\tLoss: 0.445905\tLR: 0.00023430\n",
            "Train Epoch: 9 [35584/123872 (29%)]\tLoss: 0.415812\tLR: 0.00023419\n",
            "Train Epoch: 9 [35840/123872 (29%)]\tLoss: 0.384366\tLR: 0.00023408\n",
            "Train Epoch: 9 [35840/123872 (29%)]\tLoss: 0.384366\n",
            "Train Epoch: 9 [36096/123872 (29%)]\tLoss: 0.370287\tLR: 0.00023397\n",
            "Train Epoch: 9 [36352/123872 (29%)]\tLoss: 0.445123\tLR: 0.00023386\n",
            "Train Epoch: 9 [36608/123872 (30%)]\tLoss: 0.385899\tLR: 0.00023375\n",
            "Train Epoch: 9 [36864/123872 (30%)]\tLoss: 0.367137\tLR: 0.00023364\n",
            "Train Epoch: 9 [37120/123872 (30%)]\tLoss: 0.386588\tLR: 0.00023353\n",
            "Train Epoch: 9 [37376/123872 (30%)]\tLoss: 0.372302\tLR: 0.00023342\n",
            "Train Epoch: 9 [37632/123872 (30%)]\tLoss: 0.363046\tLR: 0.00023331\n",
            "Train Epoch: 9 [37888/123872 (31%)]\tLoss: 0.427486\tLR: 0.00023320\n",
            "Train Epoch: 9 [38144/123872 (31%)]\tLoss: 0.415916\tLR: 0.00023309\n",
            "Train Epoch: 9 [38400/123872 (31%)]\tLoss: 0.368847\tLR: 0.00023298\n",
            "Train Epoch: 9 [38400/123872 (31%)]\tLoss: 0.368847\n",
            "Train Epoch: 9 [38656/123872 (31%)]\tLoss: 0.526722\tLR: 0.00023287\n",
            "Train Epoch: 9 [38912/123872 (31%)]\tLoss: 0.410010\tLR: 0.00023276\n",
            "Train Epoch: 9 [39168/123872 (32%)]\tLoss: 0.408784\tLR: 0.00023265\n",
            "Train Epoch: 9 [39424/123872 (32%)]\tLoss: 0.363612\tLR: 0.00023254\n",
            "Train Epoch: 9 [39680/123872 (32%)]\tLoss: 0.367440\tLR: 0.00023243\n",
            "Train Epoch: 9 [39936/123872 (32%)]\tLoss: 0.343217\tLR: 0.00023232\n",
            "Train Epoch: 9 [40192/123872 (32%)]\tLoss: 0.388571\tLR: 0.00023221\n",
            "Train Epoch: 9 [40448/123872 (33%)]\tLoss: 0.428492\tLR: 0.00023210\n",
            "Train Epoch: 9 [40704/123872 (33%)]\tLoss: 0.419027\tLR: 0.00023199\n",
            "Train Epoch: 9 [40960/123872 (33%)]\tLoss: 0.389084\tLR: 0.00023188\n",
            "Train Epoch: 9 [40960/123872 (33%)]\tLoss: 0.389084\n",
            "Train Epoch: 9 [41216/123872 (33%)]\tLoss: 0.328470\tLR: 0.00023177\n",
            "Train Epoch: 9 [41472/123872 (33%)]\tLoss: 0.405064\tLR: 0.00023166\n",
            "Train Epoch: 9 [41728/123872 (34%)]\tLoss: 0.393191\tLR: 0.00023155\n",
            "Train Epoch: 9 [41984/123872 (34%)]\tLoss: 0.429379\tLR: 0.00023144\n",
            "Train Epoch: 9 [42240/123872 (34%)]\tLoss: 0.423679\tLR: 0.00023133\n",
            "Train Epoch: 9 [42496/123872 (34%)]\tLoss: 0.357860\tLR: 0.00023122\n",
            "Train Epoch: 9 [42752/123872 (35%)]\tLoss: 0.418214\tLR: 0.00023111\n",
            "Train Epoch: 9 [43008/123872 (35%)]\tLoss: 0.352375\tLR: 0.00023100\n",
            "Train Epoch: 9 [43264/123872 (35%)]\tLoss: 0.385270\tLR: 0.00023089\n",
            "Train Epoch: 9 [43520/123872 (35%)]\tLoss: 0.408388\tLR: 0.00023078\n",
            "Train Epoch: 9 [43520/123872 (35%)]\tLoss: 0.408388\n",
            "Train Epoch: 9 [43776/123872 (35%)]\tLoss: 0.424687\tLR: 0.00023067\n",
            "Train Epoch: 9 [44032/123872 (36%)]\tLoss: 0.397648\tLR: 0.00023056\n",
            "Train Epoch: 9 [44288/123872 (36%)]\tLoss: 0.407695\tLR: 0.00023045\n",
            "Train Epoch: 9 [44544/123872 (36%)]\tLoss: 0.414699\tLR: 0.00023034\n",
            "Train Epoch: 9 [44800/123872 (36%)]\tLoss: 0.401575\tLR: 0.00023023\n",
            "Train Epoch: 9 [45056/123872 (36%)]\tLoss: 0.411594\tLR: 0.00023012\n",
            "Train Epoch: 9 [45312/123872 (37%)]\tLoss: 0.375662\tLR: 0.00023001\n",
            "Train Epoch: 9 [45568/123872 (37%)]\tLoss: 0.453110\tLR: 0.00022990\n",
            "Train Epoch: 9 [45824/123872 (37%)]\tLoss: 0.456554\tLR: 0.00022979\n",
            "Train Epoch: 9 [46080/123872 (37%)]\tLoss: 0.431727\tLR: 0.00022968\n",
            "Train Epoch: 9 [46080/123872 (37%)]\tLoss: 0.431727\n",
            "Train Epoch: 9 [46336/123872 (37%)]\tLoss: 0.423729\tLR: 0.00022957\n",
            "Train Epoch: 9 [46592/123872 (38%)]\tLoss: 0.438729\tLR: 0.00022946\n",
            "Train Epoch: 9 [46848/123872 (38%)]\tLoss: 0.386417\tLR: 0.00022935\n",
            "Train Epoch: 9 [47104/123872 (38%)]\tLoss: 0.386941\tLR: 0.00022924\n",
            "Train Epoch: 9 [47360/123872 (38%)]\tLoss: 0.427280\tLR: 0.00022913\n",
            "Train Epoch: 9 [47616/123872 (38%)]\tLoss: 0.452597\tLR: 0.00022902\n",
            "Train Epoch: 9 [47872/123872 (39%)]\tLoss: 0.403991\tLR: 0.00022891\n",
            "Train Epoch: 9 [48128/123872 (39%)]\tLoss: 0.384198\tLR: 0.00022881\n",
            "Train Epoch: 9 [48384/123872 (39%)]\tLoss: 0.412223\tLR: 0.00022870\n",
            "Train Epoch: 9 [48640/123872 (39%)]\tLoss: 0.478019\tLR: 0.00022859\n",
            "Train Epoch: 9 [48640/123872 (39%)]\tLoss: 0.478019\n",
            "Train Epoch: 9 [48896/123872 (39%)]\tLoss: 0.363838\tLR: 0.00022848\n",
            "Train Epoch: 9 [49152/123872 (40%)]\tLoss: 0.397613\tLR: 0.00022837\n",
            "Train Epoch: 9 [49408/123872 (40%)]\tLoss: 0.344290\tLR: 0.00022826\n",
            "Train Epoch: 9 [49664/123872 (40%)]\tLoss: 0.434281\tLR: 0.00022815\n",
            "Train Epoch: 9 [49920/123872 (40%)]\tLoss: 0.399923\tLR: 0.00022804\n",
            "Train Epoch: 9 [50176/123872 (40%)]\tLoss: 0.397806\tLR: 0.00022793\n",
            "Train Epoch: 9 [50432/123872 (41%)]\tLoss: 0.365798\tLR: 0.00022782\n",
            "Train Epoch: 9 [50688/123872 (41%)]\tLoss: 0.349871\tLR: 0.00022771\n",
            "Train Epoch: 9 [50944/123872 (41%)]\tLoss: 0.453768\tLR: 0.00022760\n",
            "Train Epoch: 9 [51200/123872 (41%)]\tLoss: 0.441257\tLR: 0.00022749\n",
            "Train Epoch: 9 [51200/123872 (41%)]\tLoss: 0.441257\n",
            "Train Epoch: 9 [51456/123872 (42%)]\tLoss: 0.367280\tLR: 0.00022738\n",
            "Train Epoch: 9 [51712/123872 (42%)]\tLoss: 0.448973\tLR: 0.00022728\n",
            "Train Epoch: 9 [51968/123872 (42%)]\tLoss: 0.410067\tLR: 0.00022717\n",
            "Train Epoch: 9 [52224/123872 (42%)]\tLoss: 0.446859\tLR: 0.00022706\n",
            "Train Epoch: 9 [52480/123872 (42%)]\tLoss: 0.432640\tLR: 0.00022695\n",
            "Train Epoch: 9 [52736/123872 (43%)]\tLoss: 0.433673\tLR: 0.00022684\n",
            "Train Epoch: 9 [52992/123872 (43%)]\tLoss: 0.395266\tLR: 0.00022673\n",
            "Train Epoch: 9 [53248/123872 (43%)]\tLoss: 0.413069\tLR: 0.00022662\n",
            "Train Epoch: 9 [53504/123872 (43%)]\tLoss: 0.417807\tLR: 0.00022651\n",
            "Train Epoch: 9 [53760/123872 (43%)]\tLoss: 0.427827\tLR: 0.00022640\n",
            "Train Epoch: 9 [53760/123872 (43%)]\tLoss: 0.427827\n",
            "Train Epoch: 9 [54016/123872 (44%)]\tLoss: 0.469756\tLR: 0.00022629\n",
            "Train Epoch: 9 [54272/123872 (44%)]\tLoss: 0.381605\tLR: 0.00022619\n",
            "Train Epoch: 9 [54528/123872 (44%)]\tLoss: 0.417934\tLR: 0.00022608\n",
            "Train Epoch: 9 [54784/123872 (44%)]\tLoss: 0.401347\tLR: 0.00022597\n",
            "Train Epoch: 9 [55040/123872 (44%)]\tLoss: 0.401317\tLR: 0.00022586\n",
            "Train Epoch: 9 [55296/123872 (45%)]\tLoss: 0.387002\tLR: 0.00022575\n",
            "Train Epoch: 9 [55552/123872 (45%)]\tLoss: 0.405412\tLR: 0.00022564\n",
            "Train Epoch: 9 [55808/123872 (45%)]\tLoss: 0.414138\tLR: 0.00022553\n",
            "Train Epoch: 9 [56064/123872 (45%)]\tLoss: 0.396364\tLR: 0.00022542\n",
            "Train Epoch: 9 [56320/123872 (45%)]\tLoss: 0.401070\tLR: 0.00022531\n",
            "Train Epoch: 9 [56320/123872 (45%)]\tLoss: 0.401070\n",
            "Train Epoch: 9 [56576/123872 (46%)]\tLoss: 0.397295\tLR: 0.00022521\n",
            "Train Epoch: 9 [56832/123872 (46%)]\tLoss: 0.407378\tLR: 0.00022510\n",
            "Train Epoch: 9 [57088/123872 (46%)]\tLoss: 0.381927\tLR: 0.00022499\n",
            "Train Epoch: 9 [57344/123872 (46%)]\tLoss: 0.396040\tLR: 0.00022488\n",
            "Train Epoch: 9 [57600/123872 (46%)]\tLoss: 0.427773\tLR: 0.00022477\n",
            "Train Epoch: 9 [57856/123872 (47%)]\tLoss: 0.382154\tLR: 0.00022466\n",
            "Train Epoch: 9 [58112/123872 (47%)]\tLoss: 0.439521\tLR: 0.00022455\n",
            "Train Epoch: 9 [58368/123872 (47%)]\tLoss: 0.387519\tLR: 0.00022444\n",
            "Train Epoch: 9 [58624/123872 (47%)]\tLoss: 0.406115\tLR: 0.00022434\n",
            "Train Epoch: 9 [58880/123872 (48%)]\tLoss: 0.430257\tLR: 0.00022423\n",
            "Train Epoch: 9 [58880/123872 (48%)]\tLoss: 0.430257\n",
            "Train Epoch: 9 [59136/123872 (48%)]\tLoss: 0.375411\tLR: 0.00022412\n",
            "Train Epoch: 9 [59392/123872 (48%)]\tLoss: 0.401207\tLR: 0.00022401\n",
            "Train Epoch: 9 [59648/123872 (48%)]\tLoss: 0.413135\tLR: 0.00022390\n",
            "Train Epoch: 9 [59904/123872 (48%)]\tLoss: 0.434074\tLR: 0.00022379\n",
            "Train Epoch: 9 [60160/123872 (49%)]\tLoss: 0.385210\tLR: 0.00022368\n",
            "Train Epoch: 9 [60416/123872 (49%)]\tLoss: 0.414243\tLR: 0.00022358\n",
            "Train Epoch: 9 [60672/123872 (49%)]\tLoss: 0.363352\tLR: 0.00022347\n",
            "Train Epoch: 9 [60928/123872 (49%)]\tLoss: 0.465511\tLR: 0.00022336\n",
            "Train Epoch: 9 [61184/123872 (49%)]\tLoss: 0.415650\tLR: 0.00022325\n",
            "Train Epoch: 9 [61440/123872 (50%)]\tLoss: 0.400673\tLR: 0.00022314\n",
            "Train Epoch: 9 [61440/123872 (50%)]\tLoss: 0.400673\n",
            "Train Epoch: 9 [61696/123872 (50%)]\tLoss: 0.405869\tLR: 0.00022303\n",
            "Train Epoch: 9 [61952/123872 (50%)]\tLoss: 0.400826\tLR: 0.00022293\n",
            "Train Epoch: 9 [62208/123872 (50%)]\tLoss: 0.462120\tLR: 0.00022282\n",
            "Train Epoch: 9 [62464/123872 (50%)]\tLoss: 0.345835\tLR: 0.00022271\n",
            "Train Epoch: 9 [62720/123872 (51%)]\tLoss: 0.390841\tLR: 0.00022260\n",
            "Train Epoch: 9 [62976/123872 (51%)]\tLoss: 0.332197\tLR: 0.00022249\n",
            "Train Epoch: 9 [63232/123872 (51%)]\tLoss: 0.328764\tLR: 0.00022238\n",
            "Train Epoch: 9 [63488/123872 (51%)]\tLoss: 0.385301\tLR: 0.00022228\n",
            "Train Epoch: 9 [63744/123872 (51%)]\tLoss: 0.425382\tLR: 0.00022217\n",
            "Train Epoch: 9 [64000/123872 (52%)]\tLoss: 0.402775\tLR: 0.00022206\n",
            "Train Epoch: 9 [64000/123872 (52%)]\tLoss: 0.402775\n",
            "Train Epoch: 9 [64256/123872 (52%)]\tLoss: 0.359982\tLR: 0.00022195\n",
            "Train Epoch: 9 [64512/123872 (52%)]\tLoss: 0.392385\tLR: 0.00022184\n",
            "Train Epoch: 9 [64768/123872 (52%)]\tLoss: 0.389029\tLR: 0.00022174\n",
            "Train Epoch: 9 [65024/123872 (52%)]\tLoss: 0.421671\tLR: 0.00022163\n",
            "Train Epoch: 9 [65280/123872 (53%)]\tLoss: 0.358687\tLR: 0.00022152\n",
            "Train Epoch: 9 [65536/123872 (53%)]\tLoss: 0.402308\tLR: 0.00022141\n",
            "Train Epoch: 9 [65792/123872 (53%)]\tLoss: 0.413965\tLR: 0.00022130\n",
            "Train Epoch: 9 [66048/123872 (53%)]\tLoss: 0.373567\tLR: 0.00022120\n",
            "Train Epoch: 9 [66304/123872 (54%)]\tLoss: 0.425134\tLR: 0.00022109\n",
            "Train Epoch: 9 [66560/123872 (54%)]\tLoss: 0.375489\tLR: 0.00022098\n",
            "Train Epoch: 9 [66560/123872 (54%)]\tLoss: 0.375489\n",
            "Train Epoch: 9 [66816/123872 (54%)]\tLoss: 0.377165\tLR: 0.00022087\n",
            "Train Epoch: 9 [67072/123872 (54%)]\tLoss: 0.460187\tLR: 0.00022076\n",
            "Train Epoch: 9 [67328/123872 (54%)]\tLoss: 0.399641\tLR: 0.00022066\n",
            "Train Epoch: 9 [67584/123872 (55%)]\tLoss: 0.464572\tLR: 0.00022055\n",
            "Train Epoch: 9 [67840/123872 (55%)]\tLoss: 0.342340\tLR: 0.00022044\n",
            "Train Epoch: 9 [68096/123872 (55%)]\tLoss: 0.405236\tLR: 0.00022033\n",
            "Train Epoch: 9 [68352/123872 (55%)]\tLoss: 0.429884\tLR: 0.00022022\n",
            "Train Epoch: 9 [68608/123872 (55%)]\tLoss: 0.468096\tLR: 0.00022012\n",
            "Train Epoch: 9 [68864/123872 (56%)]\tLoss: 0.391284\tLR: 0.00022001\n",
            "Train Epoch: 9 [69120/123872 (56%)]\tLoss: 0.380613\tLR: 0.00021990\n",
            "Train Epoch: 9 [69120/123872 (56%)]\tLoss: 0.380613\n",
            "Train Epoch: 9 [69376/123872 (56%)]\tLoss: 0.416793\tLR: 0.00021979\n",
            "Train Epoch: 9 [69632/123872 (56%)]\tLoss: 0.415762\tLR: 0.00021969\n",
            "Train Epoch: 9 [69888/123872 (56%)]\tLoss: 0.370647\tLR: 0.00021958\n",
            "Train Epoch: 9 [70144/123872 (57%)]\tLoss: 0.440453\tLR: 0.00021947\n",
            "Train Epoch: 9 [70400/123872 (57%)]\tLoss: 0.419296\tLR: 0.00021936\n",
            "Train Epoch: 9 [70656/123872 (57%)]\tLoss: 0.429341\tLR: 0.00021925\n",
            "Train Epoch: 9 [70912/123872 (57%)]\tLoss: 0.410039\tLR: 0.00021915\n",
            "Train Epoch: 9 [71168/123872 (57%)]\tLoss: 0.349276\tLR: 0.00021904\n",
            "Train Epoch: 9 [71424/123872 (58%)]\tLoss: 0.420740\tLR: 0.00021893\n",
            "Train Epoch: 9 [71680/123872 (58%)]\tLoss: 0.407274\tLR: 0.00021882\n",
            "Train Epoch: 9 [71680/123872 (58%)]\tLoss: 0.407274\n",
            "Train Epoch: 9 [71936/123872 (58%)]\tLoss: 0.427123\tLR: 0.00021872\n",
            "Train Epoch: 9 [72192/123872 (58%)]\tLoss: 0.371726\tLR: 0.00021861\n",
            "Train Epoch: 9 [72448/123872 (58%)]\tLoss: 0.434854\tLR: 0.00021850\n",
            "Train Epoch: 9 [72704/123872 (59%)]\tLoss: 0.420257\tLR: 0.00021839\n",
            "Train Epoch: 9 [72960/123872 (59%)]\tLoss: 0.407483\tLR: 0.00021829\n",
            "Train Epoch: 9 [73216/123872 (59%)]\tLoss: 0.424366\tLR: 0.00021818\n",
            "Train Epoch: 9 [73472/123872 (59%)]\tLoss: 0.444341\tLR: 0.00021807\n",
            "Train Epoch: 9 [73728/123872 (60%)]\tLoss: 0.413848\tLR: 0.00021796\n",
            "Train Epoch: 9 [73984/123872 (60%)]\tLoss: 0.447475\tLR: 0.00021786\n",
            "Train Epoch: 9 [74240/123872 (60%)]\tLoss: 0.367110\tLR: 0.00021775\n",
            "Train Epoch: 9 [74240/123872 (60%)]\tLoss: 0.367110\n",
            "Train Epoch: 9 [74496/123872 (60%)]\tLoss: 0.432670\tLR: 0.00021764\n",
            "Train Epoch: 9 [74752/123872 (60%)]\tLoss: 0.423678\tLR: 0.00021754\n",
            "Train Epoch: 9 [75008/123872 (61%)]\tLoss: 0.437943\tLR: 0.00021743\n",
            "Train Epoch: 9 [75264/123872 (61%)]\tLoss: 0.415029\tLR: 0.00021732\n",
            "Train Epoch: 9 [75520/123872 (61%)]\tLoss: 0.395257\tLR: 0.00021721\n",
            "Train Epoch: 9 [75776/123872 (61%)]\tLoss: 0.396746\tLR: 0.00021711\n",
            "Train Epoch: 9 [76032/123872 (61%)]\tLoss: 0.450920\tLR: 0.00021700\n",
            "Train Epoch: 9 [76288/123872 (62%)]\tLoss: 0.401665\tLR: 0.00021689\n",
            "Train Epoch: 9 [76544/123872 (62%)]\tLoss: 0.375136\tLR: 0.00021679\n",
            "Train Epoch: 9 [76800/123872 (62%)]\tLoss: 0.390735\tLR: 0.00021668\n",
            "Train Epoch: 9 [76800/123872 (62%)]\tLoss: 0.390735\n",
            "Train Epoch: 9 [77056/123872 (62%)]\tLoss: 0.434936\tLR: 0.00021657\n",
            "Train Epoch: 9 [77312/123872 (62%)]\tLoss: 0.394602\tLR: 0.00021646\n",
            "Train Epoch: 9 [77568/123872 (63%)]\tLoss: 0.437105\tLR: 0.00021636\n",
            "Train Epoch: 9 [77824/123872 (63%)]\tLoss: 0.380623\tLR: 0.00021625\n",
            "Train Epoch: 9 [78080/123872 (63%)]\tLoss: 0.426692\tLR: 0.00021614\n",
            "Train Epoch: 9 [78336/123872 (63%)]\tLoss: 0.391642\tLR: 0.00021604\n",
            "Train Epoch: 9 [78592/123872 (63%)]\tLoss: 0.394399\tLR: 0.00021593\n",
            "Train Epoch: 9 [78848/123872 (64%)]\tLoss: 0.427834\tLR: 0.00021582\n",
            "Train Epoch: 9 [79104/123872 (64%)]\tLoss: 0.406555\tLR: 0.00021571\n",
            "Train Epoch: 9 [79360/123872 (64%)]\tLoss: 0.451043\tLR: 0.00021561\n",
            "Train Epoch: 9 [79360/123872 (64%)]\tLoss: 0.451043\n",
            "Train Epoch: 9 [79616/123872 (64%)]\tLoss: 0.368811\tLR: 0.00021550\n",
            "Train Epoch: 9 [79872/123872 (64%)]\tLoss: 0.371176\tLR: 0.00021539\n",
            "Train Epoch: 9 [80128/123872 (65%)]\tLoss: 0.386385\tLR: 0.00021529\n",
            "Train Epoch: 9 [80384/123872 (65%)]\tLoss: 0.398436\tLR: 0.00021518\n",
            "Train Epoch: 9 [80640/123872 (65%)]\tLoss: 0.345873\tLR: 0.00021507\n",
            "Train Epoch: 9 [80896/123872 (65%)]\tLoss: 0.379001\tLR: 0.00021497\n",
            "Train Epoch: 9 [81152/123872 (65%)]\tLoss: 0.430513\tLR: 0.00021486\n",
            "Train Epoch: 9 [81408/123872 (66%)]\tLoss: 0.429611\tLR: 0.00021475\n",
            "Train Epoch: 9 [81664/123872 (66%)]\tLoss: 0.381684\tLR: 0.00021465\n",
            "Train Epoch: 9 [81920/123872 (66%)]\tLoss: 0.346518\tLR: 0.00021454\n",
            "Train Epoch: 9 [81920/123872 (66%)]\tLoss: 0.346518\n",
            "Train Epoch: 9 [82176/123872 (66%)]\tLoss: 0.448528\tLR: 0.00021443\n",
            "Train Epoch: 9 [82432/123872 (67%)]\tLoss: 0.425554\tLR: 0.00021433\n",
            "Train Epoch: 9 [82688/123872 (67%)]\tLoss: 0.315027\tLR: 0.00021422\n",
            "Train Epoch: 9 [82944/123872 (67%)]\tLoss: 0.398823\tLR: 0.00021411\n",
            "Train Epoch: 9 [83200/123872 (67%)]\tLoss: 0.395409\tLR: 0.00021401\n",
            "Train Epoch: 9 [83456/123872 (67%)]\tLoss: 0.358108\tLR: 0.00021390\n",
            "Train Epoch: 9 [83712/123872 (68%)]\tLoss: 0.386313\tLR: 0.00021379\n",
            "Train Epoch: 9 [83968/123872 (68%)]\tLoss: 0.362943\tLR: 0.00021369\n",
            "Train Epoch: 9 [84224/123872 (68%)]\tLoss: 0.351730\tLR: 0.00021358\n",
            "Train Epoch: 9 [84480/123872 (68%)]\tLoss: 0.393078\tLR: 0.00021347\n",
            "Train Epoch: 9 [84480/123872 (68%)]\tLoss: 0.393078\n",
            "Train Epoch: 9 [84736/123872 (68%)]\tLoss: 0.386313\tLR: 0.00021337\n",
            "Train Epoch: 9 [84992/123872 (69%)]\tLoss: 0.433016\tLR: 0.00021326\n",
            "Train Epoch: 9 [85248/123872 (69%)]\tLoss: 0.372182\tLR: 0.00021316\n",
            "Train Epoch: 9 [85504/123872 (69%)]\tLoss: 0.413461\tLR: 0.00021305\n",
            "Train Epoch: 9 [85760/123872 (69%)]\tLoss: 0.393771\tLR: 0.00021294\n",
            "Train Epoch: 9 [86016/123872 (69%)]\tLoss: 0.411145\tLR: 0.00021284\n",
            "Train Epoch: 9 [86272/123872 (70%)]\tLoss: 0.437494\tLR: 0.00021273\n",
            "Train Epoch: 9 [86528/123872 (70%)]\tLoss: 0.353421\tLR: 0.00021262\n",
            "Train Epoch: 9 [86784/123872 (70%)]\tLoss: 0.364891\tLR: 0.00021252\n",
            "Train Epoch: 9 [87040/123872 (70%)]\tLoss: 0.444826\tLR: 0.00021241\n",
            "Train Epoch: 9 [87040/123872 (70%)]\tLoss: 0.444826\n",
            "Train Epoch: 9 [87296/123872 (70%)]\tLoss: 0.382798\tLR: 0.00021230\n",
            "Train Epoch: 9 [87552/123872 (71%)]\tLoss: 0.419095\tLR: 0.00021220\n",
            "Train Epoch: 9 [87808/123872 (71%)]\tLoss: 0.382505\tLR: 0.00021209\n",
            "Train Epoch: 9 [88064/123872 (71%)]\tLoss: 0.442456\tLR: 0.00021199\n",
            "Train Epoch: 9 [88320/123872 (71%)]\tLoss: 0.384288\tLR: 0.00021188\n",
            "Train Epoch: 9 [88576/123872 (71%)]\tLoss: 0.319141\tLR: 0.00021177\n",
            "Train Epoch: 9 [88832/123872 (72%)]\tLoss: 0.407706\tLR: 0.00021167\n",
            "Train Epoch: 9 [89088/123872 (72%)]\tLoss: 0.398091\tLR: 0.00021156\n",
            "Train Epoch: 9 [89344/123872 (72%)]\tLoss: 0.399330\tLR: 0.00021146\n",
            "Train Epoch: 9 [89600/123872 (72%)]\tLoss: 0.391037\tLR: 0.00021135\n",
            "Train Epoch: 9 [89600/123872 (72%)]\tLoss: 0.391037\n",
            "Train Epoch: 9 [89856/123872 (73%)]\tLoss: 0.391199\tLR: 0.00021124\n",
            "Train Epoch: 9 [90112/123872 (73%)]\tLoss: 0.443380\tLR: 0.00021114\n",
            "Train Epoch: 9 [90368/123872 (73%)]\tLoss: 0.400152\tLR: 0.00021103\n",
            "Train Epoch: 9 [90624/123872 (73%)]\tLoss: 0.377745\tLR: 0.00021093\n",
            "Train Epoch: 9 [90880/123872 (73%)]\tLoss: 0.372464\tLR: 0.00021082\n",
            "Train Epoch: 9 [91136/123872 (74%)]\tLoss: 0.435660\tLR: 0.00021071\n",
            "Train Epoch: 9 [91392/123872 (74%)]\tLoss: 0.373666\tLR: 0.00021061\n",
            "Train Epoch: 9 [91648/123872 (74%)]\tLoss: 0.337036\tLR: 0.00021050\n",
            "Train Epoch: 9 [91904/123872 (74%)]\tLoss: 0.410315\tLR: 0.00021040\n",
            "Train Epoch: 9 [92160/123872 (74%)]\tLoss: 0.388709\tLR: 0.00021029\n",
            "Train Epoch: 9 [92160/123872 (74%)]\tLoss: 0.388709\n",
            "Train Epoch: 9 [92416/123872 (75%)]\tLoss: 0.383304\tLR: 0.00021019\n",
            "Train Epoch: 9 [92672/123872 (75%)]\tLoss: 0.443112\tLR: 0.00021008\n",
            "Train Epoch: 9 [92928/123872 (75%)]\tLoss: 0.510652\tLR: 0.00020997\n",
            "Train Epoch: 9 [93184/123872 (75%)]\tLoss: 0.413341\tLR: 0.00020987\n",
            "Train Epoch: 9 [93440/123872 (75%)]\tLoss: 0.402857\tLR: 0.00020976\n",
            "Train Epoch: 9 [93696/123872 (76%)]\tLoss: 0.415047\tLR: 0.00020966\n",
            "Train Epoch: 9 [93952/123872 (76%)]\tLoss: 0.331721\tLR: 0.00020955\n",
            "Train Epoch: 9 [94208/123872 (76%)]\tLoss: 0.349909\tLR: 0.00020945\n",
            "Train Epoch: 9 [94464/123872 (76%)]\tLoss: 0.431411\tLR: 0.00020934\n",
            "Train Epoch: 9 [94720/123872 (76%)]\tLoss: 0.442541\tLR: 0.00020923\n",
            "Train Epoch: 9 [94720/123872 (76%)]\tLoss: 0.442541\n",
            "Train Epoch: 9 [94976/123872 (77%)]\tLoss: 0.439777\tLR: 0.00020913\n",
            "Train Epoch: 9 [95232/123872 (77%)]\tLoss: 0.351511\tLR: 0.00020902\n",
            "Train Epoch: 9 [95488/123872 (77%)]\tLoss: 0.346933\tLR: 0.00020892\n",
            "Train Epoch: 9 [95744/123872 (77%)]\tLoss: 0.366838\tLR: 0.00020881\n",
            "Train Epoch: 9 [96000/123872 (77%)]\tLoss: 0.444297\tLR: 0.00020871\n",
            "Train Epoch: 9 [96256/123872 (78%)]\tLoss: 0.428512\tLR: 0.00020860\n",
            "Train Epoch: 9 [96512/123872 (78%)]\tLoss: 0.394815\tLR: 0.00020850\n",
            "Train Epoch: 9 [96768/123872 (78%)]\tLoss: 0.373107\tLR: 0.00020839\n",
            "Train Epoch: 9 [97024/123872 (78%)]\tLoss: 0.414075\tLR: 0.00020829\n",
            "Train Epoch: 9 [97280/123872 (79%)]\tLoss: 0.356038\tLR: 0.00020818\n",
            "Train Epoch: 9 [97280/123872 (79%)]\tLoss: 0.356038\n",
            "Train Epoch: 9 [97536/123872 (79%)]\tLoss: 0.499856\tLR: 0.00020807\n",
            "Train Epoch: 9 [97792/123872 (79%)]\tLoss: 0.409184\tLR: 0.00020797\n",
            "Train Epoch: 9 [98048/123872 (79%)]\tLoss: 0.443506\tLR: 0.00020786\n",
            "Train Epoch: 9 [98304/123872 (79%)]\tLoss: 0.437171\tLR: 0.00020776\n",
            "Train Epoch: 9 [98560/123872 (80%)]\tLoss: 0.422353\tLR: 0.00020765\n",
            "Train Epoch: 9 [98816/123872 (80%)]\tLoss: 0.317134\tLR: 0.00020755\n",
            "Train Epoch: 9 [99072/123872 (80%)]\tLoss: 0.448018\tLR: 0.00020744\n",
            "Train Epoch: 9 [99328/123872 (80%)]\tLoss: 0.371033\tLR: 0.00020734\n",
            "Train Epoch: 9 [99584/123872 (80%)]\tLoss: 0.364843\tLR: 0.00020723\n",
            "Train Epoch: 9 [99840/123872 (81%)]\tLoss: 0.386770\tLR: 0.00020713\n",
            "Train Epoch: 9 [99840/123872 (81%)]\tLoss: 0.386770\n",
            "Train Epoch: 9 [100096/123872 (81%)]\tLoss: 0.375786\tLR: 0.00020702\n",
            "Train Epoch: 9 [100352/123872 (81%)]\tLoss: 0.417699\tLR: 0.00020692\n",
            "Train Epoch: 9 [100608/123872 (81%)]\tLoss: 0.371795\tLR: 0.00020681\n",
            "Train Epoch: 9 [100864/123872 (81%)]\tLoss: 0.350683\tLR: 0.00020671\n",
            "Train Epoch: 9 [101120/123872 (82%)]\tLoss: 0.416428\tLR: 0.00020660\n",
            "Train Epoch: 9 [101376/123872 (82%)]\tLoss: 0.407312\tLR: 0.00020650\n",
            "Train Epoch: 9 [101632/123872 (82%)]\tLoss: 0.409271\tLR: 0.00020639\n",
            "Train Epoch: 9 [101888/123872 (82%)]\tLoss: 0.461728\tLR: 0.00020629\n",
            "Train Epoch: 9 [102144/123872 (82%)]\tLoss: 0.407509\tLR: 0.00020618\n",
            "Train Epoch: 9 [102400/123872 (83%)]\tLoss: 0.416757\tLR: 0.00020608\n",
            "Train Epoch: 9 [102400/123872 (83%)]\tLoss: 0.416757\n",
            "Train Epoch: 9 [102656/123872 (83%)]\tLoss: 0.413846\tLR: 0.00020597\n",
            "Train Epoch: 9 [102912/123872 (83%)]\tLoss: 0.429402\tLR: 0.00020587\n",
            "Train Epoch: 9 [103168/123872 (83%)]\tLoss: 0.382868\tLR: 0.00020576\n",
            "Train Epoch: 9 [103424/123872 (83%)]\tLoss: 0.406561\tLR: 0.00020566\n",
            "Train Epoch: 9 [103680/123872 (84%)]\tLoss: 0.401655\tLR: 0.00020555\n",
            "Train Epoch: 9 [103936/123872 (84%)]\tLoss: 0.432873\tLR: 0.00020545\n",
            "Train Epoch: 9 [104192/123872 (84%)]\tLoss: 0.396044\tLR: 0.00020535\n",
            "Train Epoch: 9 [104448/123872 (84%)]\tLoss: 0.392190\tLR: 0.00020524\n",
            "Train Epoch: 9 [104704/123872 (85%)]\tLoss: 0.453370\tLR: 0.00020514\n",
            "Train Epoch: 9 [104960/123872 (85%)]\tLoss: 0.415316\tLR: 0.00020503\n",
            "Train Epoch: 9 [104960/123872 (85%)]\tLoss: 0.415316\n",
            "Train Epoch: 9 [105216/123872 (85%)]\tLoss: 0.407845\tLR: 0.00020493\n",
            "Train Epoch: 9 [105472/123872 (85%)]\tLoss: 0.387064\tLR: 0.00020482\n",
            "Train Epoch: 9 [105728/123872 (85%)]\tLoss: 0.422480\tLR: 0.00020472\n",
            "Train Epoch: 9 [105984/123872 (86%)]\tLoss: 0.418411\tLR: 0.00020461\n",
            "Train Epoch: 9 [106240/123872 (86%)]\tLoss: 0.333282\tLR: 0.00020451\n",
            "Train Epoch: 9 [106496/123872 (86%)]\tLoss: 0.366967\tLR: 0.00020440\n",
            "Train Epoch: 9 [106752/123872 (86%)]\tLoss: 0.373990\tLR: 0.00020430\n",
            "Train Epoch: 9 [107008/123872 (86%)]\tLoss: 0.386460\tLR: 0.00020420\n",
            "Train Epoch: 9 [107264/123872 (87%)]\tLoss: 0.355296\tLR: 0.00020409\n",
            "Train Epoch: 9 [107520/123872 (87%)]\tLoss: 0.442677\tLR: 0.00020399\n",
            "Train Epoch: 9 [107520/123872 (87%)]\tLoss: 0.442677\n",
            "Train Epoch: 9 [107776/123872 (87%)]\tLoss: 0.438606\tLR: 0.00020388\n",
            "Train Epoch: 9 [108032/123872 (87%)]\tLoss: 0.407266\tLR: 0.00020378\n",
            "Train Epoch: 9 [108288/123872 (87%)]\tLoss: 0.386944\tLR: 0.00020367\n",
            "Train Epoch: 9 [108544/123872 (88%)]\tLoss: 0.417846\tLR: 0.00020357\n",
            "Train Epoch: 9 [108800/123872 (88%)]\tLoss: 0.414365\tLR: 0.00020347\n",
            "Train Epoch: 9 [109056/123872 (88%)]\tLoss: 0.459242\tLR: 0.00020336\n",
            "Train Epoch: 9 [109312/123872 (88%)]\tLoss: 0.374174\tLR: 0.00020326\n",
            "Train Epoch: 9 [109568/123872 (88%)]\tLoss: 0.415702\tLR: 0.00020315\n",
            "Train Epoch: 9 [109824/123872 (89%)]\tLoss: 0.367091\tLR: 0.00020305\n",
            "Train Epoch: 9 [110080/123872 (89%)]\tLoss: 0.399352\tLR: 0.00020294\n",
            "Train Epoch: 9 [110080/123872 (89%)]\tLoss: 0.399352\n",
            "Train Epoch: 9 [110336/123872 (89%)]\tLoss: 0.369975\tLR: 0.00020284\n",
            "Train Epoch: 9 [110592/123872 (89%)]\tLoss: 0.369808\tLR: 0.00020274\n",
            "Train Epoch: 9 [110848/123872 (89%)]\tLoss: 0.396391\tLR: 0.00020263\n",
            "Train Epoch: 9 [111104/123872 (90%)]\tLoss: 0.449502\tLR: 0.00020253\n",
            "Train Epoch: 9 [111360/123872 (90%)]\tLoss: 0.373614\tLR: 0.00020242\n",
            "Train Epoch: 9 [111616/123872 (90%)]\tLoss: 0.471134\tLR: 0.00020232\n",
            "Train Epoch: 9 [111872/123872 (90%)]\tLoss: 0.374690\tLR: 0.00020222\n",
            "Train Epoch: 9 [112128/123872 (90%)]\tLoss: 0.406189\tLR: 0.00020211\n",
            "Train Epoch: 9 [112384/123872 (91%)]\tLoss: 0.373875\tLR: 0.00020201\n",
            "Train Epoch: 9 [112640/123872 (91%)]\tLoss: 0.452247\tLR: 0.00020190\n",
            "Train Epoch: 9 [112640/123872 (91%)]\tLoss: 0.452247\n",
            "Train Epoch: 9 [112896/123872 (91%)]\tLoss: 0.343952\tLR: 0.00020180\n",
            "Train Epoch: 9 [113152/123872 (91%)]\tLoss: 0.369150\tLR: 0.00020170\n",
            "Train Epoch: 9 [113408/123872 (92%)]\tLoss: 0.394415\tLR: 0.00020159\n",
            "Train Epoch: 9 [113664/123872 (92%)]\tLoss: 0.405910\tLR: 0.00020149\n",
            "Train Epoch: 9 [113920/123872 (92%)]\tLoss: 0.370928\tLR: 0.00020138\n",
            "Train Epoch: 9 [114176/123872 (92%)]\tLoss: 0.455979\tLR: 0.00020128\n",
            "Train Epoch: 9 [114432/123872 (92%)]\tLoss: 0.475747\tLR: 0.00020118\n",
            "Train Epoch: 9 [114688/123872 (93%)]\tLoss: 0.434209\tLR: 0.00020107\n",
            "Train Epoch: 9 [114944/123872 (93%)]\tLoss: 0.399993\tLR: 0.00020097\n",
            "Train Epoch: 9 [115200/123872 (93%)]\tLoss: 0.388903\tLR: 0.00020087\n",
            "Train Epoch: 9 [115200/123872 (93%)]\tLoss: 0.388903\n",
            "Train Epoch: 9 [115456/123872 (93%)]\tLoss: 0.429365\tLR: 0.00020076\n",
            "Train Epoch: 9 [115712/123872 (93%)]\tLoss: 0.406669\tLR: 0.00020066\n",
            "Train Epoch: 9 [115968/123872 (94%)]\tLoss: 0.400589\tLR: 0.00020056\n",
            "Train Epoch: 9 [116224/123872 (94%)]\tLoss: 0.419284\tLR: 0.00020045\n",
            "Train Epoch: 9 [116480/123872 (94%)]\tLoss: 0.390570\tLR: 0.00020035\n",
            "Train Epoch: 9 [116736/123872 (94%)]\tLoss: 0.453782\tLR: 0.00020025\n",
            "Train Epoch: 9 [116992/123872 (94%)]\tLoss: 0.345186\tLR: 0.00020014\n",
            "Train Epoch: 9 [117248/123872 (95%)]\tLoss: 0.429729\tLR: 0.00020004\n",
            "Train Epoch: 9 [117504/123872 (95%)]\tLoss: 0.423196\tLR: 0.00019993\n",
            "Train Epoch: 9 [117760/123872 (95%)]\tLoss: 0.348931\tLR: 0.00019983\n",
            "Train Epoch: 9 [117760/123872 (95%)]\tLoss: 0.348931\n",
            "Train Epoch: 9 [118016/123872 (95%)]\tLoss: 0.437149\tLR: 0.00019973\n",
            "Train Epoch: 9 [118272/123872 (95%)]\tLoss: 0.358381\tLR: 0.00019962\n",
            "Train Epoch: 9 [118528/123872 (96%)]\tLoss: 0.374864\tLR: 0.00019952\n",
            "Train Epoch: 9 [118784/123872 (96%)]\tLoss: 0.441176\tLR: 0.00019942\n",
            "Train Epoch: 9 [119040/123872 (96%)]\tLoss: 0.374492\tLR: 0.00019931\n",
            "Train Epoch: 9 [119296/123872 (96%)]\tLoss: 0.388341\tLR: 0.00019921\n",
            "Train Epoch: 9 [119552/123872 (96%)]\tLoss: 0.404636\tLR: 0.00019911\n",
            "Train Epoch: 9 [119808/123872 (97%)]\tLoss: 0.362964\tLR: 0.00019901\n",
            "Train Epoch: 9 [120064/123872 (97%)]\tLoss: 0.373717\tLR: 0.00019890\n",
            "Train Epoch: 9 [120320/123872 (97%)]\tLoss: 0.443967\tLR: 0.00019880\n",
            "Train Epoch: 9 [120320/123872 (97%)]\tLoss: 0.443967\n",
            "Train Epoch: 9 [120576/123872 (97%)]\tLoss: 0.433098\tLR: 0.00019870\n",
            "Train Epoch: 9 [120832/123872 (98%)]\tLoss: 0.350242\tLR: 0.00019859\n",
            "Train Epoch: 9 [121088/123872 (98%)]\tLoss: 0.381926\tLR: 0.00019849\n",
            "Train Epoch: 9 [121344/123872 (98%)]\tLoss: 0.430362\tLR: 0.00019839\n",
            "Train Epoch: 9 [121600/123872 (98%)]\tLoss: 0.380132\tLR: 0.00019828\n",
            "Train Epoch: 9 [121856/123872 (98%)]\tLoss: 0.405264\tLR: 0.00019818\n",
            "Train Epoch: 9 [122112/123872 (99%)]\tLoss: 0.386063\tLR: 0.00019808\n",
            "Train Epoch: 9 [122368/123872 (99%)]\tLoss: 0.383634\tLR: 0.00019797\n",
            "Train Epoch: 9 [122624/123872 (99%)]\tLoss: 0.375472\tLR: 0.00019787\n",
            "Train Epoch: 9 [122880/123872 (99%)]\tLoss: 0.366374\tLR: 0.00019777\n",
            "Train Epoch: 9 [122880/123872 (99%)]\tLoss: 0.366374\n",
            "Train Epoch: 9 [123136/123872 (99%)]\tLoss: 0.388216\tLR: 0.00019767\n",
            "Train Epoch: 9 [123392/123872 (100%)]\tLoss: 0.474817\tLR: 0.00019756\n",
            "Train Epoch: 9 [108192/123872 (100%)]\tLoss: 0.354792\tLR: 0.00019746\n",
            "\n",
            "Test set: Average loss: 0.0016, Accuracy: 25171/30970 (81.28%)\n",
            "\n",
            "Train Epoch: 10 [0/123872 (0%)]\tLoss: 0.443268\tLR: 0.00019736\n",
            "Train Epoch: 10 [0/123872 (0%)]\tLoss: 0.443268\n",
            "Train Epoch: 10 [256/123872 (0%)]\tLoss: 0.347202\tLR: 0.00019725\n",
            "Train Epoch: 10 [512/123872 (0%)]\tLoss: 0.361935\tLR: 0.00019715\n",
            "Train Epoch: 10 [768/123872 (1%)]\tLoss: 0.361945\tLR: 0.00019705\n",
            "Train Epoch: 10 [1024/123872 (1%)]\tLoss: 0.382755\tLR: 0.00019695\n",
            "Train Epoch: 10 [1280/123872 (1%)]\tLoss: 0.404770\tLR: 0.00019684\n",
            "Train Epoch: 10 [1536/123872 (1%)]\tLoss: 0.474047\tLR: 0.00019674\n",
            "Train Epoch: 10 [1792/123872 (1%)]\tLoss: 0.398673\tLR: 0.00019664\n",
            "Train Epoch: 10 [2048/123872 (2%)]\tLoss: 0.392202\tLR: 0.00019654\n",
            "Train Epoch: 10 [2304/123872 (2%)]\tLoss: 0.431486\tLR: 0.00019643\n",
            "Train Epoch: 10 [2560/123872 (2%)]\tLoss: 0.395702\tLR: 0.00019633\n",
            "Train Epoch: 10 [2560/123872 (2%)]\tLoss: 0.395702\n",
            "Train Epoch: 10 [2816/123872 (2%)]\tLoss: 0.391047\tLR: 0.00019623\n",
            "Train Epoch: 10 [3072/123872 (2%)]\tLoss: 0.393608\tLR: 0.00019613\n",
            "Train Epoch: 10 [3328/123872 (3%)]\tLoss: 0.414893\tLR: 0.00019602\n",
            "Train Epoch: 10 [3584/123872 (3%)]\tLoss: 0.397637\tLR: 0.00019592\n",
            "Train Epoch: 10 [3840/123872 (3%)]\tLoss: 0.373836\tLR: 0.00019582\n",
            "Train Epoch: 10 [4096/123872 (3%)]\tLoss: 0.437935\tLR: 0.00019572\n",
            "Train Epoch: 10 [4352/123872 (4%)]\tLoss: 0.361354\tLR: 0.00019561\n",
            "Train Epoch: 10 [4608/123872 (4%)]\tLoss: 0.408796\tLR: 0.00019551\n",
            "Train Epoch: 10 [4864/123872 (4%)]\tLoss: 0.308954\tLR: 0.00019541\n",
            "Train Epoch: 10 [5120/123872 (4%)]\tLoss: 0.419470\tLR: 0.00019531\n",
            "Train Epoch: 10 [5120/123872 (4%)]\tLoss: 0.419470\n",
            "Train Epoch: 10 [5376/123872 (4%)]\tLoss: 0.386543\tLR: 0.00019520\n",
            "Train Epoch: 10 [5632/123872 (5%)]\tLoss: 0.399628\tLR: 0.00019510\n",
            "Train Epoch: 10 [5888/123872 (5%)]\tLoss: 0.479181\tLR: 0.00019500\n",
            "Train Epoch: 10 [6144/123872 (5%)]\tLoss: 0.394780\tLR: 0.00019490\n",
            "Train Epoch: 10 [6400/123872 (5%)]\tLoss: 0.359617\tLR: 0.00019480\n",
            "Train Epoch: 10 [6656/123872 (5%)]\tLoss: 0.415157\tLR: 0.00019469\n",
            "Train Epoch: 10 [6912/123872 (6%)]\tLoss: 0.411379\tLR: 0.00019459\n",
            "Train Epoch: 10 [7168/123872 (6%)]\tLoss: 0.433771\tLR: 0.00019449\n",
            "Train Epoch: 10 [7424/123872 (6%)]\tLoss: 0.363734\tLR: 0.00019439\n",
            "Train Epoch: 10 [7680/123872 (6%)]\tLoss: 0.485533\tLR: 0.00019429\n",
            "Train Epoch: 10 [7680/123872 (6%)]\tLoss: 0.485533\n",
            "Train Epoch: 10 [7936/123872 (6%)]\tLoss: 0.352538\tLR: 0.00019418\n",
            "Train Epoch: 10 [8192/123872 (7%)]\tLoss: 0.425816\tLR: 0.00019408\n",
            "Train Epoch: 10 [8448/123872 (7%)]\tLoss: 0.445638\tLR: 0.00019398\n",
            "Train Epoch: 10 [8704/123872 (7%)]\tLoss: 0.426297\tLR: 0.00019388\n",
            "Train Epoch: 10 [8960/123872 (7%)]\tLoss: 0.394944\tLR: 0.00019378\n",
            "Train Epoch: 10 [9216/123872 (7%)]\tLoss: 0.362630\tLR: 0.00019367\n",
            "Train Epoch: 10 [9472/123872 (8%)]\tLoss: 0.461141\tLR: 0.00019357\n",
            "Train Epoch: 10 [9728/123872 (8%)]\tLoss: 0.424854\tLR: 0.00019347\n",
            "Train Epoch: 10 [9984/123872 (8%)]\tLoss: 0.372970\tLR: 0.00019337\n",
            "Train Epoch: 10 [10240/123872 (8%)]\tLoss: 0.398688\tLR: 0.00019327\n",
            "Train Epoch: 10 [10240/123872 (8%)]\tLoss: 0.398688\n",
            "Train Epoch: 10 [10496/123872 (8%)]\tLoss: 0.366949\tLR: 0.00019316\n",
            "Train Epoch: 10 [10752/123872 (9%)]\tLoss: 0.368362\tLR: 0.00019306\n",
            "Train Epoch: 10 [11008/123872 (9%)]\tLoss: 0.358278\tLR: 0.00019296\n",
            "Train Epoch: 10 [11264/123872 (9%)]\tLoss: 0.448474\tLR: 0.00019286\n",
            "Train Epoch: 10 [11520/123872 (9%)]\tLoss: 0.370466\tLR: 0.00019276\n",
            "Train Epoch: 10 [11776/123872 (10%)]\tLoss: 0.439644\tLR: 0.00019266\n",
            "Train Epoch: 10 [12032/123872 (10%)]\tLoss: 0.401573\tLR: 0.00019255\n",
            "Train Epoch: 10 [12288/123872 (10%)]\tLoss: 0.412463\tLR: 0.00019245\n",
            "Train Epoch: 10 [12544/123872 (10%)]\tLoss: 0.332079\tLR: 0.00019235\n",
            "Train Epoch: 10 [12800/123872 (10%)]\tLoss: 0.337154\tLR: 0.00019225\n",
            "Train Epoch: 10 [12800/123872 (10%)]\tLoss: 0.337154\n",
            "Train Epoch: 10 [13056/123872 (11%)]\tLoss: 0.371606\tLR: 0.00019215\n",
            "Train Epoch: 10 [13312/123872 (11%)]\tLoss: 0.360946\tLR: 0.00019205\n",
            "Train Epoch: 10 [13568/123872 (11%)]\tLoss: 0.396706\tLR: 0.00019195\n",
            "Train Epoch: 10 [13824/123872 (11%)]\tLoss: 0.371313\tLR: 0.00019184\n",
            "Train Epoch: 10 [14080/123872 (11%)]\tLoss: 0.444011\tLR: 0.00019174\n",
            "Train Epoch: 10 [14336/123872 (12%)]\tLoss: 0.388436\tLR: 0.00019164\n",
            "Train Epoch: 10 [14592/123872 (12%)]\tLoss: 0.362531\tLR: 0.00019154\n",
            "Train Epoch: 10 [14848/123872 (12%)]\tLoss: 0.428406\tLR: 0.00019144\n",
            "Train Epoch: 10 [15104/123872 (12%)]\tLoss: 0.361914\tLR: 0.00019134\n",
            "Train Epoch: 10 [15360/123872 (12%)]\tLoss: 0.391596\tLR: 0.00019124\n",
            "Train Epoch: 10 [15360/123872 (12%)]\tLoss: 0.391596\n",
            "Train Epoch: 10 [15616/123872 (13%)]\tLoss: 0.409163\tLR: 0.00019114\n",
            "Train Epoch: 10 [15872/123872 (13%)]\tLoss: 0.456879\tLR: 0.00019103\n",
            "Train Epoch: 10 [16128/123872 (13%)]\tLoss: 0.488124\tLR: 0.00019093\n",
            "Train Epoch: 10 [16384/123872 (13%)]\tLoss: 0.412145\tLR: 0.00019083\n",
            "Train Epoch: 10 [16640/123872 (13%)]\tLoss: 0.407543\tLR: 0.00019073\n",
            "Train Epoch: 10 [16896/123872 (14%)]\tLoss: 0.427399\tLR: 0.00019063\n",
            "Train Epoch: 10 [17152/123872 (14%)]\tLoss: 0.432369\tLR: 0.00019053\n",
            "Train Epoch: 10 [17408/123872 (14%)]\tLoss: 0.417617\tLR: 0.00019043\n",
            "Train Epoch: 10 [17664/123872 (14%)]\tLoss: 0.354056\tLR: 0.00019033\n",
            "Train Epoch: 10 [17920/123872 (14%)]\tLoss: 0.404081\tLR: 0.00019023\n",
            "Train Epoch: 10 [17920/123872 (14%)]\tLoss: 0.404081\n",
            "Train Epoch: 10 [18176/123872 (15%)]\tLoss: 0.409022\tLR: 0.00019012\n",
            "Train Epoch: 10 [18432/123872 (15%)]\tLoss: 0.400111\tLR: 0.00019002\n",
            "Train Epoch: 10 [18688/123872 (15%)]\tLoss: 0.388460\tLR: 0.00018992\n",
            "Train Epoch: 10 [18944/123872 (15%)]\tLoss: 0.375968\tLR: 0.00018982\n",
            "Train Epoch: 10 [19200/123872 (15%)]\tLoss: 0.382572\tLR: 0.00018972\n",
            "Train Epoch: 10 [19456/123872 (16%)]\tLoss: 0.409289\tLR: 0.00018962\n",
            "Train Epoch: 10 [19712/123872 (16%)]\tLoss: 0.368509\tLR: 0.00018952\n",
            "Train Epoch: 10 [19968/123872 (16%)]\tLoss: 0.474581\tLR: 0.00018942\n",
            "Train Epoch: 10 [20224/123872 (16%)]\tLoss: 0.353878\tLR: 0.00018932\n",
            "Train Epoch: 10 [20480/123872 (17%)]\tLoss: 0.376689\tLR: 0.00018922\n",
            "Train Epoch: 10 [20480/123872 (17%)]\tLoss: 0.376689\n",
            "Train Epoch: 10 [20736/123872 (17%)]\tLoss: 0.418879\tLR: 0.00018912\n",
            "Train Epoch: 10 [20992/123872 (17%)]\tLoss: 0.381630\tLR: 0.00018902\n",
            "Train Epoch: 10 [21248/123872 (17%)]\tLoss: 0.401435\tLR: 0.00018892\n",
            "Train Epoch: 10 [21504/123872 (17%)]\tLoss: 0.403338\tLR: 0.00018881\n",
            "Train Epoch: 10 [21760/123872 (18%)]\tLoss: 0.416811\tLR: 0.00018871\n",
            "Train Epoch: 10 [22016/123872 (18%)]\tLoss: 0.409591\tLR: 0.00018861\n",
            "Train Epoch: 10 [22272/123872 (18%)]\tLoss: 0.392813\tLR: 0.00018851\n",
            "Train Epoch: 10 [22528/123872 (18%)]\tLoss: 0.414497\tLR: 0.00018841\n",
            "Train Epoch: 10 [22784/123872 (18%)]\tLoss: 0.471684\tLR: 0.00018831\n",
            "Train Epoch: 10 [23040/123872 (19%)]\tLoss: 0.405936\tLR: 0.00018821\n",
            "Train Epoch: 10 [23040/123872 (19%)]\tLoss: 0.405936\n",
            "Train Epoch: 10 [23296/123872 (19%)]\tLoss: 0.359398\tLR: 0.00018811\n",
            "Train Epoch: 10 [23552/123872 (19%)]\tLoss: 0.457612\tLR: 0.00018801\n",
            "Train Epoch: 10 [23808/123872 (19%)]\tLoss: 0.394223\tLR: 0.00018791\n",
            "Train Epoch: 10 [24064/123872 (19%)]\tLoss: 0.361981\tLR: 0.00018781\n",
            "Train Epoch: 10 [24320/123872 (20%)]\tLoss: 0.393609\tLR: 0.00018771\n",
            "Train Epoch: 10 [24576/123872 (20%)]\tLoss: 0.362965\tLR: 0.00018761\n",
            "Train Epoch: 10 [24832/123872 (20%)]\tLoss: 0.456524\tLR: 0.00018751\n",
            "Train Epoch: 10 [25088/123872 (20%)]\tLoss: 0.407693\tLR: 0.00018741\n",
            "Train Epoch: 10 [25344/123872 (20%)]\tLoss: 0.400982\tLR: 0.00018731\n",
            "Train Epoch: 10 [25600/123872 (21%)]\tLoss: 0.384601\tLR: 0.00018721\n",
            "Train Epoch: 10 [25600/123872 (21%)]\tLoss: 0.384601\n",
            "Train Epoch: 10 [25856/123872 (21%)]\tLoss: 0.427064\tLR: 0.00018711\n",
            "Train Epoch: 10 [26112/123872 (21%)]\tLoss: 0.390167\tLR: 0.00018701\n",
            "Train Epoch: 10 [26368/123872 (21%)]\tLoss: 0.373554\tLR: 0.00018691\n",
            "Train Epoch: 10 [26624/123872 (21%)]\tLoss: 0.404506\tLR: 0.00018681\n",
            "Train Epoch: 10 [26880/123872 (22%)]\tLoss: 0.415348\tLR: 0.00018671\n",
            "Train Epoch: 10 [27136/123872 (22%)]\tLoss: 0.380003\tLR: 0.00018661\n",
            "Train Epoch: 10 [27392/123872 (22%)]\tLoss: 0.354664\tLR: 0.00018651\n",
            "Train Epoch: 10 [27648/123872 (22%)]\tLoss: 0.403574\tLR: 0.00018641\n",
            "Train Epoch: 10 [27904/123872 (23%)]\tLoss: 0.391344\tLR: 0.00018631\n",
            "Train Epoch: 10 [28160/123872 (23%)]\tLoss: 0.375115\tLR: 0.00018621\n",
            "Train Epoch: 10 [28160/123872 (23%)]\tLoss: 0.375115\n",
            "Train Epoch: 10 [28416/123872 (23%)]\tLoss: 0.420458\tLR: 0.00018611\n",
            "Train Epoch: 10 [28672/123872 (23%)]\tLoss: 0.334408\tLR: 0.00018601\n",
            "Train Epoch: 10 [28928/123872 (23%)]\tLoss: 0.344795\tLR: 0.00018591\n",
            "Train Epoch: 10 [29184/123872 (24%)]\tLoss: 0.343010\tLR: 0.00018581\n",
            "Train Epoch: 10 [29440/123872 (24%)]\tLoss: 0.344196\tLR: 0.00018571\n",
            "Train Epoch: 10 [29696/123872 (24%)]\tLoss: 0.394716\tLR: 0.00018561\n",
            "Train Epoch: 10 [29952/123872 (24%)]\tLoss: 0.445389\tLR: 0.00018551\n",
            "Train Epoch: 10 [30208/123872 (24%)]\tLoss: 0.407468\tLR: 0.00018541\n",
            "Train Epoch: 10 [30464/123872 (25%)]\tLoss: 0.428575\tLR: 0.00018531\n",
            "Train Epoch: 10 [30720/123872 (25%)]\tLoss: 0.393765\tLR: 0.00018521\n",
            "Train Epoch: 10 [30720/123872 (25%)]\tLoss: 0.393765\n",
            "Train Epoch: 10 [30976/123872 (25%)]\tLoss: 0.405648\tLR: 0.00018511\n",
            "Train Epoch: 10 [31232/123872 (25%)]\tLoss: 0.334438\tLR: 0.00018501\n",
            "Train Epoch: 10 [31488/123872 (25%)]\tLoss: 0.360340\tLR: 0.00018491\n",
            "Train Epoch: 10 [31744/123872 (26%)]\tLoss: 0.375130\tLR: 0.00018481\n",
            "Train Epoch: 10 [32000/123872 (26%)]\tLoss: 0.424008\tLR: 0.00018471\n",
            "Train Epoch: 10 [32256/123872 (26%)]\tLoss: 0.359064\tLR: 0.00018461\n",
            "Train Epoch: 10 [32512/123872 (26%)]\tLoss: 0.406924\tLR: 0.00018451\n",
            "Train Epoch: 10 [32768/123872 (26%)]\tLoss: 0.437816\tLR: 0.00018442\n",
            "Train Epoch: 10 [33024/123872 (27%)]\tLoss: 0.409602\tLR: 0.00018432\n",
            "Train Epoch: 10 [33280/123872 (27%)]\tLoss: 0.389793\tLR: 0.00018422\n",
            "Train Epoch: 10 [33280/123872 (27%)]\tLoss: 0.389793\n",
            "Train Epoch: 10 [33536/123872 (27%)]\tLoss: 0.345513\tLR: 0.00018412\n",
            "Train Epoch: 10 [33792/123872 (27%)]\tLoss: 0.390999\tLR: 0.00018402\n",
            "Train Epoch: 10 [34048/123872 (27%)]\tLoss: 0.440571\tLR: 0.00018392\n",
            "Train Epoch: 10 [34304/123872 (28%)]\tLoss: 0.458818\tLR: 0.00018382\n",
            "Train Epoch: 10 [34560/123872 (28%)]\tLoss: 0.407618\tLR: 0.00018372\n",
            "Train Epoch: 10 [34816/123872 (28%)]\tLoss: 0.415016\tLR: 0.00018362\n",
            "Train Epoch: 10 [35072/123872 (28%)]\tLoss: 0.371033\tLR: 0.00018352\n",
            "Train Epoch: 10 [35328/123872 (29%)]\tLoss: 0.426915\tLR: 0.00018342\n",
            "Train Epoch: 10 [35584/123872 (29%)]\tLoss: 0.375806\tLR: 0.00018332\n",
            "Train Epoch: 10 [35840/123872 (29%)]\tLoss: 0.434751\tLR: 0.00018322\n",
            "Train Epoch: 10 [35840/123872 (29%)]\tLoss: 0.434751\n",
            "Train Epoch: 10 [36096/123872 (29%)]\tLoss: 0.396692\tLR: 0.00018313\n",
            "Train Epoch: 10 [36352/123872 (29%)]\tLoss: 0.357578\tLR: 0.00018303\n",
            "Train Epoch: 10 [36608/123872 (30%)]\tLoss: 0.321559\tLR: 0.00018293\n",
            "Train Epoch: 10 [36864/123872 (30%)]\tLoss: 0.393080\tLR: 0.00018283\n",
            "Train Epoch: 10 [37120/123872 (30%)]\tLoss: 0.434848\tLR: 0.00018273\n",
            "Train Epoch: 10 [37376/123872 (30%)]\tLoss: 0.392051\tLR: 0.00018263\n",
            "Train Epoch: 10 [37632/123872 (30%)]\tLoss: 0.395535\tLR: 0.00018253\n",
            "Train Epoch: 10 [37888/123872 (31%)]\tLoss: 0.400516\tLR: 0.00018243\n",
            "Train Epoch: 10 [38144/123872 (31%)]\tLoss: 0.422470\tLR: 0.00018233\n",
            "Train Epoch: 10 [38400/123872 (31%)]\tLoss: 0.363311\tLR: 0.00018224\n",
            "Train Epoch: 10 [38400/123872 (31%)]\tLoss: 0.363311\n",
            "Train Epoch: 10 [38656/123872 (31%)]\tLoss: 0.421279\tLR: 0.00018214\n",
            "Train Epoch: 10 [38912/123872 (31%)]\tLoss: 0.380212\tLR: 0.00018204\n",
            "Train Epoch: 10 [39168/123872 (32%)]\tLoss: 0.348478\tLR: 0.00018194\n",
            "Train Epoch: 10 [39424/123872 (32%)]\tLoss: 0.336899\tLR: 0.00018184\n",
            "Train Epoch: 10 [39680/123872 (32%)]\tLoss: 0.448257\tLR: 0.00018174\n",
            "Train Epoch: 10 [39936/123872 (32%)]\tLoss: 0.410719\tLR: 0.00018164\n",
            "Train Epoch: 10 [40192/123872 (32%)]\tLoss: 0.378917\tLR: 0.00018154\n",
            "Train Epoch: 10 [40448/123872 (33%)]\tLoss: 0.501278\tLR: 0.00018145\n",
            "Train Epoch: 10 [40704/123872 (33%)]\tLoss: 0.378199\tLR: 0.00018135\n",
            "Train Epoch: 10 [40960/123872 (33%)]\tLoss: 0.384196\tLR: 0.00018125\n",
            "Train Epoch: 10 [40960/123872 (33%)]\tLoss: 0.384196\n",
            "Train Epoch: 10 [41216/123872 (33%)]\tLoss: 0.425653\tLR: 0.00018115\n",
            "Train Epoch: 10 [41472/123872 (33%)]\tLoss: 0.374561\tLR: 0.00018105\n",
            "Train Epoch: 10 [41728/123872 (34%)]\tLoss: 0.443298\tLR: 0.00018095\n",
            "Train Epoch: 10 [41984/123872 (34%)]\tLoss: 0.372018\tLR: 0.00018086\n",
            "Train Epoch: 10 [42240/123872 (34%)]\tLoss: 0.441292\tLR: 0.00018076\n",
            "Train Epoch: 10 [42496/123872 (34%)]\tLoss: 0.397684\tLR: 0.00018066\n",
            "Train Epoch: 10 [42752/123872 (35%)]\tLoss: 0.359212\tLR: 0.00018056\n",
            "Train Epoch: 10 [43008/123872 (35%)]\tLoss: 0.359340\tLR: 0.00018046\n",
            "Train Epoch: 10 [43264/123872 (35%)]\tLoss: 0.360989\tLR: 0.00018036\n",
            "Train Epoch: 10 [43520/123872 (35%)]\tLoss: 0.351506\tLR: 0.00018027\n",
            "Train Epoch: 10 [43520/123872 (35%)]\tLoss: 0.351506\n",
            "Train Epoch: 10 [43776/123872 (35%)]\tLoss: 0.411792\tLR: 0.00018017\n",
            "Train Epoch: 10 [44032/123872 (36%)]\tLoss: 0.388760\tLR: 0.00018007\n",
            "Train Epoch: 10 [44288/123872 (36%)]\tLoss: 0.426997\tLR: 0.00017997\n",
            "Train Epoch: 10 [44544/123872 (36%)]\tLoss: 0.420874\tLR: 0.00017987\n",
            "Train Epoch: 10 [44800/123872 (36%)]\tLoss: 0.406696\tLR: 0.00017978\n",
            "Train Epoch: 10 [45056/123872 (36%)]\tLoss: 0.445438\tLR: 0.00017968\n",
            "Train Epoch: 10 [45312/123872 (37%)]\tLoss: 0.381313\tLR: 0.00017958\n",
            "Train Epoch: 10 [45568/123872 (37%)]\tLoss: 0.390525\tLR: 0.00017948\n",
            "Train Epoch: 10 [45824/123872 (37%)]\tLoss: 0.341164\tLR: 0.00017938\n",
            "Train Epoch: 10 [46080/123872 (37%)]\tLoss: 0.372237\tLR: 0.00017929\n",
            "Train Epoch: 10 [46080/123872 (37%)]\tLoss: 0.372237\n",
            "Train Epoch: 10 [46336/123872 (37%)]\tLoss: 0.394047\tLR: 0.00017919\n",
            "Train Epoch: 10 [46592/123872 (38%)]\tLoss: 0.347105\tLR: 0.00017909\n",
            "Train Epoch: 10 [46848/123872 (38%)]\tLoss: 0.405321\tLR: 0.00017899\n",
            "Train Epoch: 10 [47104/123872 (38%)]\tLoss: 0.378748\tLR: 0.00017889\n",
            "Train Epoch: 10 [47360/123872 (38%)]\tLoss: 0.441932\tLR: 0.00017880\n",
            "Train Epoch: 10 [47616/123872 (38%)]\tLoss: 0.385608\tLR: 0.00017870\n",
            "Train Epoch: 10 [47872/123872 (39%)]\tLoss: 0.409959\tLR: 0.00017860\n",
            "Train Epoch: 10 [48128/123872 (39%)]\tLoss: 0.419849\tLR: 0.00017850\n",
            "Train Epoch: 10 [48384/123872 (39%)]\tLoss: 0.454011\tLR: 0.00017841\n",
            "Train Epoch: 10 [48640/123872 (39%)]\tLoss: 0.395674\tLR: 0.00017831\n",
            "Train Epoch: 10 [48640/123872 (39%)]\tLoss: 0.395674\n",
            "Train Epoch: 10 [48896/123872 (39%)]\tLoss: 0.407516\tLR: 0.00017821\n",
            "Train Epoch: 10 [49152/123872 (40%)]\tLoss: 0.397227\tLR: 0.00017811\n",
            "Train Epoch: 10 [49408/123872 (40%)]\tLoss: 0.362581\tLR: 0.00017802\n",
            "Train Epoch: 10 [49664/123872 (40%)]\tLoss: 0.370334\tLR: 0.00017792\n",
            "Train Epoch: 10 [49920/123872 (40%)]\tLoss: 0.389615\tLR: 0.00017782\n",
            "Train Epoch: 10 [50176/123872 (40%)]\tLoss: 0.438470\tLR: 0.00017772\n",
            "Train Epoch: 10 [50432/123872 (41%)]\tLoss: 0.362258\tLR: 0.00017763\n",
            "Train Epoch: 10 [50688/123872 (41%)]\tLoss: 0.384270\tLR: 0.00017753\n",
            "Train Epoch: 10 [50944/123872 (41%)]\tLoss: 0.387385\tLR: 0.00017743\n",
            "Train Epoch: 10 [51200/123872 (41%)]\tLoss: 0.348133\tLR: 0.00017733\n",
            "Train Epoch: 10 [51200/123872 (41%)]\tLoss: 0.348133\n",
            "Train Epoch: 10 [51456/123872 (42%)]\tLoss: 0.432225\tLR: 0.00017724\n",
            "Train Epoch: 10 [51712/123872 (42%)]\tLoss: 0.424012\tLR: 0.00017714\n",
            "Train Epoch: 10 [51968/123872 (42%)]\tLoss: 0.377138\tLR: 0.00017704\n",
            "Train Epoch: 10 [52224/123872 (42%)]\tLoss: 0.381718\tLR: 0.00017694\n",
            "Train Epoch: 10 [52480/123872 (42%)]\tLoss: 0.310952\tLR: 0.00017685\n",
            "Train Epoch: 10 [52736/123872 (43%)]\tLoss: 0.354652\tLR: 0.00017675\n",
            "Train Epoch: 10 [52992/123872 (43%)]\tLoss: 0.380246\tLR: 0.00017665\n",
            "Train Epoch: 10 [53248/123872 (43%)]\tLoss: 0.394874\tLR: 0.00017656\n",
            "Train Epoch: 10 [53504/123872 (43%)]\tLoss: 0.405704\tLR: 0.00017646\n",
            "Train Epoch: 10 [53760/123872 (43%)]\tLoss: 0.419091\tLR: 0.00017636\n",
            "Train Epoch: 10 [53760/123872 (43%)]\tLoss: 0.419091\n",
            "Train Epoch: 10 [54016/123872 (44%)]\tLoss: 0.412977\tLR: 0.00017626\n",
            "Train Epoch: 10 [54272/123872 (44%)]\tLoss: 0.427325\tLR: 0.00017617\n",
            "Train Epoch: 10 [54528/123872 (44%)]\tLoss: 0.401272\tLR: 0.00017607\n",
            "Train Epoch: 10 [54784/123872 (44%)]\tLoss: 0.466305\tLR: 0.00017597\n",
            "Train Epoch: 10 [55040/123872 (44%)]\tLoss: 0.454137\tLR: 0.00017588\n",
            "Train Epoch: 10 [55296/123872 (45%)]\tLoss: 0.387688\tLR: 0.00017578\n",
            "Train Epoch: 10 [55552/123872 (45%)]\tLoss: 0.397364\tLR: 0.00017568\n",
            "Train Epoch: 10 [55808/123872 (45%)]\tLoss: 0.385115\tLR: 0.00017559\n",
            "Train Epoch: 10 [56064/123872 (45%)]\tLoss: 0.363157\tLR: 0.00017549\n",
            "Train Epoch: 10 [56320/123872 (45%)]\tLoss: 0.448101\tLR: 0.00017539\n",
            "Train Epoch: 10 [56320/123872 (45%)]\tLoss: 0.448101\n",
            "Train Epoch: 10 [56576/123872 (46%)]\tLoss: 0.383170\tLR: 0.00017530\n",
            "Train Epoch: 10 [56832/123872 (46%)]\tLoss: 0.470799\tLR: 0.00017520\n",
            "Train Epoch: 10 [57088/123872 (46%)]\tLoss: 0.411439\tLR: 0.00017510\n",
            "Train Epoch: 10 [57344/123872 (46%)]\tLoss: 0.365749\tLR: 0.00017501\n",
            "Train Epoch: 10 [57600/123872 (46%)]\tLoss: 0.365485\tLR: 0.00017491\n",
            "Train Epoch: 10 [57856/123872 (47%)]\tLoss: 0.362061\tLR: 0.00017481\n",
            "Train Epoch: 10 [58112/123872 (47%)]\tLoss: 0.397382\tLR: 0.00017472\n",
            "Train Epoch: 10 [58368/123872 (47%)]\tLoss: 0.433921\tLR: 0.00017462\n",
            "Train Epoch: 10 [58624/123872 (47%)]\tLoss: 0.415645\tLR: 0.00017452\n",
            "Train Epoch: 10 [58880/123872 (48%)]\tLoss: 0.447079\tLR: 0.00017443\n",
            "Train Epoch: 10 [58880/123872 (48%)]\tLoss: 0.447079\n",
            "Train Epoch: 10 [59136/123872 (48%)]\tLoss: 0.420084\tLR: 0.00017433\n",
            "Train Epoch: 10 [59392/123872 (48%)]\tLoss: 0.381950\tLR: 0.00017423\n",
            "Train Epoch: 10 [59648/123872 (48%)]\tLoss: 0.412454\tLR: 0.00017414\n",
            "Train Epoch: 10 [59904/123872 (48%)]\tLoss: 0.342058\tLR: 0.00017404\n",
            "Train Epoch: 10 [60160/123872 (49%)]\tLoss: 0.473554\tLR: 0.00017394\n",
            "Train Epoch: 10 [60416/123872 (49%)]\tLoss: 0.423846\tLR: 0.00017385\n",
            "Train Epoch: 10 [60672/123872 (49%)]\tLoss: 0.371735\tLR: 0.00017375\n",
            "Train Epoch: 10 [60928/123872 (49%)]\tLoss: 0.395147\tLR: 0.00017366\n",
            "Train Epoch: 10 [61184/123872 (49%)]\tLoss: 0.409143\tLR: 0.00017356\n",
            "Train Epoch: 10 [61440/123872 (50%)]\tLoss: 0.399111\tLR: 0.00017346\n",
            "Train Epoch: 10 [61440/123872 (50%)]\tLoss: 0.399111\n",
            "Train Epoch: 10 [61696/123872 (50%)]\tLoss: 0.376157\tLR: 0.00017337\n",
            "Train Epoch: 10 [61952/123872 (50%)]\tLoss: 0.366244\tLR: 0.00017327\n",
            "Train Epoch: 10 [62208/123872 (50%)]\tLoss: 0.388014\tLR: 0.00017317\n",
            "Train Epoch: 10 [62464/123872 (50%)]\tLoss: 0.355784\tLR: 0.00017308\n",
            "Train Epoch: 10 [62720/123872 (51%)]\tLoss: 0.354554\tLR: 0.00017298\n",
            "Train Epoch: 10 [62976/123872 (51%)]\tLoss: 0.373670\tLR: 0.00017289\n",
            "Train Epoch: 10 [63232/123872 (51%)]\tLoss: 0.369817\tLR: 0.00017279\n",
            "Train Epoch: 10 [63488/123872 (51%)]\tLoss: 0.336998\tLR: 0.00017270\n",
            "Train Epoch: 10 [63744/123872 (51%)]\tLoss: 0.392132\tLR: 0.00017260\n",
            "Train Epoch: 10 [64000/123872 (52%)]\tLoss: 0.408808\tLR: 0.00017250\n",
            "Train Epoch: 10 [64000/123872 (52%)]\tLoss: 0.408808\n",
            "Train Epoch: 10 [64256/123872 (52%)]\tLoss: 0.405469\tLR: 0.00017241\n",
            "Train Epoch: 10 [64512/123872 (52%)]\tLoss: 0.376170\tLR: 0.00017231\n",
            "Train Epoch: 10 [64768/123872 (52%)]\tLoss: 0.323937\tLR: 0.00017222\n",
            "Train Epoch: 10 [65024/123872 (52%)]\tLoss: 0.398577\tLR: 0.00017212\n",
            "Train Epoch: 10 [65280/123872 (53%)]\tLoss: 0.329929\tLR: 0.00017202\n",
            "Train Epoch: 10 [65536/123872 (53%)]\tLoss: 0.423469\tLR: 0.00017193\n",
            "Train Epoch: 10 [65792/123872 (53%)]\tLoss: 0.363921\tLR: 0.00017183\n",
            "Train Epoch: 10 [66048/123872 (53%)]\tLoss: 0.406129\tLR: 0.00017174\n",
            "Train Epoch: 10 [66304/123872 (54%)]\tLoss: 0.387532\tLR: 0.00017164\n",
            "Train Epoch: 10 [66560/123872 (54%)]\tLoss: 0.409396\tLR: 0.00017155\n",
            "Train Epoch: 10 [66560/123872 (54%)]\tLoss: 0.409396\n",
            "Train Epoch: 10 [66816/123872 (54%)]\tLoss: 0.367279\tLR: 0.00017145\n",
            "Train Epoch: 10 [67072/123872 (54%)]\tLoss: 0.357672\tLR: 0.00017136\n",
            "Train Epoch: 10 [67328/123872 (54%)]\tLoss: 0.417157\tLR: 0.00017126\n",
            "Train Epoch: 10 [67584/123872 (55%)]\tLoss: 0.433431\tLR: 0.00017116\n",
            "Train Epoch: 10 [67840/123872 (55%)]\tLoss: 0.378050\tLR: 0.00017107\n",
            "Train Epoch: 10 [68096/123872 (55%)]\tLoss: 0.393287\tLR: 0.00017097\n",
            "Train Epoch: 10 [68352/123872 (55%)]\tLoss: 0.395125\tLR: 0.00017088\n",
            "Train Epoch: 10 [68608/123872 (55%)]\tLoss: 0.417054\tLR: 0.00017078\n",
            "Train Epoch: 10 [68864/123872 (56%)]\tLoss: 0.439752\tLR: 0.00017069\n",
            "Train Epoch: 10 [69120/123872 (56%)]\tLoss: 0.458890\tLR: 0.00017059\n",
            "Train Epoch: 10 [69120/123872 (56%)]\tLoss: 0.458890\n",
            "Train Epoch: 10 [69376/123872 (56%)]\tLoss: 0.320358\tLR: 0.00017050\n",
            "Train Epoch: 10 [69632/123872 (56%)]\tLoss: 0.389999\tLR: 0.00017040\n",
            "Train Epoch: 10 [69888/123872 (56%)]\tLoss: 0.425556\tLR: 0.00017031\n",
            "Train Epoch: 10 [70144/123872 (57%)]\tLoss: 0.399448\tLR: 0.00017021\n",
            "Train Epoch: 10 [70400/123872 (57%)]\tLoss: 0.404759\tLR: 0.00017012\n",
            "Train Epoch: 10 [70656/123872 (57%)]\tLoss: 0.378152\tLR: 0.00017002\n",
            "Train Epoch: 10 [70912/123872 (57%)]\tLoss: 0.473071\tLR: 0.00016993\n",
            "Train Epoch: 10 [71168/123872 (57%)]\tLoss: 0.420806\tLR: 0.00016983\n",
            "Train Epoch: 10 [71424/123872 (58%)]\tLoss: 0.378704\tLR: 0.00016974\n",
            "Train Epoch: 10 [71680/123872 (58%)]\tLoss: 0.361464\tLR: 0.00016964\n",
            "Train Epoch: 10 [71680/123872 (58%)]\tLoss: 0.361464\n",
            "Train Epoch: 10 [71936/123872 (58%)]\tLoss: 0.371512\tLR: 0.00016955\n",
            "Train Epoch: 10 [72192/123872 (58%)]\tLoss: 0.413513\tLR: 0.00016945\n",
            "Train Epoch: 10 [72448/123872 (58%)]\tLoss: 0.360111\tLR: 0.00016936\n",
            "Train Epoch: 10 [72704/123872 (59%)]\tLoss: 0.403282\tLR: 0.00016926\n",
            "Train Epoch: 10 [72960/123872 (59%)]\tLoss: 0.392135\tLR: 0.00016917\n",
            "Train Epoch: 10 [73216/123872 (59%)]\tLoss: 0.376934\tLR: 0.00016907\n",
            "Train Epoch: 10 [73472/123872 (59%)]\tLoss: 0.431148\tLR: 0.00016898\n",
            "Train Epoch: 10 [73728/123872 (60%)]\tLoss: 0.315816\tLR: 0.00016888\n",
            "Train Epoch: 10 [73984/123872 (60%)]\tLoss: 0.331506\tLR: 0.00016879\n",
            "Train Epoch: 10 [74240/123872 (60%)]\tLoss: 0.358125\tLR: 0.00016869\n",
            "Train Epoch: 10 [74240/123872 (60%)]\tLoss: 0.358125\n",
            "Train Epoch: 10 [74496/123872 (60%)]\tLoss: 0.339630\tLR: 0.00016860\n",
            "Train Epoch: 10 [74752/123872 (60%)]\tLoss: 0.379220\tLR: 0.00016850\n",
            "Train Epoch: 10 [75008/123872 (61%)]\tLoss: 0.395431\tLR: 0.00016841\n",
            "Train Epoch: 10 [75264/123872 (61%)]\tLoss: 0.398090\tLR: 0.00016832\n",
            "Train Epoch: 10 [75520/123872 (61%)]\tLoss: 0.377613\tLR: 0.00016822\n",
            "Train Epoch: 10 [75776/123872 (61%)]\tLoss: 0.377911\tLR: 0.00016813\n",
            "Train Epoch: 10 [76032/123872 (61%)]\tLoss: 0.409899\tLR: 0.00016803\n",
            "Train Epoch: 10 [76288/123872 (62%)]\tLoss: 0.358058\tLR: 0.00016794\n",
            "Train Epoch: 10 [76544/123872 (62%)]\tLoss: 0.414957\tLR: 0.00016784\n",
            "Train Epoch: 10 [76800/123872 (62%)]\tLoss: 0.441733\tLR: 0.00016775\n",
            "Train Epoch: 10 [76800/123872 (62%)]\tLoss: 0.441733\n",
            "Train Epoch: 10 [77056/123872 (62%)]\tLoss: 0.442468\tLR: 0.00016765\n",
            "Train Epoch: 10 [77312/123872 (62%)]\tLoss: 0.372368\tLR: 0.00016756\n",
            "Train Epoch: 10 [77568/123872 (63%)]\tLoss: 0.370254\tLR: 0.00016747\n",
            "Train Epoch: 10 [77824/123872 (63%)]\tLoss: 0.413704\tLR: 0.00016737\n",
            "Train Epoch: 10 [78080/123872 (63%)]\tLoss: 0.397201\tLR: 0.00016728\n",
            "Train Epoch: 10 [78336/123872 (63%)]\tLoss: 0.445362\tLR: 0.00016718\n",
            "Train Epoch: 10 [78592/123872 (63%)]\tLoss: 0.443255\tLR: 0.00016709\n",
            "Train Epoch: 10 [78848/123872 (64%)]\tLoss: 0.349543\tLR: 0.00016700\n",
            "Train Epoch: 10 [79104/123872 (64%)]\tLoss: 0.451930\tLR: 0.00016690\n",
            "Train Epoch: 10 [79360/123872 (64%)]\tLoss: 0.372936\tLR: 0.00016681\n",
            "Train Epoch: 10 [79360/123872 (64%)]\tLoss: 0.372936\n",
            "Train Epoch: 10 [79616/123872 (64%)]\tLoss: 0.381679\tLR: 0.00016671\n",
            "Train Epoch: 10 [79872/123872 (64%)]\tLoss: 0.394713\tLR: 0.00016662\n",
            "Train Epoch: 10 [80128/123872 (65%)]\tLoss: 0.358993\tLR: 0.00016653\n",
            "Train Epoch: 10 [80384/123872 (65%)]\tLoss: 0.382411\tLR: 0.00016643\n",
            "Train Epoch: 10 [80640/123872 (65%)]\tLoss: 0.414714\tLR: 0.00016634\n",
            "Train Epoch: 10 [80896/123872 (65%)]\tLoss: 0.362039\tLR: 0.00016624\n",
            "Train Epoch: 10 [81152/123872 (65%)]\tLoss: 0.400259\tLR: 0.00016615\n",
            "Train Epoch: 10 [81408/123872 (66%)]\tLoss: 0.332532\tLR: 0.00016606\n",
            "Train Epoch: 10 [81664/123872 (66%)]\tLoss: 0.448438\tLR: 0.00016596\n",
            "Train Epoch: 10 [81920/123872 (66%)]\tLoss: 0.375300\tLR: 0.00016587\n",
            "Train Epoch: 10 [81920/123872 (66%)]\tLoss: 0.375300\n",
            "Train Epoch: 10 [82176/123872 (66%)]\tLoss: 0.391761\tLR: 0.00016577\n",
            "Train Epoch: 10 [82432/123872 (67%)]\tLoss: 0.396518\tLR: 0.00016568\n",
            "Train Epoch: 10 [82688/123872 (67%)]\tLoss: 0.419114\tLR: 0.00016559\n",
            "Train Epoch: 10 [82944/123872 (67%)]\tLoss: 0.381762\tLR: 0.00016549\n",
            "Train Epoch: 10 [83200/123872 (67%)]\tLoss: 0.436182\tLR: 0.00016540\n",
            "Train Epoch: 10 [83456/123872 (67%)]\tLoss: 0.402202\tLR: 0.00016531\n",
            "Train Epoch: 10 [83712/123872 (68%)]\tLoss: 0.437266\tLR: 0.00016521\n",
            "Train Epoch: 10 [83968/123872 (68%)]\tLoss: 0.446741\tLR: 0.00016512\n",
            "Train Epoch: 10 [84224/123872 (68%)]\tLoss: 0.449156\tLR: 0.00016503\n",
            "Train Epoch: 10 [84480/123872 (68%)]\tLoss: 0.373980\tLR: 0.00016493\n",
            "Train Epoch: 10 [84480/123872 (68%)]\tLoss: 0.373980\n",
            "Train Epoch: 10 [84736/123872 (68%)]\tLoss: 0.365890\tLR: 0.00016484\n",
            "Train Epoch: 10 [84992/123872 (69%)]\tLoss: 0.400421\tLR: 0.00016475\n",
            "Train Epoch: 10 [85248/123872 (69%)]\tLoss: 0.518775\tLR: 0.00016465\n",
            "Train Epoch: 10 [85504/123872 (69%)]\tLoss: 0.420685\tLR: 0.00016456\n",
            "Train Epoch: 10 [85760/123872 (69%)]\tLoss: 0.426407\tLR: 0.00016447\n",
            "Train Epoch: 10 [86016/123872 (69%)]\tLoss: 0.373602\tLR: 0.00016437\n",
            "Train Epoch: 10 [86272/123872 (70%)]\tLoss: 0.370668\tLR: 0.00016428\n",
            "Train Epoch: 10 [86528/123872 (70%)]\tLoss: 0.417614\tLR: 0.00016419\n",
            "Train Epoch: 10 [86784/123872 (70%)]\tLoss: 0.360144\tLR: 0.00016409\n",
            "Train Epoch: 10 [87040/123872 (70%)]\tLoss: 0.396704\tLR: 0.00016400\n",
            "Train Epoch: 10 [87040/123872 (70%)]\tLoss: 0.396704\n",
            "Train Epoch: 10 [87296/123872 (70%)]\tLoss: 0.408070\tLR: 0.00016391\n",
            "Train Epoch: 10 [87552/123872 (71%)]\tLoss: 0.393364\tLR: 0.00016381\n",
            "Train Epoch: 10 [87808/123872 (71%)]\tLoss: 0.431738\tLR: 0.00016372\n",
            "Train Epoch: 10 [88064/123872 (71%)]\tLoss: 0.392362\tLR: 0.00016363\n",
            "Train Epoch: 10 [88320/123872 (71%)]\tLoss: 0.446500\tLR: 0.00016354\n",
            "Train Epoch: 10 [88576/123872 (71%)]\tLoss: 0.376655\tLR: 0.00016344\n",
            "Train Epoch: 10 [88832/123872 (72%)]\tLoss: 0.384392\tLR: 0.00016335\n",
            "Train Epoch: 10 [89088/123872 (72%)]\tLoss: 0.451890\tLR: 0.00016326\n",
            "Train Epoch: 10 [89344/123872 (72%)]\tLoss: 0.424673\tLR: 0.00016316\n",
            "Train Epoch: 10 [89600/123872 (72%)]\tLoss: 0.412703\tLR: 0.00016307\n",
            "Train Epoch: 10 [89600/123872 (72%)]\tLoss: 0.412703\n",
            "Train Epoch: 10 [89856/123872 (73%)]\tLoss: 0.359372\tLR: 0.00016298\n",
            "Train Epoch: 10 [90112/123872 (73%)]\tLoss: 0.366613\tLR: 0.00016289\n",
            "Train Epoch: 10 [90368/123872 (73%)]\tLoss: 0.426703\tLR: 0.00016279\n",
            "Train Epoch: 10 [90624/123872 (73%)]\tLoss: 0.399058\tLR: 0.00016270\n",
            "Train Epoch: 10 [90880/123872 (73%)]\tLoss: 0.381304\tLR: 0.00016261\n",
            "Train Epoch: 10 [91136/123872 (74%)]\tLoss: 0.367016\tLR: 0.00016252\n",
            "Train Epoch: 10 [91392/123872 (74%)]\tLoss: 0.380462\tLR: 0.00016242\n",
            "Train Epoch: 10 [91648/123872 (74%)]\tLoss: 0.385075\tLR: 0.00016233\n",
            "Train Epoch: 10 [91904/123872 (74%)]\tLoss: 0.403518\tLR: 0.00016224\n",
            "Train Epoch: 10 [92160/123872 (74%)]\tLoss: 0.443668\tLR: 0.00016215\n",
            "Train Epoch: 10 [92160/123872 (74%)]\tLoss: 0.443668\n",
            "Train Epoch: 10 [92416/123872 (75%)]\tLoss: 0.408063\tLR: 0.00016205\n",
            "Train Epoch: 10 [92672/123872 (75%)]\tLoss: 0.374453\tLR: 0.00016196\n",
            "Train Epoch: 10 [92928/123872 (75%)]\tLoss: 0.382428\tLR: 0.00016187\n",
            "Train Epoch: 10 [93184/123872 (75%)]\tLoss: 0.431280\tLR: 0.00016178\n",
            "Train Epoch: 10 [93440/123872 (75%)]\tLoss: 0.380147\tLR: 0.00016168\n",
            "Train Epoch: 10 [93696/123872 (76%)]\tLoss: 0.363018\tLR: 0.00016159\n",
            "Train Epoch: 10 [93952/123872 (76%)]\tLoss: 0.449407\tLR: 0.00016150\n",
            "Train Epoch: 10 [94208/123872 (76%)]\tLoss: 0.425528\tLR: 0.00016141\n",
            "Train Epoch: 10 [94464/123872 (76%)]\tLoss: 0.387403\tLR: 0.00016132\n",
            "Train Epoch: 10 [94720/123872 (76%)]\tLoss: 0.384852\tLR: 0.00016122\n",
            "Train Epoch: 10 [94720/123872 (76%)]\tLoss: 0.384852\n",
            "Train Epoch: 10 [94976/123872 (77%)]\tLoss: 0.355529\tLR: 0.00016113\n",
            "Train Epoch: 10 [95232/123872 (77%)]\tLoss: 0.387353\tLR: 0.00016104\n",
            "Train Epoch: 10 [95488/123872 (77%)]\tLoss: 0.380419\tLR: 0.00016095\n",
            "Train Epoch: 10 [95744/123872 (77%)]\tLoss: 0.407548\tLR: 0.00016085\n",
            "Train Epoch: 10 [96000/123872 (77%)]\tLoss: 0.401243\tLR: 0.00016076\n",
            "Train Epoch: 10 [96256/123872 (78%)]\tLoss: 0.417469\tLR: 0.00016067\n",
            "Train Epoch: 10 [96512/123872 (78%)]\tLoss: 0.351146\tLR: 0.00016058\n",
            "Train Epoch: 10 [96768/123872 (78%)]\tLoss: 0.419265\tLR: 0.00016049\n",
            "Train Epoch: 10 [97024/123872 (78%)]\tLoss: 0.383250\tLR: 0.00016040\n",
            "Train Epoch: 10 [97280/123872 (79%)]\tLoss: 0.393828\tLR: 0.00016030\n",
            "Train Epoch: 10 [97280/123872 (79%)]\tLoss: 0.393828\n",
            "Train Epoch: 10 [97536/123872 (79%)]\tLoss: 0.378734\tLR: 0.00016021\n",
            "Train Epoch: 10 [97792/123872 (79%)]\tLoss: 0.422589\tLR: 0.00016012\n",
            "Train Epoch: 10 [98048/123872 (79%)]\tLoss: 0.345089\tLR: 0.00016003\n",
            "Train Epoch: 10 [98304/123872 (79%)]\tLoss: 0.388924\tLR: 0.00015994\n",
            "Train Epoch: 10 [98560/123872 (80%)]\tLoss: 0.277577\tLR: 0.00015985\n",
            "Train Epoch: 10 [98816/123872 (80%)]\tLoss: 0.449572\tLR: 0.00015975\n",
            "Train Epoch: 10 [99072/123872 (80%)]\tLoss: 0.379604\tLR: 0.00015966\n",
            "Train Epoch: 10 [99328/123872 (80%)]\tLoss: 0.405235\tLR: 0.00015957\n",
            "Train Epoch: 10 [99584/123872 (80%)]\tLoss: 0.424349\tLR: 0.00015948\n",
            "Train Epoch: 10 [99840/123872 (81%)]\tLoss: 0.426826\tLR: 0.00015939\n",
            "Train Epoch: 10 [99840/123872 (81%)]\tLoss: 0.426826\n",
            "Train Epoch: 10 [100096/123872 (81%)]\tLoss: 0.441317\tLR: 0.00015930\n",
            "Train Epoch: 10 [100352/123872 (81%)]\tLoss: 0.366658\tLR: 0.00015920\n",
            "Train Epoch: 10 [100608/123872 (81%)]\tLoss: 0.370434\tLR: 0.00015911\n",
            "Train Epoch: 10 [100864/123872 (81%)]\tLoss: 0.393363\tLR: 0.00015902\n",
            "Train Epoch: 10 [101120/123872 (82%)]\tLoss: 0.358112\tLR: 0.00015893\n",
            "Train Epoch: 10 [101376/123872 (82%)]\tLoss: 0.376527\tLR: 0.00015884\n",
            "Train Epoch: 10 [101632/123872 (82%)]\tLoss: 0.363755\tLR: 0.00015875\n",
            "Train Epoch: 10 [101888/123872 (82%)]\tLoss: 0.348446\tLR: 0.00015866\n",
            "Train Epoch: 10 [102144/123872 (82%)]\tLoss: 0.392657\tLR: 0.00015857\n",
            "Train Epoch: 10 [102400/123872 (83%)]\tLoss: 0.386458\tLR: 0.00015847\n",
            "Train Epoch: 10 [102400/123872 (83%)]\tLoss: 0.386458\n",
            "Train Epoch: 10 [102656/123872 (83%)]\tLoss: 0.414382\tLR: 0.00015838\n",
            "Train Epoch: 10 [102912/123872 (83%)]\tLoss: 0.468174\tLR: 0.00015829\n",
            "Train Epoch: 10 [103168/123872 (83%)]\tLoss: 0.397963\tLR: 0.00015820\n",
            "Train Epoch: 10 [103424/123872 (83%)]\tLoss: 0.380683\tLR: 0.00015811\n",
            "Train Epoch: 10 [103680/123872 (84%)]\tLoss: 0.333896\tLR: 0.00015802\n",
            "Train Epoch: 10 [103936/123872 (84%)]\tLoss: 0.391364\tLR: 0.00015793\n",
            "Train Epoch: 10 [104192/123872 (84%)]\tLoss: 0.382599\tLR: 0.00015784\n",
            "Train Epoch: 10 [104448/123872 (84%)]\tLoss: 0.410019\tLR: 0.00015775\n",
            "Train Epoch: 10 [104704/123872 (85%)]\tLoss: 0.397508\tLR: 0.00015766\n",
            "Train Epoch: 10 [104960/123872 (85%)]\tLoss: 0.362744\tLR: 0.00015756\n",
            "Train Epoch: 10 [104960/123872 (85%)]\tLoss: 0.362744\n",
            "Train Epoch: 10 [105216/123872 (85%)]\tLoss: 0.341729\tLR: 0.00015747\n",
            "Train Epoch: 10 [105472/123872 (85%)]\tLoss: 0.460989\tLR: 0.00015738\n",
            "Train Epoch: 10 [105728/123872 (85%)]\tLoss: 0.365924\tLR: 0.00015729\n",
            "Train Epoch: 10 [105984/123872 (86%)]\tLoss: 0.375683\tLR: 0.00015720\n",
            "Train Epoch: 10 [106240/123872 (86%)]\tLoss: 0.481260\tLR: 0.00015711\n",
            "Train Epoch: 10 [106496/123872 (86%)]\tLoss: 0.404847\tLR: 0.00015702\n",
            "Train Epoch: 10 [106752/123872 (86%)]\tLoss: 0.384579\tLR: 0.00015693\n",
            "Train Epoch: 10 [107008/123872 (86%)]\tLoss: 0.394690\tLR: 0.00015684\n",
            "Train Epoch: 10 [107264/123872 (87%)]\tLoss: 0.385753\tLR: 0.00015675\n",
            "Train Epoch: 10 [107520/123872 (87%)]\tLoss: 0.419964\tLR: 0.00015666\n",
            "Train Epoch: 10 [107520/123872 (87%)]\tLoss: 0.419964\n",
            "Train Epoch: 10 [107776/123872 (87%)]\tLoss: 0.425671\tLR: 0.00015657\n",
            "Train Epoch: 10 [108032/123872 (87%)]\tLoss: 0.394623\tLR: 0.00015648\n",
            "Train Epoch: 10 [108288/123872 (87%)]\tLoss: 0.460118\tLR: 0.00015639\n",
            "Train Epoch: 10 [108544/123872 (88%)]\tLoss: 0.393008\tLR: 0.00015630\n",
            "Train Epoch: 10 [108800/123872 (88%)]\tLoss: 0.358224\tLR: 0.00015621\n",
            "Train Epoch: 10 [109056/123872 (88%)]\tLoss: 0.425387\tLR: 0.00015612\n",
            "Train Epoch: 10 [109312/123872 (88%)]\tLoss: 0.348689\tLR: 0.00015603\n",
            "Train Epoch: 10 [109568/123872 (88%)]\tLoss: 0.326260\tLR: 0.00015594\n",
            "Train Epoch: 10 [109824/123872 (89%)]\tLoss: 0.343617\tLR: 0.00015585\n",
            "Train Epoch: 10 [110080/123872 (89%)]\tLoss: 0.356715\tLR: 0.00015576\n",
            "Train Epoch: 10 [110080/123872 (89%)]\tLoss: 0.356715\n",
            "Train Epoch: 10 [110336/123872 (89%)]\tLoss: 0.355110\tLR: 0.00015567\n",
            "Train Epoch: 10 [110592/123872 (89%)]\tLoss: 0.330776\tLR: 0.00015558\n",
            "Train Epoch: 10 [110848/123872 (89%)]\tLoss: 0.397174\tLR: 0.00015548\n",
            "Train Epoch: 10 [111104/123872 (90%)]\tLoss: 0.388288\tLR: 0.00015539\n",
            "Train Epoch: 10 [111360/123872 (90%)]\tLoss: 0.353348\tLR: 0.00015530\n",
            "Train Epoch: 10 [111616/123872 (90%)]\tLoss: 0.414140\tLR: 0.00015522\n",
            "Train Epoch: 10 [111872/123872 (90%)]\tLoss: 0.421961\tLR: 0.00015513\n",
            "Train Epoch: 10 [112128/123872 (90%)]\tLoss: 0.406724\tLR: 0.00015504\n",
            "Train Epoch: 10 [112384/123872 (91%)]\tLoss: 0.329689\tLR: 0.00015495\n",
            "Train Epoch: 10 [112640/123872 (91%)]\tLoss: 0.397435\tLR: 0.00015486\n",
            "Train Epoch: 10 [112640/123872 (91%)]\tLoss: 0.397435\n",
            "Train Epoch: 10 [112896/123872 (91%)]\tLoss: 0.346355\tLR: 0.00015477\n",
            "Train Epoch: 10 [113152/123872 (91%)]\tLoss: 0.422266\tLR: 0.00015468\n",
            "Train Epoch: 10 [113408/123872 (92%)]\tLoss: 0.361357\tLR: 0.00015459\n",
            "Train Epoch: 10 [113664/123872 (92%)]\tLoss: 0.402340\tLR: 0.00015450\n",
            "Train Epoch: 10 [113920/123872 (92%)]\tLoss: 0.404868\tLR: 0.00015441\n",
            "Train Epoch: 10 [114176/123872 (92%)]\tLoss: 0.429098\tLR: 0.00015432\n",
            "Train Epoch: 10 [114432/123872 (92%)]\tLoss: 0.384192\tLR: 0.00015423\n",
            "Train Epoch: 10 [114688/123872 (93%)]\tLoss: 0.468285\tLR: 0.00015414\n",
            "Train Epoch: 10 [114944/123872 (93%)]\tLoss: 0.396909\tLR: 0.00015405\n",
            "Train Epoch: 10 [115200/123872 (93%)]\tLoss: 0.434407\tLR: 0.00015396\n",
            "Train Epoch: 10 [115200/123872 (93%)]\tLoss: 0.434407\n",
            "Train Epoch: 10 [115456/123872 (93%)]\tLoss: 0.478748\tLR: 0.00015387\n",
            "Train Epoch: 10 [115712/123872 (93%)]\tLoss: 0.356424\tLR: 0.00015378\n",
            "Train Epoch: 10 [115968/123872 (94%)]\tLoss: 0.415086\tLR: 0.00015369\n",
            "Train Epoch: 10 [116224/123872 (94%)]\tLoss: 0.394108\tLR: 0.00015360\n",
            "Train Epoch: 10 [116480/123872 (94%)]\tLoss: 0.420525\tLR: 0.00015351\n",
            "Train Epoch: 10 [116736/123872 (94%)]\tLoss: 0.372187\tLR: 0.00015342\n",
            "Train Epoch: 10 [116992/123872 (94%)]\tLoss: 0.394020\tLR: 0.00015333\n",
            "Train Epoch: 10 [117248/123872 (95%)]\tLoss: 0.363441\tLR: 0.00015324\n",
            "Train Epoch: 10 [117504/123872 (95%)]\tLoss: 0.397922\tLR: 0.00015316\n",
            "Train Epoch: 10 [117760/123872 (95%)]\tLoss: 0.449350\tLR: 0.00015307\n",
            "Train Epoch: 10 [117760/123872 (95%)]\tLoss: 0.449350\n",
            "Train Epoch: 10 [118016/123872 (95%)]\tLoss: 0.342192\tLR: 0.00015298\n",
            "Train Epoch: 10 [118272/123872 (95%)]\tLoss: 0.443955\tLR: 0.00015289\n",
            "Train Epoch: 10 [118528/123872 (96%)]\tLoss: 0.380606\tLR: 0.00015280\n",
            "Train Epoch: 10 [118784/123872 (96%)]\tLoss: 0.330504\tLR: 0.00015271\n",
            "Train Epoch: 10 [119040/123872 (96%)]\tLoss: 0.305406\tLR: 0.00015262\n",
            "Train Epoch: 10 [119296/123872 (96%)]\tLoss: 0.374025\tLR: 0.00015253\n",
            "Train Epoch: 10 [119552/123872 (96%)]\tLoss: 0.385369\tLR: 0.00015244\n",
            "Train Epoch: 10 [119808/123872 (97%)]\tLoss: 0.371747\tLR: 0.00015235\n",
            "Train Epoch: 10 [120064/123872 (97%)]\tLoss: 0.472081\tLR: 0.00015227\n",
            "Train Epoch: 10 [120320/123872 (97%)]\tLoss: 0.408070\tLR: 0.00015218\n",
            "Train Epoch: 10 [120320/123872 (97%)]\tLoss: 0.408070\n",
            "Train Epoch: 10 [120576/123872 (97%)]\tLoss: 0.338828\tLR: 0.00015209\n",
            "Train Epoch: 10 [120832/123872 (98%)]\tLoss: 0.376483\tLR: 0.00015200\n",
            "Train Epoch: 10 [121088/123872 (98%)]\tLoss: 0.417299\tLR: 0.00015191\n",
            "Train Epoch: 10 [121344/123872 (98%)]\tLoss: 0.353796\tLR: 0.00015182\n",
            "Train Epoch: 10 [121600/123872 (98%)]\tLoss: 0.402519\tLR: 0.00015173\n",
            "Train Epoch: 10 [121856/123872 (98%)]\tLoss: 0.330152\tLR: 0.00015164\n",
            "Train Epoch: 10 [122112/123872 (99%)]\tLoss: 0.409529\tLR: 0.00015156\n",
            "Train Epoch: 10 [122368/123872 (99%)]\tLoss: 0.481671\tLR: 0.00015147\n",
            "Train Epoch: 10 [122624/123872 (99%)]\tLoss: 0.406978\tLR: 0.00015138\n",
            "Train Epoch: 10 [122880/123872 (99%)]\tLoss: 0.425398\tLR: 0.00015129\n",
            "Train Epoch: 10 [122880/123872 (99%)]\tLoss: 0.425398\n",
            "Train Epoch: 10 [123136/123872 (99%)]\tLoss: 0.403169\tLR: 0.00015120\n",
            "Train Epoch: 10 [123392/123872 (100%)]\tLoss: 0.458656\tLR: 0.00015111\n",
            "Train Epoch: 10 [108192/123872 (100%)]\tLoss: 0.366870\tLR: 0.00015102\n",
            "\n",
            "Test set: Average loss: 0.0016, Accuracy: 25320/30970 (81.76%)\n",
            "\n",
            "Train Epoch: 11 [0/123872 (0%)]\tLoss: 0.370138\tLR: 0.00015094\n",
            "Train Epoch: 11 [0/123872 (0%)]\tLoss: 0.370138\n",
            "Train Epoch: 11 [256/123872 (0%)]\tLoss: 0.349065\tLR: 0.00015085\n",
            "Train Epoch: 11 [512/123872 (0%)]\tLoss: 0.437196\tLR: 0.00015076\n",
            "Train Epoch: 11 [768/123872 (1%)]\tLoss: 0.364966\tLR: 0.00015067\n",
            "Train Epoch: 11 [1024/123872 (1%)]\tLoss: 0.420900\tLR: 0.00015058\n",
            "Train Epoch: 11 [1280/123872 (1%)]\tLoss: 0.327809\tLR: 0.00015050\n",
            "Train Epoch: 11 [1536/123872 (1%)]\tLoss: 0.353595\tLR: 0.00015041\n",
            "Train Epoch: 11 [1792/123872 (1%)]\tLoss: 0.381735\tLR: 0.00015032\n",
            "Train Epoch: 11 [2048/123872 (2%)]\tLoss: 0.381348\tLR: 0.00015023\n",
            "Train Epoch: 11 [2304/123872 (2%)]\tLoss: 0.358498\tLR: 0.00015014\n",
            "Train Epoch: 11 [2560/123872 (2%)]\tLoss: 0.330162\tLR: 0.00015006\n",
            "Train Epoch: 11 [2560/123872 (2%)]\tLoss: 0.330162\n",
            "Train Epoch: 11 [2816/123872 (2%)]\tLoss: 0.371948\tLR: 0.00014997\n",
            "Train Epoch: 11 [3072/123872 (2%)]\tLoss: 0.371995\tLR: 0.00014988\n",
            "Train Epoch: 11 [3328/123872 (3%)]\tLoss: 0.362879\tLR: 0.00014979\n",
            "Train Epoch: 11 [3584/123872 (3%)]\tLoss: 0.337179\tLR: 0.00014970\n",
            "Train Epoch: 11 [3840/123872 (3%)]\tLoss: 0.341573\tLR: 0.00014962\n",
            "Train Epoch: 11 [4096/123872 (3%)]\tLoss: 0.348155\tLR: 0.00014953\n",
            "Train Epoch: 11 [4352/123872 (4%)]\tLoss: 0.405282\tLR: 0.00014944\n",
            "Train Epoch: 11 [4608/123872 (4%)]\tLoss: 0.444926\tLR: 0.00014935\n",
            "Train Epoch: 11 [4864/123872 (4%)]\tLoss: 0.367179\tLR: 0.00014926\n",
            "Train Epoch: 11 [5120/123872 (4%)]\tLoss: 0.391494\tLR: 0.00014918\n",
            "Train Epoch: 11 [5120/123872 (4%)]\tLoss: 0.391494\n",
            "Train Epoch: 11 [5376/123872 (4%)]\tLoss: 0.386282\tLR: 0.00014909\n",
            "Train Epoch: 11 [5632/123872 (5%)]\tLoss: 0.314767\tLR: 0.00014900\n",
            "Train Epoch: 11 [5888/123872 (5%)]\tLoss: 0.386938\tLR: 0.00014891\n",
            "Train Epoch: 11 [6144/123872 (5%)]\tLoss: 0.401441\tLR: 0.00014883\n",
            "Train Epoch: 11 [6400/123872 (5%)]\tLoss: 0.407209\tLR: 0.00014874\n",
            "Train Epoch: 11 [6656/123872 (5%)]\tLoss: 0.436710\tLR: 0.00014865\n",
            "Train Epoch: 11 [6912/123872 (6%)]\tLoss: 0.418858\tLR: 0.00014856\n",
            "Train Epoch: 11 [7168/123872 (6%)]\tLoss: 0.423212\tLR: 0.00014848\n",
            "Train Epoch: 11 [7424/123872 (6%)]\tLoss: 0.451121\tLR: 0.00014839\n",
            "Train Epoch: 11 [7680/123872 (6%)]\tLoss: 0.374233\tLR: 0.00014830\n",
            "Train Epoch: 11 [7680/123872 (6%)]\tLoss: 0.374233\n",
            "Train Epoch: 11 [7936/123872 (6%)]\tLoss: 0.345450\tLR: 0.00014822\n",
            "Train Epoch: 11 [8192/123872 (7%)]\tLoss: 0.398686\tLR: 0.00014813\n",
            "Train Epoch: 11 [8448/123872 (7%)]\tLoss: 0.373897\tLR: 0.00014804\n",
            "Train Epoch: 11 [8704/123872 (7%)]\tLoss: 0.376282\tLR: 0.00014795\n",
            "Train Epoch: 11 [8960/123872 (7%)]\tLoss: 0.374954\tLR: 0.00014787\n",
            "Train Epoch: 11 [9216/123872 (7%)]\tLoss: 0.357455\tLR: 0.00014778\n",
            "Train Epoch: 11 [9472/123872 (8%)]\tLoss: 0.369594\tLR: 0.00014769\n",
            "Train Epoch: 11 [9728/123872 (8%)]\tLoss: 0.366944\tLR: 0.00014761\n",
            "Train Epoch: 11 [9984/123872 (8%)]\tLoss: 0.360004\tLR: 0.00014752\n",
            "Train Epoch: 11 [10240/123872 (8%)]\tLoss: 0.461331\tLR: 0.00014743\n",
            "Train Epoch: 11 [10240/123872 (8%)]\tLoss: 0.461331\n",
            "Train Epoch: 11 [10496/123872 (8%)]\tLoss: 0.364058\tLR: 0.00014734\n",
            "Train Epoch: 11 [10752/123872 (9%)]\tLoss: 0.333580\tLR: 0.00014726\n",
            "Train Epoch: 11 [11008/123872 (9%)]\tLoss: 0.357824\tLR: 0.00014717\n",
            "Train Epoch: 11 [11264/123872 (9%)]\tLoss: 0.406406\tLR: 0.00014708\n",
            "Train Epoch: 11 [11520/123872 (9%)]\tLoss: 0.386940\tLR: 0.00014700\n",
            "Train Epoch: 11 [11776/123872 (10%)]\tLoss: 0.383047\tLR: 0.00014691\n",
            "Train Epoch: 11 [12032/123872 (10%)]\tLoss: 0.417330\tLR: 0.00014682\n",
            "Train Epoch: 11 [12288/123872 (10%)]\tLoss: 0.383498\tLR: 0.00014674\n",
            "Train Epoch: 11 [12544/123872 (10%)]\tLoss: 0.405356\tLR: 0.00014665\n",
            "Train Epoch: 11 [12800/123872 (10%)]\tLoss: 0.336249\tLR: 0.00014656\n",
            "Train Epoch: 11 [12800/123872 (10%)]\tLoss: 0.336249\n",
            "Train Epoch: 11 [13056/123872 (11%)]\tLoss: 0.395717\tLR: 0.00014648\n",
            "Train Epoch: 11 [13312/123872 (11%)]\tLoss: 0.412033\tLR: 0.00014639\n",
            "Train Epoch: 11 [13568/123872 (11%)]\tLoss: 0.349321\tLR: 0.00014630\n",
            "Train Epoch: 11 [13824/123872 (11%)]\tLoss: 0.456581\tLR: 0.00014622\n",
            "Train Epoch: 11 [14080/123872 (11%)]\tLoss: 0.350825\tLR: 0.00014613\n",
            "Train Epoch: 11 [14336/123872 (12%)]\tLoss: 0.408529\tLR: 0.00014604\n",
            "Train Epoch: 11 [14592/123872 (12%)]\tLoss: 0.350225\tLR: 0.00014596\n",
            "Train Epoch: 11 [14848/123872 (12%)]\tLoss: 0.361123\tLR: 0.00014587\n",
            "Train Epoch: 11 [15104/123872 (12%)]\tLoss: 0.404100\tLR: 0.00014579\n",
            "Train Epoch: 11 [15360/123872 (12%)]\tLoss: 0.439646\tLR: 0.00014570\n",
            "Train Epoch: 11 [15360/123872 (12%)]\tLoss: 0.439646\n",
            "Train Epoch: 11 [15616/123872 (13%)]\tLoss: 0.433418\tLR: 0.00014561\n",
            "Train Epoch: 11 [15872/123872 (13%)]\tLoss: 0.371224\tLR: 0.00014553\n",
            "Train Epoch: 11 [16128/123872 (13%)]\tLoss: 0.356202\tLR: 0.00014544\n",
            "Train Epoch: 11 [16384/123872 (13%)]\tLoss: 0.423276\tLR: 0.00014535\n",
            "Train Epoch: 11 [16640/123872 (13%)]\tLoss: 0.365648\tLR: 0.00014527\n",
            "Train Epoch: 11 [16896/123872 (14%)]\tLoss: 0.356844\tLR: 0.00014518\n",
            "Train Epoch: 11 [17152/123872 (14%)]\tLoss: 0.364652\tLR: 0.00014510\n",
            "Train Epoch: 11 [17408/123872 (14%)]\tLoss: 0.351677\tLR: 0.00014501\n",
            "Train Epoch: 11 [17664/123872 (14%)]\tLoss: 0.402443\tLR: 0.00014492\n",
            "Train Epoch: 11 [17920/123872 (14%)]\tLoss: 0.355442\tLR: 0.00014484\n",
            "Train Epoch: 11 [17920/123872 (14%)]\tLoss: 0.355442\n",
            "Train Epoch: 11 [18176/123872 (15%)]\tLoss: 0.398205\tLR: 0.00014475\n",
            "Train Epoch: 11 [18432/123872 (15%)]\tLoss: 0.387797\tLR: 0.00014467\n",
            "Train Epoch: 11 [18688/123872 (15%)]\tLoss: 0.367554\tLR: 0.00014458\n",
            "Train Epoch: 11 [18944/123872 (15%)]\tLoss: 0.397843\tLR: 0.00014450\n",
            "Train Epoch: 11 [19200/123872 (15%)]\tLoss: 0.403779\tLR: 0.00014441\n",
            "Train Epoch: 11 [19456/123872 (16%)]\tLoss: 0.395394\tLR: 0.00014432\n",
            "Train Epoch: 11 [19712/123872 (16%)]\tLoss: 0.346504\tLR: 0.00014424\n",
            "Train Epoch: 11 [19968/123872 (16%)]\tLoss: 0.363712\tLR: 0.00014415\n",
            "Train Epoch: 11 [20224/123872 (16%)]\tLoss: 0.329361\tLR: 0.00014407\n",
            "Train Epoch: 11 [20480/123872 (17%)]\tLoss: 0.404743\tLR: 0.00014398\n",
            "Train Epoch: 11 [20480/123872 (17%)]\tLoss: 0.404743\n",
            "Train Epoch: 11 [20736/123872 (17%)]\tLoss: 0.439139\tLR: 0.00014390\n",
            "Train Epoch: 11 [20992/123872 (17%)]\tLoss: 0.407681\tLR: 0.00014381\n",
            "Train Epoch: 11 [21248/123872 (17%)]\tLoss: 0.443728\tLR: 0.00014373\n",
            "Train Epoch: 11 [21504/123872 (17%)]\tLoss: 0.348128\tLR: 0.00014364\n",
            "Train Epoch: 11 [21760/123872 (18%)]\tLoss: 0.363593\tLR: 0.00014355\n",
            "Train Epoch: 11 [22016/123872 (18%)]\tLoss: 0.342107\tLR: 0.00014347\n",
            "Train Epoch: 11 [22272/123872 (18%)]\tLoss: 0.407478\tLR: 0.00014338\n",
            "Train Epoch: 11 [22528/123872 (18%)]\tLoss: 0.368527\tLR: 0.00014330\n",
            "Train Epoch: 11 [22784/123872 (18%)]\tLoss: 0.377030\tLR: 0.00014321\n",
            "Train Epoch: 11 [23040/123872 (19%)]\tLoss: 0.357667\tLR: 0.00014313\n",
            "Train Epoch: 11 [23040/123872 (19%)]\tLoss: 0.357667\n",
            "Train Epoch: 11 [23296/123872 (19%)]\tLoss: 0.395308\tLR: 0.00014304\n",
            "Train Epoch: 11 [23552/123872 (19%)]\tLoss: 0.379524\tLR: 0.00014296\n",
            "Train Epoch: 11 [23808/123872 (19%)]\tLoss: 0.335571\tLR: 0.00014287\n",
            "Train Epoch: 11 [24064/123872 (19%)]\tLoss: 0.411476\tLR: 0.00014279\n",
            "Train Epoch: 11 [24320/123872 (20%)]\tLoss: 0.372166\tLR: 0.00014270\n",
            "Train Epoch: 11 [24576/123872 (20%)]\tLoss: 0.391293\tLR: 0.00014262\n",
            "Train Epoch: 11 [24832/123872 (20%)]\tLoss: 0.372025\tLR: 0.00014253\n",
            "Train Epoch: 11 [25088/123872 (20%)]\tLoss: 0.305686\tLR: 0.00014245\n",
            "Train Epoch: 11 [25344/123872 (20%)]\tLoss: 0.377282\tLR: 0.00014236\n",
            "Train Epoch: 11 [25600/123872 (21%)]\tLoss: 0.342786\tLR: 0.00014228\n",
            "Train Epoch: 11 [25600/123872 (21%)]\tLoss: 0.342786\n",
            "Train Epoch: 11 [25856/123872 (21%)]\tLoss: 0.416605\tLR: 0.00014219\n",
            "Train Epoch: 11 [26112/123872 (21%)]\tLoss: 0.339775\tLR: 0.00014211\n",
            "Train Epoch: 11 [26368/123872 (21%)]\tLoss: 0.414049\tLR: 0.00014202\n",
            "Train Epoch: 11 [26624/123872 (21%)]\tLoss: 0.363545\tLR: 0.00014194\n",
            "Train Epoch: 11 [26880/123872 (22%)]\tLoss: 0.432401\tLR: 0.00014185\n",
            "Train Epoch: 11 [27136/123872 (22%)]\tLoss: 0.334499\tLR: 0.00014177\n",
            "Train Epoch: 11 [27392/123872 (22%)]\tLoss: 0.386606\tLR: 0.00014169\n",
            "Train Epoch: 11 [27648/123872 (22%)]\tLoss: 0.400391\tLR: 0.00014160\n",
            "Train Epoch: 11 [27904/123872 (23%)]\tLoss: 0.397650\tLR: 0.00014152\n",
            "Train Epoch: 11 [28160/123872 (23%)]\tLoss: 0.350627\tLR: 0.00014143\n",
            "Train Epoch: 11 [28160/123872 (23%)]\tLoss: 0.350627\n",
            "Train Epoch: 11 [28416/123872 (23%)]\tLoss: 0.394895\tLR: 0.00014135\n",
            "Train Epoch: 11 [28672/123872 (23%)]\tLoss: 0.391317\tLR: 0.00014126\n",
            "Train Epoch: 11 [28928/123872 (23%)]\tLoss: 0.371694\tLR: 0.00014118\n",
            "Train Epoch: 11 [29184/123872 (24%)]\tLoss: 0.458833\tLR: 0.00014109\n",
            "Train Epoch: 11 [29440/123872 (24%)]\tLoss: 0.415497\tLR: 0.00014101\n",
            "Train Epoch: 11 [29696/123872 (24%)]\tLoss: 0.364790\tLR: 0.00014093\n",
            "Train Epoch: 11 [29952/123872 (24%)]\tLoss: 0.492041\tLR: 0.00014084\n",
            "Train Epoch: 11 [30208/123872 (24%)]\tLoss: 0.383477\tLR: 0.00014076\n",
            "Train Epoch: 11 [30464/123872 (25%)]\tLoss: 0.395291\tLR: 0.00014067\n",
            "Train Epoch: 11 [30720/123872 (25%)]\tLoss: 0.397310\tLR: 0.00014059\n",
            "Train Epoch: 11 [30720/123872 (25%)]\tLoss: 0.397310\n",
            "Train Epoch: 11 [30976/123872 (25%)]\tLoss: 0.358601\tLR: 0.00014050\n",
            "Train Epoch: 11 [31232/123872 (25%)]\tLoss: 0.376825\tLR: 0.00014042\n",
            "Train Epoch: 11 [31488/123872 (25%)]\tLoss: 0.383860\tLR: 0.00014034\n",
            "Train Epoch: 11 [31744/123872 (26%)]\tLoss: 0.373307\tLR: 0.00014025\n",
            "Train Epoch: 11 [32000/123872 (26%)]\tLoss: 0.351054\tLR: 0.00014017\n",
            "Train Epoch: 11 [32256/123872 (26%)]\tLoss: 0.369505\tLR: 0.00014009\n",
            "Train Epoch: 11 [32512/123872 (26%)]\tLoss: 0.416878\tLR: 0.00014000\n",
            "Train Epoch: 11 [32768/123872 (26%)]\tLoss: 0.339221\tLR: 0.00013992\n",
            "Train Epoch: 11 [33024/123872 (27%)]\tLoss: 0.338783\tLR: 0.00013983\n",
            "Train Epoch: 11 [33280/123872 (27%)]\tLoss: 0.368301\tLR: 0.00013975\n",
            "Train Epoch: 11 [33280/123872 (27%)]\tLoss: 0.368301\n",
            "Train Epoch: 11 [33536/123872 (27%)]\tLoss: 0.370029\tLR: 0.00013967\n",
            "Train Epoch: 11 [33792/123872 (27%)]\tLoss: 0.361906\tLR: 0.00013958\n",
            "Train Epoch: 11 [34048/123872 (27%)]\tLoss: 0.379054\tLR: 0.00013950\n",
            "Train Epoch: 11 [34304/123872 (28%)]\tLoss: 0.404163\tLR: 0.00013942\n",
            "Train Epoch: 11 [34560/123872 (28%)]\tLoss: 0.358129\tLR: 0.00013933\n",
            "Train Epoch: 11 [34816/123872 (28%)]\tLoss: 0.425450\tLR: 0.00013925\n",
            "Train Epoch: 11 [35072/123872 (28%)]\tLoss: 0.384985\tLR: 0.00013916\n",
            "Train Epoch: 11 [35328/123872 (29%)]\tLoss: 0.431988\tLR: 0.00013908\n",
            "Train Epoch: 11 [35584/123872 (29%)]\tLoss: 0.442172\tLR: 0.00013900\n",
            "Train Epoch: 11 [35840/123872 (29%)]\tLoss: 0.365531\tLR: 0.00013891\n",
            "Train Epoch: 11 [35840/123872 (29%)]\tLoss: 0.365531\n",
            "Train Epoch: 11 [36096/123872 (29%)]\tLoss: 0.376745\tLR: 0.00013883\n",
            "Train Epoch: 11 [36352/123872 (29%)]\tLoss: 0.357406\tLR: 0.00013875\n",
            "Train Epoch: 11 [36608/123872 (30%)]\tLoss: 0.366734\tLR: 0.00013866\n",
            "Train Epoch: 11 [36864/123872 (30%)]\tLoss: 0.385475\tLR: 0.00013858\n",
            "Train Epoch: 11 [37120/123872 (30%)]\tLoss: 0.389041\tLR: 0.00013850\n",
            "Train Epoch: 11 [37376/123872 (30%)]\tLoss: 0.386492\tLR: 0.00013841\n",
            "Train Epoch: 11 [37632/123872 (30%)]\tLoss: 0.432574\tLR: 0.00013833\n",
            "Train Epoch: 11 [37888/123872 (31%)]\tLoss: 0.427860\tLR: 0.00013825\n",
            "Train Epoch: 11 [38144/123872 (31%)]\tLoss: 0.429080\tLR: 0.00013817\n",
            "Train Epoch: 11 [38400/123872 (31%)]\tLoss: 0.380259\tLR: 0.00013808\n",
            "Train Epoch: 11 [38400/123872 (31%)]\tLoss: 0.380259\n",
            "Train Epoch: 11 [38656/123872 (31%)]\tLoss: 0.387435\tLR: 0.00013800\n",
            "Train Epoch: 11 [38912/123872 (31%)]\tLoss: 0.412087\tLR: 0.00013792\n",
            "Train Epoch: 11 [39168/123872 (32%)]\tLoss: 0.398011\tLR: 0.00013783\n",
            "Train Epoch: 11 [39424/123872 (32%)]\tLoss: 0.384045\tLR: 0.00013775\n",
            "Train Epoch: 11 [39680/123872 (32%)]\tLoss: 0.391623\tLR: 0.00013767\n",
            "Train Epoch: 11 [39936/123872 (32%)]\tLoss: 0.413056\tLR: 0.00013759\n",
            "Train Epoch: 11 [40192/123872 (32%)]\tLoss: 0.395888\tLR: 0.00013750\n",
            "Train Epoch: 11 [40448/123872 (33%)]\tLoss: 0.396038\tLR: 0.00013742\n",
            "Train Epoch: 11 [40704/123872 (33%)]\tLoss: 0.327662\tLR: 0.00013734\n",
            "Train Epoch: 11 [40960/123872 (33%)]\tLoss: 0.362580\tLR: 0.00013725\n",
            "Train Epoch: 11 [40960/123872 (33%)]\tLoss: 0.362580\n",
            "Train Epoch: 11 [41216/123872 (33%)]\tLoss: 0.369031\tLR: 0.00013717\n",
            "Train Epoch: 11 [41472/123872 (33%)]\tLoss: 0.438089\tLR: 0.00013709\n",
            "Train Epoch: 11 [41728/123872 (34%)]\tLoss: 0.339422\tLR: 0.00013701\n",
            "Train Epoch: 11 [41984/123872 (34%)]\tLoss: 0.363793\tLR: 0.00013692\n",
            "Train Epoch: 11 [42240/123872 (34%)]\tLoss: 0.384365\tLR: 0.00013684\n",
            "Train Epoch: 11 [42496/123872 (34%)]\tLoss: 0.341977\tLR: 0.00013676\n",
            "Train Epoch: 11 [42752/123872 (35%)]\tLoss: 0.406468\tLR: 0.00013668\n",
            "Train Epoch: 11 [43008/123872 (35%)]\tLoss: 0.412655\tLR: 0.00013659\n",
            "Train Epoch: 11 [43264/123872 (35%)]\tLoss: 0.407268\tLR: 0.00013651\n",
            "Train Epoch: 11 [43520/123872 (35%)]\tLoss: 0.431752\tLR: 0.00013643\n",
            "Train Epoch: 11 [43520/123872 (35%)]\tLoss: 0.431752\n",
            "Train Epoch: 11 [43776/123872 (35%)]\tLoss: 0.385202\tLR: 0.00013635\n",
            "Train Epoch: 11 [44032/123872 (36%)]\tLoss: 0.392484\tLR: 0.00013627\n",
            "Train Epoch: 11 [44288/123872 (36%)]\tLoss: 0.344710\tLR: 0.00013618\n",
            "Train Epoch: 11 [44544/123872 (36%)]\tLoss: 0.347102\tLR: 0.00013610\n",
            "Train Epoch: 11 [44800/123872 (36%)]\tLoss: 0.410464\tLR: 0.00013602\n",
            "Train Epoch: 11 [45056/123872 (36%)]\tLoss: 0.388687\tLR: 0.00013594\n",
            "Train Epoch: 11 [45312/123872 (37%)]\tLoss: 0.318153\tLR: 0.00013585\n",
            "Train Epoch: 11 [45568/123872 (37%)]\tLoss: 0.438013\tLR: 0.00013577\n",
            "Train Epoch: 11 [45824/123872 (37%)]\tLoss: 0.395713\tLR: 0.00013569\n",
            "Train Epoch: 11 [46080/123872 (37%)]\tLoss: 0.451452\tLR: 0.00013561\n",
            "Train Epoch: 11 [46080/123872 (37%)]\tLoss: 0.451452\n",
            "Train Epoch: 11 [46336/123872 (37%)]\tLoss: 0.421871\tLR: 0.00013553\n",
            "Train Epoch: 11 [46592/123872 (38%)]\tLoss: 0.422057\tLR: 0.00013545\n",
            "Train Epoch: 11 [46848/123872 (38%)]\tLoss: 0.359893\tLR: 0.00013536\n",
            "Train Epoch: 11 [47104/123872 (38%)]\tLoss: 0.405338\tLR: 0.00013528\n",
            "Train Epoch: 11 [47360/123872 (38%)]\tLoss: 0.399835\tLR: 0.00013520\n",
            "Train Epoch: 11 [47616/123872 (38%)]\tLoss: 0.329377\tLR: 0.00013512\n",
            "Train Epoch: 11 [47872/123872 (39%)]\tLoss: 0.375205\tLR: 0.00013504\n",
            "Train Epoch: 11 [48128/123872 (39%)]\tLoss: 0.328734\tLR: 0.00013495\n",
            "Train Epoch: 11 [48384/123872 (39%)]\tLoss: 0.374874\tLR: 0.00013487\n",
            "Train Epoch: 11 [48640/123872 (39%)]\tLoss: 0.362827\tLR: 0.00013479\n",
            "Train Epoch: 11 [48640/123872 (39%)]\tLoss: 0.362827\n",
            "Train Epoch: 11 [48896/123872 (39%)]\tLoss: 0.384184\tLR: 0.00013471\n",
            "Train Epoch: 11 [49152/123872 (40%)]\tLoss: 0.342727\tLR: 0.00013463\n",
            "Train Epoch: 11 [49408/123872 (40%)]\tLoss: 0.456613\tLR: 0.00013455\n",
            "Train Epoch: 11 [49664/123872 (40%)]\tLoss: 0.378162\tLR: 0.00013447\n",
            "Train Epoch: 11 [49920/123872 (40%)]\tLoss: 0.363907\tLR: 0.00013438\n",
            "Train Epoch: 11 [50176/123872 (40%)]\tLoss: 0.399798\tLR: 0.00013430\n",
            "Train Epoch: 11 [50432/123872 (41%)]\tLoss: 0.376798\tLR: 0.00013422\n",
            "Train Epoch: 11 [50688/123872 (41%)]\tLoss: 0.332229\tLR: 0.00013414\n",
            "Train Epoch: 11 [50944/123872 (41%)]\tLoss: 0.367270\tLR: 0.00013406\n",
            "Train Epoch: 11 [51200/123872 (41%)]\tLoss: 0.377476\tLR: 0.00013398\n",
            "Train Epoch: 11 [51200/123872 (41%)]\tLoss: 0.377476\n",
            "Train Epoch: 11 [51456/123872 (42%)]\tLoss: 0.440625\tLR: 0.00013390\n",
            "Train Epoch: 11 [51712/123872 (42%)]\tLoss: 0.421629\tLR: 0.00013382\n",
            "Train Epoch: 11 [51968/123872 (42%)]\tLoss: 0.415938\tLR: 0.00013374\n",
            "Train Epoch: 11 [52224/123872 (42%)]\tLoss: 0.369281\tLR: 0.00013365\n",
            "Train Epoch: 11 [52480/123872 (42%)]\tLoss: 0.420212\tLR: 0.00013357\n",
            "Train Epoch: 11 [52736/123872 (43%)]\tLoss: 0.370060\tLR: 0.00013349\n",
            "Train Epoch: 11 [52992/123872 (43%)]\tLoss: 0.419439\tLR: 0.00013341\n",
            "Train Epoch: 11 [53248/123872 (43%)]\tLoss: 0.403444\tLR: 0.00013333\n",
            "Train Epoch: 11 [53504/123872 (43%)]\tLoss: 0.424296\tLR: 0.00013325\n",
            "Train Epoch: 11 [53760/123872 (43%)]\tLoss: 0.366275\tLR: 0.00013317\n",
            "Train Epoch: 11 [53760/123872 (43%)]\tLoss: 0.366275\n",
            "Train Epoch: 11 [54016/123872 (44%)]\tLoss: 0.405754\tLR: 0.00013309\n",
            "Train Epoch: 11 [54272/123872 (44%)]\tLoss: 0.367205\tLR: 0.00013301\n",
            "Train Epoch: 11 [54528/123872 (44%)]\tLoss: 0.421707\tLR: 0.00013293\n",
            "Train Epoch: 11 [54784/123872 (44%)]\tLoss: 0.357704\tLR: 0.00013285\n",
            "Train Epoch: 11 [55040/123872 (44%)]\tLoss: 0.365117\tLR: 0.00013277\n",
            "Train Epoch: 11 [55296/123872 (45%)]\tLoss: 0.379532\tLR: 0.00013268\n",
            "Train Epoch: 11 [55552/123872 (45%)]\tLoss: 0.416240\tLR: 0.00013260\n",
            "Train Epoch: 11 [55808/123872 (45%)]\tLoss: 0.421552\tLR: 0.00013252\n",
            "Train Epoch: 11 [56064/123872 (45%)]\tLoss: 0.390378\tLR: 0.00013244\n",
            "Train Epoch: 11 [56320/123872 (45%)]\tLoss: 0.364104\tLR: 0.00013236\n",
            "Train Epoch: 11 [56320/123872 (45%)]\tLoss: 0.364104\n",
            "Train Epoch: 11 [56576/123872 (46%)]\tLoss: 0.357917\tLR: 0.00013228\n",
            "Train Epoch: 11 [56832/123872 (46%)]\tLoss: 0.396558\tLR: 0.00013220\n",
            "Train Epoch: 11 [57088/123872 (46%)]\tLoss: 0.438743\tLR: 0.00013212\n",
            "Train Epoch: 11 [57344/123872 (46%)]\tLoss: 0.363778\tLR: 0.00013204\n",
            "Train Epoch: 11 [57600/123872 (46%)]\tLoss: 0.417013\tLR: 0.00013196\n",
            "Train Epoch: 11 [57856/123872 (47%)]\tLoss: 0.346354\tLR: 0.00013188\n",
            "Train Epoch: 11 [58112/123872 (47%)]\tLoss: 0.405676\tLR: 0.00013180\n",
            "Train Epoch: 11 [58368/123872 (47%)]\tLoss: 0.307139\tLR: 0.00013172\n",
            "Train Epoch: 11 [58624/123872 (47%)]\tLoss: 0.419471\tLR: 0.00013164\n",
            "Train Epoch: 11 [58880/123872 (48%)]\tLoss: 0.352256\tLR: 0.00013156\n",
            "Train Epoch: 11 [58880/123872 (48%)]\tLoss: 0.352256\n",
            "Train Epoch: 11 [59136/123872 (48%)]\tLoss: 0.343342\tLR: 0.00013148\n",
            "Train Epoch: 11 [59392/123872 (48%)]\tLoss: 0.404550\tLR: 0.00013140\n",
            "Train Epoch: 11 [59648/123872 (48%)]\tLoss: 0.452496\tLR: 0.00013132\n",
            "Train Epoch: 11 [59904/123872 (48%)]\tLoss: 0.377941\tLR: 0.00013124\n",
            "Train Epoch: 11 [60160/123872 (49%)]\tLoss: 0.382233\tLR: 0.00013116\n",
            "Train Epoch: 11 [60416/123872 (49%)]\tLoss: 0.456512\tLR: 0.00013108\n",
            "Train Epoch: 11 [60672/123872 (49%)]\tLoss: 0.410062\tLR: 0.00013100\n",
            "Train Epoch: 11 [60928/123872 (49%)]\tLoss: 0.303892\tLR: 0.00013092\n",
            "Train Epoch: 11 [61184/123872 (49%)]\tLoss: 0.355935\tLR: 0.00013084\n",
            "Train Epoch: 11 [61440/123872 (50%)]\tLoss: 0.445227\tLR: 0.00013076\n",
            "Train Epoch: 11 [61440/123872 (50%)]\tLoss: 0.445227\n",
            "Train Epoch: 11 [61696/123872 (50%)]\tLoss: 0.389010\tLR: 0.00013068\n",
            "Train Epoch: 11 [61952/123872 (50%)]\tLoss: 0.438068\tLR: 0.00013060\n",
            "Train Epoch: 11 [62208/123872 (50%)]\tLoss: 0.409705\tLR: 0.00013052\n",
            "Train Epoch: 11 [62464/123872 (50%)]\tLoss: 0.381527\tLR: 0.00013044\n",
            "Train Epoch: 11 [62720/123872 (51%)]\tLoss: 0.326771\tLR: 0.00013036\n",
            "Train Epoch: 11 [62976/123872 (51%)]\tLoss: 0.386772\tLR: 0.00013029\n",
            "Train Epoch: 11 [63232/123872 (51%)]\tLoss: 0.431360\tLR: 0.00013021\n",
            "Train Epoch: 11 [63488/123872 (51%)]\tLoss: 0.398024\tLR: 0.00013013\n",
            "Train Epoch: 11 [63744/123872 (51%)]\tLoss: 0.404019\tLR: 0.00013005\n",
            "Train Epoch: 11 [64000/123872 (52%)]\tLoss: 0.402199\tLR: 0.00012997\n",
            "Train Epoch: 11 [64000/123872 (52%)]\tLoss: 0.402199\n",
            "Train Epoch: 11 [64256/123872 (52%)]\tLoss: 0.378321\tLR: 0.00012989\n",
            "Train Epoch: 11 [64512/123872 (52%)]\tLoss: 0.385683\tLR: 0.00012981\n",
            "Train Epoch: 11 [64768/123872 (52%)]\tLoss: 0.379684\tLR: 0.00012973\n",
            "Train Epoch: 11 [65024/123872 (52%)]\tLoss: 0.386490\tLR: 0.00012965\n",
            "Train Epoch: 11 [65280/123872 (53%)]\tLoss: 0.343625\tLR: 0.00012957\n",
            "Train Epoch: 11 [65536/123872 (53%)]\tLoss: 0.418946\tLR: 0.00012949\n",
            "Train Epoch: 11 [65792/123872 (53%)]\tLoss: 0.363381\tLR: 0.00012941\n",
            "Train Epoch: 11 [66048/123872 (53%)]\tLoss: 0.456569\tLR: 0.00012934\n",
            "Train Epoch: 11 [66304/123872 (54%)]\tLoss: 0.372351\tLR: 0.00012926\n",
            "Train Epoch: 11 [66560/123872 (54%)]\tLoss: 0.348657\tLR: 0.00012918\n",
            "Train Epoch: 11 [66560/123872 (54%)]\tLoss: 0.348657\n",
            "Train Epoch: 11 [66816/123872 (54%)]\tLoss: 0.385102\tLR: 0.00012910\n",
            "Train Epoch: 11 [67072/123872 (54%)]\tLoss: 0.435060\tLR: 0.00012902\n",
            "Train Epoch: 11 [67328/123872 (54%)]\tLoss: 0.341691\tLR: 0.00012894\n",
            "Train Epoch: 11 [67584/123872 (55%)]\tLoss: 0.418344\tLR: 0.00012886\n",
            "Train Epoch: 11 [67840/123872 (55%)]\tLoss: 0.371003\tLR: 0.00012878\n",
            "Train Epoch: 11 [68096/123872 (55%)]\tLoss: 0.334353\tLR: 0.00012870\n",
            "Train Epoch: 11 [68352/123872 (55%)]\tLoss: 0.425998\tLR: 0.00012863\n",
            "Train Epoch: 11 [68608/123872 (55%)]\tLoss: 0.435321\tLR: 0.00012855\n",
            "Train Epoch: 11 [68864/123872 (56%)]\tLoss: 0.344584\tLR: 0.00012847\n",
            "Train Epoch: 11 [69120/123872 (56%)]\tLoss: 0.325859\tLR: 0.00012839\n",
            "Train Epoch: 11 [69120/123872 (56%)]\tLoss: 0.325859\n",
            "Train Epoch: 11 [69376/123872 (56%)]\tLoss: 0.405624\tLR: 0.00012831\n",
            "Train Epoch: 11 [69632/123872 (56%)]\tLoss: 0.336888\tLR: 0.00012823\n",
            "Train Epoch: 11 [69888/123872 (56%)]\tLoss: 0.415654\tLR: 0.00012816\n",
            "Train Epoch: 11 [70144/123872 (57%)]\tLoss: 0.372336\tLR: 0.00012808\n",
            "Train Epoch: 11 [70400/123872 (57%)]\tLoss: 0.360681\tLR: 0.00012800\n",
            "Train Epoch: 11 [70656/123872 (57%)]\tLoss: 0.355629\tLR: 0.00012792\n",
            "Train Epoch: 11 [70912/123872 (57%)]\tLoss: 0.394051\tLR: 0.00012784\n",
            "Train Epoch: 11 [71168/123872 (57%)]\tLoss: 0.419341\tLR: 0.00012776\n",
            "Train Epoch: 11 [71424/123872 (58%)]\tLoss: 0.374081\tLR: 0.00012769\n",
            "Train Epoch: 11 [71680/123872 (58%)]\tLoss: 0.333639\tLR: 0.00012761\n",
            "Train Epoch: 11 [71680/123872 (58%)]\tLoss: 0.333639\n",
            "Train Epoch: 11 [71936/123872 (58%)]\tLoss: 0.372061\tLR: 0.00012753\n",
            "Train Epoch: 11 [72192/123872 (58%)]\tLoss: 0.403697\tLR: 0.00012745\n",
            "Train Epoch: 11 [72448/123872 (58%)]\tLoss: 0.394701\tLR: 0.00012737\n",
            "Train Epoch: 11 [72704/123872 (59%)]\tLoss: 0.420235\tLR: 0.00012730\n",
            "Train Epoch: 11 [72960/123872 (59%)]\tLoss: 0.411436\tLR: 0.00012722\n",
            "Train Epoch: 11 [73216/123872 (59%)]\tLoss: 0.363782\tLR: 0.00012714\n",
            "Train Epoch: 11 [73472/123872 (59%)]\tLoss: 0.360421\tLR: 0.00012706\n",
            "Train Epoch: 11 [73728/123872 (60%)]\tLoss: 0.412687\tLR: 0.00012698\n",
            "Train Epoch: 11 [73984/123872 (60%)]\tLoss: 0.399851\tLR: 0.00012691\n",
            "Train Epoch: 11 [74240/123872 (60%)]\tLoss: 0.349482\tLR: 0.00012683\n",
            "Train Epoch: 11 [74240/123872 (60%)]\tLoss: 0.349482\n",
            "Train Epoch: 11 [74496/123872 (60%)]\tLoss: 0.408187\tLR: 0.00012675\n",
            "Train Epoch: 11 [74752/123872 (60%)]\tLoss: 0.416919\tLR: 0.00012667\n",
            "Train Epoch: 11 [75008/123872 (61%)]\tLoss: 0.409560\tLR: 0.00012660\n",
            "Train Epoch: 11 [75264/123872 (61%)]\tLoss: 0.361792\tLR: 0.00012652\n",
            "Train Epoch: 11 [75520/123872 (61%)]\tLoss: 0.390959\tLR: 0.00012644\n",
            "Train Epoch: 11 [75776/123872 (61%)]\tLoss: 0.288161\tLR: 0.00012636\n",
            "Train Epoch: 11 [76032/123872 (61%)]\tLoss: 0.408205\tLR: 0.00012629\n",
            "Train Epoch: 11 [76288/123872 (62%)]\tLoss: 0.391010\tLR: 0.00012621\n",
            "Train Epoch: 11 [76544/123872 (62%)]\tLoss: 0.374308\tLR: 0.00012613\n",
            "Train Epoch: 11 [76800/123872 (62%)]\tLoss: 0.375171\tLR: 0.00012605\n",
            "Train Epoch: 11 [76800/123872 (62%)]\tLoss: 0.375171\n",
            "Train Epoch: 11 [77056/123872 (62%)]\tLoss: 0.415024\tLR: 0.00012598\n",
            "Train Epoch: 11 [77312/123872 (62%)]\tLoss: 0.510372\tLR: 0.00012590\n",
            "Train Epoch: 11 [77568/123872 (63%)]\tLoss: 0.386981\tLR: 0.00012582\n",
            "Train Epoch: 11 [77824/123872 (63%)]\tLoss: 0.382028\tLR: 0.00012574\n",
            "Train Epoch: 11 [78080/123872 (63%)]\tLoss: 0.411301\tLR: 0.00012567\n",
            "Train Epoch: 11 [78336/123872 (63%)]\tLoss: 0.363010\tLR: 0.00012559\n",
            "Train Epoch: 11 [78592/123872 (63%)]\tLoss: 0.420559\tLR: 0.00012551\n",
            "Train Epoch: 11 [78848/123872 (64%)]\tLoss: 0.345587\tLR: 0.00012544\n",
            "Train Epoch: 11 [79104/123872 (64%)]\tLoss: 0.398949\tLR: 0.00012536\n",
            "Train Epoch: 11 [79360/123872 (64%)]\tLoss: 0.391552\tLR: 0.00012528\n",
            "Train Epoch: 11 [79360/123872 (64%)]\tLoss: 0.391552\n",
            "Train Epoch: 11 [79616/123872 (64%)]\tLoss: 0.400759\tLR: 0.00012520\n",
            "Train Epoch: 11 [79872/123872 (64%)]\tLoss: 0.455221\tLR: 0.00012513\n",
            "Train Epoch: 11 [80128/123872 (65%)]\tLoss: 0.461331\tLR: 0.00012505\n",
            "Train Epoch: 11 [80384/123872 (65%)]\tLoss: 0.352108\tLR: 0.00012497\n",
            "Train Epoch: 11 [80640/123872 (65%)]\tLoss: 0.411037\tLR: 0.00012490\n",
            "Train Epoch: 11 [80896/123872 (65%)]\tLoss: 0.372094\tLR: 0.00012482\n",
            "Train Epoch: 11 [81152/123872 (65%)]\tLoss: 0.326108\tLR: 0.00012474\n",
            "Train Epoch: 11 [81408/123872 (66%)]\tLoss: 0.416837\tLR: 0.00012467\n",
            "Train Epoch: 11 [81664/123872 (66%)]\tLoss: 0.397085\tLR: 0.00012459\n",
            "Train Epoch: 11 [81920/123872 (66%)]\tLoss: 0.367358\tLR: 0.00012451\n",
            "Train Epoch: 11 [81920/123872 (66%)]\tLoss: 0.367358\n",
            "Train Epoch: 11 [82176/123872 (66%)]\tLoss: 0.393731\tLR: 0.00012444\n",
            "Train Epoch: 11 [82432/123872 (67%)]\tLoss: 0.394999\tLR: 0.00012436\n",
            "Train Epoch: 11 [82688/123872 (67%)]\tLoss: 0.384706\tLR: 0.00012428\n",
            "Train Epoch: 11 [82944/123872 (67%)]\tLoss: 0.388244\tLR: 0.00012421\n",
            "Train Epoch: 11 [83200/123872 (67%)]\tLoss: 0.376448\tLR: 0.00012413\n",
            "Train Epoch: 11 [83456/123872 (67%)]\tLoss: 0.367052\tLR: 0.00012406\n",
            "Train Epoch: 11 [83712/123872 (68%)]\tLoss: 0.445074\tLR: 0.00012398\n",
            "Train Epoch: 11 [83968/123872 (68%)]\tLoss: 0.387328\tLR: 0.00012390\n",
            "Train Epoch: 11 [84224/123872 (68%)]\tLoss: 0.345008\tLR: 0.00012383\n",
            "Train Epoch: 11 [84480/123872 (68%)]\tLoss: 0.379123\tLR: 0.00012375\n",
            "Train Epoch: 11 [84480/123872 (68%)]\tLoss: 0.379123\n",
            "Train Epoch: 11 [84736/123872 (68%)]\tLoss: 0.360729\tLR: 0.00012367\n",
            "Train Epoch: 11 [84992/123872 (69%)]\tLoss: 0.411919\tLR: 0.00012360\n",
            "Train Epoch: 11 [85248/123872 (69%)]\tLoss: 0.315872\tLR: 0.00012352\n",
            "Train Epoch: 11 [85504/123872 (69%)]\tLoss: 0.304199\tLR: 0.00012345\n",
            "Train Epoch: 11 [85760/123872 (69%)]\tLoss: 0.374095\tLR: 0.00012337\n",
            "Train Epoch: 11 [86016/123872 (69%)]\tLoss: 0.416853\tLR: 0.00012329\n",
            "Train Epoch: 11 [86272/123872 (70%)]\tLoss: 0.379410\tLR: 0.00012322\n",
            "Train Epoch: 11 [86528/123872 (70%)]\tLoss: 0.393527\tLR: 0.00012314\n",
            "Train Epoch: 11 [86784/123872 (70%)]\tLoss: 0.423837\tLR: 0.00012307\n",
            "Train Epoch: 11 [87040/123872 (70%)]\tLoss: 0.409378\tLR: 0.00012299\n",
            "Train Epoch: 11 [87040/123872 (70%)]\tLoss: 0.409378\n",
            "Train Epoch: 11 [87296/123872 (70%)]\tLoss: 0.356379\tLR: 0.00012292\n",
            "Train Epoch: 11 [87552/123872 (71%)]\tLoss: 0.374909\tLR: 0.00012284\n",
            "Train Epoch: 11 [87808/123872 (71%)]\tLoss: 0.349498\tLR: 0.00012276\n",
            "Train Epoch: 11 [88064/123872 (71%)]\tLoss: 0.386956\tLR: 0.00012269\n",
            "Train Epoch: 11 [88320/123872 (71%)]\tLoss: 0.374643\tLR: 0.00012261\n",
            "Train Epoch: 11 [88576/123872 (71%)]\tLoss: 0.454166\tLR: 0.00012254\n",
            "Train Epoch: 11 [88832/123872 (72%)]\tLoss: 0.422768\tLR: 0.00012246\n",
            "Train Epoch: 11 [89088/123872 (72%)]\tLoss: 0.422976\tLR: 0.00012239\n",
            "Train Epoch: 11 [89344/123872 (72%)]\tLoss: 0.328486\tLR: 0.00012231\n",
            "Train Epoch: 11 [89600/123872 (72%)]\tLoss: 0.408332\tLR: 0.00012224\n",
            "Train Epoch: 11 [89600/123872 (72%)]\tLoss: 0.408332\n",
            "Train Epoch: 11 [89856/123872 (73%)]\tLoss: 0.370979\tLR: 0.00012216\n",
            "Train Epoch: 11 [90112/123872 (73%)]\tLoss: 0.372470\tLR: 0.00012208\n",
            "Train Epoch: 11 [90368/123872 (73%)]\tLoss: 0.391271\tLR: 0.00012201\n",
            "Train Epoch: 11 [90624/123872 (73%)]\tLoss: 0.445532\tLR: 0.00012193\n",
            "Train Epoch: 11 [90880/123872 (73%)]\tLoss: 0.372765\tLR: 0.00012186\n",
            "Train Epoch: 11 [91136/123872 (74%)]\tLoss: 0.380537\tLR: 0.00012178\n",
            "Train Epoch: 11 [91392/123872 (74%)]\tLoss: 0.363042\tLR: 0.00012171\n",
            "Train Epoch: 11 [91648/123872 (74%)]\tLoss: 0.413358\tLR: 0.00012163\n",
            "Train Epoch: 11 [91904/123872 (74%)]\tLoss: 0.434270\tLR: 0.00012156\n",
            "Train Epoch: 11 [92160/123872 (74%)]\tLoss: 0.362830\tLR: 0.00012148\n",
            "Train Epoch: 11 [92160/123872 (74%)]\tLoss: 0.362830\n",
            "Train Epoch: 11 [92416/123872 (75%)]\tLoss: 0.405687\tLR: 0.00012141\n",
            "Train Epoch: 11 [92672/123872 (75%)]\tLoss: 0.378461\tLR: 0.00012133\n",
            "Train Epoch: 11 [92928/123872 (75%)]\tLoss: 0.371404\tLR: 0.00012126\n",
            "Train Epoch: 11 [93184/123872 (75%)]\tLoss: 0.394168\tLR: 0.00012118\n",
            "Train Epoch: 11 [93440/123872 (75%)]\tLoss: 0.406128\tLR: 0.00012111\n",
            "Train Epoch: 11 [93696/123872 (76%)]\tLoss: 0.306462\tLR: 0.00012103\n",
            "Train Epoch: 11 [93952/123872 (76%)]\tLoss: 0.403513\tLR: 0.00012096\n",
            "Train Epoch: 11 [94208/123872 (76%)]\tLoss: 0.456659\tLR: 0.00012089\n",
            "Train Epoch: 11 [94464/123872 (76%)]\tLoss: 0.415317\tLR: 0.00012081\n",
            "Train Epoch: 11 [94720/123872 (76%)]\tLoss: 0.422264\tLR: 0.00012074\n",
            "Train Epoch: 11 [94720/123872 (76%)]\tLoss: 0.422264\n",
            "Train Epoch: 11 [94976/123872 (77%)]\tLoss: 0.375049\tLR: 0.00012066\n",
            "Train Epoch: 11 [95232/123872 (77%)]\tLoss: 0.366832\tLR: 0.00012059\n",
            "Train Epoch: 11 [95488/123872 (77%)]\tLoss: 0.411020\tLR: 0.00012051\n",
            "Train Epoch: 11 [95744/123872 (77%)]\tLoss: 0.397955\tLR: 0.00012044\n",
            "Train Epoch: 11 [96000/123872 (77%)]\tLoss: 0.385383\tLR: 0.00012036\n",
            "Train Epoch: 11 [96256/123872 (78%)]\tLoss: 0.355883\tLR: 0.00012029\n",
            "Train Epoch: 11 [96512/123872 (78%)]\tLoss: 0.426346\tLR: 0.00012022\n",
            "Train Epoch: 11 [96768/123872 (78%)]\tLoss: 0.394034\tLR: 0.00012014\n",
            "Train Epoch: 11 [97024/123872 (78%)]\tLoss: 0.337171\tLR: 0.00012007\n",
            "Train Epoch: 11 [97280/123872 (79%)]\tLoss: 0.422442\tLR: 0.00011999\n",
            "Train Epoch: 11 [97280/123872 (79%)]\tLoss: 0.422442\n",
            "Train Epoch: 11 [97536/123872 (79%)]\tLoss: 0.372833\tLR: 0.00011992\n",
            "Train Epoch: 11 [97792/123872 (79%)]\tLoss: 0.371976\tLR: 0.00011984\n",
            "Train Epoch: 11 [98048/123872 (79%)]\tLoss: 0.387246\tLR: 0.00011977\n",
            "Train Epoch: 11 [98304/123872 (79%)]\tLoss: 0.399939\tLR: 0.00011970\n",
            "Train Epoch: 11 [98560/123872 (80%)]\tLoss: 0.438170\tLR: 0.00011962\n",
            "Train Epoch: 11 [98816/123872 (80%)]\tLoss: 0.366141\tLR: 0.00011955\n",
            "Train Epoch: 11 [99072/123872 (80%)]\tLoss: 0.429820\tLR: 0.00011947\n",
            "Train Epoch: 11 [99328/123872 (80%)]\tLoss: 0.339191\tLR: 0.00011940\n",
            "Train Epoch: 11 [99584/123872 (80%)]\tLoss: 0.353414\tLR: 0.00011933\n",
            "Train Epoch: 11 [99840/123872 (81%)]\tLoss: 0.380123\tLR: 0.00011925\n",
            "Train Epoch: 11 [99840/123872 (81%)]\tLoss: 0.380123\n",
            "Train Epoch: 11 [100096/123872 (81%)]\tLoss: 0.426535\tLR: 0.00011918\n",
            "Train Epoch: 11 [100352/123872 (81%)]\tLoss: 0.337034\tLR: 0.00011911\n",
            "Train Epoch: 11 [100608/123872 (81%)]\tLoss: 0.425105\tLR: 0.00011903\n",
            "Train Epoch: 11 [100864/123872 (81%)]\tLoss: 0.443672\tLR: 0.00011896\n",
            "Train Epoch: 11 [101120/123872 (82%)]\tLoss: 0.354374\tLR: 0.00011888\n",
            "Train Epoch: 11 [101376/123872 (82%)]\tLoss: 0.356859\tLR: 0.00011881\n",
            "Train Epoch: 11 [101632/123872 (82%)]\tLoss: 0.344774\tLR: 0.00011874\n",
            "Train Epoch: 11 [101888/123872 (82%)]\tLoss: 0.378461\tLR: 0.00011866\n",
            "Train Epoch: 11 [102144/123872 (82%)]\tLoss: 0.373193\tLR: 0.00011859\n",
            "Train Epoch: 11 [102400/123872 (83%)]\tLoss: 0.365744\tLR: 0.00011852\n",
            "Train Epoch: 11 [102400/123872 (83%)]\tLoss: 0.365744\n",
            "Train Epoch: 11 [102656/123872 (83%)]\tLoss: 0.401636\tLR: 0.00011844\n",
            "Train Epoch: 11 [102912/123872 (83%)]\tLoss: 0.432492\tLR: 0.00011837\n",
            "Train Epoch: 11 [103168/123872 (83%)]\tLoss: 0.420646\tLR: 0.00011830\n",
            "Train Epoch: 11 [103424/123872 (83%)]\tLoss: 0.352776\tLR: 0.00011822\n",
            "Train Epoch: 11 [103680/123872 (84%)]\tLoss: 0.332087\tLR: 0.00011815\n",
            "Train Epoch: 11 [103936/123872 (84%)]\tLoss: 0.421639\tLR: 0.00011808\n",
            "Train Epoch: 11 [104192/123872 (84%)]\tLoss: 0.385232\tLR: 0.00011800\n",
            "Train Epoch: 11 [104448/123872 (84%)]\tLoss: 0.450548\tLR: 0.00011793\n",
            "Train Epoch: 11 [104704/123872 (85%)]\tLoss: 0.303919\tLR: 0.00011786\n",
            "Train Epoch: 11 [104960/123872 (85%)]\tLoss: 0.396014\tLR: 0.00011779\n",
            "Train Epoch: 11 [104960/123872 (85%)]\tLoss: 0.396014\n",
            "Train Epoch: 11 [105216/123872 (85%)]\tLoss: 0.360138\tLR: 0.00011771\n",
            "Train Epoch: 11 [105472/123872 (85%)]\tLoss: 0.425457\tLR: 0.00011764\n",
            "Train Epoch: 11 [105728/123872 (85%)]\tLoss: 0.382448\tLR: 0.00011757\n",
            "Train Epoch: 11 [105984/123872 (86%)]\tLoss: 0.388607\tLR: 0.00011749\n",
            "Train Epoch: 11 [106240/123872 (86%)]\tLoss: 0.432643\tLR: 0.00011742\n",
            "Train Epoch: 11 [106496/123872 (86%)]\tLoss: 0.431542\tLR: 0.00011735\n",
            "Train Epoch: 11 [106752/123872 (86%)]\tLoss: 0.358462\tLR: 0.00011728\n",
            "Train Epoch: 11 [107008/123872 (86%)]\tLoss: 0.365189\tLR: 0.00011720\n",
            "Train Epoch: 11 [107264/123872 (87%)]\tLoss: 0.395498\tLR: 0.00011713\n",
            "Train Epoch: 11 [107520/123872 (87%)]\tLoss: 0.421834\tLR: 0.00011706\n",
            "Train Epoch: 11 [107520/123872 (87%)]\tLoss: 0.421834\n",
            "Train Epoch: 11 [107776/123872 (87%)]\tLoss: 0.371080\tLR: 0.00011699\n",
            "Train Epoch: 11 [108032/123872 (87%)]\tLoss: 0.412786\tLR: 0.00011691\n",
            "Train Epoch: 11 [108288/123872 (87%)]\tLoss: 0.392962\tLR: 0.00011684\n",
            "Train Epoch: 11 [108544/123872 (88%)]\tLoss: 0.447053\tLR: 0.00011677\n",
            "Train Epoch: 11 [108800/123872 (88%)]\tLoss: 0.398111\tLR: 0.00011670\n",
            "Train Epoch: 11 [109056/123872 (88%)]\tLoss: 0.402600\tLR: 0.00011662\n",
            "Train Epoch: 11 [109312/123872 (88%)]\tLoss: 0.380617\tLR: 0.00011655\n",
            "Train Epoch: 11 [109568/123872 (88%)]\tLoss: 0.347657\tLR: 0.00011648\n",
            "Train Epoch: 11 [109824/123872 (89%)]\tLoss: 0.438648\tLR: 0.00011641\n",
            "Train Epoch: 11 [110080/123872 (89%)]\tLoss: 0.399246\tLR: 0.00011633\n",
            "Train Epoch: 11 [110080/123872 (89%)]\tLoss: 0.399246\n",
            "Train Epoch: 11 [110336/123872 (89%)]\tLoss: 0.349732\tLR: 0.00011626\n",
            "Train Epoch: 11 [110592/123872 (89%)]\tLoss: 0.339709\tLR: 0.00011619\n",
            "Train Epoch: 11 [110848/123872 (89%)]\tLoss: 0.374733\tLR: 0.00011612\n",
            "Train Epoch: 11 [111104/123872 (90%)]\tLoss: 0.368270\tLR: 0.00011605\n",
            "Train Epoch: 11 [111360/123872 (90%)]\tLoss: 0.419463\tLR: 0.00011597\n",
            "Train Epoch: 11 [111616/123872 (90%)]\tLoss: 0.424936\tLR: 0.00011590\n",
            "Train Epoch: 11 [111872/123872 (90%)]\tLoss: 0.447370\tLR: 0.00011583\n",
            "Train Epoch: 11 [112128/123872 (90%)]\tLoss: 0.381129\tLR: 0.00011576\n",
            "Train Epoch: 11 [112384/123872 (91%)]\tLoss: 0.361006\tLR: 0.00011569\n",
            "Train Epoch: 11 [112640/123872 (91%)]\tLoss: 0.391372\tLR: 0.00011562\n",
            "Train Epoch: 11 [112640/123872 (91%)]\tLoss: 0.391372\n",
            "Train Epoch: 11 [112896/123872 (91%)]\tLoss: 0.317425\tLR: 0.00011554\n",
            "Train Epoch: 11 [113152/123872 (91%)]\tLoss: 0.367599\tLR: 0.00011547\n",
            "Train Epoch: 11 [113408/123872 (92%)]\tLoss: 0.351353\tLR: 0.00011540\n",
            "Train Epoch: 11 [113664/123872 (92%)]\tLoss: 0.383012\tLR: 0.00011533\n",
            "Train Epoch: 11 [113920/123872 (92%)]\tLoss: 0.392856\tLR: 0.00011526\n",
            "Train Epoch: 11 [114176/123872 (92%)]\tLoss: 0.380520\tLR: 0.00011519\n",
            "Train Epoch: 11 [114432/123872 (92%)]\tLoss: 0.400172\tLR: 0.00011511\n",
            "Train Epoch: 11 [114688/123872 (93%)]\tLoss: 0.397230\tLR: 0.00011504\n",
            "Train Epoch: 11 [114944/123872 (93%)]\tLoss: 0.422746\tLR: 0.00011497\n",
            "Train Epoch: 11 [115200/123872 (93%)]\tLoss: 0.392034\tLR: 0.00011490\n",
            "Train Epoch: 11 [115200/123872 (93%)]\tLoss: 0.392034\n",
            "Train Epoch: 11 [115456/123872 (93%)]\tLoss: 0.414609\tLR: 0.00011483\n",
            "Train Epoch: 11 [115712/123872 (93%)]\tLoss: 0.437902\tLR: 0.00011476\n",
            "Train Epoch: 11 [115968/123872 (94%)]\tLoss: 0.332462\tLR: 0.00011469\n",
            "Train Epoch: 11 [116224/123872 (94%)]\tLoss: 0.390204\tLR: 0.00011461\n",
            "Train Epoch: 11 [116480/123872 (94%)]\tLoss: 0.348945\tLR: 0.00011454\n",
            "Train Epoch: 11 [116736/123872 (94%)]\tLoss: 0.379650\tLR: 0.00011447\n",
            "Train Epoch: 11 [116992/123872 (94%)]\tLoss: 0.409491\tLR: 0.00011440\n",
            "Train Epoch: 11 [117248/123872 (95%)]\tLoss: 0.411774\tLR: 0.00011433\n",
            "Train Epoch: 11 [117504/123872 (95%)]\tLoss: 0.385936\tLR: 0.00011426\n",
            "Train Epoch: 11 [117760/123872 (95%)]\tLoss: 0.366900\tLR: 0.00011419\n",
            "Train Epoch: 11 [117760/123872 (95%)]\tLoss: 0.366900\n",
            "Train Epoch: 11 [118016/123872 (95%)]\tLoss: 0.339204\tLR: 0.00011412\n",
            "Train Epoch: 11 [118272/123872 (95%)]\tLoss: 0.389244\tLR: 0.00011405\n",
            "Train Epoch: 11 [118528/123872 (96%)]\tLoss: 0.444173\tLR: 0.00011398\n",
            "Train Epoch: 11 [118784/123872 (96%)]\tLoss: 0.344157\tLR: 0.00011391\n",
            "Train Epoch: 11 [119040/123872 (96%)]\tLoss: 0.326531\tLR: 0.00011383\n",
            "Train Epoch: 11 [119296/123872 (96%)]\tLoss: 0.419643\tLR: 0.00011376\n",
            "Train Epoch: 11 [119552/123872 (96%)]\tLoss: 0.404970\tLR: 0.00011369\n",
            "Train Epoch: 11 [119808/123872 (97%)]\tLoss: 0.399267\tLR: 0.00011362\n",
            "Train Epoch: 11 [120064/123872 (97%)]\tLoss: 0.403263\tLR: 0.00011355\n",
            "Train Epoch: 11 [120320/123872 (97%)]\tLoss: 0.340817\tLR: 0.00011348\n",
            "Train Epoch: 11 [120320/123872 (97%)]\tLoss: 0.340817\n",
            "Train Epoch: 11 [120576/123872 (97%)]\tLoss: 0.388472\tLR: 0.00011341\n",
            "Train Epoch: 11 [120832/123872 (98%)]\tLoss: 0.394960\tLR: 0.00011334\n",
            "Train Epoch: 11 [121088/123872 (98%)]\tLoss: 0.361037\tLR: 0.00011327\n",
            "Train Epoch: 11 [121344/123872 (98%)]\tLoss: 0.399000\tLR: 0.00011320\n",
            "Train Epoch: 11 [121600/123872 (98%)]\tLoss: 0.335101\tLR: 0.00011313\n",
            "Train Epoch: 11 [121856/123872 (98%)]\tLoss: 0.422006\tLR: 0.00011306\n",
            "Train Epoch: 11 [122112/123872 (99%)]\tLoss: 0.402297\tLR: 0.00011299\n",
            "Train Epoch: 11 [122368/123872 (99%)]\tLoss: 0.367422\tLR: 0.00011292\n",
            "Train Epoch: 11 [122624/123872 (99%)]\tLoss: 0.372031\tLR: 0.00011285\n",
            "Train Epoch: 11 [122880/123872 (99%)]\tLoss: 0.403983\tLR: 0.00011278\n",
            "Train Epoch: 11 [122880/123872 (99%)]\tLoss: 0.403983\n",
            "Train Epoch: 11 [123136/123872 (99%)]\tLoss: 0.403711\tLR: 0.00011271\n",
            "Train Epoch: 11 [123392/123872 (100%)]\tLoss: 0.390830\tLR: 0.00011264\n",
            "Train Epoch: 11 [108192/123872 (100%)]\tLoss: 0.362879\tLR: 0.00011257\n",
            "\n",
            "Test set: Average loss: 0.0016, Accuracy: 25388/30970 (81.98%)\n",
            "\n",
            "Train Epoch: 12 [0/123872 (0%)]\tLoss: 0.436493\tLR: 0.00011250\n",
            "Train Epoch: 12 [0/123872 (0%)]\tLoss: 0.436493\n",
            "Train Epoch: 12 [256/123872 (0%)]\tLoss: 0.402804\tLR: 0.00011243\n",
            "Train Epoch: 12 [512/123872 (0%)]\tLoss: 0.344269\tLR: 0.00011236\n",
            "Train Epoch: 12 [768/123872 (1%)]\tLoss: 0.404172\tLR: 0.00011229\n",
            "Train Epoch: 12 [1024/123872 (1%)]\tLoss: 0.360753\tLR: 0.00011222\n",
            "Train Epoch: 12 [1280/123872 (1%)]\tLoss: 0.354998\tLR: 0.00011215\n",
            "Train Epoch: 12 [1536/123872 (1%)]\tLoss: 0.351489\tLR: 0.00011208\n",
            "Train Epoch: 12 [1792/123872 (1%)]\tLoss: 0.371922\tLR: 0.00011201\n",
            "Train Epoch: 12 [2048/123872 (2%)]\tLoss: 0.340345\tLR: 0.00011194\n",
            "Train Epoch: 12 [2304/123872 (2%)]\tLoss: 0.371295\tLR: 0.00011187\n",
            "Train Epoch: 12 [2560/123872 (2%)]\tLoss: 0.378562\tLR: 0.00011180\n",
            "Train Epoch: 12 [2560/123872 (2%)]\tLoss: 0.378562\n",
            "Train Epoch: 12 [2816/123872 (2%)]\tLoss: 0.368226\tLR: 0.00011173\n",
            "Train Epoch: 12 [3072/123872 (2%)]\tLoss: 0.318842\tLR: 0.00011166\n",
            "Train Epoch: 12 [3328/123872 (3%)]\tLoss: 0.355436\tLR: 0.00011159\n",
            "Train Epoch: 12 [3584/123872 (3%)]\tLoss: 0.344206\tLR: 0.00011152\n",
            "Train Epoch: 12 [3840/123872 (3%)]\tLoss: 0.384530\tLR: 0.00011145\n",
            "Train Epoch: 12 [4096/123872 (3%)]\tLoss: 0.334186\tLR: 0.00011139\n",
            "Train Epoch: 12 [4352/123872 (4%)]\tLoss: 0.367877\tLR: 0.00011132\n",
            "Train Epoch: 12 [4608/123872 (4%)]\tLoss: 0.353678\tLR: 0.00011125\n",
            "Train Epoch: 12 [4864/123872 (4%)]\tLoss: 0.380936\tLR: 0.00011118\n",
            "Train Epoch: 12 [5120/123872 (4%)]\tLoss: 0.374250\tLR: 0.00011111\n",
            "Train Epoch: 12 [5120/123872 (4%)]\tLoss: 0.374250\n",
            "Train Epoch: 12 [5376/123872 (4%)]\tLoss: 0.412693\tLR: 0.00011104\n",
            "Train Epoch: 12 [5632/123872 (5%)]\tLoss: 0.377778\tLR: 0.00011097\n",
            "Train Epoch: 12 [5888/123872 (5%)]\tLoss: 0.338517\tLR: 0.00011090\n",
            "Train Epoch: 12 [6144/123872 (5%)]\tLoss: 0.355569\tLR: 0.00011083\n",
            "Train Epoch: 12 [6400/123872 (5%)]\tLoss: 0.382751\tLR: 0.00011076\n",
            "Train Epoch: 12 [6656/123872 (5%)]\tLoss: 0.345939\tLR: 0.00011069\n",
            "Train Epoch: 12 [6912/123872 (6%)]\tLoss: 0.407810\tLR: 0.00011063\n",
            "Train Epoch: 12 [7168/123872 (6%)]\tLoss: 0.392429\tLR: 0.00011056\n",
            "Train Epoch: 12 [7424/123872 (6%)]\tLoss: 0.381371\tLR: 0.00011049\n",
            "Train Epoch: 12 [7680/123872 (6%)]\tLoss: 0.482870\tLR: 0.00011042\n",
            "Train Epoch: 12 [7680/123872 (6%)]\tLoss: 0.482870\n",
            "Train Epoch: 12 [7936/123872 (6%)]\tLoss: 0.416929\tLR: 0.00011035\n",
            "Train Epoch: 12 [8192/123872 (7%)]\tLoss: 0.370160\tLR: 0.00011028\n",
            "Train Epoch: 12 [8448/123872 (7%)]\tLoss: 0.417555\tLR: 0.00011021\n",
            "Train Epoch: 12 [8704/123872 (7%)]\tLoss: 0.376856\tLR: 0.00011015\n",
            "Train Epoch: 12 [8960/123872 (7%)]\tLoss: 0.333033\tLR: 0.00011008\n",
            "Train Epoch: 12 [9216/123872 (7%)]\tLoss: 0.367964\tLR: 0.00011001\n",
            "Train Epoch: 12 [9472/123872 (8%)]\tLoss: 0.356219\tLR: 0.00010994\n",
            "Train Epoch: 12 [9728/123872 (8%)]\tLoss: 0.353602\tLR: 0.00010987\n",
            "Train Epoch: 12 [9984/123872 (8%)]\tLoss: 0.362004\tLR: 0.00010980\n",
            "Train Epoch: 12 [10240/123872 (8%)]\tLoss: 0.414387\tLR: 0.00010974\n",
            "Train Epoch: 12 [10240/123872 (8%)]\tLoss: 0.414387\n",
            "Train Epoch: 12 [10496/123872 (8%)]\tLoss: 0.406635\tLR: 0.00010967\n",
            "Train Epoch: 12 [10752/123872 (9%)]\tLoss: 0.368345\tLR: 0.00010960\n",
            "Train Epoch: 12 [11008/123872 (9%)]\tLoss: 0.375573\tLR: 0.00010953\n",
            "Train Epoch: 12 [11264/123872 (9%)]\tLoss: 0.381109\tLR: 0.00010946\n",
            "Train Epoch: 12 [11520/123872 (9%)]\tLoss: 0.409561\tLR: 0.00010939\n",
            "Train Epoch: 12 [11776/123872 (10%)]\tLoss: 0.386216\tLR: 0.00010933\n",
            "Train Epoch: 12 [12032/123872 (10%)]\tLoss: 0.340972\tLR: 0.00010926\n",
            "Train Epoch: 12 [12288/123872 (10%)]\tLoss: 0.387698\tLR: 0.00010919\n",
            "Train Epoch: 12 [12544/123872 (10%)]\tLoss: 0.377034\tLR: 0.00010912\n",
            "Train Epoch: 12 [12800/123872 (10%)]\tLoss: 0.411820\tLR: 0.00010905\n",
            "Train Epoch: 12 [12800/123872 (10%)]\tLoss: 0.411820\n",
            "Train Epoch: 12 [13056/123872 (11%)]\tLoss: 0.361077\tLR: 0.00010899\n",
            "Train Epoch: 12 [13312/123872 (11%)]\tLoss: 0.430030\tLR: 0.00010892\n",
            "Train Epoch: 12 [13568/123872 (11%)]\tLoss: 0.377707\tLR: 0.00010885\n",
            "Train Epoch: 12 [13824/123872 (11%)]\tLoss: 0.383485\tLR: 0.00010878\n",
            "Train Epoch: 12 [14080/123872 (11%)]\tLoss: 0.386499\tLR: 0.00010872\n",
            "Train Epoch: 12 [14336/123872 (12%)]\tLoss: 0.410366\tLR: 0.00010865\n",
            "Train Epoch: 12 [14592/123872 (12%)]\tLoss: 0.373171\tLR: 0.00010858\n",
            "Train Epoch: 12 [14848/123872 (12%)]\tLoss: 0.374808\tLR: 0.00010851\n",
            "Train Epoch: 12 [15104/123872 (12%)]\tLoss: 0.365274\tLR: 0.00010845\n",
            "Train Epoch: 12 [15360/123872 (12%)]\tLoss: 0.385479\tLR: 0.00010838\n",
            "Train Epoch: 12 [15360/123872 (12%)]\tLoss: 0.385479\n",
            "Train Epoch: 12 [15616/123872 (13%)]\tLoss: 0.428137\tLR: 0.00010831\n",
            "Train Epoch: 12 [15872/123872 (13%)]\tLoss: 0.371806\tLR: 0.00010824\n",
            "Train Epoch: 12 [16128/123872 (13%)]\tLoss: 0.379863\tLR: 0.00010818\n",
            "Train Epoch: 12 [16384/123872 (13%)]\tLoss: 0.359900\tLR: 0.00010811\n",
            "Train Epoch: 12 [16640/123872 (13%)]\tLoss: 0.381568\tLR: 0.00010804\n",
            "Train Epoch: 12 [16896/123872 (14%)]\tLoss: 0.354855\tLR: 0.00010797\n",
            "Train Epoch: 12 [17152/123872 (14%)]\tLoss: 0.329144\tLR: 0.00010791\n",
            "Train Epoch: 12 [17408/123872 (14%)]\tLoss: 0.386920\tLR: 0.00010784\n",
            "Train Epoch: 12 [17664/123872 (14%)]\tLoss: 0.314795\tLR: 0.00010777\n",
            "Train Epoch: 12 [17920/123872 (14%)]\tLoss: 0.411534\tLR: 0.00010771\n",
            "Train Epoch: 12 [17920/123872 (14%)]\tLoss: 0.411534\n",
            "Train Epoch: 12 [18176/123872 (15%)]\tLoss: 0.378249\tLR: 0.00010764\n",
            "Train Epoch: 12 [18432/123872 (15%)]\tLoss: 0.385570\tLR: 0.00010757\n",
            "Train Epoch: 12 [18688/123872 (15%)]\tLoss: 0.460225\tLR: 0.00010751\n",
            "Train Epoch: 12 [18944/123872 (15%)]\tLoss: 0.399422\tLR: 0.00010744\n",
            "Train Epoch: 12 [19200/123872 (15%)]\tLoss: 0.425634\tLR: 0.00010737\n",
            "Train Epoch: 12 [19456/123872 (16%)]\tLoss: 0.342054\tLR: 0.00010731\n",
            "Train Epoch: 12 [19712/123872 (16%)]\tLoss: 0.409641\tLR: 0.00010724\n",
            "Train Epoch: 12 [19968/123872 (16%)]\tLoss: 0.389803\tLR: 0.00010717\n",
            "Train Epoch: 12 [20224/123872 (16%)]\tLoss: 0.381158\tLR: 0.00010711\n",
            "Train Epoch: 12 [20480/123872 (17%)]\tLoss: 0.330745\tLR: 0.00010704\n",
            "Train Epoch: 12 [20480/123872 (17%)]\tLoss: 0.330745\n",
            "Train Epoch: 12 [20736/123872 (17%)]\tLoss: 0.387002\tLR: 0.00010697\n",
            "Train Epoch: 12 [20992/123872 (17%)]\tLoss: 0.358905\tLR: 0.00010691\n",
            "Train Epoch: 12 [21248/123872 (17%)]\tLoss: 0.315599\tLR: 0.00010684\n",
            "Train Epoch: 12 [21504/123872 (17%)]\tLoss: 0.379544\tLR: 0.00010677\n",
            "Train Epoch: 12 [21760/123872 (18%)]\tLoss: 0.329702\tLR: 0.00010671\n",
            "Train Epoch: 12 [22016/123872 (18%)]\tLoss: 0.309064\tLR: 0.00010664\n",
            "Train Epoch: 12 [22272/123872 (18%)]\tLoss: 0.357255\tLR: 0.00010657\n",
            "Train Epoch: 12 [22528/123872 (18%)]\tLoss: 0.387368\tLR: 0.00010651\n",
            "Train Epoch: 12 [22784/123872 (18%)]\tLoss: 0.357220\tLR: 0.00010644\n",
            "Train Epoch: 12 [23040/123872 (19%)]\tLoss: 0.369837\tLR: 0.00010638\n",
            "Train Epoch: 12 [23040/123872 (19%)]\tLoss: 0.369837\n",
            "Train Epoch: 12 [23296/123872 (19%)]\tLoss: 0.388249\tLR: 0.00010631\n",
            "Train Epoch: 12 [23552/123872 (19%)]\tLoss: 0.410006\tLR: 0.00010624\n",
            "Train Epoch: 12 [23808/123872 (19%)]\tLoss: 0.403934\tLR: 0.00010618\n",
            "Train Epoch: 12 [24064/123872 (19%)]\tLoss: 0.444196\tLR: 0.00010611\n",
            "Train Epoch: 12 [24320/123872 (20%)]\tLoss: 0.403584\tLR: 0.00010605\n",
            "Train Epoch: 12 [24576/123872 (20%)]\tLoss: 0.396426\tLR: 0.00010598\n",
            "Train Epoch: 12 [24832/123872 (20%)]\tLoss: 0.404025\tLR: 0.00010591\n",
            "Train Epoch: 12 [25088/123872 (20%)]\tLoss: 0.436488\tLR: 0.00010585\n",
            "Train Epoch: 12 [25344/123872 (20%)]\tLoss: 0.347223\tLR: 0.00010578\n",
            "Train Epoch: 12 [25600/123872 (21%)]\tLoss: 0.327678\tLR: 0.00010572\n",
            "Train Epoch: 12 [25600/123872 (21%)]\tLoss: 0.327678\n",
            "Train Epoch: 12 [25856/123872 (21%)]\tLoss: 0.440033\tLR: 0.00010565\n",
            "Train Epoch: 12 [26112/123872 (21%)]\tLoss: 0.443123\tLR: 0.00010559\n",
            "Train Epoch: 12 [26368/123872 (21%)]\tLoss: 0.364808\tLR: 0.00010552\n",
            "Train Epoch: 12 [26624/123872 (21%)]\tLoss: 0.353324\tLR: 0.00010545\n",
            "Train Epoch: 12 [26880/123872 (22%)]\tLoss: 0.381901\tLR: 0.00010539\n",
            "Train Epoch: 12 [27136/123872 (22%)]\tLoss: 0.408817\tLR: 0.00010532\n",
            "Train Epoch: 12 [27392/123872 (22%)]\tLoss: 0.357974\tLR: 0.00010526\n",
            "Train Epoch: 12 [27648/123872 (22%)]\tLoss: 0.370961\tLR: 0.00010519\n",
            "Train Epoch: 12 [27904/123872 (23%)]\tLoss: 0.393782\tLR: 0.00010513\n",
            "Train Epoch: 12 [28160/123872 (23%)]\tLoss: 0.376779\tLR: 0.00010506\n",
            "Train Epoch: 12 [28160/123872 (23%)]\tLoss: 0.376779\n",
            "Train Epoch: 12 [28416/123872 (23%)]\tLoss: 0.392972\tLR: 0.00010500\n",
            "Train Epoch: 12 [28672/123872 (23%)]\tLoss: 0.327797\tLR: 0.00010493\n",
            "Train Epoch: 12 [28928/123872 (23%)]\tLoss: 0.417487\tLR: 0.00010487\n",
            "Train Epoch: 12 [29184/123872 (24%)]\tLoss: 0.403337\tLR: 0.00010480\n",
            "Train Epoch: 12 [29440/123872 (24%)]\tLoss: 0.361433\tLR: 0.00010474\n",
            "Train Epoch: 12 [29696/123872 (24%)]\tLoss: 0.388510\tLR: 0.00010467\n",
            "Train Epoch: 12 [29952/123872 (24%)]\tLoss: 0.364484\tLR: 0.00010461\n",
            "Train Epoch: 12 [30208/123872 (24%)]\tLoss: 0.320031\tLR: 0.00010454\n",
            "Train Epoch: 12 [30464/123872 (25%)]\tLoss: 0.316008\tLR: 0.00010448\n",
            "Train Epoch: 12 [30720/123872 (25%)]\tLoss: 0.331066\tLR: 0.00010441\n",
            "Train Epoch: 12 [30720/123872 (25%)]\tLoss: 0.331066\n",
            "Train Epoch: 12 [30976/123872 (25%)]\tLoss: 0.342764\tLR: 0.00010435\n",
            "Train Epoch: 12 [31232/123872 (25%)]\tLoss: 0.347776\tLR: 0.00010428\n",
            "Train Epoch: 12 [31488/123872 (25%)]\tLoss: 0.391944\tLR: 0.00010422\n",
            "Train Epoch: 12 [31744/123872 (26%)]\tLoss: 0.320973\tLR: 0.00010415\n",
            "Train Epoch: 12 [32000/123872 (26%)]\tLoss: 0.370881\tLR: 0.00010409\n",
            "Train Epoch: 12 [32256/123872 (26%)]\tLoss: 0.416507\tLR: 0.00010402\n",
            "Train Epoch: 12 [32512/123872 (26%)]\tLoss: 0.451868\tLR: 0.00010396\n",
            "Train Epoch: 12 [32768/123872 (26%)]\tLoss: 0.403660\tLR: 0.00010389\n",
            "Train Epoch: 12 [33024/123872 (27%)]\tLoss: 0.338297\tLR: 0.00010383\n",
            "Train Epoch: 12 [33280/123872 (27%)]\tLoss: 0.432629\tLR: 0.00010376\n",
            "Train Epoch: 12 [33280/123872 (27%)]\tLoss: 0.432629\n",
            "Train Epoch: 12 [33536/123872 (27%)]\tLoss: 0.424014\tLR: 0.00010370\n",
            "Train Epoch: 12 [33792/123872 (27%)]\tLoss: 0.321959\tLR: 0.00010364\n",
            "Train Epoch: 12 [34048/123872 (27%)]\tLoss: 0.411905\tLR: 0.00010357\n",
            "Train Epoch: 12 [34304/123872 (28%)]\tLoss: 0.375723\tLR: 0.00010351\n",
            "Train Epoch: 12 [34560/123872 (28%)]\tLoss: 0.371673\tLR: 0.00010344\n",
            "Train Epoch: 12 [34816/123872 (28%)]\tLoss: 0.398276\tLR: 0.00010338\n",
            "Train Epoch: 12 [35072/123872 (28%)]\tLoss: 0.391909\tLR: 0.00010332\n",
            "Train Epoch: 12 [35328/123872 (29%)]\tLoss: 0.370746\tLR: 0.00010325\n",
            "Train Epoch: 12 [35584/123872 (29%)]\tLoss: 0.328337\tLR: 0.00010319\n",
            "Train Epoch: 12 [35840/123872 (29%)]\tLoss: 0.368955\tLR: 0.00010312\n",
            "Train Epoch: 12 [35840/123872 (29%)]\tLoss: 0.368955\n",
            "Train Epoch: 12 [36096/123872 (29%)]\tLoss: 0.346932\tLR: 0.00010306\n",
            "Train Epoch: 12 [36352/123872 (29%)]\tLoss: 0.386040\tLR: 0.00010300\n",
            "Train Epoch: 12 [36608/123872 (30%)]\tLoss: 0.384181\tLR: 0.00010293\n",
            "Train Epoch: 12 [36864/123872 (30%)]\tLoss: 0.389078\tLR: 0.00010287\n",
            "Train Epoch: 12 [37120/123872 (30%)]\tLoss: 0.379716\tLR: 0.00010280\n",
            "Train Epoch: 12 [37376/123872 (30%)]\tLoss: 0.390941\tLR: 0.00010274\n",
            "Train Epoch: 12 [37632/123872 (30%)]\tLoss: 0.408911\tLR: 0.00010268\n",
            "Train Epoch: 12 [37888/123872 (31%)]\tLoss: 0.365366\tLR: 0.00010261\n",
            "Train Epoch: 12 [38144/123872 (31%)]\tLoss: 0.362538\tLR: 0.00010255\n",
            "Train Epoch: 12 [38400/123872 (31%)]\tLoss: 0.351278\tLR: 0.00010249\n",
            "Train Epoch: 12 [38400/123872 (31%)]\tLoss: 0.351278\n",
            "Train Epoch: 12 [38656/123872 (31%)]\tLoss: 0.438773\tLR: 0.00010242\n",
            "Train Epoch: 12 [38912/123872 (31%)]\tLoss: 0.431879\tLR: 0.00010236\n",
            "Train Epoch: 12 [39168/123872 (32%)]\tLoss: 0.392352\tLR: 0.00010229\n",
            "Train Epoch: 12 [39424/123872 (32%)]\tLoss: 0.357988\tLR: 0.00010223\n",
            "Train Epoch: 12 [39680/123872 (32%)]\tLoss: 0.373911\tLR: 0.00010217\n",
            "Train Epoch: 12 [39936/123872 (32%)]\tLoss: 0.385131\tLR: 0.00010210\n",
            "Train Epoch: 12 [40192/123872 (32%)]\tLoss: 0.346577\tLR: 0.00010204\n",
            "Train Epoch: 12 [40448/123872 (33%)]\tLoss: 0.399139\tLR: 0.00010198\n",
            "Train Epoch: 12 [40704/123872 (33%)]\tLoss: 0.361478\tLR: 0.00010192\n",
            "Train Epoch: 12 [40960/123872 (33%)]\tLoss: 0.388194\tLR: 0.00010185\n",
            "Train Epoch: 12 [40960/123872 (33%)]\tLoss: 0.388194\n",
            "Train Epoch: 12 [41216/123872 (33%)]\tLoss: 0.327771\tLR: 0.00010179\n",
            "Train Epoch: 12 [41472/123872 (33%)]\tLoss: 0.385053\tLR: 0.00010173\n",
            "Train Epoch: 12 [41728/123872 (34%)]\tLoss: 0.364324\tLR: 0.00010166\n",
            "Train Epoch: 12 [41984/123872 (34%)]\tLoss: 0.379578\tLR: 0.00010160\n",
            "Train Epoch: 12 [42240/123872 (34%)]\tLoss: 0.420261\tLR: 0.00010154\n",
            "Train Epoch: 12 [42496/123872 (34%)]\tLoss: 0.389995\tLR: 0.00010147\n",
            "Train Epoch: 12 [42752/123872 (35%)]\tLoss: 0.429696\tLR: 0.00010141\n",
            "Train Epoch: 12 [43008/123872 (35%)]\tLoss: 0.383056\tLR: 0.00010135\n",
            "Train Epoch: 12 [43264/123872 (35%)]\tLoss: 0.368335\tLR: 0.00010129\n",
            "Train Epoch: 12 [43520/123872 (35%)]\tLoss: 0.448833\tLR: 0.00010122\n",
            "Train Epoch: 12 [43520/123872 (35%)]\tLoss: 0.448833\n",
            "Train Epoch: 12 [43776/123872 (35%)]\tLoss: 0.407286\tLR: 0.00010116\n",
            "Train Epoch: 12 [44032/123872 (36%)]\tLoss: 0.352453\tLR: 0.00010110\n",
            "Train Epoch: 12 [44288/123872 (36%)]\tLoss: 0.375156\tLR: 0.00010104\n",
            "Train Epoch: 12 [44544/123872 (36%)]\tLoss: 0.382950\tLR: 0.00010097\n",
            "Train Epoch: 12 [44800/123872 (36%)]\tLoss: 0.361950\tLR: 0.00010091\n",
            "Train Epoch: 12 [45056/123872 (36%)]\tLoss: 0.344271\tLR: 0.00010085\n",
            "Train Epoch: 12 [45312/123872 (37%)]\tLoss: 0.363848\tLR: 0.00010079\n",
            "Train Epoch: 12 [45568/123872 (37%)]\tLoss: 0.390497\tLR: 0.00010072\n",
            "Train Epoch: 12 [45824/123872 (37%)]\tLoss: 0.422995\tLR: 0.00010066\n",
            "Train Epoch: 12 [46080/123872 (37%)]\tLoss: 0.372812\tLR: 0.00010060\n",
            "Train Epoch: 12 [46080/123872 (37%)]\tLoss: 0.372812\n",
            "Train Epoch: 12 [46336/123872 (37%)]\tLoss: 0.359031\tLR: 0.00010054\n",
            "Train Epoch: 12 [46592/123872 (38%)]\tLoss: 0.322031\tLR: 0.00010047\n",
            "Train Epoch: 12 [46848/123872 (38%)]\tLoss: 0.371952\tLR: 0.00010041\n",
            "Train Epoch: 12 [47104/123872 (38%)]\tLoss: 0.407312\tLR: 0.00010035\n",
            "Train Epoch: 12 [47360/123872 (38%)]\tLoss: 0.356784\tLR: 0.00010029\n",
            "Train Epoch: 12 [47616/123872 (38%)]\tLoss: 0.394675\tLR: 0.00010023\n",
            "Train Epoch: 12 [47872/123872 (39%)]\tLoss: 0.366146\tLR: 0.00010016\n",
            "Train Epoch: 12 [48128/123872 (39%)]\tLoss: 0.389285\tLR: 0.00010010\n",
            "Train Epoch: 12 [48384/123872 (39%)]\tLoss: 0.367560\tLR: 0.00010004\n",
            "Train Epoch: 12 [48640/123872 (39%)]\tLoss: 0.359392\tLR: 0.00009998\n",
            "Train Epoch: 12 [48640/123872 (39%)]\tLoss: 0.359392\n",
            "Train Epoch: 12 [48896/123872 (39%)]\tLoss: 0.404505\tLR: 0.00009992\n",
            "Train Epoch: 12 [49152/123872 (40%)]\tLoss: 0.411430\tLR: 0.00009986\n",
            "Train Epoch: 12 [49408/123872 (40%)]\tLoss: 0.375561\tLR: 0.00009979\n",
            "Train Epoch: 12 [49664/123872 (40%)]\tLoss: 0.326596\tLR: 0.00009973\n",
            "Train Epoch: 12 [49920/123872 (40%)]\tLoss: 0.389849\tLR: 0.00009967\n",
            "Train Epoch: 12 [50176/123872 (40%)]\tLoss: 0.391537\tLR: 0.00009961\n",
            "Train Epoch: 12 [50432/123872 (41%)]\tLoss: 0.365922\tLR: 0.00009955\n",
            "Train Epoch: 12 [50688/123872 (41%)]\tLoss: 0.426560\tLR: 0.00009949\n",
            "Train Epoch: 12 [50944/123872 (41%)]\tLoss: 0.396793\tLR: 0.00009942\n",
            "Train Epoch: 12 [51200/123872 (41%)]\tLoss: 0.353444\tLR: 0.00009936\n",
            "Train Epoch: 12 [51200/123872 (41%)]\tLoss: 0.353444\n",
            "Train Epoch: 12 [51456/123872 (42%)]\tLoss: 0.313477\tLR: 0.00009930\n",
            "Train Epoch: 12 [51712/123872 (42%)]\tLoss: 0.374142\tLR: 0.00009924\n",
            "Train Epoch: 12 [51968/123872 (42%)]\tLoss: 0.413974\tLR: 0.00009918\n",
            "Train Epoch: 12 [52224/123872 (42%)]\tLoss: 0.371192\tLR: 0.00009912\n",
            "Train Epoch: 12 [52480/123872 (42%)]\tLoss: 0.341725\tLR: 0.00009906\n",
            "Train Epoch: 12 [52736/123872 (43%)]\tLoss: 0.388144\tLR: 0.00009900\n",
            "Train Epoch: 12 [52992/123872 (43%)]\tLoss: 0.329664\tLR: 0.00009893\n",
            "Train Epoch: 12 [53248/123872 (43%)]\tLoss: 0.378371\tLR: 0.00009887\n",
            "Train Epoch: 12 [53504/123872 (43%)]\tLoss: 0.327017\tLR: 0.00009881\n",
            "Train Epoch: 12 [53760/123872 (43%)]\tLoss: 0.362400\tLR: 0.00009875\n",
            "Train Epoch: 12 [53760/123872 (43%)]\tLoss: 0.362400\n",
            "Train Epoch: 12 [54016/123872 (44%)]\tLoss: 0.430285\tLR: 0.00009869\n",
            "Train Epoch: 12 [54272/123872 (44%)]\tLoss: 0.345945\tLR: 0.00009863\n",
            "Train Epoch: 12 [54528/123872 (44%)]\tLoss: 0.360873\tLR: 0.00009857\n",
            "Train Epoch: 12 [54784/123872 (44%)]\tLoss: 0.408792\tLR: 0.00009851\n",
            "Train Epoch: 12 [55040/123872 (44%)]\tLoss: 0.392006\tLR: 0.00009845\n",
            "Train Epoch: 12 [55296/123872 (45%)]\tLoss: 0.411144\tLR: 0.00009839\n",
            "Train Epoch: 12 [55552/123872 (45%)]\tLoss: 0.363473\tLR: 0.00009833\n",
            "Train Epoch: 12 [55808/123872 (45%)]\tLoss: 0.363907\tLR: 0.00009827\n",
            "Train Epoch: 12 [56064/123872 (45%)]\tLoss: 0.448895\tLR: 0.00009820\n",
            "Train Epoch: 12 [56320/123872 (45%)]\tLoss: 0.369792\tLR: 0.00009814\n",
            "Train Epoch: 12 [56320/123872 (45%)]\tLoss: 0.369792\n",
            "Train Epoch: 12 [56576/123872 (46%)]\tLoss: 0.394061\tLR: 0.00009808\n",
            "Train Epoch: 12 [56832/123872 (46%)]\tLoss: 0.358589\tLR: 0.00009802\n",
            "Train Epoch: 12 [57088/123872 (46%)]\tLoss: 0.360598\tLR: 0.00009796\n",
            "Train Epoch: 12 [57344/123872 (46%)]\tLoss: 0.279010\tLR: 0.00009790\n",
            "Train Epoch: 12 [57600/123872 (46%)]\tLoss: 0.401972\tLR: 0.00009784\n",
            "Train Epoch: 12 [57856/123872 (47%)]\tLoss: 0.363969\tLR: 0.00009778\n",
            "Train Epoch: 12 [58112/123872 (47%)]\tLoss: 0.414730\tLR: 0.00009772\n",
            "Train Epoch: 12 [58368/123872 (47%)]\tLoss: 0.408680\tLR: 0.00009766\n",
            "Train Epoch: 12 [58624/123872 (47%)]\tLoss: 0.374758\tLR: 0.00009760\n",
            "Train Epoch: 12 [58880/123872 (48%)]\tLoss: 0.386189\tLR: 0.00009754\n",
            "Train Epoch: 12 [58880/123872 (48%)]\tLoss: 0.386189\n",
            "Train Epoch: 12 [59136/123872 (48%)]\tLoss: 0.396901\tLR: 0.00009748\n",
            "Train Epoch: 12 [59392/123872 (48%)]\tLoss: 0.367039\tLR: 0.00009742\n",
            "Train Epoch: 12 [59648/123872 (48%)]\tLoss: 0.338569\tLR: 0.00009736\n",
            "Train Epoch: 12 [59904/123872 (48%)]\tLoss: 0.427819\tLR: 0.00009730\n",
            "Train Epoch: 12 [60160/123872 (49%)]\tLoss: 0.406143\tLR: 0.00009724\n",
            "Train Epoch: 12 [60416/123872 (49%)]\tLoss: 0.383235\tLR: 0.00009718\n",
            "Train Epoch: 12 [60672/123872 (49%)]\tLoss: 0.349375\tLR: 0.00009712\n",
            "Train Epoch: 12 [60928/123872 (49%)]\tLoss: 0.304751\tLR: 0.00009706\n",
            "Train Epoch: 12 [61184/123872 (49%)]\tLoss: 0.357501\tLR: 0.00009700\n",
            "Train Epoch: 12 [61440/123872 (50%)]\tLoss: 0.411419\tLR: 0.00009694\n",
            "Train Epoch: 12 [61440/123872 (50%)]\tLoss: 0.411419\n",
            "Train Epoch: 12 [61696/123872 (50%)]\tLoss: 0.292187\tLR: 0.00009688\n",
            "Train Epoch: 12 [61952/123872 (50%)]\tLoss: 0.329974\tLR: 0.00009682\n",
            "Train Epoch: 12 [62208/123872 (50%)]\tLoss: 0.375463\tLR: 0.00009677\n",
            "Train Epoch: 12 [62464/123872 (50%)]\tLoss: 0.393638\tLR: 0.00009671\n",
            "Train Epoch: 12 [62720/123872 (51%)]\tLoss: 0.366110\tLR: 0.00009665\n",
            "Train Epoch: 12 [62976/123872 (51%)]\tLoss: 0.382338\tLR: 0.00009659\n",
            "Train Epoch: 12 [63232/123872 (51%)]\tLoss: 0.412547\tLR: 0.00009653\n",
            "Train Epoch: 12 [63488/123872 (51%)]\tLoss: 0.445158\tLR: 0.00009647\n",
            "Train Epoch: 12 [63744/123872 (51%)]\tLoss: 0.320076\tLR: 0.00009641\n",
            "Train Epoch: 12 [64000/123872 (52%)]\tLoss: 0.398949\tLR: 0.00009635\n",
            "Train Epoch: 12 [64000/123872 (52%)]\tLoss: 0.398949\n",
            "Train Epoch: 12 [64256/123872 (52%)]\tLoss: 0.420306\tLR: 0.00009629\n",
            "Train Epoch: 12 [64512/123872 (52%)]\tLoss: 0.344604\tLR: 0.00009623\n",
            "Train Epoch: 12 [64768/123872 (52%)]\tLoss: 0.355198\tLR: 0.00009617\n",
            "Train Epoch: 12 [65024/123872 (52%)]\tLoss: 0.442769\tLR: 0.00009611\n",
            "Train Epoch: 12 [65280/123872 (53%)]\tLoss: 0.314627\tLR: 0.00009606\n",
            "Train Epoch: 12 [65536/123872 (53%)]\tLoss: 0.345414\tLR: 0.00009600\n",
            "Train Epoch: 12 [65792/123872 (53%)]\tLoss: 0.432210\tLR: 0.00009594\n",
            "Train Epoch: 12 [66048/123872 (53%)]\tLoss: 0.345271\tLR: 0.00009588\n",
            "Train Epoch: 12 [66304/123872 (54%)]\tLoss: 0.402685\tLR: 0.00009582\n",
            "Train Epoch: 12 [66560/123872 (54%)]\tLoss: 0.333325\tLR: 0.00009576\n",
            "Train Epoch: 12 [66560/123872 (54%)]\tLoss: 0.333325\n",
            "Train Epoch: 12 [66816/123872 (54%)]\tLoss: 0.335684\tLR: 0.00009570\n",
            "Train Epoch: 12 [67072/123872 (54%)]\tLoss: 0.382210\tLR: 0.00009564\n",
            "Train Epoch: 12 [67328/123872 (54%)]\tLoss: 0.355346\tLR: 0.00009559\n",
            "Train Epoch: 12 [67584/123872 (55%)]\tLoss: 0.381631\tLR: 0.00009553\n",
            "Train Epoch: 12 [67840/123872 (55%)]\tLoss: 0.370538\tLR: 0.00009547\n",
            "Train Epoch: 12 [68096/123872 (55%)]\tLoss: 0.392676\tLR: 0.00009541\n",
            "Train Epoch: 12 [68352/123872 (55%)]\tLoss: 0.346981\tLR: 0.00009535\n",
            "Train Epoch: 12 [68608/123872 (55%)]\tLoss: 0.394181\tLR: 0.00009529\n",
            "Train Epoch: 12 [68864/123872 (56%)]\tLoss: 0.406337\tLR: 0.00009523\n",
            "Train Epoch: 12 [69120/123872 (56%)]\tLoss: 0.409043\tLR: 0.00009518\n",
            "Train Epoch: 12 [69120/123872 (56%)]\tLoss: 0.409043\n",
            "Train Epoch: 12 [69376/123872 (56%)]\tLoss: 0.348932\tLR: 0.00009512\n",
            "Train Epoch: 12 [69632/123872 (56%)]\tLoss: 0.376772\tLR: 0.00009506\n",
            "Train Epoch: 12 [69888/123872 (56%)]\tLoss: 0.394217\tLR: 0.00009500\n",
            "Train Epoch: 12 [70144/123872 (57%)]\tLoss: 0.389520\tLR: 0.00009494\n",
            "Train Epoch: 12 [70400/123872 (57%)]\tLoss: 0.381993\tLR: 0.00009489\n",
            "Train Epoch: 12 [70656/123872 (57%)]\tLoss: 0.353545\tLR: 0.00009483\n",
            "Train Epoch: 12 [70912/123872 (57%)]\tLoss: 0.381966\tLR: 0.00009477\n",
            "Train Epoch: 12 [71168/123872 (57%)]\tLoss: 0.387791\tLR: 0.00009471\n",
            "Train Epoch: 12 [71424/123872 (58%)]\tLoss: 0.390979\tLR: 0.00009465\n",
            "Train Epoch: 12 [71680/123872 (58%)]\tLoss: 0.372540\tLR: 0.00009460\n",
            "Train Epoch: 12 [71680/123872 (58%)]\tLoss: 0.372540\n",
            "Train Epoch: 12 [71936/123872 (58%)]\tLoss: 0.403656\tLR: 0.00009454\n",
            "Train Epoch: 12 [72192/123872 (58%)]\tLoss: 0.364695\tLR: 0.00009448\n",
            "Train Epoch: 12 [72448/123872 (58%)]\tLoss: 0.328101\tLR: 0.00009442\n",
            "Train Epoch: 12 [72704/123872 (59%)]\tLoss: 0.361212\tLR: 0.00009437\n",
            "Train Epoch: 12 [72960/123872 (59%)]\tLoss: 0.368171\tLR: 0.00009431\n",
            "Train Epoch: 12 [73216/123872 (59%)]\tLoss: 0.381779\tLR: 0.00009425\n",
            "Train Epoch: 12 [73472/123872 (59%)]\tLoss: 0.407109\tLR: 0.00009419\n",
            "Train Epoch: 12 [73728/123872 (60%)]\tLoss: 0.416366\tLR: 0.00009414\n",
            "Train Epoch: 12 [73984/123872 (60%)]\tLoss: 0.380494\tLR: 0.00009408\n",
            "Train Epoch: 12 [74240/123872 (60%)]\tLoss: 0.316052\tLR: 0.00009402\n",
            "Train Epoch: 12 [74240/123872 (60%)]\tLoss: 0.316052\n",
            "Train Epoch: 12 [74496/123872 (60%)]\tLoss: 0.343472\tLR: 0.00009396\n",
            "Train Epoch: 12 [74752/123872 (60%)]\tLoss: 0.355623\tLR: 0.00009391\n",
            "Train Epoch: 12 [75008/123872 (61%)]\tLoss: 0.421141\tLR: 0.00009385\n",
            "Train Epoch: 12 [75264/123872 (61%)]\tLoss: 0.441240\tLR: 0.00009379\n",
            "Train Epoch: 12 [75520/123872 (61%)]\tLoss: 0.345436\tLR: 0.00009373\n",
            "Train Epoch: 12 [75776/123872 (61%)]\tLoss: 0.410183\tLR: 0.00009368\n",
            "Train Epoch: 12 [76032/123872 (61%)]\tLoss: 0.371124\tLR: 0.00009362\n",
            "Train Epoch: 12 [76288/123872 (62%)]\tLoss: 0.395392\tLR: 0.00009356\n",
            "Train Epoch: 12 [76544/123872 (62%)]\tLoss: 0.415772\tLR: 0.00009351\n",
            "Train Epoch: 12 [76800/123872 (62%)]\tLoss: 0.347727\tLR: 0.00009345\n",
            "Train Epoch: 12 [76800/123872 (62%)]\tLoss: 0.347727\n",
            "Train Epoch: 12 [77056/123872 (62%)]\tLoss: 0.358437\tLR: 0.00009339\n",
            "Train Epoch: 12 [77312/123872 (62%)]\tLoss: 0.377543\tLR: 0.00009334\n",
            "Train Epoch: 12 [77568/123872 (63%)]\tLoss: 0.427435\tLR: 0.00009328\n",
            "Train Epoch: 12 [77824/123872 (63%)]\tLoss: 0.346944\tLR: 0.00009322\n",
            "Train Epoch: 12 [78080/123872 (63%)]\tLoss: 0.373425\tLR: 0.00009317\n",
            "Train Epoch: 12 [78336/123872 (63%)]\tLoss: 0.327287\tLR: 0.00009311\n",
            "Train Epoch: 12 [78592/123872 (63%)]\tLoss: 0.355599\tLR: 0.00009305\n",
            "Train Epoch: 12 [78848/123872 (64%)]\tLoss: 0.353556\tLR: 0.00009300\n",
            "Train Epoch: 12 [79104/123872 (64%)]\tLoss: 0.409722\tLR: 0.00009294\n",
            "Train Epoch: 12 [79360/123872 (64%)]\tLoss: 0.379293\tLR: 0.00009288\n",
            "Train Epoch: 12 [79360/123872 (64%)]\tLoss: 0.379293\n",
            "Train Epoch: 12 [79616/123872 (64%)]\tLoss: 0.392816\tLR: 0.00009283\n",
            "Train Epoch: 12 [79872/123872 (64%)]\tLoss: 0.356970\tLR: 0.00009277\n",
            "Train Epoch: 12 [80128/123872 (65%)]\tLoss: 0.393518\tLR: 0.00009271\n",
            "Train Epoch: 12 [80384/123872 (65%)]\tLoss: 0.373998\tLR: 0.00009266\n",
            "Train Epoch: 12 [80640/123872 (65%)]\tLoss: 0.416013\tLR: 0.00009260\n",
            "Train Epoch: 12 [80896/123872 (65%)]\tLoss: 0.410332\tLR: 0.00009255\n",
            "Train Epoch: 12 [81152/123872 (65%)]\tLoss: 0.400808\tLR: 0.00009249\n",
            "Train Epoch: 12 [81408/123872 (66%)]\tLoss: 0.393125\tLR: 0.00009243\n",
            "Train Epoch: 12 [81664/123872 (66%)]\tLoss: 0.358605\tLR: 0.00009238\n",
            "Train Epoch: 12 [81920/123872 (66%)]\tLoss: 0.368161\tLR: 0.00009232\n",
            "Train Epoch: 12 [81920/123872 (66%)]\tLoss: 0.368161\n",
            "Train Epoch: 12 [82176/123872 (66%)]\tLoss: 0.393496\tLR: 0.00009226\n",
            "Train Epoch: 12 [82432/123872 (67%)]\tLoss: 0.420901\tLR: 0.00009221\n",
            "Train Epoch: 12 [82688/123872 (67%)]\tLoss: 0.352505\tLR: 0.00009215\n",
            "Train Epoch: 12 [82944/123872 (67%)]\tLoss: 0.353653\tLR: 0.00009210\n",
            "Train Epoch: 12 [83200/123872 (67%)]\tLoss: 0.378918\tLR: 0.00009204\n",
            "Train Epoch: 12 [83456/123872 (67%)]\tLoss: 0.404648\tLR: 0.00009199\n",
            "Train Epoch: 12 [83712/123872 (68%)]\tLoss: 0.352714\tLR: 0.00009193\n",
            "Train Epoch: 12 [83968/123872 (68%)]\tLoss: 0.389592\tLR: 0.00009187\n",
            "Train Epoch: 12 [84224/123872 (68%)]\tLoss: 0.361106\tLR: 0.00009182\n",
            "Train Epoch: 12 [84480/123872 (68%)]\tLoss: 0.378429\tLR: 0.00009176\n",
            "Train Epoch: 12 [84480/123872 (68%)]\tLoss: 0.378429\n",
            "Train Epoch: 12 [84736/123872 (68%)]\tLoss: 0.400102\tLR: 0.00009171\n",
            "Train Epoch: 12 [84992/123872 (69%)]\tLoss: 0.375950\tLR: 0.00009165\n",
            "Train Epoch: 12 [85248/123872 (69%)]\tLoss: 0.473962\tLR: 0.00009160\n",
            "Train Epoch: 12 [85504/123872 (69%)]\tLoss: 0.374318\tLR: 0.00009154\n",
            "Train Epoch: 12 [85760/123872 (69%)]\tLoss: 0.334766\tLR: 0.00009149\n",
            "Train Epoch: 12 [86016/123872 (69%)]\tLoss: 0.355252\tLR: 0.00009143\n",
            "Train Epoch: 12 [86272/123872 (70%)]\tLoss: 0.419829\tLR: 0.00009138\n",
            "Train Epoch: 12 [86528/123872 (70%)]\tLoss: 0.373689\tLR: 0.00009132\n",
            "Train Epoch: 12 [86784/123872 (70%)]\tLoss: 0.427495\tLR: 0.00009127\n",
            "Train Epoch: 12 [87040/123872 (70%)]\tLoss: 0.380290\tLR: 0.00009121\n",
            "Train Epoch: 12 [87040/123872 (70%)]\tLoss: 0.380290\n",
            "Train Epoch: 12 [87296/123872 (70%)]\tLoss: 0.326620\tLR: 0.00009116\n",
            "Train Epoch: 12 [87552/123872 (71%)]\tLoss: 0.432159\tLR: 0.00009110\n",
            "Train Epoch: 12 [87808/123872 (71%)]\tLoss: 0.347469\tLR: 0.00009105\n",
            "Train Epoch: 12 [88064/123872 (71%)]\tLoss: 0.398421\tLR: 0.00009099\n",
            "Train Epoch: 12 [88320/123872 (71%)]\tLoss: 0.345971\tLR: 0.00009094\n",
            "Train Epoch: 12 [88576/123872 (71%)]\tLoss: 0.380306\tLR: 0.00009088\n",
            "Train Epoch: 12 [88832/123872 (72%)]\tLoss: 0.341282\tLR: 0.00009083\n",
            "Train Epoch: 12 [89088/123872 (72%)]\tLoss: 0.359721\tLR: 0.00009077\n",
            "Train Epoch: 12 [89344/123872 (72%)]\tLoss: 0.327337\tLR: 0.00009072\n",
            "Train Epoch: 12 [89600/123872 (72%)]\tLoss: 0.368600\tLR: 0.00009066\n",
            "Train Epoch: 12 [89600/123872 (72%)]\tLoss: 0.368600\n",
            "Train Epoch: 12 [89856/123872 (73%)]\tLoss: 0.413656\tLR: 0.00009061\n",
            "Train Epoch: 12 [90112/123872 (73%)]\tLoss: 0.314254\tLR: 0.00009055\n",
            "Train Epoch: 12 [90368/123872 (73%)]\tLoss: 0.363513\tLR: 0.00009050\n",
            "Train Epoch: 12 [90624/123872 (73%)]\tLoss: 0.343267\tLR: 0.00009044\n",
            "Train Epoch: 12 [90880/123872 (73%)]\tLoss: 0.353902\tLR: 0.00009039\n",
            "Train Epoch: 12 [91136/123872 (74%)]\tLoss: 0.384690\tLR: 0.00009033\n",
            "Train Epoch: 12 [91392/123872 (74%)]\tLoss: 0.325533\tLR: 0.00009028\n",
            "Train Epoch: 12 [91648/123872 (74%)]\tLoss: 0.406057\tLR: 0.00009023\n",
            "Train Epoch: 12 [91904/123872 (74%)]\tLoss: 0.355494\tLR: 0.00009017\n",
            "Train Epoch: 12 [92160/123872 (74%)]\tLoss: 0.407290\tLR: 0.00009012\n",
            "Train Epoch: 12 [92160/123872 (74%)]\tLoss: 0.407290\n",
            "Train Epoch: 12 [92416/123872 (75%)]\tLoss: 0.385627\tLR: 0.00009006\n",
            "Train Epoch: 12 [92672/123872 (75%)]\tLoss: 0.410429\tLR: 0.00009001\n",
            "Train Epoch: 12 [92928/123872 (75%)]\tLoss: 0.355190\tLR: 0.00008996\n",
            "Train Epoch: 12 [93184/123872 (75%)]\tLoss: 0.315664\tLR: 0.00008990\n",
            "Train Epoch: 12 [93440/123872 (75%)]\tLoss: 0.398590\tLR: 0.00008985\n",
            "Train Epoch: 12 [93696/123872 (76%)]\tLoss: 0.373625\tLR: 0.00008979\n",
            "Train Epoch: 12 [93952/123872 (76%)]\tLoss: 0.385599\tLR: 0.00008974\n",
            "Train Epoch: 12 [94208/123872 (76%)]\tLoss: 0.386355\tLR: 0.00008969\n",
            "Train Epoch: 12 [94464/123872 (76%)]\tLoss: 0.396570\tLR: 0.00008963\n",
            "Train Epoch: 12 [94720/123872 (76%)]\tLoss: 0.410019\tLR: 0.00008958\n",
            "Train Epoch: 12 [94720/123872 (76%)]\tLoss: 0.410019\n",
            "Train Epoch: 12 [94976/123872 (77%)]\tLoss: 0.345305\tLR: 0.00008952\n",
            "Train Epoch: 12 [95232/123872 (77%)]\tLoss: 0.311552\tLR: 0.00008947\n",
            "Train Epoch: 12 [95488/123872 (77%)]\tLoss: 0.461150\tLR: 0.00008942\n",
            "Train Epoch: 12 [95744/123872 (77%)]\tLoss: 0.357057\tLR: 0.00008936\n",
            "Train Epoch: 12 [96000/123872 (77%)]\tLoss: 0.323827\tLR: 0.00008931\n",
            "Train Epoch: 12 [96256/123872 (78%)]\tLoss: 0.398494\tLR: 0.00008926\n",
            "Train Epoch: 12 [96512/123872 (78%)]\tLoss: 0.392503\tLR: 0.00008920\n",
            "Train Epoch: 12 [96768/123872 (78%)]\tLoss: 0.417402\tLR: 0.00008915\n",
            "Train Epoch: 12 [97024/123872 (78%)]\tLoss: 0.373815\tLR: 0.00008910\n",
            "Train Epoch: 12 [97280/123872 (79%)]\tLoss: 0.374072\tLR: 0.00008904\n",
            "Train Epoch: 12 [97280/123872 (79%)]\tLoss: 0.374072\n",
            "Train Epoch: 12 [97536/123872 (79%)]\tLoss: 0.386020\tLR: 0.00008899\n",
            "Train Epoch: 12 [97792/123872 (79%)]\tLoss: 0.394445\tLR: 0.00008894\n",
            "Train Epoch: 12 [98048/123872 (79%)]\tLoss: 0.351807\tLR: 0.00008888\n",
            "Train Epoch: 12 [98304/123872 (79%)]\tLoss: 0.371530\tLR: 0.00008883\n",
            "Train Epoch: 12 [98560/123872 (80%)]\tLoss: 0.341109\tLR: 0.00008878\n",
            "Train Epoch: 12 [98816/123872 (80%)]\tLoss: 0.393787\tLR: 0.00008873\n",
            "Train Epoch: 12 [99072/123872 (80%)]\tLoss: 0.415382\tLR: 0.00008867\n",
            "Train Epoch: 12 [99328/123872 (80%)]\tLoss: 0.417067\tLR: 0.00008862\n",
            "Train Epoch: 12 [99584/123872 (80%)]\tLoss: 0.368034\tLR: 0.00008857\n",
            "Train Epoch: 12 [99840/123872 (81%)]\tLoss: 0.411496\tLR: 0.00008851\n",
            "Train Epoch: 12 [99840/123872 (81%)]\tLoss: 0.411496\n",
            "Train Epoch: 12 [100096/123872 (81%)]\tLoss: 0.375976\tLR: 0.00008846\n",
            "Train Epoch: 12 [100352/123872 (81%)]\tLoss: 0.342339\tLR: 0.00008841\n",
            "Train Epoch: 12 [100608/123872 (81%)]\tLoss: 0.312277\tLR: 0.00008836\n",
            "Train Epoch: 12 [100864/123872 (81%)]\tLoss: 0.388824\tLR: 0.00008830\n",
            "Train Epoch: 12 [101120/123872 (82%)]\tLoss: 0.392839\tLR: 0.00008825\n",
            "Train Epoch: 12 [101376/123872 (82%)]\tLoss: 0.326510\tLR: 0.00008820\n",
            "Train Epoch: 12 [101632/123872 (82%)]\tLoss: 0.369232\tLR: 0.00008815\n",
            "Train Epoch: 12 [101888/123872 (82%)]\tLoss: 0.375708\tLR: 0.00008809\n",
            "Train Epoch: 12 [102144/123872 (82%)]\tLoss: 0.437533\tLR: 0.00008804\n",
            "Train Epoch: 12 [102400/123872 (83%)]\tLoss: 0.347341\tLR: 0.00008799\n",
            "Train Epoch: 12 [102400/123872 (83%)]\tLoss: 0.347341\n",
            "Train Epoch: 12 [102656/123872 (83%)]\tLoss: 0.406275\tLR: 0.00008794\n",
            "Train Epoch: 12 [102912/123872 (83%)]\tLoss: 0.410622\tLR: 0.00008788\n",
            "Train Epoch: 12 [103168/123872 (83%)]\tLoss: 0.344354\tLR: 0.00008783\n",
            "Train Epoch: 12 [103424/123872 (83%)]\tLoss: 0.360067\tLR: 0.00008778\n",
            "Train Epoch: 12 [103680/123872 (84%)]\tLoss: 0.359231\tLR: 0.00008773\n",
            "Train Epoch: 12 [103936/123872 (84%)]\tLoss: 0.487323\tLR: 0.00008767\n",
            "Train Epoch: 12 [104192/123872 (84%)]\tLoss: 0.373164\tLR: 0.00008762\n",
            "Train Epoch: 12 [104448/123872 (84%)]\tLoss: 0.336121\tLR: 0.00008757\n",
            "Train Epoch: 12 [104704/123872 (85%)]\tLoss: 0.364807\tLR: 0.00008752\n",
            "Train Epoch: 12 [104960/123872 (85%)]\tLoss: 0.385130\tLR: 0.00008747\n",
            "Train Epoch: 12 [104960/123872 (85%)]\tLoss: 0.385130\n",
            "Train Epoch: 12 [105216/123872 (85%)]\tLoss: 0.383911\tLR: 0.00008742\n",
            "Train Epoch: 12 [105472/123872 (85%)]\tLoss: 0.373997\tLR: 0.00008736\n",
            "Train Epoch: 12 [105728/123872 (85%)]\tLoss: 0.361429\tLR: 0.00008731\n",
            "Train Epoch: 12 [105984/123872 (86%)]\tLoss: 0.397993\tLR: 0.00008726\n",
            "Train Epoch: 12 [106240/123872 (86%)]\tLoss: 0.312570\tLR: 0.00008721\n",
            "Train Epoch: 12 [106496/123872 (86%)]\tLoss: 0.376993\tLR: 0.00008716\n",
            "Train Epoch: 12 [106752/123872 (86%)]\tLoss: 0.416707\tLR: 0.00008711\n",
            "Train Epoch: 12 [107008/123872 (86%)]\tLoss: 0.352967\tLR: 0.00008705\n",
            "Train Epoch: 12 [107264/123872 (87%)]\tLoss: 0.365861\tLR: 0.00008700\n",
            "Train Epoch: 12 [107520/123872 (87%)]\tLoss: 0.386067\tLR: 0.00008695\n",
            "Train Epoch: 12 [107520/123872 (87%)]\tLoss: 0.386067\n",
            "Train Epoch: 12 [107776/123872 (87%)]\tLoss: 0.342241\tLR: 0.00008690\n",
            "Train Epoch: 12 [108032/123872 (87%)]\tLoss: 0.375583\tLR: 0.00008685\n",
            "Train Epoch: 12 [108288/123872 (87%)]\tLoss: 0.375327\tLR: 0.00008680\n",
            "Train Epoch: 12 [108544/123872 (88%)]\tLoss: 0.367338\tLR: 0.00008675\n",
            "Train Epoch: 12 [108800/123872 (88%)]\tLoss: 0.350329\tLR: 0.00008669\n",
            "Train Epoch: 12 [109056/123872 (88%)]\tLoss: 0.400843\tLR: 0.00008664\n",
            "Train Epoch: 12 [109312/123872 (88%)]\tLoss: 0.382048\tLR: 0.00008659\n",
            "Train Epoch: 12 [109568/123872 (88%)]\tLoss: 0.333499\tLR: 0.00008654\n",
            "Train Epoch: 12 [109824/123872 (89%)]\tLoss: 0.309795\tLR: 0.00008649\n",
            "Train Epoch: 12 [110080/123872 (89%)]\tLoss: 0.413903\tLR: 0.00008644\n",
            "Train Epoch: 12 [110080/123872 (89%)]\tLoss: 0.413903\n",
            "Train Epoch: 12 [110336/123872 (89%)]\tLoss: 0.374463\tLR: 0.00008639\n",
            "Train Epoch: 12 [110592/123872 (89%)]\tLoss: 0.367014\tLR: 0.00008634\n",
            "Train Epoch: 12 [110848/123872 (89%)]\tLoss: 0.394223\tLR: 0.00008629\n",
            "Train Epoch: 12 [111104/123872 (90%)]\tLoss: 0.377907\tLR: 0.00008624\n",
            "Train Epoch: 12 [111360/123872 (90%)]\tLoss: 0.398088\tLR: 0.00008619\n",
            "Train Epoch: 12 [111616/123872 (90%)]\tLoss: 0.412899\tLR: 0.00008613\n",
            "Train Epoch: 12 [111872/123872 (90%)]\tLoss: 0.341696\tLR: 0.00008608\n",
            "Train Epoch: 12 [112128/123872 (90%)]\tLoss: 0.381998\tLR: 0.00008603\n",
            "Train Epoch: 12 [112384/123872 (91%)]\tLoss: 0.331817\tLR: 0.00008598\n",
            "Train Epoch: 12 [112640/123872 (91%)]\tLoss: 0.375408\tLR: 0.00008593\n",
            "Train Epoch: 12 [112640/123872 (91%)]\tLoss: 0.375408\n",
            "Train Epoch: 12 [112896/123872 (91%)]\tLoss: 0.430127\tLR: 0.00008588\n",
            "Train Epoch: 12 [113152/123872 (91%)]\tLoss: 0.343256\tLR: 0.00008583\n",
            "Train Epoch: 12 [113408/123872 (92%)]\tLoss: 0.331807\tLR: 0.00008578\n",
            "Train Epoch: 12 [113664/123872 (92%)]\tLoss: 0.412376\tLR: 0.00008573\n",
            "Train Epoch: 12 [113920/123872 (92%)]\tLoss: 0.360151\tLR: 0.00008568\n",
            "Train Epoch: 12 [114176/123872 (92%)]\tLoss: 0.423848\tLR: 0.00008563\n",
            "Train Epoch: 12 [114432/123872 (92%)]\tLoss: 0.329929\tLR: 0.00008558\n",
            "Train Epoch: 12 [114688/123872 (93%)]\tLoss: 0.383062\tLR: 0.00008553\n",
            "Train Epoch: 12 [114944/123872 (93%)]\tLoss: 0.447112\tLR: 0.00008548\n",
            "Train Epoch: 12 [115200/123872 (93%)]\tLoss: 0.350528\tLR: 0.00008543\n",
            "Train Epoch: 12 [115200/123872 (93%)]\tLoss: 0.350528\n",
            "Train Epoch: 12 [115456/123872 (93%)]\tLoss: 0.393875\tLR: 0.00008538\n",
            "Train Epoch: 12 [115712/123872 (93%)]\tLoss: 0.397680\tLR: 0.00008533\n",
            "Train Epoch: 12 [115968/123872 (94%)]\tLoss: 0.361584\tLR: 0.00008528\n",
            "Train Epoch: 12 [116224/123872 (94%)]\tLoss: 0.315925\tLR: 0.00008523\n",
            "Train Epoch: 12 [116480/123872 (94%)]\tLoss: 0.353382\tLR: 0.00008518\n",
            "Train Epoch: 12 [116736/123872 (94%)]\tLoss: 0.390965\tLR: 0.00008513\n",
            "Train Epoch: 12 [116992/123872 (94%)]\tLoss: 0.366728\tLR: 0.00008508\n",
            "Train Epoch: 12 [117248/123872 (95%)]\tLoss: 0.420033\tLR: 0.00008503\n",
            "Train Epoch: 12 [117504/123872 (95%)]\tLoss: 0.404950\tLR: 0.00008498\n",
            "Train Epoch: 12 [117760/123872 (95%)]\tLoss: 0.440140\tLR: 0.00008493\n",
            "Train Epoch: 12 [117760/123872 (95%)]\tLoss: 0.440140\n",
            "Train Epoch: 12 [118016/123872 (95%)]\tLoss: 0.364437\tLR: 0.00008488\n",
            "Train Epoch: 12 [118272/123872 (95%)]\tLoss: 0.375119\tLR: 0.00008483\n",
            "Train Epoch: 12 [118528/123872 (96%)]\tLoss: 0.354007\tLR: 0.00008478\n",
            "Train Epoch: 12 [118784/123872 (96%)]\tLoss: 0.368493\tLR: 0.00008473\n",
            "Train Epoch: 12 [119040/123872 (96%)]\tLoss: 0.371192\tLR: 0.00008469\n",
            "Train Epoch: 12 [119296/123872 (96%)]\tLoss: 0.367762\tLR: 0.00008464\n",
            "Train Epoch: 12 [119552/123872 (96%)]\tLoss: 0.370803\tLR: 0.00008459\n",
            "Train Epoch: 12 [119808/123872 (97%)]\tLoss: 0.370040\tLR: 0.00008454\n",
            "Train Epoch: 12 [120064/123872 (97%)]\tLoss: 0.315560\tLR: 0.00008449\n",
            "Train Epoch: 12 [120320/123872 (97%)]\tLoss: 0.381692\tLR: 0.00008444\n",
            "Train Epoch: 12 [120320/123872 (97%)]\tLoss: 0.381692\n",
            "Train Epoch: 12 [120576/123872 (97%)]\tLoss: 0.433592\tLR: 0.00008439\n",
            "Train Epoch: 12 [120832/123872 (98%)]\tLoss: 0.409569\tLR: 0.00008434\n",
            "Train Epoch: 12 [121088/123872 (98%)]\tLoss: 0.394811\tLR: 0.00008429\n",
            "Train Epoch: 12 [121344/123872 (98%)]\tLoss: 0.305636\tLR: 0.00008424\n",
            "Train Epoch: 12 [121600/123872 (98%)]\tLoss: 0.343591\tLR: 0.00008419\n",
            "Train Epoch: 12 [121856/123872 (98%)]\tLoss: 0.353178\tLR: 0.00008415\n",
            "Train Epoch: 12 [122112/123872 (99%)]\tLoss: 0.309780\tLR: 0.00008410\n",
            "Train Epoch: 12 [122368/123872 (99%)]\tLoss: 0.357846\tLR: 0.00008405\n",
            "Train Epoch: 12 [122624/123872 (99%)]\tLoss: 0.416142\tLR: 0.00008400\n",
            "Train Epoch: 12 [122880/123872 (99%)]\tLoss: 0.420647\tLR: 0.00008395\n",
            "Train Epoch: 12 [122880/123872 (99%)]\tLoss: 0.420647\n",
            "Train Epoch: 12 [123136/123872 (99%)]\tLoss: 0.357406\tLR: 0.00008390\n",
            "Train Epoch: 12 [123392/123872 (100%)]\tLoss: 0.358503\tLR: 0.00008385\n",
            "Train Epoch: 12 [108192/123872 (100%)]\tLoss: 0.406711\tLR: 0.00008381\n",
            "\n",
            "Test set: Average loss: 0.0015, Accuracy: 25487/30970 (82.30%)\n",
            "\n",
            "Train Epoch: 13 [0/123872 (0%)]\tLoss: 0.379396\tLR: 0.00008376\n",
            "Train Epoch: 13 [0/123872 (0%)]\tLoss: 0.379396\n",
            "Train Epoch: 13 [256/123872 (0%)]\tLoss: 0.401293\tLR: 0.00008371\n",
            "Train Epoch: 13 [512/123872 (0%)]\tLoss: 0.336655\tLR: 0.00008366\n",
            "Train Epoch: 13 [768/123872 (1%)]\tLoss: 0.370495\tLR: 0.00008361\n",
            "Train Epoch: 13 [1024/123872 (1%)]\tLoss: 0.358099\tLR: 0.00008356\n",
            "Train Epoch: 13 [1280/123872 (1%)]\tLoss: 0.326951\tLR: 0.00008352\n",
            "Train Epoch: 13 [1536/123872 (1%)]\tLoss: 0.369308\tLR: 0.00008347\n",
            "Train Epoch: 13 [1792/123872 (1%)]\tLoss: 0.340820\tLR: 0.00008342\n",
            "Train Epoch: 13 [2048/123872 (2%)]\tLoss: 0.370522\tLR: 0.00008337\n",
            "Train Epoch: 13 [2304/123872 (2%)]\tLoss: 0.358498\tLR: 0.00008332\n",
            "Train Epoch: 13 [2560/123872 (2%)]\tLoss: 0.341225\tLR: 0.00008328\n",
            "Train Epoch: 13 [2560/123872 (2%)]\tLoss: 0.341225\n",
            "Train Epoch: 13 [2816/123872 (2%)]\tLoss: 0.348088\tLR: 0.00008323\n",
            "Train Epoch: 13 [3072/123872 (2%)]\tLoss: 0.311317\tLR: 0.00008318\n",
            "Train Epoch: 13 [3328/123872 (3%)]\tLoss: 0.392344\tLR: 0.00008313\n",
            "Train Epoch: 13 [3584/123872 (3%)]\tLoss: 0.327241\tLR: 0.00008308\n",
            "Train Epoch: 13 [3840/123872 (3%)]\tLoss: 0.385109\tLR: 0.00008304\n",
            "Train Epoch: 13 [4096/123872 (3%)]\tLoss: 0.388329\tLR: 0.00008299\n",
            "Train Epoch: 13 [4352/123872 (4%)]\tLoss: 0.402882\tLR: 0.00008294\n",
            "Train Epoch: 13 [4608/123872 (4%)]\tLoss: 0.348010\tLR: 0.00008289\n",
            "Train Epoch: 13 [4864/123872 (4%)]\tLoss: 0.372407\tLR: 0.00008285\n",
            "Train Epoch: 13 [5120/123872 (4%)]\tLoss: 0.411915\tLR: 0.00008280\n",
            "Train Epoch: 13 [5120/123872 (4%)]\tLoss: 0.411915\n",
            "Train Epoch: 13 [5376/123872 (4%)]\tLoss: 0.372205\tLR: 0.00008275\n",
            "Train Epoch: 13 [5632/123872 (5%)]\tLoss: 0.345444\tLR: 0.00008270\n",
            "Train Epoch: 13 [5888/123872 (5%)]\tLoss: 0.363505\tLR: 0.00008266\n",
            "Train Epoch: 13 [6144/123872 (5%)]\tLoss: 0.308189\tLR: 0.00008261\n",
            "Train Epoch: 13 [6400/123872 (5%)]\tLoss: 0.295641\tLR: 0.00008256\n",
            "Train Epoch: 13 [6656/123872 (5%)]\tLoss: 0.338259\tLR: 0.00008251\n",
            "Train Epoch: 13 [6912/123872 (6%)]\tLoss: 0.392529\tLR: 0.00008247\n",
            "Train Epoch: 13 [7168/123872 (6%)]\tLoss: 0.383895\tLR: 0.00008242\n",
            "Train Epoch: 13 [7424/123872 (6%)]\tLoss: 0.323891\tLR: 0.00008237\n",
            "Train Epoch: 13 [7680/123872 (6%)]\tLoss: 0.385185\tLR: 0.00008233\n",
            "Train Epoch: 13 [7680/123872 (6%)]\tLoss: 0.385185\n",
            "Train Epoch: 13 [7936/123872 (6%)]\tLoss: 0.410500\tLR: 0.00008228\n",
            "Train Epoch: 13 [8192/123872 (7%)]\tLoss: 0.366199\tLR: 0.00008223\n",
            "Train Epoch: 13 [8448/123872 (7%)]\tLoss: 0.358516\tLR: 0.00008219\n",
            "Train Epoch: 13 [8704/123872 (7%)]\tLoss: 0.371420\tLR: 0.00008214\n",
            "Train Epoch: 13 [8960/123872 (7%)]\tLoss: 0.409333\tLR: 0.00008209\n",
            "Train Epoch: 13 [9216/123872 (7%)]\tLoss: 0.350845\tLR: 0.00008205\n",
            "Train Epoch: 13 [9472/123872 (8%)]\tLoss: 0.385549\tLR: 0.00008200\n",
            "Train Epoch: 13 [9728/123872 (8%)]\tLoss: 0.368572\tLR: 0.00008195\n",
            "Train Epoch: 13 [9984/123872 (8%)]\tLoss: 0.375759\tLR: 0.00008191\n",
            "Train Epoch: 13 [10240/123872 (8%)]\tLoss: 0.362051\tLR: 0.00008186\n",
            "Train Epoch: 13 [10240/123872 (8%)]\tLoss: 0.362051\n",
            "Train Epoch: 13 [10496/123872 (8%)]\tLoss: 0.369341\tLR: 0.00008181\n",
            "Train Epoch: 13 [10752/123872 (9%)]\tLoss: 0.394219\tLR: 0.00008177\n",
            "Train Epoch: 13 [11008/123872 (9%)]\tLoss: 0.388156\tLR: 0.00008172\n",
            "Train Epoch: 13 [11264/123872 (9%)]\tLoss: 0.333050\tLR: 0.00008167\n",
            "Train Epoch: 13 [11520/123872 (9%)]\tLoss: 0.388134\tLR: 0.00008163\n",
            "Train Epoch: 13 [11776/123872 (10%)]\tLoss: 0.362002\tLR: 0.00008158\n",
            "Train Epoch: 13 [12032/123872 (10%)]\tLoss: 0.399802\tLR: 0.00008153\n",
            "Train Epoch: 13 [12288/123872 (10%)]\tLoss: 0.400693\tLR: 0.00008149\n",
            "Train Epoch: 13 [12544/123872 (10%)]\tLoss: 0.369599\tLR: 0.00008144\n",
            "Train Epoch: 13 [12800/123872 (10%)]\tLoss: 0.423418\tLR: 0.00008140\n",
            "Train Epoch: 13 [12800/123872 (10%)]\tLoss: 0.423418\n",
            "Train Epoch: 13 [13056/123872 (11%)]\tLoss: 0.339007\tLR: 0.00008135\n",
            "Train Epoch: 13 [13312/123872 (11%)]\tLoss: 0.351685\tLR: 0.00008130\n",
            "Train Epoch: 13 [13568/123872 (11%)]\tLoss: 0.334301\tLR: 0.00008126\n",
            "Train Epoch: 13 [13824/123872 (11%)]\tLoss: 0.341183\tLR: 0.00008121\n",
            "Train Epoch: 13 [14080/123872 (11%)]\tLoss: 0.405126\tLR: 0.00008117\n",
            "Train Epoch: 13 [14336/123872 (12%)]\tLoss: 0.472195\tLR: 0.00008112\n",
            "Train Epoch: 13 [14592/123872 (12%)]\tLoss: 0.359275\tLR: 0.00008108\n",
            "Train Epoch: 13 [14848/123872 (12%)]\tLoss: 0.405920\tLR: 0.00008103\n",
            "Train Epoch: 13 [15104/123872 (12%)]\tLoss: 0.394201\tLR: 0.00008098\n",
            "Train Epoch: 13 [15360/123872 (12%)]\tLoss: 0.369889\tLR: 0.00008094\n",
            "Train Epoch: 13 [15360/123872 (12%)]\tLoss: 0.369889\n",
            "Train Epoch: 13 [15616/123872 (13%)]\tLoss: 0.341157\tLR: 0.00008089\n",
            "Train Epoch: 13 [15872/123872 (13%)]\tLoss: 0.379252\tLR: 0.00008085\n",
            "Train Epoch: 13 [16128/123872 (13%)]\tLoss: 0.381330\tLR: 0.00008080\n",
            "Train Epoch: 13 [16384/123872 (13%)]\tLoss: 0.324497\tLR: 0.00008076\n",
            "Train Epoch: 13 [16640/123872 (13%)]\tLoss: 0.362921\tLR: 0.00008071\n",
            "Train Epoch: 13 [16896/123872 (14%)]\tLoss: 0.414735\tLR: 0.00008067\n",
            "Train Epoch: 13 [17152/123872 (14%)]\tLoss: 0.373654\tLR: 0.00008062\n",
            "Train Epoch: 13 [17408/123872 (14%)]\tLoss: 0.409335\tLR: 0.00008058\n",
            "Train Epoch: 13 [17664/123872 (14%)]\tLoss: 0.450748\tLR: 0.00008053\n",
            "Train Epoch: 13 [17920/123872 (14%)]\tLoss: 0.318162\tLR: 0.00008048\n",
            "Train Epoch: 13 [17920/123872 (14%)]\tLoss: 0.318162\n",
            "Train Epoch: 13 [18176/123872 (15%)]\tLoss: 0.359183\tLR: 0.00008044\n",
            "Train Epoch: 13 [18432/123872 (15%)]\tLoss: 0.365694\tLR: 0.00008039\n",
            "Train Epoch: 13 [18688/123872 (15%)]\tLoss: 0.334613\tLR: 0.00008035\n",
            "Train Epoch: 13 [18944/123872 (15%)]\tLoss: 0.400389\tLR: 0.00008030\n",
            "Train Epoch: 13 [19200/123872 (15%)]\tLoss: 0.381240\tLR: 0.00008026\n",
            "Train Epoch: 13 [19456/123872 (16%)]\tLoss: 0.386345\tLR: 0.00008022\n",
            "Train Epoch: 13 [19712/123872 (16%)]\tLoss: 0.336188\tLR: 0.00008017\n",
            "Train Epoch: 13 [19968/123872 (16%)]\tLoss: 0.361763\tLR: 0.00008013\n",
            "Train Epoch: 13 [20224/123872 (16%)]\tLoss: 0.376908\tLR: 0.00008008\n",
            "Train Epoch: 13 [20480/123872 (17%)]\tLoss: 0.404831\tLR: 0.00008004\n",
            "Train Epoch: 13 [20480/123872 (17%)]\tLoss: 0.404831\n",
            "Train Epoch: 13 [20736/123872 (17%)]\tLoss: 0.413629\tLR: 0.00007999\n",
            "Train Epoch: 13 [20992/123872 (17%)]\tLoss: 0.365436\tLR: 0.00007995\n",
            "Train Epoch: 13 [21248/123872 (17%)]\tLoss: 0.332585\tLR: 0.00007990\n",
            "Train Epoch: 13 [21504/123872 (17%)]\tLoss: 0.400323\tLR: 0.00007986\n",
            "Train Epoch: 13 [21760/123872 (18%)]\tLoss: 0.384637\tLR: 0.00007981\n",
            "Train Epoch: 13 [22016/123872 (18%)]\tLoss: 0.348763\tLR: 0.00007977\n",
            "Train Epoch: 13 [22272/123872 (18%)]\tLoss: 0.285790\tLR: 0.00007973\n",
            "Train Epoch: 13 [22528/123872 (18%)]\tLoss: 0.385945\tLR: 0.00007968\n",
            "Train Epoch: 13 [22784/123872 (18%)]\tLoss: 0.409789\tLR: 0.00007964\n",
            "Train Epoch: 13 [23040/123872 (19%)]\tLoss: 0.434966\tLR: 0.00007959\n",
            "Train Epoch: 13 [23040/123872 (19%)]\tLoss: 0.434966\n",
            "Train Epoch: 13 [23296/123872 (19%)]\tLoss: 0.449416\tLR: 0.00007955\n",
            "Train Epoch: 13 [23552/123872 (19%)]\tLoss: 0.376023\tLR: 0.00007950\n",
            "Train Epoch: 13 [23808/123872 (19%)]\tLoss: 0.376571\tLR: 0.00007946\n",
            "Train Epoch: 13 [24064/123872 (19%)]\tLoss: 0.311392\tLR: 0.00007942\n",
            "Train Epoch: 13 [24320/123872 (20%)]\tLoss: 0.297935\tLR: 0.00007937\n",
            "Train Epoch: 13 [24576/123872 (20%)]\tLoss: 0.350924\tLR: 0.00007933\n",
            "Train Epoch: 13 [24832/123872 (20%)]\tLoss: 0.356416\tLR: 0.00007928\n",
            "Train Epoch: 13 [25088/123872 (20%)]\tLoss: 0.347705\tLR: 0.00007924\n",
            "Train Epoch: 13 [25344/123872 (20%)]\tLoss: 0.316132\tLR: 0.00007920\n",
            "Train Epoch: 13 [25600/123872 (21%)]\tLoss: 0.328136\tLR: 0.00007915\n",
            "Train Epoch: 13 [25600/123872 (21%)]\tLoss: 0.328136\n",
            "Train Epoch: 13 [25856/123872 (21%)]\tLoss: 0.394628\tLR: 0.00007911\n",
            "Train Epoch: 13 [26112/123872 (21%)]\tLoss: 0.373425\tLR: 0.00007907\n",
            "Train Epoch: 13 [26368/123872 (21%)]\tLoss: 0.369442\tLR: 0.00007902\n",
            "Train Epoch: 13 [26624/123872 (21%)]\tLoss: 0.336086\tLR: 0.00007898\n",
            "Train Epoch: 13 [26880/123872 (22%)]\tLoss: 0.416628\tLR: 0.00007894\n",
            "Train Epoch: 13 [27136/123872 (22%)]\tLoss: 0.324775\tLR: 0.00007889\n",
            "Train Epoch: 13 [27392/123872 (22%)]\tLoss: 0.354436\tLR: 0.00007885\n",
            "Train Epoch: 13 [27648/123872 (22%)]\tLoss: 0.432425\tLR: 0.00007881\n",
            "Train Epoch: 13 [27904/123872 (23%)]\tLoss: 0.331514\tLR: 0.00007876\n",
            "Train Epoch: 13 [28160/123872 (23%)]\tLoss: 0.380404\tLR: 0.00007872\n",
            "Train Epoch: 13 [28160/123872 (23%)]\tLoss: 0.380404\n",
            "Train Epoch: 13 [28416/123872 (23%)]\tLoss: 0.387765\tLR: 0.00007868\n",
            "Train Epoch: 13 [28672/123872 (23%)]\tLoss: 0.381630\tLR: 0.00007863\n",
            "Train Epoch: 13 [28928/123872 (23%)]\tLoss: 0.504145\tLR: 0.00007859\n",
            "Train Epoch: 13 [29184/123872 (24%)]\tLoss: 0.376347\tLR: 0.00007855\n",
            "Train Epoch: 13 [29440/123872 (24%)]\tLoss: 0.400131\tLR: 0.00007850\n",
            "Train Epoch: 13 [29696/123872 (24%)]\tLoss: 0.356930\tLR: 0.00007846\n",
            "Train Epoch: 13 [29952/123872 (24%)]\tLoss: 0.335829\tLR: 0.00007842\n",
            "Train Epoch: 13 [30208/123872 (24%)]\tLoss: 0.403461\tLR: 0.00007838\n",
            "Train Epoch: 13 [30464/123872 (25%)]\tLoss: 0.396013\tLR: 0.00007833\n",
            "Train Epoch: 13 [30720/123872 (25%)]\tLoss: 0.346005\tLR: 0.00007829\n",
            "Train Epoch: 13 [30720/123872 (25%)]\tLoss: 0.346005\n",
            "Train Epoch: 13 [30976/123872 (25%)]\tLoss: 0.421939\tLR: 0.00007825\n",
            "Train Epoch: 13 [31232/123872 (25%)]\tLoss: 0.333559\tLR: 0.00007820\n",
            "Train Epoch: 13 [31488/123872 (25%)]\tLoss: 0.357212\tLR: 0.00007816\n",
            "Train Epoch: 13 [31744/123872 (26%)]\tLoss: 0.340349\tLR: 0.00007812\n",
            "Train Epoch: 13 [32000/123872 (26%)]\tLoss: 0.403421\tLR: 0.00007808\n",
            "Train Epoch: 13 [32256/123872 (26%)]\tLoss: 0.308768\tLR: 0.00007803\n",
            "Train Epoch: 13 [32512/123872 (26%)]\tLoss: 0.381022\tLR: 0.00007799\n",
            "Train Epoch: 13 [32768/123872 (26%)]\tLoss: 0.363107\tLR: 0.00007795\n",
            "Train Epoch: 13 [33024/123872 (27%)]\tLoss: 0.344524\tLR: 0.00007791\n",
            "Train Epoch: 13 [33280/123872 (27%)]\tLoss: 0.344274\tLR: 0.00007787\n",
            "Train Epoch: 13 [33280/123872 (27%)]\tLoss: 0.344274\n",
            "Train Epoch: 13 [33536/123872 (27%)]\tLoss: 0.393918\tLR: 0.00007782\n",
            "Train Epoch: 13 [33792/123872 (27%)]\tLoss: 0.374117\tLR: 0.00007778\n",
            "Train Epoch: 13 [34048/123872 (27%)]\tLoss: 0.381512\tLR: 0.00007774\n",
            "Train Epoch: 13 [34304/123872 (28%)]\tLoss: 0.348872\tLR: 0.00007770\n",
            "Train Epoch: 13 [34560/123872 (28%)]\tLoss: 0.352900\tLR: 0.00007766\n",
            "Train Epoch: 13 [34816/123872 (28%)]\tLoss: 0.409656\tLR: 0.00007761\n",
            "Train Epoch: 13 [35072/123872 (28%)]\tLoss: 0.321267\tLR: 0.00007757\n",
            "Train Epoch: 13 [35328/123872 (29%)]\tLoss: 0.352329\tLR: 0.00007753\n",
            "Train Epoch: 13 [35584/123872 (29%)]\tLoss: 0.336198\tLR: 0.00007749\n",
            "Train Epoch: 13 [35840/123872 (29%)]\tLoss: 0.376672\tLR: 0.00007745\n",
            "Train Epoch: 13 [35840/123872 (29%)]\tLoss: 0.376672\n",
            "Train Epoch: 13 [36096/123872 (29%)]\tLoss: 0.351775\tLR: 0.00007740\n",
            "Train Epoch: 13 [36352/123872 (29%)]\tLoss: 0.396045\tLR: 0.00007736\n",
            "Train Epoch: 13 [36608/123872 (30%)]\tLoss: 0.374040\tLR: 0.00007732\n",
            "Train Epoch: 13 [36864/123872 (30%)]\tLoss: 0.391151\tLR: 0.00007728\n",
            "Train Epoch: 13 [37120/123872 (30%)]\tLoss: 0.301872\tLR: 0.00007724\n",
            "Train Epoch: 13 [37376/123872 (30%)]\tLoss: 0.377631\tLR: 0.00007720\n",
            "Train Epoch: 13 [37632/123872 (30%)]\tLoss: 0.357505\tLR: 0.00007716\n",
            "Train Epoch: 13 [37888/123872 (31%)]\tLoss: 0.333437\tLR: 0.00007711\n",
            "Train Epoch: 13 [38144/123872 (31%)]\tLoss: 0.428920\tLR: 0.00007707\n",
            "Train Epoch: 13 [38400/123872 (31%)]\tLoss: 0.365416\tLR: 0.00007703\n",
            "Train Epoch: 13 [38400/123872 (31%)]\tLoss: 0.365416\n",
            "Train Epoch: 13 [38656/123872 (31%)]\tLoss: 0.364948\tLR: 0.00007699\n",
            "Train Epoch: 13 [38912/123872 (31%)]\tLoss: 0.367518\tLR: 0.00007695\n",
            "Train Epoch: 13 [39168/123872 (32%)]\tLoss: 0.388338\tLR: 0.00007691\n",
            "Train Epoch: 13 [39424/123872 (32%)]\tLoss: 0.383669\tLR: 0.00007687\n",
            "Train Epoch: 13 [39680/123872 (32%)]\tLoss: 0.426902\tLR: 0.00007683\n",
            "Train Epoch: 13 [39936/123872 (32%)]\tLoss: 0.431023\tLR: 0.00007678\n",
            "Train Epoch: 13 [40192/123872 (32%)]\tLoss: 0.399969\tLR: 0.00007674\n",
            "Train Epoch: 13 [40448/123872 (33%)]\tLoss: 0.401773\tLR: 0.00007670\n",
            "Train Epoch: 13 [40704/123872 (33%)]\tLoss: 0.360202\tLR: 0.00007666\n",
            "Train Epoch: 13 [40960/123872 (33%)]\tLoss: 0.372208\tLR: 0.00007662\n",
            "Train Epoch: 13 [40960/123872 (33%)]\tLoss: 0.372208\n",
            "Train Epoch: 13 [41216/123872 (33%)]\tLoss: 0.309922\tLR: 0.00007658\n",
            "Train Epoch: 13 [41472/123872 (33%)]\tLoss: 0.352038\tLR: 0.00007654\n",
            "Train Epoch: 13 [41728/123872 (34%)]\tLoss: 0.399605\tLR: 0.00007650\n",
            "Train Epoch: 13 [41984/123872 (34%)]\tLoss: 0.413647\tLR: 0.00007646\n",
            "Train Epoch: 13 [42240/123872 (34%)]\tLoss: 0.364083\tLR: 0.00007642\n",
            "Train Epoch: 13 [42496/123872 (34%)]\tLoss: 0.370327\tLR: 0.00007638\n",
            "Train Epoch: 13 [42752/123872 (35%)]\tLoss: 0.364395\tLR: 0.00007634\n",
            "Train Epoch: 13 [43008/123872 (35%)]\tLoss: 0.353000\tLR: 0.00007630\n",
            "Train Epoch: 13 [43264/123872 (35%)]\tLoss: 0.345640\tLR: 0.00007626\n",
            "Train Epoch: 13 [43520/123872 (35%)]\tLoss: 0.389829\tLR: 0.00007622\n",
            "Train Epoch: 13 [43520/123872 (35%)]\tLoss: 0.389829\n",
            "Train Epoch: 13 [43776/123872 (35%)]\tLoss: 0.319119\tLR: 0.00007618\n",
            "Train Epoch: 13 [44032/123872 (36%)]\tLoss: 0.379277\tLR: 0.00007614\n",
            "Train Epoch: 13 [44288/123872 (36%)]\tLoss: 0.378226\tLR: 0.00007610\n",
            "Train Epoch: 13 [44544/123872 (36%)]\tLoss: 0.377627\tLR: 0.00007606\n",
            "Train Epoch: 13 [44800/123872 (36%)]\tLoss: 0.379115\tLR: 0.00007601\n",
            "Train Epoch: 13 [45056/123872 (36%)]\tLoss: 0.372670\tLR: 0.00007597\n",
            "Train Epoch: 13 [45312/123872 (37%)]\tLoss: 0.338983\tLR: 0.00007594\n",
            "Train Epoch: 13 [45568/123872 (37%)]\tLoss: 0.310944\tLR: 0.00007590\n",
            "Train Epoch: 13 [45824/123872 (37%)]\tLoss: 0.360831\tLR: 0.00007586\n",
            "Train Epoch: 13 [46080/123872 (37%)]\tLoss: 0.378627\tLR: 0.00007582\n",
            "Train Epoch: 13 [46080/123872 (37%)]\tLoss: 0.378627\n",
            "Train Epoch: 13 [46336/123872 (37%)]\tLoss: 0.368990\tLR: 0.00007578\n",
            "Train Epoch: 13 [46592/123872 (38%)]\tLoss: 0.329607\tLR: 0.00007574\n",
            "Train Epoch: 13 [46848/123872 (38%)]\tLoss: 0.366772\tLR: 0.00007570\n",
            "Train Epoch: 13 [47104/123872 (38%)]\tLoss: 0.351498\tLR: 0.00007566\n",
            "Train Epoch: 13 [47360/123872 (38%)]\tLoss: 0.385187\tLR: 0.00007562\n",
            "Train Epoch: 13 [47616/123872 (38%)]\tLoss: 0.366242\tLR: 0.00007558\n",
            "Train Epoch: 13 [47872/123872 (39%)]\tLoss: 0.421597\tLR: 0.00007554\n",
            "Train Epoch: 13 [48128/123872 (39%)]\tLoss: 0.425798\tLR: 0.00007550\n",
            "Train Epoch: 13 [48384/123872 (39%)]\tLoss: 0.373805\tLR: 0.00007546\n",
            "Train Epoch: 13 [48640/123872 (39%)]\tLoss: 0.372666\tLR: 0.00007542\n",
            "Train Epoch: 13 [48640/123872 (39%)]\tLoss: 0.372666\n",
            "Train Epoch: 13 [48896/123872 (39%)]\tLoss: 0.332667\tLR: 0.00007538\n",
            "Train Epoch: 13 [49152/123872 (40%)]\tLoss: 0.335867\tLR: 0.00007534\n",
            "Train Epoch: 13 [49408/123872 (40%)]\tLoss: 0.370048\tLR: 0.00007530\n",
            "Train Epoch: 13 [49664/123872 (40%)]\tLoss: 0.373629\tLR: 0.00007526\n",
            "Train Epoch: 13 [49920/123872 (40%)]\tLoss: 0.377497\tLR: 0.00007522\n",
            "Train Epoch: 13 [50176/123872 (40%)]\tLoss: 0.343101\tLR: 0.00007518\n",
            "Train Epoch: 13 [50432/123872 (41%)]\tLoss: 0.367822\tLR: 0.00007515\n",
            "Train Epoch: 13 [50688/123872 (41%)]\tLoss: 0.403224\tLR: 0.00007511\n",
            "Train Epoch: 13 [50944/123872 (41%)]\tLoss: 0.305705\tLR: 0.00007507\n",
            "Train Epoch: 13 [51200/123872 (41%)]\tLoss: 0.387050\tLR: 0.00007503\n",
            "Train Epoch: 13 [51200/123872 (41%)]\tLoss: 0.387050\n",
            "Train Epoch: 13 [51456/123872 (42%)]\tLoss: 0.346465\tLR: 0.00007499\n",
            "Train Epoch: 13 [51712/123872 (42%)]\tLoss: 0.402005\tLR: 0.00007495\n",
            "Train Epoch: 13 [51968/123872 (42%)]\tLoss: 0.416407\tLR: 0.00007491\n",
            "Train Epoch: 13 [52224/123872 (42%)]\tLoss: 0.389118\tLR: 0.00007487\n",
            "Train Epoch: 13 [52480/123872 (42%)]\tLoss: 0.391827\tLR: 0.00007484\n",
            "Train Epoch: 13 [52736/123872 (43%)]\tLoss: 0.393388\tLR: 0.00007480\n",
            "Train Epoch: 13 [52992/123872 (43%)]\tLoss: 0.351123\tLR: 0.00007476\n",
            "Train Epoch: 13 [53248/123872 (43%)]\tLoss: 0.365015\tLR: 0.00007472\n",
            "Train Epoch: 13 [53504/123872 (43%)]\tLoss: 0.352626\tLR: 0.00007468\n",
            "Train Epoch: 13 [53760/123872 (43%)]\tLoss: 0.390432\tLR: 0.00007464\n",
            "Train Epoch: 13 [53760/123872 (43%)]\tLoss: 0.390432\n",
            "Train Epoch: 13 [54016/123872 (44%)]\tLoss: 0.373719\tLR: 0.00007461\n",
            "Train Epoch: 13 [54272/123872 (44%)]\tLoss: 0.383106\tLR: 0.00007457\n",
            "Train Epoch: 13 [54528/123872 (44%)]\tLoss: 0.413804\tLR: 0.00007453\n",
            "Train Epoch: 13 [54784/123872 (44%)]\tLoss: 0.369675\tLR: 0.00007449\n",
            "Train Epoch: 13 [55040/123872 (44%)]\tLoss: 0.371645\tLR: 0.00007445\n",
            "Train Epoch: 13 [55296/123872 (45%)]\tLoss: 0.396422\tLR: 0.00007441\n",
            "Train Epoch: 13 [55552/123872 (45%)]\tLoss: 0.367526\tLR: 0.00007438\n",
            "Train Epoch: 13 [55808/123872 (45%)]\tLoss: 0.335597\tLR: 0.00007434\n",
            "Train Epoch: 13 [56064/123872 (45%)]\tLoss: 0.288028\tLR: 0.00007430\n",
            "Train Epoch: 13 [56320/123872 (45%)]\tLoss: 0.343189\tLR: 0.00007426\n",
            "Train Epoch: 13 [56320/123872 (45%)]\tLoss: 0.343189\n",
            "Train Epoch: 13 [56576/123872 (46%)]\tLoss: 0.344839\tLR: 0.00007422\n",
            "Train Epoch: 13 [56832/123872 (46%)]\tLoss: 0.327489\tLR: 0.00007419\n",
            "Train Epoch: 13 [57088/123872 (46%)]\tLoss: 0.368989\tLR: 0.00007415\n",
            "Train Epoch: 13 [57344/123872 (46%)]\tLoss: 0.432575\tLR: 0.00007411\n",
            "Train Epoch: 13 [57600/123872 (46%)]\tLoss: 0.392701\tLR: 0.00007407\n",
            "Train Epoch: 13 [57856/123872 (47%)]\tLoss: 0.400711\tLR: 0.00007404\n",
            "Train Epoch: 13 [58112/123872 (47%)]\tLoss: 0.367518\tLR: 0.00007400\n",
            "Train Epoch: 13 [58368/123872 (47%)]\tLoss: 0.423621\tLR: 0.00007396\n",
            "Train Epoch: 13 [58624/123872 (47%)]\tLoss: 0.309131\tLR: 0.00007392\n",
            "Train Epoch: 13 [58880/123872 (48%)]\tLoss: 0.376787\tLR: 0.00007389\n",
            "Train Epoch: 13 [58880/123872 (48%)]\tLoss: 0.376787\n",
            "Train Epoch: 13 [59136/123872 (48%)]\tLoss: 0.404054\tLR: 0.00007385\n",
            "Train Epoch: 13 [59392/123872 (48%)]\tLoss: 0.368381\tLR: 0.00007381\n",
            "Train Epoch: 13 [59648/123872 (48%)]\tLoss: 0.383977\tLR: 0.00007377\n",
            "Train Epoch: 13 [59904/123872 (48%)]\tLoss: 0.399172\tLR: 0.00007374\n",
            "Train Epoch: 13 [60160/123872 (49%)]\tLoss: 0.339948\tLR: 0.00007370\n",
            "Train Epoch: 13 [60416/123872 (49%)]\tLoss: 0.407735\tLR: 0.00007366\n",
            "Train Epoch: 13 [60672/123872 (49%)]\tLoss: 0.384730\tLR: 0.00007363\n",
            "Train Epoch: 13 [60928/123872 (49%)]\tLoss: 0.400519\tLR: 0.00007359\n",
            "Train Epoch: 13 [61184/123872 (49%)]\tLoss: 0.387517\tLR: 0.00007355\n",
            "Train Epoch: 13 [61440/123872 (50%)]\tLoss: 0.339477\tLR: 0.00007352\n",
            "Train Epoch: 13 [61440/123872 (50%)]\tLoss: 0.339477\n",
            "Train Epoch: 13 [61696/123872 (50%)]\tLoss: 0.342731\tLR: 0.00007348\n",
            "Train Epoch: 13 [61952/123872 (50%)]\tLoss: 0.368633\tLR: 0.00007344\n",
            "Train Epoch: 13 [62208/123872 (50%)]\tLoss: 0.336641\tLR: 0.00007341\n",
            "Train Epoch: 13 [62464/123872 (50%)]\tLoss: 0.346058\tLR: 0.00007337\n",
            "Train Epoch: 13 [62720/123872 (51%)]\tLoss: 0.351968\tLR: 0.00007333\n",
            "Train Epoch: 13 [62976/123872 (51%)]\tLoss: 0.315344\tLR: 0.00007330\n",
            "Train Epoch: 13 [63232/123872 (51%)]\tLoss: 0.363178\tLR: 0.00007326\n",
            "Train Epoch: 13 [63488/123872 (51%)]\tLoss: 0.393643\tLR: 0.00007322\n",
            "Train Epoch: 13 [63744/123872 (51%)]\tLoss: 0.327176\tLR: 0.00007319\n",
            "Train Epoch: 13 [64000/123872 (52%)]\tLoss: 0.403205\tLR: 0.00007315\n",
            "Train Epoch: 13 [64000/123872 (52%)]\tLoss: 0.403205\n",
            "Train Epoch: 13 [64256/123872 (52%)]\tLoss: 0.294393\tLR: 0.00007311\n",
            "Train Epoch: 13 [64512/123872 (52%)]\tLoss: 0.398618\tLR: 0.00007308\n",
            "Train Epoch: 13 [64768/123872 (52%)]\tLoss: 0.420749\tLR: 0.00007304\n",
            "Train Epoch: 13 [65024/123872 (52%)]\tLoss: 0.334299\tLR: 0.00007300\n",
            "Train Epoch: 13 [65280/123872 (53%)]\tLoss: 0.387492\tLR: 0.00007297\n",
            "Train Epoch: 13 [65536/123872 (53%)]\tLoss: 0.305439\tLR: 0.00007293\n",
            "Train Epoch: 13 [65792/123872 (53%)]\tLoss: 0.374150\tLR: 0.00007290\n",
            "Train Epoch: 13 [66048/123872 (53%)]\tLoss: 0.389479\tLR: 0.00007286\n",
            "Train Epoch: 13 [66304/123872 (54%)]\tLoss: 0.378323\tLR: 0.00007282\n",
            "Train Epoch: 13 [66560/123872 (54%)]\tLoss: 0.381833\tLR: 0.00007279\n",
            "Train Epoch: 13 [66560/123872 (54%)]\tLoss: 0.381833\n",
            "Train Epoch: 13 [66816/123872 (54%)]\tLoss: 0.407422\tLR: 0.00007275\n",
            "Train Epoch: 13 [67072/123872 (54%)]\tLoss: 0.435444\tLR: 0.00007272\n",
            "Train Epoch: 13 [67328/123872 (54%)]\tLoss: 0.366491\tLR: 0.00007268\n",
            "Train Epoch: 13 [67584/123872 (55%)]\tLoss: 0.394157\tLR: 0.00007264\n",
            "Train Epoch: 13 [67840/123872 (55%)]\tLoss: 0.332718\tLR: 0.00007261\n",
            "Train Epoch: 13 [68096/123872 (55%)]\tLoss: 0.406169\tLR: 0.00007257\n",
            "Train Epoch: 13 [68352/123872 (55%)]\tLoss: 0.348088\tLR: 0.00007254\n",
            "Train Epoch: 13 [68608/123872 (55%)]\tLoss: 0.407749\tLR: 0.00007250\n",
            "Train Epoch: 13 [68864/123872 (56%)]\tLoss: 0.347531\tLR: 0.00007247\n",
            "Train Epoch: 13 [69120/123872 (56%)]\tLoss: 0.363311\tLR: 0.00007243\n",
            "Train Epoch: 13 [69120/123872 (56%)]\tLoss: 0.363311\n",
            "Train Epoch: 13 [69376/123872 (56%)]\tLoss: 0.297555\tLR: 0.00007240\n",
            "Train Epoch: 13 [69632/123872 (56%)]\tLoss: 0.447136\tLR: 0.00007236\n",
            "Train Epoch: 13 [69888/123872 (56%)]\tLoss: 0.351484\tLR: 0.00007233\n",
            "Train Epoch: 13 [70144/123872 (57%)]\tLoss: 0.325889\tLR: 0.00007229\n",
            "Train Epoch: 13 [70400/123872 (57%)]\tLoss: 0.403093\tLR: 0.00007226\n",
            "Train Epoch: 13 [70656/123872 (57%)]\tLoss: 0.414740\tLR: 0.00007222\n",
            "Train Epoch: 13 [70912/123872 (57%)]\tLoss: 0.358587\tLR: 0.00007218\n",
            "Train Epoch: 13 [71168/123872 (57%)]\tLoss: 0.446726\tLR: 0.00007215\n",
            "Train Epoch: 13 [71424/123872 (58%)]\tLoss: 0.356817\tLR: 0.00007211\n",
            "Train Epoch: 13 [71680/123872 (58%)]\tLoss: 0.409777\tLR: 0.00007208\n",
            "Train Epoch: 13 [71680/123872 (58%)]\tLoss: 0.409777\n",
            "Train Epoch: 13 [71936/123872 (58%)]\tLoss: 0.334419\tLR: 0.00007205\n",
            "Train Epoch: 13 [72192/123872 (58%)]\tLoss: 0.397011\tLR: 0.00007201\n",
            "Train Epoch: 13 [72448/123872 (58%)]\tLoss: 0.359949\tLR: 0.00007198\n",
            "Train Epoch: 13 [72704/123872 (59%)]\tLoss: 0.396367\tLR: 0.00007194\n",
            "Train Epoch: 13 [72960/123872 (59%)]\tLoss: 0.424152\tLR: 0.00007191\n",
            "Train Epoch: 13 [73216/123872 (59%)]\tLoss: 0.370026\tLR: 0.00007187\n",
            "Train Epoch: 13 [73472/123872 (59%)]\tLoss: 0.380348\tLR: 0.00007184\n",
            "Train Epoch: 13 [73728/123872 (60%)]\tLoss: 0.352943\tLR: 0.00007180\n",
            "Train Epoch: 13 [73984/123872 (60%)]\tLoss: 0.388746\tLR: 0.00007177\n",
            "Train Epoch: 13 [74240/123872 (60%)]\tLoss: 0.384826\tLR: 0.00007173\n",
            "Train Epoch: 13 [74240/123872 (60%)]\tLoss: 0.384826\n",
            "Train Epoch: 13 [74496/123872 (60%)]\tLoss: 0.355791\tLR: 0.00007170\n",
            "Train Epoch: 13 [74752/123872 (60%)]\tLoss: 0.381661\tLR: 0.00007166\n",
            "Train Epoch: 13 [75008/123872 (61%)]\tLoss: 0.357473\tLR: 0.00007163\n",
            "Train Epoch: 13 [75264/123872 (61%)]\tLoss: 0.388142\tLR: 0.00007160\n",
            "Train Epoch: 13 [75520/123872 (61%)]\tLoss: 0.384649\tLR: 0.00007156\n",
            "Train Epoch: 13 [75776/123872 (61%)]\tLoss: 0.327545\tLR: 0.00007153\n",
            "Train Epoch: 13 [76032/123872 (61%)]\tLoss: 0.355844\tLR: 0.00007149\n",
            "Train Epoch: 13 [76288/123872 (62%)]\tLoss: 0.379809\tLR: 0.00007146\n",
            "Train Epoch: 13 [76544/123872 (62%)]\tLoss: 0.391459\tLR: 0.00007143\n",
            "Train Epoch: 13 [76800/123872 (62%)]\tLoss: 0.379415\tLR: 0.00007139\n",
            "Train Epoch: 13 [76800/123872 (62%)]\tLoss: 0.379415\n",
            "Train Epoch: 13 [77056/123872 (62%)]\tLoss: 0.391717\tLR: 0.00007136\n",
            "Train Epoch: 13 [77312/123872 (62%)]\tLoss: 0.373295\tLR: 0.00007132\n",
            "Train Epoch: 13 [77568/123872 (63%)]\tLoss: 0.365313\tLR: 0.00007129\n",
            "Train Epoch: 13 [77824/123872 (63%)]\tLoss: 0.347332\tLR: 0.00007126\n",
            "Train Epoch: 13 [78080/123872 (63%)]\tLoss: 0.322447\tLR: 0.00007122\n",
            "Train Epoch: 13 [78336/123872 (63%)]\tLoss: 0.363997\tLR: 0.00007119\n",
            "Train Epoch: 13 [78592/123872 (63%)]\tLoss: 0.409770\tLR: 0.00007116\n",
            "Train Epoch: 13 [78848/123872 (64%)]\tLoss: 0.374472\tLR: 0.00007112\n",
            "Train Epoch: 13 [79104/123872 (64%)]\tLoss: 0.380093\tLR: 0.00007109\n",
            "Train Epoch: 13 [79360/123872 (64%)]\tLoss: 0.320067\tLR: 0.00007106\n",
            "Train Epoch: 13 [79360/123872 (64%)]\tLoss: 0.320067\n",
            "Train Epoch: 13 [79616/123872 (64%)]\tLoss: 0.366168\tLR: 0.00007102\n",
            "Train Epoch: 13 [79872/123872 (64%)]\tLoss: 0.438889\tLR: 0.00007099\n",
            "Train Epoch: 13 [80128/123872 (65%)]\tLoss: 0.366345\tLR: 0.00007096\n",
            "Train Epoch: 13 [80384/123872 (65%)]\tLoss: 0.404958\tLR: 0.00007092\n",
            "Train Epoch: 13 [80640/123872 (65%)]\tLoss: 0.348919\tLR: 0.00007089\n",
            "Train Epoch: 13 [80896/123872 (65%)]\tLoss: 0.382000\tLR: 0.00007086\n",
            "Train Epoch: 13 [81152/123872 (65%)]\tLoss: 0.366782\tLR: 0.00007082\n",
            "Train Epoch: 13 [81408/123872 (66%)]\tLoss: 0.354136\tLR: 0.00007079\n",
            "Train Epoch: 13 [81664/123872 (66%)]\tLoss: 0.352098\tLR: 0.00007076\n",
            "Train Epoch: 13 [81920/123872 (66%)]\tLoss: 0.382684\tLR: 0.00007072\n",
            "Train Epoch: 13 [81920/123872 (66%)]\tLoss: 0.382684\n",
            "Train Epoch: 13 [82176/123872 (66%)]\tLoss: 0.328495\tLR: 0.00007069\n",
            "Train Epoch: 13 [82432/123872 (67%)]\tLoss: 0.354573\tLR: 0.00007066\n",
            "Train Epoch: 13 [82688/123872 (67%)]\tLoss: 0.388421\tLR: 0.00007062\n",
            "Train Epoch: 13 [82944/123872 (67%)]\tLoss: 0.326025\tLR: 0.00007059\n",
            "Train Epoch: 13 [83200/123872 (67%)]\tLoss: 0.333480\tLR: 0.00007056\n",
            "Train Epoch: 13 [83456/123872 (67%)]\tLoss: 0.373849\tLR: 0.00007053\n",
            "Train Epoch: 13 [83712/123872 (68%)]\tLoss: 0.331174\tLR: 0.00007049\n",
            "Train Epoch: 13 [83968/123872 (68%)]\tLoss: 0.354518\tLR: 0.00007046\n",
            "Train Epoch: 13 [84224/123872 (68%)]\tLoss: 0.353479\tLR: 0.00007043\n",
            "Train Epoch: 13 [84480/123872 (68%)]\tLoss: 0.329309\tLR: 0.00007040\n",
            "Train Epoch: 13 [84480/123872 (68%)]\tLoss: 0.329309\n",
            "Train Epoch: 13 [84736/123872 (68%)]\tLoss: 0.374281\tLR: 0.00007036\n",
            "Train Epoch: 13 [84992/123872 (69%)]\tLoss: 0.403555\tLR: 0.00007033\n",
            "Train Epoch: 13 [85248/123872 (69%)]\tLoss: 0.430208\tLR: 0.00007030\n",
            "Train Epoch: 13 [85504/123872 (69%)]\tLoss: 0.353604\tLR: 0.00007027\n",
            "Train Epoch: 13 [85760/123872 (69%)]\tLoss: 0.404616\tLR: 0.00007024\n",
            "Train Epoch: 13 [86016/123872 (69%)]\tLoss: 0.336598\tLR: 0.00007020\n",
            "Train Epoch: 13 [86272/123872 (70%)]\tLoss: 0.388057\tLR: 0.00007017\n",
            "Train Epoch: 13 [86528/123872 (70%)]\tLoss: 0.377313\tLR: 0.00007014\n",
            "Train Epoch: 13 [86784/123872 (70%)]\tLoss: 0.391563\tLR: 0.00007011\n",
            "Train Epoch: 13 [87040/123872 (70%)]\tLoss: 0.308049\tLR: 0.00007007\n",
            "Train Epoch: 13 [87040/123872 (70%)]\tLoss: 0.308049\n",
            "Train Epoch: 13 [87296/123872 (70%)]\tLoss: 0.379029\tLR: 0.00007004\n",
            "Train Epoch: 13 [87552/123872 (71%)]\tLoss: 0.316864\tLR: 0.00007001\n",
            "Train Epoch: 13 [87808/123872 (71%)]\tLoss: 0.371160\tLR: 0.00006998\n",
            "Train Epoch: 13 [88064/123872 (71%)]\tLoss: 0.300276\tLR: 0.00006995\n",
            "Train Epoch: 13 [88320/123872 (71%)]\tLoss: 0.384575\tLR: 0.00006992\n",
            "Train Epoch: 13 [88576/123872 (71%)]\tLoss: 0.362222\tLR: 0.00006988\n",
            "Train Epoch: 13 [88832/123872 (72%)]\tLoss: 0.386169\tLR: 0.00006985\n",
            "Train Epoch: 13 [89088/123872 (72%)]\tLoss: 0.408813\tLR: 0.00006982\n",
            "Train Epoch: 13 [89344/123872 (72%)]\tLoss: 0.406930\tLR: 0.00006979\n",
            "Train Epoch: 13 [89600/123872 (72%)]\tLoss: 0.326767\tLR: 0.00006976\n",
            "Train Epoch: 13 [89600/123872 (72%)]\tLoss: 0.326767\n",
            "Train Epoch: 13 [89856/123872 (73%)]\tLoss: 0.316056\tLR: 0.00006973\n",
            "Train Epoch: 13 [90112/123872 (73%)]\tLoss: 0.423454\tLR: 0.00006970\n",
            "Train Epoch: 13 [90368/123872 (73%)]\tLoss: 0.410306\tLR: 0.00006966\n",
            "Train Epoch: 13 [90624/123872 (73%)]\tLoss: 0.420106\tLR: 0.00006963\n",
            "Train Epoch: 13 [90880/123872 (73%)]\tLoss: 0.395723\tLR: 0.00006960\n",
            "Train Epoch: 13 [91136/123872 (74%)]\tLoss: 0.413738\tLR: 0.00006957\n",
            "Train Epoch: 13 [91392/123872 (74%)]\tLoss: 0.383169\tLR: 0.00006954\n",
            "Train Epoch: 13 [91648/123872 (74%)]\tLoss: 0.381285\tLR: 0.00006951\n",
            "Train Epoch: 13 [91904/123872 (74%)]\tLoss: 0.371513\tLR: 0.00006948\n",
            "Train Epoch: 13 [92160/123872 (74%)]\tLoss: 0.346081\tLR: 0.00006945\n",
            "Train Epoch: 13 [92160/123872 (74%)]\tLoss: 0.346081\n",
            "Train Epoch: 13 [92416/123872 (75%)]\tLoss: 0.390251\tLR: 0.00006942\n",
            "Train Epoch: 13 [92672/123872 (75%)]\tLoss: 0.362828\tLR: 0.00006938\n",
            "Train Epoch: 13 [92928/123872 (75%)]\tLoss: 0.415126\tLR: 0.00006935\n",
            "Train Epoch: 13 [93184/123872 (75%)]\tLoss: 0.429650\tLR: 0.00006932\n",
            "Train Epoch: 13 [93440/123872 (75%)]\tLoss: 0.337686\tLR: 0.00006929\n",
            "Train Epoch: 13 [93696/123872 (76%)]\tLoss: 0.395801\tLR: 0.00006926\n",
            "Train Epoch: 13 [93952/123872 (76%)]\tLoss: 0.388818\tLR: 0.00006923\n",
            "Train Epoch: 13 [94208/123872 (76%)]\tLoss: 0.409480\tLR: 0.00006920\n",
            "Train Epoch: 13 [94464/123872 (76%)]\tLoss: 0.351115\tLR: 0.00006917\n",
            "Train Epoch: 13 [94720/123872 (76%)]\tLoss: 0.356604\tLR: 0.00006914\n",
            "Train Epoch: 13 [94720/123872 (76%)]\tLoss: 0.356604\n",
            "Train Epoch: 13 [94976/123872 (77%)]\tLoss: 0.390289\tLR: 0.00006911\n",
            "Train Epoch: 13 [95232/123872 (77%)]\tLoss: 0.364456\tLR: 0.00006908\n",
            "Train Epoch: 13 [95488/123872 (77%)]\tLoss: 0.353555\tLR: 0.00006905\n",
            "Train Epoch: 13 [95744/123872 (77%)]\tLoss: 0.392160\tLR: 0.00006902\n",
            "Train Epoch: 13 [96000/123872 (77%)]\tLoss: 0.361515\tLR: 0.00006899\n",
            "Train Epoch: 13 [96256/123872 (78%)]\tLoss: 0.413622\tLR: 0.00006896\n",
            "Train Epoch: 13 [96512/123872 (78%)]\tLoss: 0.322364\tLR: 0.00006893\n",
            "Train Epoch: 13 [96768/123872 (78%)]\tLoss: 0.312121\tLR: 0.00006890\n",
            "Train Epoch: 13 [97024/123872 (78%)]\tLoss: 0.403829\tLR: 0.00006887\n",
            "Train Epoch: 13 [97280/123872 (79%)]\tLoss: 0.345594\tLR: 0.00006884\n",
            "Train Epoch: 13 [97280/123872 (79%)]\tLoss: 0.345594\n",
            "Train Epoch: 13 [97536/123872 (79%)]\tLoss: 0.411146\tLR: 0.00006881\n",
            "Train Epoch: 13 [97792/123872 (79%)]\tLoss: 0.330548\tLR: 0.00006878\n",
            "Train Epoch: 13 [98048/123872 (79%)]\tLoss: 0.400883\tLR: 0.00006875\n",
            "Train Epoch: 13 [98304/123872 (79%)]\tLoss: 0.326802\tLR: 0.00006872\n",
            "Train Epoch: 13 [98560/123872 (80%)]\tLoss: 0.367009\tLR: 0.00006869\n",
            "Train Epoch: 13 [98816/123872 (80%)]\tLoss: 0.407453\tLR: 0.00006866\n",
            "Train Epoch: 13 [99072/123872 (80%)]\tLoss: 0.352902\tLR: 0.00006863\n",
            "Train Epoch: 13 [99328/123872 (80%)]\tLoss: 0.310198\tLR: 0.00006860\n",
            "Train Epoch: 13 [99584/123872 (80%)]\tLoss: 0.335310\tLR: 0.00006857\n",
            "Train Epoch: 13 [99840/123872 (81%)]\tLoss: 0.343399\tLR: 0.00006854\n",
            "Train Epoch: 13 [99840/123872 (81%)]\tLoss: 0.343399\n",
            "Train Epoch: 13 [100096/123872 (81%)]\tLoss: 0.380014\tLR: 0.00006851\n",
            "Train Epoch: 13 [100352/123872 (81%)]\tLoss: 0.340111\tLR: 0.00006848\n",
            "Train Epoch: 13 [100608/123872 (81%)]\tLoss: 0.350474\tLR: 0.00006845\n",
            "Train Epoch: 13 [100864/123872 (81%)]\tLoss: 0.316848\tLR: 0.00006842\n",
            "Train Epoch: 13 [101120/123872 (82%)]\tLoss: 0.408032\tLR: 0.00006839\n",
            "Train Epoch: 13 [101376/123872 (82%)]\tLoss: 0.417061\tLR: 0.00006836\n",
            "Train Epoch: 13 [101632/123872 (82%)]\tLoss: 0.373530\tLR: 0.00006834\n",
            "Train Epoch: 13 [101888/123872 (82%)]\tLoss: 0.351163\tLR: 0.00006831\n",
            "Train Epoch: 13 [102144/123872 (82%)]\tLoss: 0.370846\tLR: 0.00006828\n",
            "Train Epoch: 13 [102400/123872 (83%)]\tLoss: 0.430241\tLR: 0.00006825\n",
            "Train Epoch: 13 [102400/123872 (83%)]\tLoss: 0.430241\n",
            "Train Epoch: 13 [102656/123872 (83%)]\tLoss: 0.345147\tLR: 0.00006822\n",
            "Train Epoch: 13 [102912/123872 (83%)]\tLoss: 0.366808\tLR: 0.00006819\n",
            "Train Epoch: 13 [103168/123872 (83%)]\tLoss: 0.372848\tLR: 0.00006816\n",
            "Train Epoch: 13 [103424/123872 (83%)]\tLoss: 0.356265\tLR: 0.00006813\n",
            "Train Epoch: 13 [103680/123872 (84%)]\tLoss: 0.377039\tLR: 0.00006810\n",
            "Train Epoch: 13 [103936/123872 (84%)]\tLoss: 0.380873\tLR: 0.00006808\n",
            "Train Epoch: 13 [104192/123872 (84%)]\tLoss: 0.374003\tLR: 0.00006805\n",
            "Train Epoch: 13 [104448/123872 (84%)]\tLoss: 0.323740\tLR: 0.00006802\n",
            "Train Epoch: 13 [104704/123872 (85%)]\tLoss: 0.349662\tLR: 0.00006799\n",
            "Train Epoch: 13 [104960/123872 (85%)]\tLoss: 0.348586\tLR: 0.00006796\n",
            "Train Epoch: 13 [104960/123872 (85%)]\tLoss: 0.348586\n",
            "Train Epoch: 13 [105216/123872 (85%)]\tLoss: 0.338543\tLR: 0.00006793\n",
            "Train Epoch: 13 [105472/123872 (85%)]\tLoss: 0.412333\tLR: 0.00006790\n",
            "Train Epoch: 13 [105728/123872 (85%)]\tLoss: 0.359558\tLR: 0.00006788\n",
            "Train Epoch: 13 [105984/123872 (86%)]\tLoss: 0.374548\tLR: 0.00006785\n",
            "Train Epoch: 13 [106240/123872 (86%)]\tLoss: 0.334372\tLR: 0.00006782\n",
            "Train Epoch: 13 [106496/123872 (86%)]\tLoss: 0.407489\tLR: 0.00006779\n",
            "Train Epoch: 13 [106752/123872 (86%)]\tLoss: 0.432927\tLR: 0.00006776\n",
            "Train Epoch: 13 [107008/123872 (86%)]\tLoss: 0.322790\tLR: 0.00006774\n",
            "Train Epoch: 13 [107264/123872 (87%)]\tLoss: 0.408613\tLR: 0.00006771\n",
            "Train Epoch: 13 [107520/123872 (87%)]\tLoss: 0.303442\tLR: 0.00006768\n",
            "Train Epoch: 13 [107520/123872 (87%)]\tLoss: 0.303442\n",
            "Train Epoch: 13 [107776/123872 (87%)]\tLoss: 0.386833\tLR: 0.00006765\n",
            "Train Epoch: 13 [108032/123872 (87%)]\tLoss: 0.358570\tLR: 0.00006762\n",
            "Train Epoch: 13 [108288/123872 (87%)]\tLoss: 0.365065\tLR: 0.00006760\n",
            "Train Epoch: 13 [108544/123872 (88%)]\tLoss: 0.332810\tLR: 0.00006757\n",
            "Train Epoch: 13 [108800/123872 (88%)]\tLoss: 0.342627\tLR: 0.00006754\n",
            "Train Epoch: 13 [109056/123872 (88%)]\tLoss: 0.427318\tLR: 0.00006751\n",
            "Train Epoch: 13 [109312/123872 (88%)]\tLoss: 0.364687\tLR: 0.00006748\n",
            "Train Epoch: 13 [109568/123872 (88%)]\tLoss: 0.389936\tLR: 0.00006746\n",
            "Train Epoch: 13 [109824/123872 (89%)]\tLoss: 0.358894\tLR: 0.00006743\n",
            "Train Epoch: 13 [110080/123872 (89%)]\tLoss: 0.355444\tLR: 0.00006740\n",
            "Train Epoch: 13 [110080/123872 (89%)]\tLoss: 0.355444\n",
            "Train Epoch: 13 [110336/123872 (89%)]\tLoss: 0.355164\tLR: 0.00006737\n",
            "Train Epoch: 13 [110592/123872 (89%)]\tLoss: 0.395978\tLR: 0.00006735\n",
            "Train Epoch: 13 [110848/123872 (89%)]\tLoss: 0.332412\tLR: 0.00006732\n",
            "Train Epoch: 13 [111104/123872 (90%)]\tLoss: 0.384208\tLR: 0.00006729\n",
            "Train Epoch: 13 [111360/123872 (90%)]\tLoss: 0.409524\tLR: 0.00006727\n",
            "Train Epoch: 13 [111616/123872 (90%)]\tLoss: 0.390285\tLR: 0.00006724\n",
            "Train Epoch: 13 [111872/123872 (90%)]\tLoss: 0.398321\tLR: 0.00006721\n",
            "Train Epoch: 13 [112128/123872 (90%)]\tLoss: 0.354981\tLR: 0.00006718\n",
            "Train Epoch: 13 [112384/123872 (91%)]\tLoss: 0.310133\tLR: 0.00006716\n",
            "Train Epoch: 13 [112640/123872 (91%)]\tLoss: 0.372338\tLR: 0.00006713\n",
            "Train Epoch: 13 [112640/123872 (91%)]\tLoss: 0.372338\n",
            "Train Epoch: 13 [112896/123872 (91%)]\tLoss: 0.359329\tLR: 0.00006710\n",
            "Train Epoch: 13 [113152/123872 (91%)]\tLoss: 0.379940\tLR: 0.00006708\n",
            "Train Epoch: 13 [113408/123872 (92%)]\tLoss: 0.350207\tLR: 0.00006705\n",
            "Train Epoch: 13 [113664/123872 (92%)]\tLoss: 0.340696\tLR: 0.00006702\n",
            "Train Epoch: 13 [113920/123872 (92%)]\tLoss: 0.394930\tLR: 0.00006700\n",
            "Train Epoch: 13 [114176/123872 (92%)]\tLoss: 0.358773\tLR: 0.00006697\n",
            "Train Epoch: 13 [114432/123872 (92%)]\tLoss: 0.366115\tLR: 0.00006694\n",
            "Train Epoch: 13 [114688/123872 (93%)]\tLoss: 0.333231\tLR: 0.00006692\n",
            "Train Epoch: 13 [114944/123872 (93%)]\tLoss: 0.329891\tLR: 0.00006689\n",
            "Train Epoch: 13 [115200/123872 (93%)]\tLoss: 0.369056\tLR: 0.00006686\n",
            "Train Epoch: 13 [115200/123872 (93%)]\tLoss: 0.369056\n",
            "Train Epoch: 13 [115456/123872 (93%)]\tLoss: 0.374479\tLR: 0.00006684\n",
            "Train Epoch: 13 [115712/123872 (93%)]\tLoss: 0.316168\tLR: 0.00006681\n",
            "Train Epoch: 13 [115968/123872 (94%)]\tLoss: 0.389855\tLR: 0.00006678\n",
            "Train Epoch: 13 [116224/123872 (94%)]\tLoss: 0.447710\tLR: 0.00006676\n",
            "Train Epoch: 13 [116480/123872 (94%)]\tLoss: 0.409596\tLR: 0.00006673\n",
            "Train Epoch: 13 [116736/123872 (94%)]\tLoss: 0.373523\tLR: 0.00006671\n",
            "Train Epoch: 13 [116992/123872 (94%)]\tLoss: 0.435331\tLR: 0.00006668\n",
            "Train Epoch: 13 [117248/123872 (95%)]\tLoss: 0.374160\tLR: 0.00006665\n",
            "Train Epoch: 13 [117504/123872 (95%)]\tLoss: 0.358362\tLR: 0.00006663\n",
            "Train Epoch: 13 [117760/123872 (95%)]\tLoss: 0.383349\tLR: 0.00006660\n",
            "Train Epoch: 13 [117760/123872 (95%)]\tLoss: 0.383349\n",
            "Train Epoch: 13 [118016/123872 (95%)]\tLoss: 0.350156\tLR: 0.00006658\n",
            "Train Epoch: 13 [118272/123872 (95%)]\tLoss: 0.377911\tLR: 0.00006655\n",
            "Train Epoch: 13 [118528/123872 (96%)]\tLoss: 0.359430\tLR: 0.00006652\n",
            "Train Epoch: 13 [118784/123872 (96%)]\tLoss: 0.369355\tLR: 0.00006650\n",
            "Train Epoch: 13 [119040/123872 (96%)]\tLoss: 0.356973\tLR: 0.00006647\n",
            "Train Epoch: 13 [119296/123872 (96%)]\tLoss: 0.385994\tLR: 0.00006645\n",
            "Train Epoch: 13 [119552/123872 (96%)]\tLoss: 0.374076\tLR: 0.00006642\n",
            "Train Epoch: 13 [119808/123872 (97%)]\tLoss: 0.382607\tLR: 0.00006640\n",
            "Train Epoch: 13 [120064/123872 (97%)]\tLoss: 0.375606\tLR: 0.00006637\n",
            "Train Epoch: 13 [120320/123872 (97%)]\tLoss: 0.382487\tLR: 0.00006634\n",
            "Train Epoch: 13 [120320/123872 (97%)]\tLoss: 0.382487\n",
            "Train Epoch: 13 [120576/123872 (97%)]\tLoss: 0.371758\tLR: 0.00006632\n",
            "Train Epoch: 13 [120832/123872 (98%)]\tLoss: 0.377048\tLR: 0.00006629\n",
            "Train Epoch: 13 [121088/123872 (98%)]\tLoss: 0.437627\tLR: 0.00006627\n",
            "Train Epoch: 13 [121344/123872 (98%)]\tLoss: 0.316816\tLR: 0.00006624\n",
            "Train Epoch: 13 [121600/123872 (98%)]\tLoss: 0.359512\tLR: 0.00006622\n",
            "Train Epoch: 13 [121856/123872 (98%)]\tLoss: 0.328800\tLR: 0.00006619\n",
            "Train Epoch: 13 [122112/123872 (99%)]\tLoss: 0.328191\tLR: 0.00006617\n",
            "Train Epoch: 13 [122368/123872 (99%)]\tLoss: 0.441686\tLR: 0.00006614\n",
            "Train Epoch: 13 [122624/123872 (99%)]\tLoss: 0.391566\tLR: 0.00006612\n",
            "Train Epoch: 13 [122880/123872 (99%)]\tLoss: 0.310556\tLR: 0.00006609\n",
            "Train Epoch: 13 [122880/123872 (99%)]\tLoss: 0.310556\n",
            "Train Epoch: 13 [123136/123872 (99%)]\tLoss: 0.406874\tLR: 0.00006607\n",
            "Train Epoch: 13 [123392/123872 (100%)]\tLoss: 0.380366\tLR: 0.00006604\n",
            "Train Epoch: 13 [108192/123872 (100%)]\tLoss: 0.447169\tLR: 0.00006602\n",
            "\n",
            "Test set: Average loss: 0.0015, Accuracy: 25636/30970 (82.78%)\n",
            "\n",
            "Train Epoch: 14 [0/123872 (0%)]\tLoss: 0.330623\tLR: 0.00006599\n",
            "Train Epoch: 14 [0/123872 (0%)]\tLoss: 0.330623\n",
            "Train Epoch: 14 [256/123872 (0%)]\tLoss: 0.337309\tLR: 0.00006597\n",
            "Train Epoch: 14 [512/123872 (0%)]\tLoss: 0.389334\tLR: 0.00006594\n",
            "Train Epoch: 14 [768/123872 (1%)]\tLoss: 0.342011\tLR: 0.00006592\n",
            "Train Epoch: 14 [1024/123872 (1%)]\tLoss: 0.341932\tLR: 0.00006590\n",
            "Train Epoch: 14 [1280/123872 (1%)]\tLoss: 0.346011\tLR: 0.00006587\n",
            "Train Epoch: 14 [1536/123872 (1%)]\tLoss: 0.371800\tLR: 0.00006585\n",
            "Train Epoch: 14 [1792/123872 (1%)]\tLoss: 0.387219\tLR: 0.00006582\n",
            "Train Epoch: 14 [2048/123872 (2%)]\tLoss: 0.394989\tLR: 0.00006580\n",
            "Train Epoch: 14 [2304/123872 (2%)]\tLoss: 0.350872\tLR: 0.00006577\n",
            "Train Epoch: 14 [2560/123872 (2%)]\tLoss: 0.381221\tLR: 0.00006575\n",
            "Train Epoch: 14 [2560/123872 (2%)]\tLoss: 0.381221\n",
            "Train Epoch: 14 [2816/123872 (2%)]\tLoss: 0.415736\tLR: 0.00006572\n",
            "Train Epoch: 14 [3072/123872 (2%)]\tLoss: 0.399768\tLR: 0.00006570\n",
            "Train Epoch: 14 [3328/123872 (3%)]\tLoss: 0.370633\tLR: 0.00006568\n",
            "Train Epoch: 14 [3584/123872 (3%)]\tLoss: 0.348849\tLR: 0.00006565\n",
            "Train Epoch: 14 [3840/123872 (3%)]\tLoss: 0.401185\tLR: 0.00006563\n",
            "Train Epoch: 14 [4096/123872 (3%)]\tLoss: 0.369607\tLR: 0.00006560\n",
            "Train Epoch: 14 [4352/123872 (4%)]\tLoss: 0.341174\tLR: 0.00006558\n",
            "Train Epoch: 14 [4608/123872 (4%)]\tLoss: 0.365673\tLR: 0.00006556\n",
            "Train Epoch: 14 [4864/123872 (4%)]\tLoss: 0.373662\tLR: 0.00006553\n",
            "Train Epoch: 14 [5120/123872 (4%)]\tLoss: 0.324509\tLR: 0.00006551\n",
            "Train Epoch: 14 [5120/123872 (4%)]\tLoss: 0.324509\n",
            "Train Epoch: 14 [5376/123872 (4%)]\tLoss: 0.414193\tLR: 0.00006549\n",
            "Train Epoch: 14 [5632/123872 (5%)]\tLoss: 0.360485\tLR: 0.00006546\n",
            "Train Epoch: 14 [5888/123872 (5%)]\tLoss: 0.367679\tLR: 0.00006544\n",
            "Train Epoch: 14 [6144/123872 (5%)]\tLoss: 0.371711\tLR: 0.00006541\n",
            "Train Epoch: 14 [6400/123872 (5%)]\tLoss: 0.421434\tLR: 0.00006539\n",
            "Train Epoch: 14 [6656/123872 (5%)]\tLoss: 0.351288\tLR: 0.00006537\n",
            "Train Epoch: 14 [6912/123872 (6%)]\tLoss: 0.392028\tLR: 0.00006534\n",
            "Train Epoch: 14 [7168/123872 (6%)]\tLoss: 0.362083\tLR: 0.00006532\n",
            "Train Epoch: 14 [7424/123872 (6%)]\tLoss: 0.369166\tLR: 0.00006530\n",
            "Train Epoch: 14 [7680/123872 (6%)]\tLoss: 0.390890\tLR: 0.00006527\n",
            "Train Epoch: 14 [7680/123872 (6%)]\tLoss: 0.390890\n",
            "Train Epoch: 14 [7936/123872 (6%)]\tLoss: 0.345063\tLR: 0.00006525\n",
            "Train Epoch: 14 [8192/123872 (7%)]\tLoss: 0.435880\tLR: 0.00006523\n",
            "Train Epoch: 14 [8448/123872 (7%)]\tLoss: 0.373718\tLR: 0.00006521\n",
            "Train Epoch: 14 [8704/123872 (7%)]\tLoss: 0.372080\tLR: 0.00006518\n",
            "Train Epoch: 14 [8960/123872 (7%)]\tLoss: 0.331833\tLR: 0.00006516\n",
            "Train Epoch: 14 [9216/123872 (7%)]\tLoss: 0.333307\tLR: 0.00006514\n",
            "Train Epoch: 14 [9472/123872 (8%)]\tLoss: 0.384187\tLR: 0.00006511\n",
            "Train Epoch: 14 [9728/123872 (8%)]\tLoss: 0.339309\tLR: 0.00006509\n",
            "Train Epoch: 14 [9984/123872 (8%)]\tLoss: 0.371918\tLR: 0.00006507\n",
            "Train Epoch: 14 [10240/123872 (8%)]\tLoss: 0.363247\tLR: 0.00006505\n",
            "Train Epoch: 14 [10240/123872 (8%)]\tLoss: 0.363247\n",
            "Train Epoch: 14 [10496/123872 (8%)]\tLoss: 0.346923\tLR: 0.00006502\n",
            "Train Epoch: 14 [10752/123872 (9%)]\tLoss: 0.384257\tLR: 0.00006500\n",
            "Train Epoch: 14 [11008/123872 (9%)]\tLoss: 0.439186\tLR: 0.00006498\n",
            "Train Epoch: 14 [11264/123872 (9%)]\tLoss: 0.370846\tLR: 0.00006495\n",
            "Train Epoch: 14 [11520/123872 (9%)]\tLoss: 0.310961\tLR: 0.00006493\n",
            "Train Epoch: 14 [11776/123872 (10%)]\tLoss: 0.347410\tLR: 0.00006491\n",
            "Train Epoch: 14 [12032/123872 (10%)]\tLoss: 0.329998\tLR: 0.00006489\n",
            "Train Epoch: 14 [12288/123872 (10%)]\tLoss: 0.315674\tLR: 0.00006487\n",
            "Train Epoch: 14 [12544/123872 (10%)]\tLoss: 0.320271\tLR: 0.00006484\n",
            "Train Epoch: 14 [12800/123872 (10%)]\tLoss: 0.313537\tLR: 0.00006482\n",
            "Train Epoch: 14 [12800/123872 (10%)]\tLoss: 0.313537\n",
            "Train Epoch: 14 [13056/123872 (11%)]\tLoss: 0.361685\tLR: 0.00006480\n",
            "Train Epoch: 14 [13312/123872 (11%)]\tLoss: 0.389978\tLR: 0.00006478\n",
            "Train Epoch: 14 [13568/123872 (11%)]\tLoss: 0.342427\tLR: 0.00006475\n",
            "Train Epoch: 14 [13824/123872 (11%)]\tLoss: 0.379769\tLR: 0.00006473\n",
            "Train Epoch: 14 [14080/123872 (11%)]\tLoss: 0.411335\tLR: 0.00006471\n",
            "Train Epoch: 14 [14336/123872 (12%)]\tLoss: 0.344693\tLR: 0.00006469\n",
            "Train Epoch: 14 [14592/123872 (12%)]\tLoss: 0.315438\tLR: 0.00006467\n",
            "Train Epoch: 14 [14848/123872 (12%)]\tLoss: 0.477860\tLR: 0.00006464\n",
            "Train Epoch: 14 [15104/123872 (12%)]\tLoss: 0.384568\tLR: 0.00006462\n",
            "Train Epoch: 14 [15360/123872 (12%)]\tLoss: 0.337378\tLR: 0.00006460\n",
            "Train Epoch: 14 [15360/123872 (12%)]\tLoss: 0.337378\n",
            "Train Epoch: 14 [15616/123872 (13%)]\tLoss: 0.310438\tLR: 0.00006458\n",
            "Train Epoch: 14 [15872/123872 (13%)]\tLoss: 0.407493\tLR: 0.00006456\n",
            "Train Epoch: 14 [16128/123872 (13%)]\tLoss: 0.393865\tLR: 0.00006454\n",
            "Train Epoch: 14 [16384/123872 (13%)]\tLoss: 0.315927\tLR: 0.00006451\n",
            "Train Epoch: 14 [16640/123872 (13%)]\tLoss: 0.347402\tLR: 0.00006449\n",
            "Train Epoch: 14 [16896/123872 (14%)]\tLoss: 0.366872\tLR: 0.00006447\n",
            "Train Epoch: 14 [17152/123872 (14%)]\tLoss: 0.377878\tLR: 0.00006445\n",
            "Train Epoch: 14 [17408/123872 (14%)]\tLoss: 0.311265\tLR: 0.00006443\n",
            "Train Epoch: 14 [17664/123872 (14%)]\tLoss: 0.398578\tLR: 0.00006441\n",
            "Train Epoch: 14 [17920/123872 (14%)]\tLoss: 0.308347\tLR: 0.00006439\n",
            "Train Epoch: 14 [17920/123872 (14%)]\tLoss: 0.308347\n",
            "Train Epoch: 14 [18176/123872 (15%)]\tLoss: 0.390196\tLR: 0.00006437\n",
            "Train Epoch: 14 [18432/123872 (15%)]\tLoss: 0.388752\tLR: 0.00006434\n",
            "Train Epoch: 14 [18688/123872 (15%)]\tLoss: 0.309988\tLR: 0.00006432\n",
            "Train Epoch: 14 [18944/123872 (15%)]\tLoss: 0.392085\tLR: 0.00006430\n",
            "Train Epoch: 14 [19200/123872 (15%)]\tLoss: 0.385712\tLR: 0.00006428\n",
            "Train Epoch: 14 [19456/123872 (16%)]\tLoss: 0.400263\tLR: 0.00006426\n",
            "Train Epoch: 14 [19712/123872 (16%)]\tLoss: 0.303999\tLR: 0.00006424\n",
            "Train Epoch: 14 [19968/123872 (16%)]\tLoss: 0.390474\tLR: 0.00006422\n",
            "Train Epoch: 14 [20224/123872 (16%)]\tLoss: 0.378742\tLR: 0.00006420\n",
            "Train Epoch: 14 [20480/123872 (17%)]\tLoss: 0.380018\tLR: 0.00006418\n",
            "Train Epoch: 14 [20480/123872 (17%)]\tLoss: 0.380018\n",
            "Train Epoch: 14 [20736/123872 (17%)]\tLoss: 0.389789\tLR: 0.00006416\n",
            "Train Epoch: 14 [20992/123872 (17%)]\tLoss: 0.360240\tLR: 0.00006414\n",
            "Train Epoch: 14 [21248/123872 (17%)]\tLoss: 0.378328\tLR: 0.00006412\n",
            "Train Epoch: 14 [21504/123872 (17%)]\tLoss: 0.338821\tLR: 0.00006410\n",
            "Train Epoch: 14 [21760/123872 (18%)]\tLoss: 0.386925\tLR: 0.00006407\n",
            "Train Epoch: 14 [22016/123872 (18%)]\tLoss: 0.326896\tLR: 0.00006405\n",
            "Train Epoch: 14 [22272/123872 (18%)]\tLoss: 0.342157\tLR: 0.00006403\n",
            "Train Epoch: 14 [22528/123872 (18%)]\tLoss: 0.263977\tLR: 0.00006401\n",
            "Train Epoch: 14 [22784/123872 (18%)]\tLoss: 0.431016\tLR: 0.00006399\n",
            "Train Epoch: 14 [23040/123872 (19%)]\tLoss: 0.348640\tLR: 0.00006397\n",
            "Train Epoch: 14 [23040/123872 (19%)]\tLoss: 0.348640\n",
            "Train Epoch: 14 [23296/123872 (19%)]\tLoss: 0.352921\tLR: 0.00006395\n",
            "Train Epoch: 14 [23552/123872 (19%)]\tLoss: 0.382373\tLR: 0.00006393\n",
            "Train Epoch: 14 [23808/123872 (19%)]\tLoss: 0.395740\tLR: 0.00006391\n",
            "Train Epoch: 14 [24064/123872 (19%)]\tLoss: 0.345853\tLR: 0.00006389\n",
            "Train Epoch: 14 [24320/123872 (20%)]\tLoss: 0.378662\tLR: 0.00006387\n",
            "Train Epoch: 14 [24576/123872 (20%)]\tLoss: 0.332951\tLR: 0.00006385\n",
            "Train Epoch: 14 [24832/123872 (20%)]\tLoss: 0.347929\tLR: 0.00006383\n",
            "Train Epoch: 14 [25088/123872 (20%)]\tLoss: 0.369350\tLR: 0.00006381\n",
            "Train Epoch: 14 [25344/123872 (20%)]\tLoss: 0.349717\tLR: 0.00006379\n",
            "Train Epoch: 14 [25600/123872 (21%)]\tLoss: 0.328904\tLR: 0.00006377\n",
            "Train Epoch: 14 [25600/123872 (21%)]\tLoss: 0.328904\n",
            "Train Epoch: 14 [25856/123872 (21%)]\tLoss: 0.352815\tLR: 0.00006375\n",
            "Train Epoch: 14 [26112/123872 (21%)]\tLoss: 0.365886\tLR: 0.00006373\n",
            "Train Epoch: 14 [26368/123872 (21%)]\tLoss: 0.393573\tLR: 0.00006372\n",
            "Train Epoch: 14 [26624/123872 (21%)]\tLoss: 0.403613\tLR: 0.00006370\n",
            "Train Epoch: 14 [26880/123872 (22%)]\tLoss: 0.330463\tLR: 0.00006368\n",
            "Train Epoch: 14 [27136/123872 (22%)]\tLoss: 0.345658\tLR: 0.00006366\n",
            "Train Epoch: 14 [27392/123872 (22%)]\tLoss: 0.288280\tLR: 0.00006364\n",
            "Train Epoch: 14 [27648/123872 (22%)]\tLoss: 0.389907\tLR: 0.00006362\n",
            "Train Epoch: 14 [27904/123872 (23%)]\tLoss: 0.363839\tLR: 0.00006360\n",
            "Train Epoch: 14 [28160/123872 (23%)]\tLoss: 0.471557\tLR: 0.00006358\n",
            "Train Epoch: 14 [28160/123872 (23%)]\tLoss: 0.471557\n",
            "Train Epoch: 14 [28416/123872 (23%)]\tLoss: 0.369110\tLR: 0.00006356\n",
            "Train Epoch: 14 [28672/123872 (23%)]\tLoss: 0.331349\tLR: 0.00006354\n",
            "Train Epoch: 14 [28928/123872 (23%)]\tLoss: 0.362202\tLR: 0.00006352\n",
            "Train Epoch: 14 [29184/123872 (24%)]\tLoss: 0.382740\tLR: 0.00006350\n",
            "Train Epoch: 14 [29440/123872 (24%)]\tLoss: 0.337434\tLR: 0.00006348\n",
            "Train Epoch: 14 [29696/123872 (24%)]\tLoss: 0.368306\tLR: 0.00006347\n",
            "Train Epoch: 14 [29952/123872 (24%)]\tLoss: 0.366254\tLR: 0.00006345\n",
            "Train Epoch: 14 [30208/123872 (24%)]\tLoss: 0.384750\tLR: 0.00006343\n",
            "Train Epoch: 14 [30464/123872 (25%)]\tLoss: 0.313987\tLR: 0.00006341\n",
            "Train Epoch: 14 [30720/123872 (25%)]\tLoss: 0.317423\tLR: 0.00006339\n",
            "Train Epoch: 14 [30720/123872 (25%)]\tLoss: 0.317423\n",
            "Train Epoch: 14 [30976/123872 (25%)]\tLoss: 0.324673\tLR: 0.00006337\n",
            "Train Epoch: 14 [31232/123872 (25%)]\tLoss: 0.387101\tLR: 0.00006335\n",
            "Train Epoch: 14 [31488/123872 (25%)]\tLoss: 0.396097\tLR: 0.00006334\n",
            "Train Epoch: 14 [31744/123872 (26%)]\tLoss: 0.374166\tLR: 0.00006332\n",
            "Train Epoch: 14 [32000/123872 (26%)]\tLoss: 0.330474\tLR: 0.00006330\n",
            "Train Epoch: 14 [32256/123872 (26%)]\tLoss: 0.369285\tLR: 0.00006328\n",
            "Train Epoch: 14 [32512/123872 (26%)]\tLoss: 0.356198\tLR: 0.00006326\n",
            "Train Epoch: 14 [32768/123872 (26%)]\tLoss: 0.356512\tLR: 0.00006324\n",
            "Train Epoch: 14 [33024/123872 (27%)]\tLoss: 0.392374\tLR: 0.00006323\n",
            "Train Epoch: 14 [33280/123872 (27%)]\tLoss: 0.351954\tLR: 0.00006321\n",
            "Train Epoch: 14 [33280/123872 (27%)]\tLoss: 0.351954\n",
            "Train Epoch: 14 [33536/123872 (27%)]\tLoss: 0.356104\tLR: 0.00006319\n",
            "Train Epoch: 14 [33792/123872 (27%)]\tLoss: 0.303153\tLR: 0.00006317\n",
            "Train Epoch: 14 [34048/123872 (27%)]\tLoss: 0.320260\tLR: 0.00006315\n",
            "Train Epoch: 14 [34304/123872 (28%)]\tLoss: 0.398837\tLR: 0.00006313\n",
            "Train Epoch: 14 [34560/123872 (28%)]\tLoss: 0.340535\tLR: 0.00006312\n",
            "Train Epoch: 14 [34816/123872 (28%)]\tLoss: 0.361619\tLR: 0.00006310\n",
            "Train Epoch: 14 [35072/123872 (28%)]\tLoss: 0.390702\tLR: 0.00006308\n",
            "Train Epoch: 14 [35328/123872 (29%)]\tLoss: 0.388404\tLR: 0.00006306\n",
            "Train Epoch: 14 [35584/123872 (29%)]\tLoss: 0.414649\tLR: 0.00006305\n",
            "Train Epoch: 14 [35840/123872 (29%)]\tLoss: 0.339705\tLR: 0.00006303\n",
            "Train Epoch: 14 [35840/123872 (29%)]\tLoss: 0.339705\n",
            "Train Epoch: 14 [36096/123872 (29%)]\tLoss: 0.381114\tLR: 0.00006301\n",
            "Train Epoch: 14 [36352/123872 (29%)]\tLoss: 0.353329\tLR: 0.00006299\n",
            "Train Epoch: 14 [36608/123872 (30%)]\tLoss: 0.339476\tLR: 0.00006298\n",
            "Train Epoch: 14 [36864/123872 (30%)]\tLoss: 0.330564\tLR: 0.00006296\n",
            "Train Epoch: 14 [37120/123872 (30%)]\tLoss: 0.361257\tLR: 0.00006294\n",
            "Train Epoch: 14 [37376/123872 (30%)]\tLoss: 0.385388\tLR: 0.00006292\n",
            "Train Epoch: 14 [37632/123872 (30%)]\tLoss: 0.319146\tLR: 0.00006291\n",
            "Train Epoch: 14 [37888/123872 (31%)]\tLoss: 0.300015\tLR: 0.00006289\n",
            "Train Epoch: 14 [38144/123872 (31%)]\tLoss: 0.356347\tLR: 0.00006287\n",
            "Train Epoch: 14 [38400/123872 (31%)]\tLoss: 0.377010\tLR: 0.00006285\n",
            "Train Epoch: 14 [38400/123872 (31%)]\tLoss: 0.377010\n",
            "Train Epoch: 14 [38656/123872 (31%)]\tLoss: 0.369437\tLR: 0.00006284\n",
            "Train Epoch: 14 [38912/123872 (31%)]\tLoss: 0.412160\tLR: 0.00006282\n",
            "Train Epoch: 14 [39168/123872 (32%)]\tLoss: 0.350176\tLR: 0.00006280\n",
            "Train Epoch: 14 [39424/123872 (32%)]\tLoss: 0.347585\tLR: 0.00006279\n",
            "Train Epoch: 14 [39680/123872 (32%)]\tLoss: 0.344887\tLR: 0.00006277\n",
            "Train Epoch: 14 [39936/123872 (32%)]\tLoss: 0.323839\tLR: 0.00006275\n",
            "Train Epoch: 14 [40192/123872 (32%)]\tLoss: 0.391402\tLR: 0.00006274\n",
            "Train Epoch: 14 [40448/123872 (33%)]\tLoss: 0.304336\tLR: 0.00006272\n",
            "Train Epoch: 14 [40704/123872 (33%)]\tLoss: 0.360867\tLR: 0.00006270\n",
            "Train Epoch: 14 [40960/123872 (33%)]\tLoss: 0.382400\tLR: 0.00006269\n",
            "Train Epoch: 14 [40960/123872 (33%)]\tLoss: 0.382400\n",
            "Train Epoch: 14 [41216/123872 (33%)]\tLoss: 0.327550\tLR: 0.00006267\n",
            "Train Epoch: 14 [41472/123872 (33%)]\tLoss: 0.399581\tLR: 0.00006265\n",
            "Train Epoch: 14 [41728/123872 (34%)]\tLoss: 0.407719\tLR: 0.00006264\n",
            "Train Epoch: 14 [41984/123872 (34%)]\tLoss: 0.381984\tLR: 0.00006262\n",
            "Train Epoch: 14 [42240/123872 (34%)]\tLoss: 0.354478\tLR: 0.00006260\n",
            "Train Epoch: 14 [42496/123872 (34%)]\tLoss: 0.407191\tLR: 0.00006259\n",
            "Train Epoch: 14 [42752/123872 (35%)]\tLoss: 0.408474\tLR: 0.00006257\n",
            "Train Epoch: 14 [43008/123872 (35%)]\tLoss: 0.391784\tLR: 0.00006255\n",
            "Train Epoch: 14 [43264/123872 (35%)]\tLoss: 0.309989\tLR: 0.00006254\n",
            "Train Epoch: 14 [43520/123872 (35%)]\tLoss: 0.356132\tLR: 0.00006252\n",
            "Train Epoch: 14 [43520/123872 (35%)]\tLoss: 0.356132\n",
            "Train Epoch: 14 [43776/123872 (35%)]\tLoss: 0.381504\tLR: 0.00006251\n",
            "Train Epoch: 14 [44032/123872 (36%)]\tLoss: 0.323194\tLR: 0.00006249\n",
            "Train Epoch: 14 [44288/123872 (36%)]\tLoss: 0.355048\tLR: 0.00006247\n",
            "Train Epoch: 14 [44544/123872 (36%)]\tLoss: 0.329195\tLR: 0.00006246\n",
            "Train Epoch: 14 [44800/123872 (36%)]\tLoss: 0.327595\tLR: 0.00006244\n",
            "Train Epoch: 14 [45056/123872 (36%)]\tLoss: 0.368109\tLR: 0.00006243\n",
            "Train Epoch: 14 [45312/123872 (37%)]\tLoss: 0.358236\tLR: 0.00006241\n",
            "Train Epoch: 14 [45568/123872 (37%)]\tLoss: 0.330655\tLR: 0.00006240\n",
            "Train Epoch: 14 [45824/123872 (37%)]\tLoss: 0.365369\tLR: 0.00006238\n",
            "Train Epoch: 14 [46080/123872 (37%)]\tLoss: 0.375124\tLR: 0.00006236\n",
            "Train Epoch: 14 [46080/123872 (37%)]\tLoss: 0.375124\n",
            "Train Epoch: 14 [46336/123872 (37%)]\tLoss: 0.371235\tLR: 0.00006235\n",
            "Train Epoch: 14 [46592/123872 (38%)]\tLoss: 0.331793\tLR: 0.00006233\n",
            "Train Epoch: 14 [46848/123872 (38%)]\tLoss: 0.389052\tLR: 0.00006232\n",
            "Train Epoch: 14 [47104/123872 (38%)]\tLoss: 0.337357\tLR: 0.00006230\n",
            "Train Epoch: 14 [47360/123872 (38%)]\tLoss: 0.332475\tLR: 0.00006229\n",
            "Train Epoch: 14 [47616/123872 (38%)]\tLoss: 0.355167\tLR: 0.00006227\n",
            "Train Epoch: 14 [47872/123872 (39%)]\tLoss: 0.403757\tLR: 0.00006226\n",
            "Train Epoch: 14 [48128/123872 (39%)]\tLoss: 0.396139\tLR: 0.00006224\n",
            "Train Epoch: 14 [48384/123872 (39%)]\tLoss: 0.387156\tLR: 0.00006223\n",
            "Train Epoch: 14 [48640/123872 (39%)]\tLoss: 0.406833\tLR: 0.00006221\n",
            "Train Epoch: 14 [48640/123872 (39%)]\tLoss: 0.406833\n",
            "Train Epoch: 14 [48896/123872 (39%)]\tLoss: 0.376862\tLR: 0.00006220\n",
            "Train Epoch: 14 [49152/123872 (40%)]\tLoss: 0.350590\tLR: 0.00006218\n",
            "Train Epoch: 14 [49408/123872 (40%)]\tLoss: 0.392630\tLR: 0.00006217\n",
            "Train Epoch: 14 [49664/123872 (40%)]\tLoss: 0.352722\tLR: 0.00006215\n",
            "Train Epoch: 14 [49920/123872 (40%)]\tLoss: 0.358048\tLR: 0.00006214\n",
            "Train Epoch: 14 [50176/123872 (40%)]\tLoss: 0.343734\tLR: 0.00006212\n",
            "Train Epoch: 14 [50432/123872 (41%)]\tLoss: 0.360622\tLR: 0.00006211\n",
            "Train Epoch: 14 [50688/123872 (41%)]\tLoss: 0.345616\tLR: 0.00006209\n",
            "Train Epoch: 14 [50944/123872 (41%)]\tLoss: 0.428841\tLR: 0.00006208\n",
            "Train Epoch: 14 [51200/123872 (41%)]\tLoss: 0.357273\tLR: 0.00006206\n",
            "Train Epoch: 14 [51200/123872 (41%)]\tLoss: 0.357273\n",
            "Train Epoch: 14 [51456/123872 (42%)]\tLoss: 0.425710\tLR: 0.00006205\n",
            "Train Epoch: 14 [51712/123872 (42%)]\tLoss: 0.405526\tLR: 0.00006203\n",
            "Train Epoch: 14 [51968/123872 (42%)]\tLoss: 0.341534\tLR: 0.00006202\n",
            "Train Epoch: 14 [52224/123872 (42%)]\tLoss: 0.336347\tLR: 0.00006200\n",
            "Train Epoch: 14 [52480/123872 (42%)]\tLoss: 0.335350\tLR: 0.00006199\n",
            "Train Epoch: 14 [52736/123872 (43%)]\tLoss: 0.394212\tLR: 0.00006198\n",
            "Train Epoch: 14 [52992/123872 (43%)]\tLoss: 0.376850\tLR: 0.00006196\n",
            "Train Epoch: 14 [53248/123872 (43%)]\tLoss: 0.343193\tLR: 0.00006195\n",
            "Train Epoch: 14 [53504/123872 (43%)]\tLoss: 0.342243\tLR: 0.00006193\n",
            "Train Epoch: 14 [53760/123872 (43%)]\tLoss: 0.401322\tLR: 0.00006192\n",
            "Train Epoch: 14 [53760/123872 (43%)]\tLoss: 0.401322\n",
            "Train Epoch: 14 [54016/123872 (44%)]\tLoss: 0.416092\tLR: 0.00006191\n",
            "Train Epoch: 14 [54272/123872 (44%)]\tLoss: 0.361459\tLR: 0.00006189\n",
            "Train Epoch: 14 [54528/123872 (44%)]\tLoss: 0.341172\tLR: 0.00006188\n",
            "Train Epoch: 14 [54784/123872 (44%)]\tLoss: 0.307636\tLR: 0.00006186\n",
            "Train Epoch: 14 [55040/123872 (44%)]\tLoss: 0.373639\tLR: 0.00006185\n",
            "Train Epoch: 14 [55296/123872 (45%)]\tLoss: 0.349136\tLR: 0.00006184\n",
            "Train Epoch: 14 [55552/123872 (45%)]\tLoss: 0.459986\tLR: 0.00006182\n",
            "Train Epoch: 14 [55808/123872 (45%)]\tLoss: 0.339348\tLR: 0.00006181\n",
            "Train Epoch: 14 [56064/123872 (45%)]\tLoss: 0.352254\tLR: 0.00006180\n",
            "Train Epoch: 14 [56320/123872 (45%)]\tLoss: 0.421002\tLR: 0.00006178\n",
            "Train Epoch: 14 [56320/123872 (45%)]\tLoss: 0.421002\n",
            "Train Epoch: 14 [56576/123872 (46%)]\tLoss: 0.267457\tLR: 0.00006177\n",
            "Train Epoch: 14 [56832/123872 (46%)]\tLoss: 0.367062\tLR: 0.00006175\n",
            "Train Epoch: 14 [57088/123872 (46%)]\tLoss: 0.348603\tLR: 0.00006174\n",
            "Train Epoch: 14 [57344/123872 (46%)]\tLoss: 0.394871\tLR: 0.00006173\n",
            "Train Epoch: 14 [57600/123872 (46%)]\tLoss: 0.379702\tLR: 0.00006171\n",
            "Train Epoch: 14 [57856/123872 (47%)]\tLoss: 0.403819\tLR: 0.00006170\n",
            "Train Epoch: 14 [58112/123872 (47%)]\tLoss: 0.329788\tLR: 0.00006169\n",
            "Train Epoch: 14 [58368/123872 (47%)]\tLoss: 0.346940\tLR: 0.00006168\n",
            "Train Epoch: 14 [58624/123872 (47%)]\tLoss: 0.394406\tLR: 0.00006166\n",
            "Train Epoch: 14 [58880/123872 (48%)]\tLoss: 0.373406\tLR: 0.00006165\n",
            "Train Epoch: 14 [58880/123872 (48%)]\tLoss: 0.373406\n",
            "Train Epoch: 14 [59136/123872 (48%)]\tLoss: 0.363232\tLR: 0.00006164\n",
            "Train Epoch: 14 [59392/123872 (48%)]\tLoss: 0.369987\tLR: 0.00006162\n",
            "Train Epoch: 14 [59648/123872 (48%)]\tLoss: 0.389205\tLR: 0.00006161\n",
            "Train Epoch: 14 [59904/123872 (48%)]\tLoss: 0.383418\tLR: 0.00006160\n",
            "Train Epoch: 14 [60160/123872 (49%)]\tLoss: 0.352999\tLR: 0.00006158\n",
            "Train Epoch: 14 [60416/123872 (49%)]\tLoss: 0.423600\tLR: 0.00006157\n",
            "Train Epoch: 14 [60672/123872 (49%)]\tLoss: 0.365889\tLR: 0.00006156\n",
            "Train Epoch: 14 [60928/123872 (49%)]\tLoss: 0.372848\tLR: 0.00006155\n",
            "Train Epoch: 14 [61184/123872 (49%)]\tLoss: 0.480466\tLR: 0.00006153\n",
            "Train Epoch: 14 [61440/123872 (50%)]\tLoss: 0.358290\tLR: 0.00006152\n",
            "Train Epoch: 14 [61440/123872 (50%)]\tLoss: 0.358290\n",
            "Train Epoch: 14 [61696/123872 (50%)]\tLoss: 0.344183\tLR: 0.00006151\n",
            "Train Epoch: 14 [61952/123872 (50%)]\tLoss: 0.362385\tLR: 0.00006150\n",
            "Train Epoch: 14 [62208/123872 (50%)]\tLoss: 0.299167\tLR: 0.00006148\n",
            "Train Epoch: 14 [62464/123872 (50%)]\tLoss: 0.357408\tLR: 0.00006147\n",
            "Train Epoch: 14 [62720/123872 (51%)]\tLoss: 0.363525\tLR: 0.00006146\n",
            "Train Epoch: 14 [62976/123872 (51%)]\tLoss: 0.330449\tLR: 0.00006145\n",
            "Train Epoch: 14 [63232/123872 (51%)]\tLoss: 0.368942\tLR: 0.00006144\n",
            "Train Epoch: 14 [63488/123872 (51%)]\tLoss: 0.379465\tLR: 0.00006142\n",
            "Train Epoch: 14 [63744/123872 (51%)]\tLoss: 0.263346\tLR: 0.00006141\n",
            "Train Epoch: 14 [64000/123872 (52%)]\tLoss: 0.342245\tLR: 0.00006140\n",
            "Train Epoch: 14 [64000/123872 (52%)]\tLoss: 0.342245\n",
            "Train Epoch: 14 [64256/123872 (52%)]\tLoss: 0.361781\tLR: 0.00006139\n",
            "Train Epoch: 14 [64512/123872 (52%)]\tLoss: 0.332767\tLR: 0.00006137\n",
            "Train Epoch: 14 [64768/123872 (52%)]\tLoss: 0.319941\tLR: 0.00006136\n",
            "Train Epoch: 14 [65024/123872 (52%)]\tLoss: 0.376416\tLR: 0.00006135\n",
            "Train Epoch: 14 [65280/123872 (53%)]\tLoss: 0.378399\tLR: 0.00006134\n",
            "Train Epoch: 14 [65536/123872 (53%)]\tLoss: 0.367320\tLR: 0.00006133\n",
            "Train Epoch: 14 [65792/123872 (53%)]\tLoss: 0.413985\tLR: 0.00006132\n",
            "Train Epoch: 14 [66048/123872 (53%)]\tLoss: 0.364987\tLR: 0.00006130\n",
            "Train Epoch: 14 [66304/123872 (54%)]\tLoss: 0.324434\tLR: 0.00006129\n",
            "Train Epoch: 14 [66560/123872 (54%)]\tLoss: 0.386898\tLR: 0.00006128\n",
            "Train Epoch: 14 [66560/123872 (54%)]\tLoss: 0.386898\n",
            "Train Epoch: 14 [66816/123872 (54%)]\tLoss: 0.466286\tLR: 0.00006127\n",
            "Train Epoch: 14 [67072/123872 (54%)]\tLoss: 0.348526\tLR: 0.00006126\n",
            "Train Epoch: 14 [67328/123872 (54%)]\tLoss: 0.414645\tLR: 0.00006125\n",
            "Train Epoch: 14 [67584/123872 (55%)]\tLoss: 0.395095\tLR: 0.00006124\n",
            "Train Epoch: 14 [67840/123872 (55%)]\tLoss: 0.368771\tLR: 0.00006122\n",
            "Train Epoch: 14 [68096/123872 (55%)]\tLoss: 0.400531\tLR: 0.00006121\n",
            "Train Epoch: 14 [68352/123872 (55%)]\tLoss: 0.376437\tLR: 0.00006120\n",
            "Train Epoch: 14 [68608/123872 (55%)]\tLoss: 0.387305\tLR: 0.00006119\n",
            "Train Epoch: 14 [68864/123872 (56%)]\tLoss: 0.336200\tLR: 0.00006118\n",
            "Train Epoch: 14 [69120/123872 (56%)]\tLoss: 0.391767\tLR: 0.00006117\n",
            "Train Epoch: 14 [69120/123872 (56%)]\tLoss: 0.391767\n",
            "Train Epoch: 14 [69376/123872 (56%)]\tLoss: 0.360382\tLR: 0.00006116\n",
            "Train Epoch: 14 [69632/123872 (56%)]\tLoss: 0.345134\tLR: 0.00006115\n",
            "Train Epoch: 14 [69888/123872 (56%)]\tLoss: 0.379602\tLR: 0.00006114\n",
            "Train Epoch: 14 [70144/123872 (57%)]\tLoss: 0.314432\tLR: 0.00006113\n",
            "Train Epoch: 14 [70400/123872 (57%)]\tLoss: 0.414438\tLR: 0.00006111\n",
            "Train Epoch: 14 [70656/123872 (57%)]\tLoss: 0.356933\tLR: 0.00006110\n",
            "Train Epoch: 14 [70912/123872 (57%)]\tLoss: 0.354433\tLR: 0.00006109\n",
            "Train Epoch: 14 [71168/123872 (57%)]\tLoss: 0.403605\tLR: 0.00006108\n",
            "Train Epoch: 14 [71424/123872 (58%)]\tLoss: 0.372309\tLR: 0.00006107\n",
            "Train Epoch: 14 [71680/123872 (58%)]\tLoss: 0.372024\tLR: 0.00006106\n",
            "Train Epoch: 14 [71680/123872 (58%)]\tLoss: 0.372024\n",
            "Train Epoch: 14 [71936/123872 (58%)]\tLoss: 0.379823\tLR: 0.00006105\n",
            "Train Epoch: 14 [72192/123872 (58%)]\tLoss: 0.361634\tLR: 0.00006104\n",
            "Train Epoch: 14 [72448/123872 (58%)]\tLoss: 0.368805\tLR: 0.00006103\n",
            "Train Epoch: 14 [72704/123872 (59%)]\tLoss: 0.379131\tLR: 0.00006102\n",
            "Train Epoch: 14 [72960/123872 (59%)]\tLoss: 0.318238\tLR: 0.00006101\n",
            "Train Epoch: 14 [73216/123872 (59%)]\tLoss: 0.358659\tLR: 0.00006100\n",
            "Train Epoch: 14 [73472/123872 (59%)]\tLoss: 0.332150\tLR: 0.00006099\n",
            "Train Epoch: 14 [73728/123872 (60%)]\tLoss: 0.358223\tLR: 0.00006098\n",
            "Train Epoch: 14 [73984/123872 (60%)]\tLoss: 0.389692\tLR: 0.00006097\n",
            "Train Epoch: 14 [74240/123872 (60%)]\tLoss: 0.360405\tLR: 0.00006096\n",
            "Train Epoch: 14 [74240/123872 (60%)]\tLoss: 0.360405\n",
            "Train Epoch: 14 [74496/123872 (60%)]\tLoss: 0.398746\tLR: 0.00006095\n",
            "Train Epoch: 14 [74752/123872 (60%)]\tLoss: 0.302002\tLR: 0.00006094\n",
            "Train Epoch: 14 [75008/123872 (61%)]\tLoss: 0.384845\tLR: 0.00006093\n",
            "Train Epoch: 14 [75264/123872 (61%)]\tLoss: 0.348417\tLR: 0.00006092\n",
            "Train Epoch: 14 [75520/123872 (61%)]\tLoss: 0.321437\tLR: 0.00006091\n",
            "Train Epoch: 14 [75776/123872 (61%)]\tLoss: 0.328963\tLR: 0.00006090\n",
            "Train Epoch: 14 [76032/123872 (61%)]\tLoss: 0.392159\tLR: 0.00006089\n",
            "Train Epoch: 14 [76288/123872 (62%)]\tLoss: 0.321938\tLR: 0.00006088\n",
            "Train Epoch: 14 [76544/123872 (62%)]\tLoss: 0.296664\tLR: 0.00006087\n",
            "Train Epoch: 14 [76800/123872 (62%)]\tLoss: 0.379266\tLR: 0.00006086\n",
            "Train Epoch: 14 [76800/123872 (62%)]\tLoss: 0.379266\n",
            "Train Epoch: 14 [77056/123872 (62%)]\tLoss: 0.395906\tLR: 0.00006085\n",
            "Train Epoch: 14 [77312/123872 (62%)]\tLoss: 0.370768\tLR: 0.00006084\n",
            "Train Epoch: 14 [77568/123872 (63%)]\tLoss: 0.348043\tLR: 0.00006084\n",
            "Train Epoch: 14 [77824/123872 (63%)]\tLoss: 0.362939\tLR: 0.00006083\n",
            "Train Epoch: 14 [78080/123872 (63%)]\tLoss: 0.348084\tLR: 0.00006082\n",
            "Train Epoch: 14 [78336/123872 (63%)]\tLoss: 0.404373\tLR: 0.00006081\n",
            "Train Epoch: 14 [78592/123872 (63%)]\tLoss: 0.372963\tLR: 0.00006080\n",
            "Train Epoch: 14 [78848/123872 (64%)]\tLoss: 0.349433\tLR: 0.00006079\n",
            "Train Epoch: 14 [79104/123872 (64%)]\tLoss: 0.327016\tLR: 0.00006078\n",
            "Train Epoch: 14 [79360/123872 (64%)]\tLoss: 0.290850\tLR: 0.00006077\n",
            "Train Epoch: 14 [79360/123872 (64%)]\tLoss: 0.290850\n",
            "Train Epoch: 14 [79616/123872 (64%)]\tLoss: 0.398334\tLR: 0.00006076\n",
            "Train Epoch: 14 [79872/123872 (64%)]\tLoss: 0.365950\tLR: 0.00006075\n",
            "Train Epoch: 14 [80128/123872 (65%)]\tLoss: 0.395117\tLR: 0.00006074\n",
            "Train Epoch: 14 [80384/123872 (65%)]\tLoss: 0.437530\tLR: 0.00006074\n",
            "Train Epoch: 14 [80640/123872 (65%)]\tLoss: 0.328194\tLR: 0.00006073\n",
            "Train Epoch: 14 [80896/123872 (65%)]\tLoss: 0.368472\tLR: 0.00006072\n",
            "Train Epoch: 14 [81152/123872 (65%)]\tLoss: 0.279486\tLR: 0.00006071\n",
            "Train Epoch: 14 [81408/123872 (66%)]\tLoss: 0.328337\tLR: 0.00006070\n",
            "Train Epoch: 14 [81664/123872 (66%)]\tLoss: 0.348034\tLR: 0.00006069\n",
            "Train Epoch: 14 [81920/123872 (66%)]\tLoss: 0.363558\tLR: 0.00006068\n",
            "Train Epoch: 14 [81920/123872 (66%)]\tLoss: 0.363558\n",
            "Train Epoch: 14 [82176/123872 (66%)]\tLoss: 0.416861\tLR: 0.00006068\n",
            "Train Epoch: 14 [82432/123872 (67%)]\tLoss: 0.395600\tLR: 0.00006067\n",
            "Train Epoch: 14 [82688/123872 (67%)]\tLoss: 0.353099\tLR: 0.00006066\n",
            "Train Epoch: 14 [82944/123872 (67%)]\tLoss: 0.397860\tLR: 0.00006065\n",
            "Train Epoch: 14 [83200/123872 (67%)]\tLoss: 0.399225\tLR: 0.00006064\n",
            "Train Epoch: 14 [83456/123872 (67%)]\tLoss: 0.368699\tLR: 0.00006064\n",
            "Train Epoch: 14 [83712/123872 (68%)]\tLoss: 0.327031\tLR: 0.00006063\n",
            "Train Epoch: 14 [83968/123872 (68%)]\tLoss: 0.364779\tLR: 0.00006062\n",
            "Train Epoch: 14 [84224/123872 (68%)]\tLoss: 0.348742\tLR: 0.00006061\n",
            "Train Epoch: 14 [84480/123872 (68%)]\tLoss: 0.349736\tLR: 0.00006060\n",
            "Train Epoch: 14 [84480/123872 (68%)]\tLoss: 0.349736\n",
            "Train Epoch: 14 [84736/123872 (68%)]\tLoss: 0.366389\tLR: 0.00006060\n",
            "Train Epoch: 14 [84992/123872 (69%)]\tLoss: 0.335498\tLR: 0.00006059\n",
            "Train Epoch: 14 [85248/123872 (69%)]\tLoss: 0.431767\tLR: 0.00006058\n",
            "Train Epoch: 14 [85504/123872 (69%)]\tLoss: 0.373700\tLR: 0.00006057\n",
            "Train Epoch: 14 [85760/123872 (69%)]\tLoss: 0.339324\tLR: 0.00006056\n",
            "Train Epoch: 14 [86016/123872 (69%)]\tLoss: 0.373263\tLR: 0.00006056\n",
            "Train Epoch: 14 [86272/123872 (70%)]\tLoss: 0.341596\tLR: 0.00006055\n",
            "Train Epoch: 14 [86528/123872 (70%)]\tLoss: 0.366906\tLR: 0.00006054\n",
            "Train Epoch: 14 [86784/123872 (70%)]\tLoss: 0.432481\tLR: 0.00006053\n",
            "Train Epoch: 14 [87040/123872 (70%)]\tLoss: 0.380228\tLR: 0.00006053\n",
            "Train Epoch: 14 [87040/123872 (70%)]\tLoss: 0.380228\n",
            "Train Epoch: 14 [87296/123872 (70%)]\tLoss: 0.397090\tLR: 0.00006052\n",
            "Train Epoch: 14 [87552/123872 (71%)]\tLoss: 0.391285\tLR: 0.00006051\n",
            "Train Epoch: 14 [87808/123872 (71%)]\tLoss: 0.371015\tLR: 0.00006051\n",
            "Train Epoch: 14 [88064/123872 (71%)]\tLoss: 0.450205\tLR: 0.00006050\n",
            "Train Epoch: 14 [88320/123872 (71%)]\tLoss: 0.364670\tLR: 0.00006049\n",
            "Train Epoch: 14 [88576/123872 (71%)]\tLoss: 0.380276\tLR: 0.00006048\n",
            "Train Epoch: 14 [88832/123872 (72%)]\tLoss: 0.391376\tLR: 0.00006048\n",
            "Train Epoch: 14 [89088/123872 (72%)]\tLoss: 0.404357\tLR: 0.00006047\n",
            "Train Epoch: 14 [89344/123872 (72%)]\tLoss: 0.406391\tLR: 0.00006046\n",
            "Train Epoch: 14 [89600/123872 (72%)]\tLoss: 0.374462\tLR: 0.00006046\n",
            "Train Epoch: 14 [89600/123872 (72%)]\tLoss: 0.374462\n",
            "Train Epoch: 14 [89856/123872 (73%)]\tLoss: 0.290160\tLR: 0.00006045\n",
            "Train Epoch: 14 [90112/123872 (73%)]\tLoss: 0.343607\tLR: 0.00006044\n",
            "Train Epoch: 14 [90368/123872 (73%)]\tLoss: 0.334944\tLR: 0.00006044\n",
            "Train Epoch: 14 [90624/123872 (73%)]\tLoss: 0.320354\tLR: 0.00006043\n",
            "Train Epoch: 14 [90880/123872 (73%)]\tLoss: 0.408702\tLR: 0.00006042\n",
            "Train Epoch: 14 [91136/123872 (74%)]\tLoss: 0.332740\tLR: 0.00006042\n",
            "Train Epoch: 14 [91392/123872 (74%)]\tLoss: 0.388257\tLR: 0.00006041\n",
            "Train Epoch: 14 [91648/123872 (74%)]\tLoss: 0.412448\tLR: 0.00006040\n",
            "Train Epoch: 14 [91904/123872 (74%)]\tLoss: 0.384215\tLR: 0.00006040\n",
            "Train Epoch: 14 [92160/123872 (74%)]\tLoss: 0.398945\tLR: 0.00006039\n",
            "Train Epoch: 14 [92160/123872 (74%)]\tLoss: 0.398945\n",
            "Train Epoch: 14 [92416/123872 (75%)]\tLoss: 0.348260\tLR: 0.00006038\n",
            "Train Epoch: 14 [92672/123872 (75%)]\tLoss: 0.361471\tLR: 0.00006038\n",
            "Train Epoch: 14 [92928/123872 (75%)]\tLoss: 0.325955\tLR: 0.00006037\n",
            "Train Epoch: 14 [93184/123872 (75%)]\tLoss: 0.434212\tLR: 0.00006037\n",
            "Train Epoch: 14 [93440/123872 (75%)]\tLoss: 0.353009\tLR: 0.00006036\n",
            "Train Epoch: 14 [93696/123872 (76%)]\tLoss: 0.390415\tLR: 0.00006035\n",
            "Train Epoch: 14 [93952/123872 (76%)]\tLoss: 0.357152\tLR: 0.00006035\n",
            "Train Epoch: 14 [94208/123872 (76%)]\tLoss: 0.377001\tLR: 0.00006034\n",
            "Train Epoch: 14 [94464/123872 (76%)]\tLoss: 0.343623\tLR: 0.00006034\n",
            "Train Epoch: 14 [94720/123872 (76%)]\tLoss: 0.348354\tLR: 0.00006033\n",
            "Train Epoch: 14 [94720/123872 (76%)]\tLoss: 0.348354\n",
            "Train Epoch: 14 [94976/123872 (77%)]\tLoss: 0.319105\tLR: 0.00006032\n",
            "Train Epoch: 14 [95232/123872 (77%)]\tLoss: 0.390357\tLR: 0.00006032\n",
            "Train Epoch: 14 [95488/123872 (77%)]\tLoss: 0.403733\tLR: 0.00006031\n",
            "Train Epoch: 14 [95744/123872 (77%)]\tLoss: 0.333991\tLR: 0.00006031\n",
            "Train Epoch: 14 [96000/123872 (77%)]\tLoss: 0.345922\tLR: 0.00006030\n",
            "Train Epoch: 14 [96256/123872 (78%)]\tLoss: 0.329872\tLR: 0.00006030\n",
            "Train Epoch: 14 [96512/123872 (78%)]\tLoss: 0.380064\tLR: 0.00006029\n",
            "Train Epoch: 14 [96768/123872 (78%)]\tLoss: 0.351166\tLR: 0.00006028\n",
            "Train Epoch: 14 [97024/123872 (78%)]\tLoss: 0.381899\tLR: 0.00006028\n",
            "Train Epoch: 14 [97280/123872 (79%)]\tLoss: 0.304954\tLR: 0.00006027\n",
            "Train Epoch: 14 [97280/123872 (79%)]\tLoss: 0.304954\n",
            "Train Epoch: 14 [97536/123872 (79%)]\tLoss: 0.385134\tLR: 0.00006027\n",
            "Train Epoch: 14 [97792/123872 (79%)]\tLoss: 0.378001\tLR: 0.00006026\n",
            "Train Epoch: 14 [98048/123872 (79%)]\tLoss: 0.417127\tLR: 0.00006026\n",
            "Train Epoch: 14 [98304/123872 (79%)]\tLoss: 0.333082\tLR: 0.00006025\n",
            "Train Epoch: 14 [98560/123872 (80%)]\tLoss: 0.317531\tLR: 0.00006025\n",
            "Train Epoch: 14 [98816/123872 (80%)]\tLoss: 0.426280\tLR: 0.00006024\n",
            "Train Epoch: 14 [99072/123872 (80%)]\tLoss: 0.374657\tLR: 0.00006024\n",
            "Train Epoch: 14 [99328/123872 (80%)]\tLoss: 0.396707\tLR: 0.00006023\n",
            "Train Epoch: 14 [99584/123872 (80%)]\tLoss: 0.323760\tLR: 0.00006023\n",
            "Train Epoch: 14 [99840/123872 (81%)]\tLoss: 0.373945\tLR: 0.00006022\n",
            "Train Epoch: 14 [99840/123872 (81%)]\tLoss: 0.373945\n",
            "Train Epoch: 14 [100096/123872 (81%)]\tLoss: 0.388080\tLR: 0.00006022\n",
            "Train Epoch: 14 [100352/123872 (81%)]\tLoss: 0.367214\tLR: 0.00006021\n",
            "Train Epoch: 14 [100608/123872 (81%)]\tLoss: 0.412208\tLR: 0.00006021\n",
            "Train Epoch: 14 [100864/123872 (81%)]\tLoss: 0.376079\tLR: 0.00006020\n",
            "Train Epoch: 14 [101120/123872 (82%)]\tLoss: 0.350112\tLR: 0.00006020\n",
            "Train Epoch: 14 [101376/123872 (82%)]\tLoss: 0.411921\tLR: 0.00006020\n",
            "Train Epoch: 14 [101632/123872 (82%)]\tLoss: 0.334287\tLR: 0.00006019\n",
            "Train Epoch: 14 [101888/123872 (82%)]\tLoss: 0.376995\tLR: 0.00006019\n",
            "Train Epoch: 14 [102144/123872 (82%)]\tLoss: 0.303467\tLR: 0.00006018\n",
            "Train Epoch: 14 [102400/123872 (83%)]\tLoss: 0.307528\tLR: 0.00006018\n",
            "Train Epoch: 14 [102400/123872 (83%)]\tLoss: 0.307528\n",
            "Train Epoch: 14 [102656/123872 (83%)]\tLoss: 0.375368\tLR: 0.00006017\n",
            "Train Epoch: 14 [102912/123872 (83%)]\tLoss: 0.386438\tLR: 0.00006017\n",
            "Train Epoch: 14 [103168/123872 (83%)]\tLoss: 0.319167\tLR: 0.00006017\n",
            "Train Epoch: 14 [103424/123872 (83%)]\tLoss: 0.349242\tLR: 0.00006016\n",
            "Train Epoch: 14 [103680/123872 (84%)]\tLoss: 0.351785\tLR: 0.00006016\n",
            "Train Epoch: 14 [103936/123872 (84%)]\tLoss: 0.360825\tLR: 0.00006015\n",
            "Train Epoch: 14 [104192/123872 (84%)]\tLoss: 0.405801\tLR: 0.00006015\n",
            "Train Epoch: 14 [104448/123872 (84%)]\tLoss: 0.345705\tLR: 0.00006015\n",
            "Train Epoch: 14 [104704/123872 (85%)]\tLoss: 0.345477\tLR: 0.00006014\n",
            "Train Epoch: 14 [104960/123872 (85%)]\tLoss: 0.357021\tLR: 0.00006014\n",
            "Train Epoch: 14 [104960/123872 (85%)]\tLoss: 0.357021\n",
            "Train Epoch: 14 [105216/123872 (85%)]\tLoss: 0.325708\tLR: 0.00006013\n",
            "Train Epoch: 14 [105472/123872 (85%)]\tLoss: 0.426776\tLR: 0.00006013\n",
            "Train Epoch: 14 [105728/123872 (85%)]\tLoss: 0.298329\tLR: 0.00006013\n",
            "Train Epoch: 14 [105984/123872 (86%)]\tLoss: 0.334264\tLR: 0.00006012\n",
            "Train Epoch: 14 [106240/123872 (86%)]\tLoss: 0.377416\tLR: 0.00006012\n",
            "Train Epoch: 14 [106496/123872 (86%)]\tLoss: 0.404975\tLR: 0.00006012\n",
            "Train Epoch: 14 [106752/123872 (86%)]\tLoss: 0.405340\tLR: 0.00006011\n",
            "Train Epoch: 14 [107008/123872 (86%)]\tLoss: 0.355532\tLR: 0.00006011\n",
            "Train Epoch: 14 [107264/123872 (87%)]\tLoss: 0.309572\tLR: 0.00006011\n",
            "Train Epoch: 14 [107520/123872 (87%)]\tLoss: 0.381611\tLR: 0.00006010\n",
            "Train Epoch: 14 [107520/123872 (87%)]\tLoss: 0.381611\n",
            "Train Epoch: 14 [107776/123872 (87%)]\tLoss: 0.349652\tLR: 0.00006010\n",
            "Train Epoch: 14 [108032/123872 (87%)]\tLoss: 0.382802\tLR: 0.00006010\n",
            "Train Epoch: 14 [108288/123872 (87%)]\tLoss: 0.382889\tLR: 0.00006009\n",
            "Train Epoch: 14 [108544/123872 (88%)]\tLoss: 0.355358\tLR: 0.00006009\n",
            "Train Epoch: 14 [108800/123872 (88%)]\tLoss: 0.317790\tLR: 0.00006009\n",
            "Train Epoch: 14 [109056/123872 (88%)]\tLoss: 0.362557\tLR: 0.00006008\n",
            "Train Epoch: 14 [109312/123872 (88%)]\tLoss: 0.355042\tLR: 0.00006008\n",
            "Train Epoch: 14 [109568/123872 (88%)]\tLoss: 0.364360\tLR: 0.00006008\n",
            "Train Epoch: 14 [109824/123872 (89%)]\tLoss: 0.347725\tLR: 0.00006008\n",
            "Train Epoch: 14 [110080/123872 (89%)]\tLoss: 0.431577\tLR: 0.00006007\n",
            "Train Epoch: 14 [110080/123872 (89%)]\tLoss: 0.431577\n",
            "Train Epoch: 14 [110336/123872 (89%)]\tLoss: 0.339183\tLR: 0.00006007\n",
            "Train Epoch: 14 [110592/123872 (89%)]\tLoss: 0.380868\tLR: 0.00006007\n",
            "Train Epoch: 14 [110848/123872 (89%)]\tLoss: 0.340232\tLR: 0.00006006\n",
            "Train Epoch: 14 [111104/123872 (90%)]\tLoss: 0.371270\tLR: 0.00006006\n",
            "Train Epoch: 14 [111360/123872 (90%)]\tLoss: 0.363129\tLR: 0.00006006\n",
            "Train Epoch: 14 [111616/123872 (90%)]\tLoss: 0.343286\tLR: 0.00006006\n",
            "Train Epoch: 14 [111872/123872 (90%)]\tLoss: 0.335871\tLR: 0.00006005\n",
            "Train Epoch: 14 [112128/123872 (90%)]\tLoss: 0.398129\tLR: 0.00006005\n",
            "Train Epoch: 14 [112384/123872 (91%)]\tLoss: 0.329740\tLR: 0.00006005\n",
            "Train Epoch: 14 [112640/123872 (91%)]\tLoss: 0.419653\tLR: 0.00006005\n",
            "Train Epoch: 14 [112640/123872 (91%)]\tLoss: 0.419653\n",
            "Train Epoch: 14 [112896/123872 (91%)]\tLoss: 0.457660\tLR: 0.00006005\n",
            "Train Epoch: 14 [113152/123872 (91%)]\tLoss: 0.351228\tLR: 0.00006004\n",
            "Train Epoch: 14 [113408/123872 (92%)]\tLoss: 0.417598\tLR: 0.00006004\n",
            "Train Epoch: 14 [113664/123872 (92%)]\tLoss: 0.325934\tLR: 0.00006004\n",
            "Train Epoch: 14 [113920/123872 (92%)]\tLoss: 0.453549\tLR: 0.00006004\n",
            "Train Epoch: 14 [114176/123872 (92%)]\tLoss: 0.394708\tLR: 0.00006004\n",
            "Train Epoch: 14 [114432/123872 (92%)]\tLoss: 0.381884\tLR: 0.00006003\n",
            "Train Epoch: 14 [114688/123872 (93%)]\tLoss: 0.361879\tLR: 0.00006003\n",
            "Train Epoch: 14 [114944/123872 (93%)]\tLoss: 0.324269\tLR: 0.00006003\n",
            "Train Epoch: 14 [115200/123872 (93%)]\tLoss: 0.395629\tLR: 0.00006003\n",
            "Train Epoch: 14 [115200/123872 (93%)]\tLoss: 0.395629\n",
            "Train Epoch: 14 [115456/123872 (93%)]\tLoss: 0.317622\tLR: 0.00006003\n",
            "Train Epoch: 14 [115712/123872 (93%)]\tLoss: 0.340864\tLR: 0.00006002\n",
            "Train Epoch: 14 [115968/123872 (94%)]\tLoss: 0.414526\tLR: 0.00006002\n",
            "Train Epoch: 14 [116224/123872 (94%)]\tLoss: 0.368199\tLR: 0.00006002\n",
            "Train Epoch: 14 [116480/123872 (94%)]\tLoss: 0.426704\tLR: 0.00006002\n",
            "Train Epoch: 14 [116736/123872 (94%)]\tLoss: 0.304284\tLR: 0.00006002\n",
            "Train Epoch: 14 [116992/123872 (94%)]\tLoss: 0.354831\tLR: 0.00006002\n",
            "Train Epoch: 14 [117248/123872 (95%)]\tLoss: 0.321763\tLR: 0.00006002\n",
            "Train Epoch: 14 [117504/123872 (95%)]\tLoss: 0.444056\tLR: 0.00006001\n",
            "Train Epoch: 14 [117760/123872 (95%)]\tLoss: 0.391620\tLR: 0.00006001\n",
            "Train Epoch: 14 [117760/123872 (95%)]\tLoss: 0.391620\n",
            "Train Epoch: 14 [118016/123872 (95%)]\tLoss: 0.348156\tLR: 0.00006001\n",
            "Train Epoch: 14 [118272/123872 (95%)]\tLoss: 0.388092\tLR: 0.00006001\n",
            "Train Epoch: 14 [118528/123872 (96%)]\tLoss: 0.385498\tLR: 0.00006001\n",
            "Train Epoch: 14 [118784/123872 (96%)]\tLoss: 0.389115\tLR: 0.00006001\n",
            "Train Epoch: 14 [119040/123872 (96%)]\tLoss: 0.400981\tLR: 0.00006001\n",
            "Train Epoch: 14 [119296/123872 (96%)]\tLoss: 0.349135\tLR: 0.00006001\n",
            "Train Epoch: 14 [119552/123872 (96%)]\tLoss: 0.376841\tLR: 0.00006001\n",
            "Train Epoch: 14 [119808/123872 (97%)]\tLoss: 0.335948\tLR: 0.00006001\n",
            "Train Epoch: 14 [120064/123872 (97%)]\tLoss: 0.354776\tLR: 0.00006001\n",
            "Train Epoch: 14 [120320/123872 (97%)]\tLoss: 0.391556\tLR: 0.00006000\n",
            "Train Epoch: 14 [120320/123872 (97%)]\tLoss: 0.391556\n",
            "Train Epoch: 14 [120576/123872 (97%)]\tLoss: 0.325735\tLR: 0.00006000\n",
            "Train Epoch: 14 [120832/123872 (98%)]\tLoss: 0.335002\tLR: 0.00006000\n",
            "Train Epoch: 14 [121088/123872 (98%)]\tLoss: 0.326026\tLR: 0.00006000\n",
            "Train Epoch: 14 [121344/123872 (98%)]\tLoss: 0.353205\tLR: 0.00006000\n",
            "Train Epoch: 14 [121600/123872 (98%)]\tLoss: 0.394230\tLR: 0.00006000\n",
            "Train Epoch: 14 [121856/123872 (98%)]\tLoss: 0.401598\tLR: 0.00006000\n",
            "Train Epoch: 14 [122112/123872 (99%)]\tLoss: 0.356252\tLR: 0.00006000\n",
            "Train Epoch: 14 [122368/123872 (99%)]\tLoss: 0.351595\tLR: 0.00006000\n",
            "Train Epoch: 14 [122624/123872 (99%)]\tLoss: 0.325465\tLR: 0.00006000\n",
            "Train Epoch: 14 [122880/123872 (99%)]\tLoss: 0.330044\tLR: 0.00006000\n",
            "Train Epoch: 14 [122880/123872 (99%)]\tLoss: 0.330044\n",
            "Train Epoch: 14 [123136/123872 (99%)]\tLoss: 0.261102\tLR: 0.00006000\n",
            "Train Epoch: 14 [123392/123872 (100%)]\tLoss: 0.321704\tLR: 0.00006000\n",
            "Train Epoch: 14 [108192/123872 (100%)]\tLoss: 0.364496\tLR: 0.00006000\n",
            "\n",
            "Test set: Average loss: 0.0015, Accuracy: 25583/30970 (82.61%)\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to trained_model_15_human_enhancers_ensembl_22_1M.pt\n",
            "Accuracies saved to accuracies_15_human_enhancers_ensembl_22_1M.json\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train Epoch: 5 [83712/123872 (68%)]\tLoss: 0.462693\tLR: 0.00043545\n",
            "Train Epoch: 5 [83968/123872 (68%)]\tLoss: 0.416194\tLR: 0.00043534\n",
            "Train Epoch: 5 [84224/123872 (68%)]\tLoss: 0.458452\tLR: 0.00043523\n",
            "Train Epoch: 5 [84480/123872 (68%)]\tLoss: 0.477435\tLR: 0.00043512\n",
            "Train Epoch: 5 [84480/123872 (68%)]\tLoss: 0.477435\n",
            "Train Epoch: 5 [84736/123872 (68%)]\tLoss: 0.449231\tLR: 0.00043501\n",
            "Train Epoch: 5 [84992/123872 (69%)]\tLoss: 0.424831\tLR: 0.00043490\n",
            "Train Epoch: 5 [85248/123872 (69%)]\tLoss: 0.415082\tLR: 0.00043479\n",
            "Train Epoch: 5 [85504/123872 (69%)]\tLoss: 0.442300\tLR: 0.00043469\n",
            "Train Epoch: 5 [85760/123872 (69%)]\tLoss: 0.421714\tLR: 0.00043458\n",
            "Train Epoch: 5 [86016/123872 (69%)]\tLoss: 0.428402\tLR: 0.00043447\n",
            "Train Epoch: 5 [86272/123872 (70%)]\tLoss: 0.438274\tLR: 0.00043436\n",
            "Train Epoch: 5 [86528/123872 (70%)]\tLoss: 0.343986\tLR: 0.00043425\n",
            "Train Epoch: 5 [86784/123872 (70%)]\tLoss: 0.399783\tLR: 0.00043414\n",
            "Train Epoch: 5 [87040/123872 (70%)]\tLoss: 0.371602\tLR: 0.00043403\n",
            "Train Epoch: 5 [87040/123872 (70%)]\tLoss: 0.371602\n",
            "Train Epoch: 5 [87296/123872 (70%)]\tLoss: 0.408145\tLR: 0.00043392\n",
            "Train Epoch: 5 [87552/123872 (71%)]\tLoss: 0.458357\tLR: 0.00043381\n",
            "Train Epoch: 5 [87808/123872 (71%)]\tLoss: 0.468651\tLR: 0.00043371\n",
            "Train Epoch: 5 [88064/123872 (71%)]\tLoss: 0.382819\tLR: 0.00043360\n",
            "Train Epoch: 5 [88320/123872 (71%)]\tLoss: 0.391993\tLR: 0.00043349\n",
            "Train Epoch: 5 [88576/123872 (71%)]\tLoss: 0.349656\tLR: 0.00043338\n",
            "Train Epoch: 5 [88832/123872 (72%)]\tLoss: 0.430115\tLR: 0.00043327\n",
            "Train Epoch: 5 [89088/123872 (72%)]\tLoss: 0.402636\tLR: 0.00043316\n",
            "Train Epoch: 5 [89344/123872 (72%)]\tLoss: 0.381897\tLR: 0.00043305\n",
            "Train Epoch: 5 [89600/123872 (72%)]\tLoss: 0.359315\tLR: 0.00043294\n",
            "Train Epoch: 5 [89600/123872 (72%)]\tLoss: 0.359315\n",
            "Train Epoch: 5 [89856/123872 (73%)]\tLoss: 0.324460\tLR: 0.00043283\n",
            "Train Epoch: 5 [90112/123872 (73%)]\tLoss: 0.357992\tLR: 0.00043272\n",
            "Train Epoch: 5 [90368/123872 (73%)]\tLoss: 0.493398\tLR: 0.00043262\n",
            "Train Epoch: 5 [90624/123872 (73%)]\tLoss: 0.378392\tLR: 0.00043251\n",
            "Train Epoch: 5 [90880/123872 (73%)]\tLoss: 0.394050\tLR: 0.00043240\n",
            "Train Epoch: 5 [91136/123872 (74%)]\tLoss: 0.400572\tLR: 0.00043229\n",
            "Train Epoch: 5 [91392/123872 (74%)]\tLoss: 0.427315\tLR: 0.00043218\n",
            "Train Epoch: 5 [91648/123872 (74%)]\tLoss: 0.506325\tLR: 0.00043207\n",
            "Train Epoch: 5 [91904/123872 (74%)]\tLoss: 0.443109\tLR: 0.00043196\n",
            "Train Epoch: 5 [92160/123872 (74%)]\tLoss: 0.391079\tLR: 0.00043185\n",
            "Train Epoch: 5 [92160/123872 (74%)]\tLoss: 0.391079\n",
            "Train Epoch: 5 [92416/123872 (75%)]\tLoss: 0.386659\tLR: 0.00043174\n",
            "Train Epoch: 5 [92672/123872 (75%)]\tLoss: 0.410934\tLR: 0.00043163\n",
            "Train Epoch: 5 [92928/123872 (75%)]\tLoss: 0.389492\tLR: 0.00043152\n",
            "Train Epoch: 5 [93184/123872 (75%)]\tLoss: 0.443022\tLR: 0.00043141\n",
            "Train Epoch: 5 [93440/123872 (75%)]\tLoss: 0.429854\tLR: 0.00043130\n",
            "Train Epoch: 5 [93696/123872 (76%)]\tLoss: 0.410834\tLR: 0.00043119\n",
            "Train Epoch: 5 [93952/123872 (76%)]\tLoss: 0.390009\tLR: 0.00043109\n",
            "Train Epoch: 5 [94208/123872 (76%)]\tLoss: 0.457232\tLR: 0.00043098\n",
            "Train Epoch: 5 [94464/123872 (76%)]\tLoss: 0.419042\tLR: 0.00043087\n",
            "Train Epoch: 5 [94720/123872 (76%)]\tLoss: 0.388154\tLR: 0.00043076\n",
            "Train Epoch: 5 [94720/123872 (76%)]\tLoss: 0.388154\n",
            "Train Epoch: 5 [94976/123872 (77%)]\tLoss: 0.392846\tLR: 0.00043065\n",
            "Train Epoch: 5 [95232/123872 (77%)]\tLoss: 0.365332\tLR: 0.00043054\n",
            "Train Epoch: 5 [95488/123872 (77%)]\tLoss: 0.426662\tLR: 0.00043043\n",
            "Train Epoch: 5 [95744/123872 (77%)]\tLoss: 0.463740\tLR: 0.00043032\n",
            "Train Epoch: 5 [96000/123872 (77%)]\tLoss: 0.478834\tLR: 0.00043021\n",
            "Train Epoch: 5 [96256/123872 (78%)]\tLoss: 0.438968\tLR: 0.00043010\n",
            "Train Epoch: 5 [96512/123872 (78%)]\tLoss: 0.444187\tLR: 0.00042999\n",
            "Train Epoch: 5 [96768/123872 (78%)]\tLoss: 0.437060\tLR: 0.00042988\n",
            "Train Epoch: 5 [97024/123872 (78%)]\tLoss: 0.441310\tLR: 0.00042977\n",
            "Train Epoch: 5 [97280/123872 (79%)]\tLoss: 0.455762\tLR: 0.00042966\n",
            "Train Epoch: 5 [97280/123872 (79%)]\tLoss: 0.455762\n",
            "Train Epoch: 5 [97536/123872 (79%)]\tLoss: 0.464432\tLR: 0.00042955\n",
            "Train Epoch: 5 [97792/123872 (79%)]\tLoss: 0.438778\tLR: 0.00042944\n",
            "Train Epoch: 5 [98048/123872 (79%)]\tLoss: 0.467123\tLR: 0.00042933\n",
            "Train Epoch: 5 [98304/123872 (79%)]\tLoss: 0.491967\tLR: 0.00042922\n",
            "Train Epoch: 5 [98560/123872 (80%)]\tLoss: 0.411779\tLR: 0.00042911\n",
            "Train Epoch: 5 [98816/123872 (80%)]\tLoss: 0.404413\tLR: 0.00042900\n",
            "Train Epoch: 5 [99072/123872 (80%)]\tLoss: 0.452299\tLR: 0.00042889\n",
            "Train Epoch: 5 [99328/123872 (80%)]\tLoss: 0.426488\tLR: 0.00042878\n",
            "Train Epoch: 5 [99584/123872 (80%)]\tLoss: 0.434692\tLR: 0.00042867\n",
            "Train Epoch: 5 [99840/123872 (81%)]\tLoss: 0.423061\tLR: 0.00042856\n",
            "Train Epoch: 5 [99840/123872 (81%)]\tLoss: 0.423061\n",
            "Train Epoch: 5 [100096/123872 (81%)]\tLoss: 0.449253\tLR: 0.00042845\n",
            "Train Epoch: 5 [100352/123872 (81%)]\tLoss: 0.374618\tLR: 0.00042834\n",
            "Train Epoch: 5 [100608/123872 (81%)]\tLoss: 0.442739\tLR: 0.00042823\n",
            "Train Epoch: 5 [100864/123872 (81%)]\tLoss: 0.448247\tLR: 0.00042812\n",
            "Train Epoch: 5 [101120/123872 (82%)]\tLoss: 0.462122\tLR: 0.00042801\n",
            "Train Epoch: 5 [101376/123872 (82%)]\tLoss: 0.444923\tLR: 0.00042790\n",
            "Train Epoch: 5 [101632/123872 (82%)]\tLoss: 0.447420\tLR: 0.00042779\n",
            "Train Epoch: 5 [101888/123872 (82%)]\tLoss: 0.450730\tLR: 0.00042768\n",
            "Train Epoch: 5 [102144/123872 (82%)]\tLoss: 0.434817\tLR: 0.00042757\n",
            "Train Epoch: 5 [102400/123872 (83%)]\tLoss: 0.432188\tLR: 0.00042746\n",
            "Train Epoch: 5 [102400/123872 (83%)]\tLoss: 0.432188\n",
            "Train Epoch: 5 [102656/123872 (83%)]\tLoss: 0.415407\tLR: 0.00042735\n",
            "Train Epoch: 5 [102912/123872 (83%)]\tLoss: 0.418292\tLR: 0.00042724\n",
            "Train Epoch: 5 [103168/123872 (83%)]\tLoss: 0.444286\tLR: 0.00042713\n",
            "Train Epoch: 5 [103424/123872 (83%)]\tLoss: 0.463217\tLR: 0.00042702\n",
            "Train Epoch: 5 [103680/123872 (84%)]\tLoss: 0.402553\tLR: 0.00042691\n",
            "Train Epoch: 5 [103936/123872 (84%)]\tLoss: 0.459860\tLR: 0.00042680\n",
            "Train Epoch: 5 [104192/123872 (84%)]\tLoss: 0.457585\tLR: 0.00042669\n",
            "Train Epoch: 5 [104448/123872 (84%)]\tLoss: 0.442848\tLR: 0.00042658\n",
            "Train Epoch: 5 [104704/123872 (85%)]\tLoss: 0.371758\tLR: 0.00042647\n",
            "Train Epoch: 5 [104960/123872 (85%)]\tLoss: 0.375725\tLR: 0.00042636\n",
            "Train Epoch: 5 [104960/123872 (85%)]\tLoss: 0.375725\n",
            "Train Epoch: 5 [105216/123872 (85%)]\tLoss: 0.516453\tLR: 0.00042625\n",
            "Train Epoch: 5 [105472/123872 (85%)]\tLoss: 0.371893\tLR: 0.00042614\n",
            "Train Epoch: 5 [105728/123872 (85%)]\tLoss: 0.424810\tLR: 0.00042603\n",
            "Train Epoch: 5 [105984/123872 (86%)]\tLoss: 0.469558\tLR: 0.00042592\n",
            "Train Epoch: 5 [106240/123872 (86%)]\tLoss: 0.454664\tLR: 0.00042581\n",
            "Train Epoch: 5 [106496/123872 (86%)]\tLoss: 0.424978\tLR: 0.00042570\n",
            "Train Epoch: 5 [106752/123872 (86%)]\tLoss: 0.411538\tLR: 0.00042559\n",
            "Train Epoch: 5 [107008/123872 (86%)]\tLoss: 0.411884\tLR: 0.00042548\n",
            "Train Epoch: 5 [107264/123872 (87%)]\tLoss: 0.491889\tLR: 0.00042537\n",
            "Train Epoch: 5 [107520/123872 (87%)]\tLoss: 0.447924\tLR: 0.00042526\n",
            "Train Epoch: 5 [107520/123872 (87%)]\tLoss: 0.447924\n",
            "Train Epoch: 5 [107776/123872 (87%)]\tLoss: 0.419828\tLR: 0.00042515\n",
            "Train Epoch: 5 [108032/123872 (87%)]\tLoss: 0.460300\tLR: 0.00042504\n",
            "Train Epoch: 5 [108288/123872 (87%)]\tLoss: 0.409756\tLR: 0.00042493\n",
            "Train Epoch: 5 [108544/123872 (88%)]\tLoss: 0.409957\tLR: 0.00042482\n",
            "Train Epoch: 5 [108800/123872 (88%)]\tLoss: 0.411403\tLR: 0.00042471\n",
            "Train Epoch: 5 [109056/123872 (88%)]\tLoss: 0.349421\tLR: 0.00042460\n",
            "Train Epoch: 5 [109312/123872 (88%)]\tLoss: 0.404966\tLR: 0.00042449\n",
            "Train Epoch: 5 [109568/123872 (88%)]\tLoss: 0.362671\tLR: 0.00042438\n",
            "Train Epoch: 5 [109824/123872 (89%)]\tLoss: 0.460506\tLR: 0.00042426\n",
            "Train Epoch: 5 [110080/123872 (89%)]\tLoss: 0.378409\tLR: 0.00042415\n",
            "Train Epoch: 5 [110080/123872 (89%)]\tLoss: 0.378409\n",
            "Train Epoch: 5 [110336/123872 (89%)]\tLoss: 0.463863\tLR: 0.00042404\n",
            "Train Epoch: 5 [110592/123872 (89%)]\tLoss: 0.442366\tLR: 0.00042393\n",
            "Train Epoch: 5 [110848/123872 (89%)]\tLoss: 0.426257\tLR: 0.00042382\n",
            "Train Epoch: 5 [111104/123872 (90%)]\tLoss: 0.415826\tLR: 0.00042371\n",
            "Train Epoch: 5 [111360/123872 (90%)]\tLoss: 0.433463\tLR: 0.00042360\n",
            "Train Epoch: 5 [111616/123872 (90%)]\tLoss: 0.411778\tLR: 0.00042349\n",
            "Train Epoch: 5 [111872/123872 (90%)]\tLoss: 0.431673\tLR: 0.00042338\n",
            "Train Epoch: 5 [112128/123872 (90%)]\tLoss: 0.400079\tLR: 0.00042327\n",
            "Train Epoch: 5 [112384/123872 (91%)]\tLoss: 0.409253\tLR: 0.00042316\n",
            "Train Epoch: 5 [112640/123872 (91%)]\tLoss: 0.506484\tLR: 0.00042305\n",
            "Train Epoch: 5 [112640/123872 (91%)]\tLoss: 0.506484\n",
            "Train Epoch: 5 [112896/123872 (91%)]\tLoss: 0.424890\tLR: 0.00042294\n",
            "Train Epoch: 5 [113152/123872 (91%)]\tLoss: 0.433508\tLR: 0.00042283\n",
            "Train Epoch: 5 [113408/123872 (92%)]\tLoss: 0.387370\tLR: 0.00042271\n",
            "Train Epoch: 5 [113664/123872 (92%)]\tLoss: 0.448873\tLR: 0.00042260\n",
            "Train Epoch: 5 [113920/123872 (92%)]\tLoss: 0.416246\tLR: 0.00042249\n",
            "Train Epoch: 5 [114176/123872 (92%)]\tLoss: 0.457913\tLR: 0.00042238\n",
            "Train Epoch: 5 [114432/123872 (92%)]\tLoss: 0.437678\tLR: 0.00042227\n",
            "Train Epoch: 5 [114688/123872 (93%)]\tLoss: 0.427218\tLR: 0.00042216\n",
            "Train Epoch: 5 [114944/123872 (93%)]\tLoss: 0.410142\tLR: 0.00042205\n",
            "Train Epoch: 5 [115200/123872 (93%)]\tLoss: 0.404640\tLR: 0.00042194\n",
            "Train Epoch: 5 [115200/123872 (93%)]\tLoss: 0.404640\n",
            "Train Epoch: 5 [115456/123872 (93%)]\tLoss: 0.358595\tLR: 0.00042183\n",
            "Train Epoch: 5 [115712/123872 (93%)]\tLoss: 0.420546\tLR: 0.00042172\n",
            "Train Epoch: 5 [115968/123872 (94%)]\tLoss: 0.448630\tLR: 0.00042161\n",
            "Train Epoch: 5 [116224/123872 (94%)]\tLoss: 0.473984\tLR: 0.00042149\n",
            "Train Epoch: 5 [116480/123872 (94%)]\tLoss: 0.393899\tLR: 0.00042138\n",
            "Train Epoch: 5 [116736/123872 (94%)]\tLoss: 0.325584\tLR: 0.00042127\n",
            "Train Epoch: 5 [116992/123872 (94%)]\tLoss: 0.451789\tLR: 0.00042116\n",
            "Train Epoch: 5 [117248/123872 (95%)]\tLoss: 0.449116\tLR: 0.00042105\n",
            "Train Epoch: 5 [117504/123872 (95%)]\tLoss: 0.462345\tLR: 0.00042094\n",
            "Train Epoch: 5 [117760/123872 (95%)]\tLoss: 0.486795\tLR: 0.00042083\n",
            "Train Epoch: 5 [117760/123872 (95%)]\tLoss: 0.486795\n",
            "Train Epoch: 5 [118016/123872 (95%)]\tLoss: 0.467272\tLR: 0.00042072\n",
            "Train Epoch: 5 [118272/123872 (95%)]\tLoss: 0.421936\tLR: 0.00042061\n",
            "Train Epoch: 5 [118528/123872 (96%)]\tLoss: 0.379948\tLR: 0.00042049\n",
            "Train Epoch: 5 [118784/123872 (96%)]\tLoss: 0.446666\tLR: 0.00042038\n",
            "Train Epoch: 5 [119040/123872 (96%)]\tLoss: 0.402731\tLR: 0.00042027\n",
            "Train Epoch: 5 [119296/123872 (96%)]\tLoss: 0.448608\tLR: 0.00042016\n",
            "Train Epoch: 5 [119552/123872 (96%)]\tLoss: 0.419107\tLR: 0.00042005\n",
            "Train Epoch: 5 [119808/123872 (97%)]\tLoss: 0.396662\tLR: 0.00041994\n",
            "Train Epoch: 5 [120064/123872 (97%)]\tLoss: 0.369955\tLR: 0.00041983\n",
            "Train Epoch: 5 [120320/123872 (97%)]\tLoss: 0.462052\tLR: 0.00041972\n",
            "Train Epoch: 5 [120320/123872 (97%)]\tLoss: 0.462052\n",
            "Train Epoch: 5 [120576/123872 (97%)]\tLoss: 0.381725\tLR: 0.00041960\n",
            "Train Epoch: 5 [120832/123872 (98%)]\tLoss: 0.368879\tLR: 0.00041949\n",
            "Train Epoch: 5 [121088/123872 (98%)]\tLoss: 0.400360\tLR: 0.00041938\n",
            "Train Epoch: 5 [121344/123872 (98%)]\tLoss: 0.437626\tLR: 0.00041927\n",
            "Train Epoch: 5 [121600/123872 (98%)]\tLoss: 0.412980\tLR: 0.00041916\n",
            "Train Epoch: 5 [121856/123872 (98%)]\tLoss: 0.397173\tLR: 0.00041905\n",
            "Train Epoch: 5 [122112/123872 (99%)]\tLoss: 0.512782\tLR: 0.00041894\n",
            "Train Epoch: 5 [122368/123872 (99%)]\tLoss: 0.444951\tLR: 0.00041883\n",
            "Train Epoch: 5 [122624/123872 (99%)]\tLoss: 0.397433\tLR: 0.00041871\n",
            "Train Epoch: 5 [122880/123872 (99%)]\tLoss: 0.403333\tLR: 0.00041860\n",
            "Train Epoch: 5 [122880/123872 (99%)]\tLoss: 0.403333\n",
            "Train Epoch: 5 [123136/123872 (99%)]\tLoss: 0.407494\tLR: 0.00041849\n",
            "Train Epoch: 5 [123392/123872 (100%)]\tLoss: 0.340012\tLR: 0.00041838\n",
            "Train Epoch: 5 [108192/123872 (100%)]\tLoss: 0.417826\tLR: 0.00041827\n",
            "\n",
            "Test set: Average loss: 0.0016, Accuracy: 25059/30970 (80.91%)\n",
            "\n",
            "Train Epoch: 6 [0/123872 (0%)]\tLoss: 0.427306\tLR: 0.00041816\n",
            "Train Epoch: 6 [0/123872 (0%)]\tLoss: 0.427306\n",
            "Train Epoch: 6 [256/123872 (0%)]\tLoss: 0.369876\tLR: 0.00041804\n",
            "Train Epoch: 6 [512/123872 (0%)]\tLoss: 0.375715\tLR: 0.00041793\n",
            "Train Epoch: 6 [768/123872 (1%)]\tLoss: 0.446459\tLR: 0.00041782\n",
            "Train Epoch: 6 [1024/123872 (1%)]\tLoss: 0.378915\tLR: 0.00041771\n",
            "Train Epoch: 6 [1280/123872 (1%)]\tLoss: 0.381589\tLR: 0.00041760\n",
            "Train Epoch: 6 [1536/123872 (1%)]\tLoss: 0.473920\tLR: 0.00041749\n",
            "Train Epoch: 6 [1792/123872 (1%)]\tLoss: 0.451346\tLR: 0.00041738\n",
            "Train Epoch: 6 [2048/123872 (2%)]\tLoss: 0.446427\tLR: 0.00041726\n",
            "Train Epoch: 6 [2304/123872 (2%)]\tLoss: 0.383980\tLR: 0.00041715\n",
            "Train Epoch: 6 [2560/123872 (2%)]\tLoss: 0.440649\tLR: 0.00041704\n",
            "Train Epoch: 6 [2560/123872 (2%)]\tLoss: 0.440649\n",
            "Train Epoch: 6 [2816/123872 (2%)]\tLoss: 0.435035\tLR: 0.00041693\n",
            "Train Epoch: 6 [3072/123872 (2%)]\tLoss: 0.434184\tLR: 0.00041682\n",
            "Train Epoch: 6 [3328/123872 (3%)]\tLoss: 0.411148\tLR: 0.00041670\n",
            "Train Epoch: 6 [3584/123872 (3%)]\tLoss: 0.380299\tLR: 0.00041659\n",
            "Train Epoch: 6 [3840/123872 (3%)]\tLoss: 0.436076\tLR: 0.00041648\n",
            "Train Epoch: 6 [4096/123872 (3%)]\tLoss: 0.432484\tLR: 0.00041637\n",
            "Train Epoch: 6 [4352/123872 (4%)]\tLoss: 0.372300\tLR: 0.00041626\n",
            "Train Epoch: 6 [4608/123872 (4%)]\tLoss: 0.459888\tLR: 0.00041615\n",
            "Train Epoch: 6 [4864/123872 (4%)]\tLoss: 0.395209\tLR: 0.00041603\n",
            "Train Epoch: 6 [5120/123872 (4%)]\tLoss: 0.457088\tLR: 0.00041592\n",
            "Train Epoch: 6 [5120/123872 (4%)]\tLoss: 0.457088\n",
            "Train Epoch: 6 [5376/123872 (4%)]\tLoss: 0.413409\tLR: 0.00041581\n",
            "Train Epoch: 6 [5632/123872 (5%)]\tLoss: 0.387860\tLR: 0.00041570\n",
            "Train Epoch: 6 [5888/123872 (5%)]\tLoss: 0.400007\tLR: 0.00041559\n",
            "Train Epoch: 6 [6144/123872 (5%)]\tLoss: 0.385812\tLR: 0.00041547\n",
            "Train Epoch: 6 [6400/123872 (5%)]\tLoss: 0.375550\tLR: 0.00041536\n",
            "Train Epoch: 6 [6656/123872 (5%)]\tLoss: 0.404197\tLR: 0.00041525\n",
            "Train Epoch: 6 [6912/123872 (6%)]\tLoss: 0.509509\tLR: 0.00041514\n",
            "Train Epoch: 6 [7168/123872 (6%)]\tLoss: 0.456338\tLR: 0.00041503\n",
            "Train Epoch: 6 [7424/123872 (6%)]\tLoss: 0.404539\tLR: 0.00041491\n",
            "Train Epoch: 6 [7680/123872 (6%)]\tLoss: 0.430721\tLR: 0.00041480\n",
            "Train Epoch: 6 [7680/123872 (6%)]\tLoss: 0.430721\n",
            "Train Epoch: 6 [7936/123872 (6%)]\tLoss: 0.354793\tLR: 0.00041469\n",
            "Train Epoch: 6 [8192/123872 (7%)]\tLoss: 0.429880\tLR: 0.00041458\n",
            "Train Epoch: 6 [8448/123872 (7%)]\tLoss: 0.416095\tLR: 0.00041447\n",
            "Train Epoch: 6 [8704/123872 (7%)]\tLoss: 0.412381\tLR: 0.00041435\n",
            "Train Epoch: 6 [8960/123872 (7%)]\tLoss: 0.493845\tLR: 0.00041424\n",
            "Train Epoch: 6 [9216/123872 (7%)]\tLoss: 0.400070\tLR: 0.00041413\n",
            "Train Epoch: 6 [9472/123872 (8%)]\tLoss: 0.440882\tLR: 0.00041402\n",
            "Train Epoch: 6 [9728/123872 (8%)]\tLoss: 0.370056\tLR: 0.00041391\n",
            "Train Epoch: 6 [9984/123872 (8%)]\tLoss: 0.364869\tLR: 0.00041379\n",
            "Train Epoch: 6 [10240/123872 (8%)]\tLoss: 0.368708\tLR: 0.00041368\n",
            "Train Epoch: 6 [10240/123872 (8%)]\tLoss: 0.368708\n",
            "Train Epoch: 6 [10496/123872 (8%)]\tLoss: 0.444195\tLR: 0.00041357\n",
            "Train Epoch: 6 [10752/123872 (9%)]\tLoss: 0.460825\tLR: 0.00041346\n",
            "Train Epoch: 6 [11008/123872 (9%)]\tLoss: 0.361744\tLR: 0.00041334\n",
            "Train Epoch: 6 [11264/123872 (9%)]\tLoss: 0.385864\tLR: 0.00041323\n",
            "Train Epoch: 6 [11520/123872 (9%)]\tLoss: 0.411900\tLR: 0.00041312\n",
            "Train Epoch: 6 [11776/123872 (10%)]\tLoss: 0.447419\tLR: 0.00041301\n",
            "Train Epoch: 6 [12032/123872 (10%)]\tLoss: 0.426043\tLR: 0.00041290\n",
            "Train Epoch: 6 [12288/123872 (10%)]\tLoss: 0.455305\tLR: 0.00041278\n",
            "Train Epoch: 6 [12544/123872 (10%)]\tLoss: 0.462381\tLR: 0.00041267\n",
            "Train Epoch: 6 [12800/123872 (10%)]\tLoss: 0.393938\tLR: 0.00041256\n",
            "Train Epoch: 6 [12800/123872 (10%)]\tLoss: 0.393938\n",
            "Train Epoch: 6 [13056/123872 (11%)]\tLoss: 0.452016\tLR: 0.00041245\n",
            "Train Epoch: 6 [13312/123872 (11%)]\tLoss: 0.368076\tLR: 0.00041233\n",
            "Train Epoch: 6 [13568/123872 (11%)]\tLoss: 0.449107\tLR: 0.00041222\n",
            "Train Epoch: 6 [13824/123872 (11%)]\tLoss: 0.430414\tLR: 0.00041211\n",
            "Train Epoch: 6 [14080/123872 (11%)]\tLoss: 0.451751\tLR: 0.00041200\n",
            "Train Epoch: 6 [14336/123872 (12%)]\tLoss: 0.335070\tLR: 0.00041188\n",
            "Train Epoch: 6 [14592/123872 (12%)]\tLoss: 0.435929\tLR: 0.00041177\n",
            "Train Epoch: 6 [14848/123872 (12%)]\tLoss: 0.498640\tLR: 0.00041166\n",
            "Train Epoch: 6 [15104/123872 (12%)]\tLoss: 0.390589\tLR: 0.00041155\n",
            "Train Epoch: 6 [15360/123872 (12%)]\tLoss: 0.418113\tLR: 0.00041143\n",
            "Train Epoch: 6 [15360/123872 (12%)]\tLoss: 0.418113\n",
            "Train Epoch: 6 [15616/123872 (13%)]\tLoss: 0.440415\tLR: 0.00041132\n",
            "Train Epoch: 6 [15872/123872 (13%)]\tLoss: 0.394640\tLR: 0.00041121\n",
            "Train Epoch: 6 [16128/123872 (13%)]\tLoss: 0.418593\tLR: 0.00041110\n",
            "Train Epoch: 6 [16384/123872 (13%)]\tLoss: 0.369709\tLR: 0.00041098\n",
            "Train Epoch: 6 [16640/123872 (13%)]\tLoss: 0.397366\tLR: 0.00041087\n",
            "Train Epoch: 6 [16896/123872 (14%)]\tLoss: 0.401207\tLR: 0.00041076\n",
            "Train Epoch: 6 [17152/123872 (14%)]\tLoss: 0.406986\tLR: 0.00041065\n",
            "Train Epoch: 6 [17408/123872 (14%)]\tLoss: 0.381495\tLR: 0.00041053\n",
            "Train Epoch: 6 [17664/123872 (14%)]\tLoss: 0.408461\tLR: 0.00041042\n",
            "Train Epoch: 6 [17920/123872 (14%)]\tLoss: 0.405888\tLR: 0.00041031\n",
            "Train Epoch: 6 [17920/123872 (14%)]\tLoss: 0.405888\n",
            "Train Epoch: 6 [18176/123872 (15%)]\tLoss: 0.343297\tLR: 0.00041020\n",
            "Train Epoch: 6 [18432/123872 (15%)]\tLoss: 0.415559\tLR: 0.00041008\n",
            "Train Epoch: 6 [18688/123872 (15%)]\tLoss: 0.441990\tLR: 0.00040997\n",
            "Train Epoch: 6 [18944/123872 (15%)]\tLoss: 0.378291\tLR: 0.00040986\n",
            "Train Epoch: 6 [19200/123872 (15%)]\tLoss: 0.404461\tLR: 0.00040974\n",
            "Train Epoch: 6 [19456/123872 (16%)]\tLoss: 0.420090\tLR: 0.00040963\n",
            "Train Epoch: 6 [19712/123872 (16%)]\tLoss: 0.409504\tLR: 0.00040952\n",
            "Train Epoch: 6 [19968/123872 (16%)]\tLoss: 0.418160\tLR: 0.00040941\n",
            "Train Epoch: 6 [20224/123872 (16%)]\tLoss: 0.415722\tLR: 0.00040929\n",
            "Train Epoch: 6 [20480/123872 (17%)]\tLoss: 0.394455\tLR: 0.00040918\n",
            "Train Epoch: 6 [20480/123872 (17%)]\tLoss: 0.394455\n",
            "Train Epoch: 6 [20736/123872 (17%)]\tLoss: 0.393943\tLR: 0.00040907\n",
            "Train Epoch: 6 [20992/123872 (17%)]\tLoss: 0.435253\tLR: 0.00040896\n",
            "Train Epoch: 6 [21248/123872 (17%)]\tLoss: 0.404941\tLR: 0.00040884\n",
            "Train Epoch: 6 [21504/123872 (17%)]\tLoss: 0.394469\tLR: 0.00040873\n",
            "Train Epoch: 6 [21760/123872 (18%)]\tLoss: 0.401442\tLR: 0.00040862\n",
            "Train Epoch: 6 [22016/123872 (18%)]\tLoss: 0.417202\tLR: 0.00040850\n",
            "Train Epoch: 6 [22272/123872 (18%)]\tLoss: 0.377037\tLR: 0.00040839\n",
            "Train Epoch: 6 [22528/123872 (18%)]\tLoss: 0.396633\tLR: 0.00040828\n",
            "Train Epoch: 6 [22784/123872 (18%)]\tLoss: 0.400979\tLR: 0.00040817\n",
            "Train Epoch: 6 [23040/123872 (19%)]\tLoss: 0.419751\tLR: 0.00040805\n",
            "Train Epoch: 6 [23040/123872 (19%)]\tLoss: 0.419751\n",
            "Train Epoch: 6 [23296/123872 (19%)]\tLoss: 0.399969\tLR: 0.00040794\n",
            "Train Epoch: 6 [23552/123872 (19%)]\tLoss: 0.391488\tLR: 0.00040783\n",
            "Train Epoch: 6 [23808/123872 (19%)]\tLoss: 0.340258\tLR: 0.00040771\n",
            "Train Epoch: 6 [24064/123872 (19%)]\tLoss: 0.400757\tLR: 0.00040760\n",
            "Train Epoch: 6 [24320/123872 (20%)]\tLoss: 0.457179\tLR: 0.00040749\n",
            "Train Epoch: 6 [24576/123872 (20%)]\tLoss: 0.409508\tLR: 0.00040737\n",
            "Train Epoch: 6 [24832/123872 (20%)]\tLoss: 0.383862\tLR: 0.00040726\n",
            "Train Epoch: 6 [25088/123872 (20%)]\tLoss: 0.433667\tLR: 0.00040715\n",
            "Train Epoch: 6 [25344/123872 (20%)]\tLoss: 0.410200\tLR: 0.00040703\n",
            "Train Epoch: 6 [25600/123872 (21%)]\tLoss: 0.370677\tLR: 0.00040692\n",
            "Train Epoch: 6 [25600/123872 (21%)]\tLoss: 0.370677\n",
            "Train Epoch: 6 [25856/123872 (21%)]\tLoss: 0.431872\tLR: 0.00040681\n",
            "Train Epoch: 6 [26112/123872 (21%)]\tLoss: 0.377471\tLR: 0.00040670\n",
            "Train Epoch: 6 [26368/123872 (21%)]\tLoss: 0.469743\tLR: 0.00040658\n",
            "Train Epoch: 6 [26624/123872 (21%)]\tLoss: 0.430006\tLR: 0.00040647\n",
            "Train Epoch: 6 [26880/123872 (22%)]\tLoss: 0.345589\tLR: 0.00040636\n",
            "Train Epoch: 6 [27136/123872 (22%)]\tLoss: 0.384335\tLR: 0.00040624\n",
            "Train Epoch: 6 [27392/123872 (22%)]\tLoss: 0.376757\tLR: 0.00040613\n",
            "Train Epoch: 6 [27648/123872 (22%)]\tLoss: 0.383038\tLR: 0.00040602\n",
            "Train Epoch: 6 [27904/123872 (23%)]\tLoss: 0.324187\tLR: 0.00040590\n",
            "Train Epoch: 6 [28160/123872 (23%)]\tLoss: 0.344359\tLR: 0.00040579\n",
            "Train Epoch: 6 [28160/123872 (23%)]\tLoss: 0.344359\n",
            "Train Epoch: 6 [28416/123872 (23%)]\tLoss: 0.396516\tLR: 0.00040568\n",
            "Train Epoch: 6 [28672/123872 (23%)]\tLoss: 0.394343\tLR: 0.00040556\n",
            "Train Epoch: 6 [28928/123872 (23%)]\tLoss: 0.382428\tLR: 0.00040545\n",
            "Train Epoch: 6 [29184/123872 (24%)]\tLoss: 0.437564\tLR: 0.00040534\n",
            "Train Epoch: 6 [29440/123872 (24%)]\tLoss: 0.411671\tLR: 0.00040522\n",
            "Train Epoch: 6 [29696/123872 (24%)]\tLoss: 0.392600\tLR: 0.00040511\n",
            "Train Epoch: 6 [29952/123872 (24%)]\tLoss: 0.477802\tLR: 0.00040500\n",
            "Train Epoch: 6 [30208/123872 (24%)]\tLoss: 0.406107\tLR: 0.00040488\n",
            "Train Epoch: 6 [30464/123872 (25%)]\tLoss: 0.451229\tLR: 0.00040477\n",
            "Train Epoch: 6 [30720/123872 (25%)]\tLoss: 0.412965\tLR: 0.00040466\n",
            "Train Epoch: 6 [30720/123872 (25%)]\tLoss: 0.412965\n",
            "Train Epoch: 6 [30976/123872 (25%)]\tLoss: 0.458474\tLR: 0.00040454\n",
            "Train Epoch: 6 [31232/123872 (25%)]\tLoss: 0.473096\tLR: 0.00040443\n",
            "Train Epoch: 6 [31488/123872 (25%)]\tLoss: 0.368599\tLR: 0.00040432\n",
            "Train Epoch: 6 [31744/123872 (26%)]\tLoss: 0.388870\tLR: 0.00040420\n",
            "Train Epoch: 6 [32000/123872 (26%)]\tLoss: 0.422001\tLR: 0.00040409\n",
            "Train Epoch: 6 [32256/123872 (26%)]\tLoss: 0.404550\tLR: 0.00040398\n",
            "Train Epoch: 6 [32512/123872 (26%)]\tLoss: 0.378062\tLR: 0.00040386\n",
            "Train Epoch: 6 [32768/123872 (26%)]\tLoss: 0.409821\tLR: 0.00040375\n",
            "Train Epoch: 6 [33024/123872 (27%)]\tLoss: 0.396527\tLR: 0.00040364\n",
            "Train Epoch: 6 [33280/123872 (27%)]\tLoss: 0.417002\tLR: 0.00040352\n",
            "Train Epoch: 6 [33280/123872 (27%)]\tLoss: 0.417002\n",
            "Train Epoch: 6 [33536/123872 (27%)]\tLoss: 0.439843\tLR: 0.00040341\n",
            "Train Epoch: 6 [33792/123872 (27%)]\tLoss: 0.371006\tLR: 0.00040329\n",
            "Train Epoch: 6 [34048/123872 (27%)]\tLoss: 0.406530\tLR: 0.00040318\n",
            "Train Epoch: 6 [34304/123872 (28%)]\tLoss: 0.452302\tLR: 0.00040307\n",
            "Train Epoch: 6 [34560/123872 (28%)]\tLoss: 0.419411\tLR: 0.00040295\n",
            "Train Epoch: 6 [34816/123872 (28%)]\tLoss: 0.424631\tLR: 0.00040284\n",
            "Train Epoch: 6 [35072/123872 (28%)]\tLoss: 0.362866\tLR: 0.00040273\n",
            "Train Epoch: 6 [35328/123872 (29%)]\tLoss: 0.500194\tLR: 0.00040261\n",
            "Train Epoch: 6 [35584/123872 (29%)]\tLoss: 0.400459\tLR: 0.00040250\n",
            "Train Epoch: 6 [35840/123872 (29%)]\tLoss: 0.456917\tLR: 0.00040239\n",
            "Train Epoch: 6 [35840/123872 (29%)]\tLoss: 0.456917\n",
            "Train Epoch: 6 [36096/123872 (29%)]\tLoss: 0.433018\tLR: 0.00040227\n",
            "Train Epoch: 6 [36352/123872 (29%)]\tLoss: 0.420757\tLR: 0.00040216\n",
            "Train Epoch: 6 [36608/123872 (30%)]\tLoss: 0.373122\tLR: 0.00040204\n",
            "Train Epoch: 6 [36864/123872 (30%)]\tLoss: 0.383433\tLR: 0.00040193\n",
            "Train Epoch: 6 [37120/123872 (30%)]\tLoss: 0.466057\tLR: 0.00040182\n",
            "Train Epoch: 6 [37376/123872 (30%)]\tLoss: 0.344048\tLR: 0.00040170\n",
            "Train Epoch: 6 [37632/123872 (30%)]\tLoss: 0.385569\tLR: 0.00040159\n",
            "Train Epoch: 6 [37888/123872 (31%)]\tLoss: 0.447613\tLR: 0.00040148\n",
            "Train Epoch: 6 [38144/123872 (31%)]\tLoss: 0.398601\tLR: 0.00040136\n",
            "Train Epoch: 6 [38400/123872 (31%)]\tLoss: 0.491912\tLR: 0.00040125\n",
            "Train Epoch: 6 [38400/123872 (31%)]\tLoss: 0.491912\n",
            "Train Epoch: 6 [38656/123872 (31%)]\tLoss: 0.406147\tLR: 0.00040113\n",
            "Train Epoch: 6 [38912/123872 (31%)]\tLoss: 0.381980\tLR: 0.00040102\n",
            "Train Epoch: 6 [39168/123872 (32%)]\tLoss: 0.446345\tLR: 0.00040091\n",
            "Train Epoch: 6 [39424/123872 (32%)]\tLoss: 0.458910\tLR: 0.00040079\n",
            "Train Epoch: 6 [39680/123872 (32%)]\tLoss: 0.372887\tLR: 0.00040068\n",
            "Train Epoch: 6 [39936/123872 (32%)]\tLoss: 0.472130\tLR: 0.00040056\n",
            "Train Epoch: 6 [40192/123872 (32%)]\tLoss: 0.430792\tLR: 0.00040045\n",
            "Train Epoch: 6 [40448/123872 (33%)]\tLoss: 0.451448\tLR: 0.00040034\n",
            "Train Epoch: 6 [40704/123872 (33%)]\tLoss: 0.412107\tLR: 0.00040022\n",
            "Train Epoch: 6 [40960/123872 (33%)]\tLoss: 0.419956\tLR: 0.00040011\n",
            "Train Epoch: 6 [40960/123872 (33%)]\tLoss: 0.419956\n",
            "Train Epoch: 6 [41216/123872 (33%)]\tLoss: 0.394581\tLR: 0.00040000\n",
            "Train Epoch: 6 [41472/123872 (33%)]\tLoss: 0.436900\tLR: 0.00039988\n",
            "Train Epoch: 6 [41728/123872 (34%)]\tLoss: 0.417549\tLR: 0.00039977\n",
            "Train Epoch: 6 [41984/123872 (34%)]\tLoss: 0.415182\tLR: 0.00039965\n",
            "Train Epoch: 6 [42240/123872 (34%)]\tLoss: 0.419920\tLR: 0.00039954\n",
            "Train Epoch: 6 [42496/123872 (34%)]\tLoss: 0.441967\tLR: 0.00039943\n",
            "Train Epoch: 6 [42752/123872 (35%)]\tLoss: 0.388489\tLR: 0.00039931\n",
            "Train Epoch: 6 [43008/123872 (35%)]\tLoss: 0.441095\tLR: 0.00039920\n",
            "Train Epoch: 6 [43264/123872 (35%)]\tLoss: 0.405257\tLR: 0.00039908\n",
            "Train Epoch: 6 [43520/123872 (35%)]\tLoss: 0.406595\tLR: 0.00039897\n",
            "Train Epoch: 6 [43520/123872 (35%)]\tLoss: 0.406595\n",
            "Train Epoch: 6 [43776/123872 (35%)]\tLoss: 0.418567\tLR: 0.00039885\n",
            "Train Epoch: 6 [44032/123872 (36%)]\tLoss: 0.412340\tLR: 0.00039874\n",
            "Train Epoch: 6 [44288/123872 (36%)]\tLoss: 0.419320\tLR: 0.00039863\n",
            "Train Epoch: 6 [44544/123872 (36%)]\tLoss: 0.419975\tLR: 0.00039851\n",
            "Train Epoch: 6 [44800/123872 (36%)]\tLoss: 0.402571\tLR: 0.00039840\n",
            "Train Epoch: 6 [45056/123872 (36%)]\tLoss: 0.459542\tLR: 0.00039828\n",
            "Train Epoch: 6 [45312/123872 (37%)]\tLoss: 0.337739\tLR: 0.00039817\n",
            "Train Epoch: 6 [45568/123872 (37%)]\tLoss: 0.423960\tLR: 0.00039806\n",
            "Train Epoch: 6 [45824/123872 (37%)]\tLoss: 0.466633\tLR: 0.00039794\n",
            "Train Epoch: 6 [46080/123872 (37%)]\tLoss: 0.479981\tLR: 0.00039783\n",
            "Train Epoch: 6 [46080/123872 (37%)]\tLoss: 0.479981\n",
            "Train Epoch: 6 [46336/123872 (37%)]\tLoss: 0.389721\tLR: 0.00039771\n",
            "Train Epoch: 6 [46592/123872 (38%)]\tLoss: 0.394610\tLR: 0.00039760\n",
            "Train Epoch: 6 [46848/123872 (38%)]\tLoss: 0.375429\tLR: 0.00039748\n",
            "Train Epoch: 6 [47104/123872 (38%)]\tLoss: 0.396969\tLR: 0.00039737\n",
            "Train Epoch: 6 [47360/123872 (38%)]\tLoss: 0.441741\tLR: 0.00039726\n",
            "Train Epoch: 6 [47616/123872 (38%)]\tLoss: 0.402910\tLR: 0.00039714\n",
            "Train Epoch: 6 [47872/123872 (39%)]\tLoss: 0.385841\tLR: 0.00039703\n",
            "Train Epoch: 6 [48128/123872 (39%)]\tLoss: 0.338719\tLR: 0.00039691\n",
            "Train Epoch: 6 [48384/123872 (39%)]\tLoss: 0.517555\tLR: 0.00039680\n",
            "Train Epoch: 6 [48640/123872 (39%)]\tLoss: 0.404652\tLR: 0.00039668\n",
            "Train Epoch: 6 [48640/123872 (39%)]\tLoss: 0.404652\n",
            "Train Epoch: 6 [48896/123872 (39%)]\tLoss: 0.361317\tLR: 0.00039657\n",
            "Train Epoch: 6 [49152/123872 (40%)]\tLoss: 0.397159\tLR: 0.00039646\n",
            "Train Epoch: 6 [49408/123872 (40%)]\tLoss: 0.367690\tLR: 0.00039634\n",
            "Train Epoch: 6 [49664/123872 (40%)]\tLoss: 0.417373\tLR: 0.00039623\n",
            "Train Epoch: 6 [49920/123872 (40%)]\tLoss: 0.375239\tLR: 0.00039611\n",
            "Train Epoch: 6 [50176/123872 (40%)]\tLoss: 0.432298\tLR: 0.00039600\n",
            "Train Epoch: 6 [50432/123872 (41%)]\tLoss: 0.376199\tLR: 0.00039588\n",
            "Train Epoch: 6 [50688/123872 (41%)]\tLoss: 0.414765\tLR: 0.00039577\n",
            "Train Epoch: 6 [50944/123872 (41%)]\tLoss: 0.473601\tLR: 0.00039565\n",
            "Train Epoch: 6 [51200/123872 (41%)]\tLoss: 0.391867\tLR: 0.00039554\n",
            "Train Epoch: 6 [51200/123872 (41%)]\tLoss: 0.391867\n",
            "Train Epoch: 6 [51456/123872 (42%)]\tLoss: 0.425172\tLR: 0.00039543\n",
            "Train Epoch: 6 [51712/123872 (42%)]\tLoss: 0.372837\tLR: 0.00039531\n",
            "Train Epoch: 6 [51968/123872 (42%)]\tLoss: 0.452112\tLR: 0.00039520\n",
            "Train Epoch: 6 [52224/123872 (42%)]\tLoss: 0.413496\tLR: 0.00039508\n",
            "Train Epoch: 6 [52480/123872 (42%)]\tLoss: 0.437210\tLR: 0.00039497\n",
            "Train Epoch: 6 [52736/123872 (43%)]\tLoss: 0.415891\tLR: 0.00039485\n",
            "Train Epoch: 6 [52992/123872 (43%)]\tLoss: 0.442144\tLR: 0.00039474\n",
            "Train Epoch: 6 [53248/123872 (43%)]\tLoss: 0.457228\tLR: 0.00039462\n",
            "Train Epoch: 6 [53504/123872 (43%)]\tLoss: 0.366121\tLR: 0.00039451\n",
            "Train Epoch: 6 [53760/123872 (43%)]\tLoss: 0.450307\tLR: 0.00039439\n",
            "Train Epoch: 6 [53760/123872 (43%)]\tLoss: 0.450307\n",
            "Train Epoch: 6 [54016/123872 (44%)]\tLoss: 0.382742\tLR: 0.00039428\n",
            "Train Epoch: 6 [54272/123872 (44%)]\tLoss: 0.403947\tLR: 0.00039417\n",
            "Train Epoch: 6 [54528/123872 (44%)]\tLoss: 0.408781\tLR: 0.00039405\n",
            "Train Epoch: 6 [54784/123872 (44%)]\tLoss: 0.368598\tLR: 0.00039394\n",
            "Train Epoch: 6 [55040/123872 (44%)]\tLoss: 0.431010\tLR: 0.00039382\n",
            "Train Epoch: 6 [55296/123872 (45%)]\tLoss: 0.453993\tLR: 0.00039371\n",
            "Train Epoch: 6 [55552/123872 (45%)]\tLoss: 0.444874\tLR: 0.00039359\n",
            "Train Epoch: 6 [55808/123872 (45%)]\tLoss: 0.510615\tLR: 0.00039348\n",
            "Train Epoch: 6 [56064/123872 (45%)]\tLoss: 0.411416\tLR: 0.00039336\n",
            "Train Epoch: 6 [56320/123872 (45%)]\tLoss: 0.412979\tLR: 0.00039325\n",
            "Train Epoch: 6 [56320/123872 (45%)]\tLoss: 0.412979\n",
            "Train Epoch: 6 [56576/123872 (46%)]\tLoss: 0.382159\tLR: 0.00039313\n",
            "Train Epoch: 6 [56832/123872 (46%)]\tLoss: 0.409312\tLR: 0.00039302\n",
            "Train Epoch: 6 [57088/123872 (46%)]\tLoss: 0.402926\tLR: 0.00039290\n",
            "Train Epoch: 6 [57344/123872 (46%)]\tLoss: 0.430104\tLR: 0.00039279\n",
            "Train Epoch: 6 [57600/123872 (46%)]\tLoss: 0.447477\tLR: 0.00039267\n",
            "Train Epoch: 6 [57856/123872 (47%)]\tLoss: 0.454367\tLR: 0.00039256\n",
            "Train Epoch: 6 [58112/123872 (47%)]\tLoss: 0.406456\tLR: 0.00039244\n",
            "Train Epoch: 6 [58368/123872 (47%)]\tLoss: 0.407246\tLR: 0.00039233\n",
            "Train Epoch: 6 [58624/123872 (47%)]\tLoss: 0.388257\tLR: 0.00039222\n",
            "Train Epoch: 6 [58880/123872 (48%)]\tLoss: 0.372036\tLR: 0.00039210\n",
            "Train Epoch: 6 [58880/123872 (48%)]\tLoss: 0.372036\n",
            "Train Epoch: 6 [59136/123872 (48%)]\tLoss: 0.412571\tLR: 0.00039199\n",
            "Train Epoch: 6 [59392/123872 (48%)]\tLoss: 0.386660\tLR: 0.00039187\n",
            "Train Epoch: 6 [59648/123872 (48%)]\tLoss: 0.366977\tLR: 0.00039176\n",
            "Train Epoch: 6 [59904/123872 (48%)]\tLoss: 0.436778\tLR: 0.00039164\n",
            "Train Epoch: 6 [60160/123872 (49%)]\tLoss: 0.374242\tLR: 0.00039153\n",
            "Train Epoch: 6 [60416/123872 (49%)]\tLoss: 0.385421\tLR: 0.00039141\n",
            "Train Epoch: 6 [60672/123872 (49%)]\tLoss: 0.415993\tLR: 0.00039130\n",
            "Train Epoch: 6 [60928/123872 (49%)]\tLoss: 0.417455\tLR: 0.00039118\n",
            "Train Epoch: 6 [61184/123872 (49%)]\tLoss: 0.406737\tLR: 0.00039107\n",
            "Train Epoch: 6 [61440/123872 (50%)]\tLoss: 0.439029\tLR: 0.00039095\n",
            "Train Epoch: 6 [61440/123872 (50%)]\tLoss: 0.439029\n",
            "Train Epoch: 6 [61696/123872 (50%)]\tLoss: 0.477075\tLR: 0.00039084\n",
            "Train Epoch: 6 [61952/123872 (50%)]\tLoss: 0.413232\tLR: 0.00039072\n",
            "Train Epoch: 6 [62208/123872 (50%)]\tLoss: 0.383304\tLR: 0.00039061\n",
            "Train Epoch: 6 [62464/123872 (50%)]\tLoss: 0.397217\tLR: 0.00039049\n",
            "Train Epoch: 6 [62720/123872 (51%)]\tLoss: 0.434001\tLR: 0.00039038\n",
            "Train Epoch: 6 [62976/123872 (51%)]\tLoss: 0.373710\tLR: 0.00039026\n",
            "Train Epoch: 6 [63232/123872 (51%)]\tLoss: 0.362863\tLR: 0.00039015\n",
            "Train Epoch: 6 [63488/123872 (51%)]\tLoss: 0.376067\tLR: 0.00039003\n",
            "Train Epoch: 6 [63744/123872 (51%)]\tLoss: 0.476351\tLR: 0.00038992\n",
            "Train Epoch: 6 [64000/123872 (52%)]\tLoss: 0.406294\tLR: 0.00038980\n",
            "Train Epoch: 6 [64000/123872 (52%)]\tLoss: 0.406294\n",
            "Train Epoch: 6 [64256/123872 (52%)]\tLoss: 0.381216\tLR: 0.00038969\n",
            "Train Epoch: 6 [64512/123872 (52%)]\tLoss: 0.424346\tLR: 0.00038957\n",
            "Train Epoch: 6 [64768/123872 (52%)]\tLoss: 0.419381\tLR: 0.00038946\n",
            "Train Epoch: 6 [65024/123872 (52%)]\tLoss: 0.381988\tLR: 0.00038934\n",
            "Train Epoch: 6 [65280/123872 (53%)]\tLoss: 0.403242\tLR: 0.00038923\n",
            "Train Epoch: 6 [65536/123872 (53%)]\tLoss: 0.416603\tLR: 0.00038911\n",
            "Train Epoch: 6 [65792/123872 (53%)]\tLoss: 0.392813\tLR: 0.00038900\n",
            "Train Epoch: 6 [66048/123872 (53%)]\tLoss: 0.350133\tLR: 0.00038888\n",
            "Train Epoch: 6 [66304/123872 (54%)]\tLoss: 0.407623\tLR: 0.00038877\n",
            "Train Epoch: 6 [66560/123872 (54%)]\tLoss: 0.369353\tLR: 0.00038865\n",
            "Train Epoch: 6 [66560/123872 (54%)]\tLoss: 0.369353\n",
            "Train Epoch: 6 [66816/123872 (54%)]\tLoss: 0.391258\tLR: 0.00038853\n",
            "Train Epoch: 6 [67072/123872 (54%)]\tLoss: 0.400317\tLR: 0.00038842\n",
            "Train Epoch: 6 [67328/123872 (54%)]\tLoss: 0.354175\tLR: 0.00038830\n",
            "Train Epoch: 6 [67584/123872 (55%)]\tLoss: 0.423428\tLR: 0.00038819\n",
            "Train Epoch: 6 [67840/123872 (55%)]\tLoss: 0.385773\tLR: 0.00038807\n",
            "Train Epoch: 6 [68096/123872 (55%)]\tLoss: 0.391612\tLR: 0.00038796\n",
            "Train Epoch: 6 [68352/123872 (55%)]\tLoss: 0.424600\tLR: 0.00038784\n",
            "Train Epoch: 6 [68608/123872 (55%)]\tLoss: 0.410000\tLR: 0.00038773\n",
            "Train Epoch: 6 [68864/123872 (56%)]\tLoss: 0.406776\tLR: 0.00038761\n",
            "Train Epoch: 6 [69120/123872 (56%)]\tLoss: 0.394984\tLR: 0.00038750\n",
            "Train Epoch: 6 [69120/123872 (56%)]\tLoss: 0.394984\n",
            "Train Epoch: 6 [69376/123872 (56%)]\tLoss: 0.432928\tLR: 0.00038738\n",
            "Train Epoch: 6 [69632/123872 (56%)]\tLoss: 0.505590\tLR: 0.00038727\n",
            "Train Epoch: 6 [69888/123872 (56%)]\tLoss: 0.400846\tLR: 0.00038715\n",
            "Train Epoch: 6 [70144/123872 (57%)]\tLoss: 0.438052\tLR: 0.00038704\n",
            "Train Epoch: 6 [70400/123872 (57%)]\tLoss: 0.495285\tLR: 0.00038692\n",
            "Train Epoch: 6 [70656/123872 (57%)]\tLoss: 0.453906\tLR: 0.00038681\n",
            "Train Epoch: 6 [70912/123872 (57%)]\tLoss: 0.419969\tLR: 0.00038669\n",
            "Train Epoch: 6 [71168/123872 (57%)]\tLoss: 0.451175\tLR: 0.00038657\n",
            "Train Epoch: 6 [71424/123872 (58%)]\tLoss: 0.447589\tLR: 0.00038646\n",
            "Train Epoch: 6 [71680/123872 (58%)]\tLoss: 0.405861\tLR: 0.00038634\n",
            "Train Epoch: 6 [71680/123872 (58%)]\tLoss: 0.405861\n",
            "Train Epoch: 6 [71936/123872 (58%)]\tLoss: 0.356649\tLR: 0.00038623\n",
            "Train Epoch: 6 [72192/123872 (58%)]\tLoss: 0.413702\tLR: 0.00038611\n",
            "Train Epoch: 6 [72448/123872 (58%)]\tLoss: 0.405997\tLR: 0.00038600\n",
            "Train Epoch: 6 [72704/123872 (59%)]\tLoss: 0.389440\tLR: 0.00038588\n",
            "Train Epoch: 6 [72960/123872 (59%)]\tLoss: 0.408697\tLR: 0.00038577\n",
            "Train Epoch: 6 [73216/123872 (59%)]\tLoss: 0.418748\tLR: 0.00038565\n",
            "Train Epoch: 6 [73472/123872 (59%)]\tLoss: 0.426187\tLR: 0.00038554\n",
            "Train Epoch: 6 [73728/123872 (60%)]\tLoss: 0.445906\tLR: 0.00038542\n",
            "Train Epoch: 6 [73984/123872 (60%)]\tLoss: 0.354097\tLR: 0.00038530\n",
            "Train Epoch: 6 [74240/123872 (60%)]\tLoss: 0.450378\tLR: 0.00038519\n",
            "Train Epoch: 6 [74240/123872 (60%)]\tLoss: 0.450378\n",
            "Train Epoch: 6 [74496/123872 (60%)]\tLoss: 0.395801\tLR: 0.00038507\n",
            "Train Epoch: 6 [74752/123872 (60%)]\tLoss: 0.373198\tLR: 0.00038496\n",
            "Train Epoch: 6 [75008/123872 (61%)]\tLoss: 0.362031\tLR: 0.00038484\n",
            "Train Epoch: 6 [75264/123872 (61%)]\tLoss: 0.367281\tLR: 0.00038473\n",
            "Train Epoch: 6 [75520/123872 (61%)]\tLoss: 0.384944\tLR: 0.00038461\n",
            "Train Epoch: 6 [75776/123872 (61%)]\tLoss: 0.409077\tLR: 0.00038450\n",
            "Train Epoch: 6 [76032/123872 (61%)]\tLoss: 0.446483\tLR: 0.00038438\n",
            "Train Epoch: 6 [76288/123872 (62%)]\tLoss: 0.438765\tLR: 0.00038426\n",
            "Train Epoch: 6 [76544/123872 (62%)]\tLoss: 0.388695\tLR: 0.00038415\n",
            "Train Epoch: 6 [76800/123872 (62%)]\tLoss: 0.317246\tLR: 0.00038403\n",
            "Train Epoch: 6 [76800/123872 (62%)]\tLoss: 0.317246\n",
            "Train Epoch: 6 [77056/123872 (62%)]\tLoss: 0.413158\tLR: 0.00038392\n",
            "Train Epoch: 6 [77312/123872 (62%)]\tLoss: 0.407979\tLR: 0.00038380\n",
            "Train Epoch: 6 [77568/123872 (63%)]\tLoss: 0.383608\tLR: 0.00038369\n",
            "Train Epoch: 6 [77824/123872 (63%)]\tLoss: 0.409742\tLR: 0.00038357\n",
            "Train Epoch: 6 [78080/123872 (63%)]\tLoss: 0.350469\tLR: 0.00038346\n",
            "Train Epoch: 6 [78336/123872 (63%)]\tLoss: 0.423624\tLR: 0.00038334\n",
            "Train Epoch: 6 [78592/123872 (63%)]\tLoss: 0.397714\tLR: 0.00038322\n",
            "Train Epoch: 6 [78848/123872 (64%)]\tLoss: 0.456110\tLR: 0.00038311\n",
            "Train Epoch: 6 [79104/123872 (64%)]\tLoss: 0.430294\tLR: 0.00038299\n",
            "Train Epoch: 6 [79360/123872 (64%)]\tLoss: 0.426573\tLR: 0.00038288\n",
            "Train Epoch: 6 [79360/123872 (64%)]\tLoss: 0.426573\n",
            "Train Epoch: 6 [79616/123872 (64%)]\tLoss: 0.350428\tLR: 0.00038276\n",
            "Train Epoch: 6 [79872/123872 (64%)]\tLoss: 0.412814\tLR: 0.00038265\n",
            "Train Epoch: 6 [80128/123872 (65%)]\tLoss: 0.440615\tLR: 0.00038253\n",
            "Train Epoch: 6 [80384/123872 (65%)]\tLoss: 0.445766\tLR: 0.00038241\n",
            "Train Epoch: 6 [80640/123872 (65%)]\tLoss: 0.395587\tLR: 0.00038230\n",
            "Train Epoch: 6 [80896/123872 (65%)]\tLoss: 0.369552\tLR: 0.00038218\n",
            "Train Epoch: 6 [81152/123872 (65%)]\tLoss: 0.406393\tLR: 0.00038207\n",
            "Train Epoch: 6 [81408/123872 (66%)]\tLoss: 0.381255\tLR: 0.00038195\n",
            "Train Epoch: 6 [81664/123872 (66%)]\tLoss: 0.408044\tLR: 0.00038184\n",
            "Train Epoch: 6 [81920/123872 (66%)]\tLoss: 0.416967\tLR: 0.00038172\n",
            "Train Epoch: 6 [81920/123872 (66%)]\tLoss: 0.416967\n",
            "Train Epoch: 6 [82176/123872 (66%)]\tLoss: 0.424727\tLR: 0.00038160\n",
            "Train Epoch: 6 [82432/123872 (67%)]\tLoss: 0.432058\tLR: 0.00038149\n",
            "Train Epoch: 6 [82688/123872 (67%)]\tLoss: 0.398235\tLR: 0.00038137\n",
            "Train Epoch: 6 [82944/123872 (67%)]\tLoss: 0.375461\tLR: 0.00038126\n",
            "Train Epoch: 6 [83200/123872 (67%)]\tLoss: 0.398005\tLR: 0.00038114\n",
            "Train Epoch: 6 [83456/123872 (67%)]\tLoss: 0.400492\tLR: 0.00038102\n",
            "Train Epoch: 6 [83712/123872 (68%)]\tLoss: 0.410500\tLR: 0.00038091\n",
            "Train Epoch: 6 [83968/123872 (68%)]\tLoss: 0.350291\tLR: 0.00038079\n",
            "Train Epoch: 6 [84224/123872 (68%)]\tLoss: 0.439913\tLR: 0.00038068\n",
            "Train Epoch: 6 [84480/123872 (68%)]\tLoss: 0.412922\tLR: 0.00038056\n",
            "Train Epoch: 6 [84480/123872 (68%)]\tLoss: 0.412922\n",
            "Train Epoch: 6 [84736/123872 (68%)]\tLoss: 0.441763\tLR: 0.00038044\n",
            "Train Epoch: 6 [84992/123872 (69%)]\tLoss: 0.455014\tLR: 0.00038033\n",
            "Train Epoch: 6 [85248/123872 (69%)]\tLoss: 0.484909\tLR: 0.00038021\n",
            "Train Epoch: 6 [85504/123872 (69%)]\tLoss: 0.446948\tLR: 0.00038010\n",
            "Train Epoch: 6 [85760/123872 (69%)]\tLoss: 0.415652\tLR: 0.00037998\n",
            "Train Epoch: 6 [86016/123872 (69%)]\tLoss: 0.477750\tLR: 0.00037986\n",
            "Train Epoch: 6 [86272/123872 (70%)]\tLoss: 0.409134\tLR: 0.00037975\n",
            "Train Epoch: 6 [86528/123872 (70%)]\tLoss: 0.412904\tLR: 0.00037963\n",
            "Train Epoch: 6 [86784/123872 (70%)]\tLoss: 0.422230\tLR: 0.00037952\n",
            "Train Epoch: 6 [87040/123872 (70%)]\tLoss: 0.452675\tLR: 0.00037940\n",
            "Train Epoch: 6 [87040/123872 (70%)]\tLoss: 0.452675\n",
            "Train Epoch: 6 [87296/123872 (70%)]\tLoss: 0.351812\tLR: 0.00037928\n",
            "Train Epoch: 6 [87552/123872 (71%)]\tLoss: 0.431420\tLR: 0.00037917\n",
            "Train Epoch: 6 [87808/123872 (71%)]\tLoss: 0.414337\tLR: 0.00037905\n",
            "Train Epoch: 6 [88064/123872 (71%)]\tLoss: 0.368840\tLR: 0.00037894\n",
            "Train Epoch: 6 [88320/123872 (71%)]\tLoss: 0.426328\tLR: 0.00037882\n",
            "Train Epoch: 6 [88576/123872 (71%)]\tLoss: 0.363950\tLR: 0.00037870\n",
            "Train Epoch: 6 [88832/123872 (72%)]\tLoss: 0.450885\tLR: 0.00037859\n",
            "Train Epoch: 6 [89088/123872 (72%)]\tLoss: 0.389385\tLR: 0.00037847\n",
            "Train Epoch: 6 [89344/123872 (72%)]\tLoss: 0.361388\tLR: 0.00037836\n",
            "Train Epoch: 6 [89600/123872 (72%)]\tLoss: 0.375828\tLR: 0.00037824\n",
            "Train Epoch: 6 [89600/123872 (72%)]\tLoss: 0.375828\n",
            "Train Epoch: 6 [89856/123872 (73%)]\tLoss: 0.426564\tLR: 0.00037812\n",
            "Train Epoch: 6 [90112/123872 (73%)]\tLoss: 0.412446\tLR: 0.00037801\n",
            "Train Epoch: 6 [90368/123872 (73%)]\tLoss: 0.314147\tLR: 0.00037789\n",
            "Train Epoch: 6 [90624/123872 (73%)]\tLoss: 0.399323\tLR: 0.00037778\n",
            "Train Epoch: 6 [90880/123872 (73%)]\tLoss: 0.463347\tLR: 0.00037766\n",
            "Train Epoch: 6 [91136/123872 (74%)]\tLoss: 0.387480\tLR: 0.00037754\n",
            "Train Epoch: 6 [91392/123872 (74%)]\tLoss: 0.413431\tLR: 0.00037743\n",
            "Train Epoch: 6 [91648/123872 (74%)]\tLoss: 0.412282\tLR: 0.00037731\n",
            "Train Epoch: 6 [91904/123872 (74%)]\tLoss: 0.399290\tLR: 0.00037719\n",
            "Train Epoch: 6 [92160/123872 (74%)]\tLoss: 0.397337\tLR: 0.00037708\n",
            "Train Epoch: 6 [92160/123872 (74%)]\tLoss: 0.397337\n",
            "Train Epoch: 6 [92416/123872 (75%)]\tLoss: 0.418278\tLR: 0.00037696\n",
            "Train Epoch: 6 [92672/123872 (75%)]\tLoss: 0.408913\tLR: 0.00037685\n",
            "Train Epoch: 6 [92928/123872 (75%)]\tLoss: 0.392225\tLR: 0.00037673\n",
            "Train Epoch: 6 [93184/123872 (75%)]\tLoss: 0.405584\tLR: 0.00037661\n",
            "Train Epoch: 6 [93440/123872 (75%)]\tLoss: 0.430940\tLR: 0.00037650\n",
            "Train Epoch: 6 [93696/123872 (76%)]\tLoss: 0.401340\tLR: 0.00037638\n",
            "Train Epoch: 6 [93952/123872 (76%)]\tLoss: 0.384877\tLR: 0.00037627\n",
            "Train Epoch: 6 [94208/123872 (76%)]\tLoss: 0.355323\tLR: 0.00037615\n",
            "Train Epoch: 6 [94464/123872 (76%)]\tLoss: 0.416968\tLR: 0.00037603\n",
            "Train Epoch: 6 [94720/123872 (76%)]\tLoss: 0.374873\tLR: 0.00037592\n",
            "Train Epoch: 6 [94720/123872 (76%)]\tLoss: 0.374873\n",
            "Train Epoch: 6 [94976/123872 (77%)]\tLoss: 0.421127\tLR: 0.00037580\n",
            "Train Epoch: 6 [95232/123872 (77%)]\tLoss: 0.395347\tLR: 0.00037568\n",
            "Train Epoch: 6 [95488/123872 (77%)]\tLoss: 0.471337\tLR: 0.00037557\n",
            "Train Epoch: 6 [95744/123872 (77%)]\tLoss: 0.449833\tLR: 0.00037545\n",
            "Train Epoch: 6 [96000/123872 (77%)]\tLoss: 0.484013\tLR: 0.00037533\n",
            "Train Epoch: 6 [96256/123872 (78%)]\tLoss: 0.388425\tLR: 0.00037522\n",
            "Train Epoch: 6 [96512/123872 (78%)]\tLoss: 0.469047\tLR: 0.00037510\n",
            "Train Epoch: 6 [96768/123872 (78%)]\tLoss: 0.396600\tLR: 0.00037499\n",
            "Train Epoch: 6 [97024/123872 (78%)]\tLoss: 0.401119\tLR: 0.00037487\n",
            "Train Epoch: 6 [97280/123872 (79%)]\tLoss: 0.375672\tLR: 0.00037475\n",
            "Train Epoch: 6 [97280/123872 (79%)]\tLoss: 0.375672\n",
            "Train Epoch: 6 [97536/123872 (79%)]\tLoss: 0.453477\tLR: 0.00037464\n",
            "Train Epoch: 6 [97792/123872 (79%)]\tLoss: 0.397118\tLR: 0.00037452\n",
            "Train Epoch: 6 [98048/123872 (79%)]\tLoss: 0.390601\tLR: 0.00037440\n",
            "Train Epoch: 6 [98304/123872 (79%)]\tLoss: 0.373927\tLR: 0.00037429\n",
            "Train Epoch: 6 [98560/123872 (80%)]\tLoss: 0.420277\tLR: 0.00037417\n",
            "Train Epoch: 6 [98816/123872 (80%)]\tLoss: 0.444472\tLR: 0.00037405\n",
            "Train Epoch: 6 [99072/123872 (80%)]\tLoss: 0.494527\tLR: 0.00037394\n",
            "Train Epoch: 6 [99328/123872 (80%)]\tLoss: 0.469628\tLR: 0.00037382\n",
            "Train Epoch: 6 [99584/123872 (80%)]\tLoss: 0.416808\tLR: 0.00037371\n",
            "Train Epoch: 6 [99840/123872 (81%)]\tLoss: 0.382536\tLR: 0.00037359\n",
            "Train Epoch: 6 [99840/123872 (81%)]\tLoss: 0.382536\n",
            "Train Epoch: 6 [100096/123872 (81%)]\tLoss: 0.408471\tLR: 0.00037347\n",
            "Train Epoch: 6 [100352/123872 (81%)]\tLoss: 0.406584\tLR: 0.00037336\n",
            "Train Epoch: 6 [100608/123872 (81%)]\tLoss: 0.410378\tLR: 0.00037324\n",
            "Train Epoch: 6 [100864/123872 (81%)]\tLoss: 0.451021\tLR: 0.00037312\n",
            "Train Epoch: 6 [101120/123872 (82%)]\tLoss: 0.408115\tLR: 0.00037301\n",
            "Train Epoch: 6 [101376/123872 (82%)]\tLoss: 0.467502\tLR: 0.00037289\n",
            "Train Epoch: 6 [101632/123872 (82%)]\tLoss: 0.362061\tLR: 0.00037277\n",
            "Train Epoch: 6 [101888/123872 (82%)]\tLoss: 0.472975\tLR: 0.00037266\n",
            "Train Epoch: 6 [102144/123872 (82%)]\tLoss: 0.404614\tLR: 0.00037254\n",
            "Train Epoch: 6 [102400/123872 (83%)]\tLoss: 0.411685\tLR: 0.00037242\n",
            "Train Epoch: 6 [102400/123872 (83%)]\tLoss: 0.411685\n",
            "Train Epoch: 6 [102656/123872 (83%)]\tLoss: 0.427236\tLR: 0.00037231\n",
            "Train Epoch: 6 [102912/123872 (83%)]\tLoss: 0.358428\tLR: 0.00037219\n",
            "Train Epoch: 6 [103168/123872 (83%)]\tLoss: 0.373232\tLR: 0.00037207\n",
            "Train Epoch: 6 [103424/123872 (83%)]\tLoss: 0.428530\tLR: 0.00037196\n",
            "Train Epoch: 6 [103680/123872 (84%)]\tLoss: 0.389695\tLR: 0.00037184\n",
            "Train Epoch: 6 [103936/123872 (84%)]\tLoss: 0.383646\tLR: 0.00037172\n",
            "Train Epoch: 6 [104192/123872 (84%)]\tLoss: 0.406162\tLR: 0.00037161\n",
            "Train Epoch: 6 [104448/123872 (84%)]\tLoss: 0.449930\tLR: 0.00037149\n",
            "Train Epoch: 6 [104704/123872 (85%)]\tLoss: 0.465655\tLR: 0.00037137\n",
            "Train Epoch: 6 [104960/123872 (85%)]\tLoss: 0.437200\tLR: 0.00037126\n",
            "Train Epoch: 6 [104960/123872 (85%)]\tLoss: 0.437200\n",
            "Train Epoch: 6 [105216/123872 (85%)]\tLoss: 0.351196\tLR: 0.00037114\n",
            "Train Epoch: 6 [105472/123872 (85%)]\tLoss: 0.411153\tLR: 0.00037102\n",
            "Train Epoch: 6 [105728/123872 (85%)]\tLoss: 0.435352\tLR: 0.00037091\n",
            "Train Epoch: 6 [105984/123872 (86%)]\tLoss: 0.382788\tLR: 0.00037079\n",
            "Train Epoch: 6 [106240/123872 (86%)]\tLoss: 0.424217\tLR: 0.00037067\n",
            "Train Epoch: 6 [106496/123872 (86%)]\tLoss: 0.360105\tLR: 0.00037056\n",
            "Train Epoch: 6 [106752/123872 (86%)]\tLoss: 0.383722\tLR: 0.00037044\n",
            "Train Epoch: 6 [107008/123872 (86%)]\tLoss: 0.418466\tLR: 0.00037032\n",
            "Train Epoch: 6 [107264/123872 (87%)]\tLoss: 0.460115\tLR: 0.00037021\n",
            "Train Epoch: 6 [107520/123872 (87%)]\tLoss: 0.356237\tLR: 0.00037009\n",
            "Train Epoch: 6 [107520/123872 (87%)]\tLoss: 0.356237\n",
            "Train Epoch: 6 [107776/123872 (87%)]\tLoss: 0.396895\tLR: 0.00036997\n",
            "Train Epoch: 6 [108032/123872 (87%)]\tLoss: 0.397741\tLR: 0.00036986\n",
            "Train Epoch: 6 [108288/123872 (87%)]\tLoss: 0.437703\tLR: 0.00036974\n",
            "Train Epoch: 6 [108544/123872 (88%)]\tLoss: 0.410339\tLR: 0.00036962\n",
            "Train Epoch: 6 [108800/123872 (88%)]\tLoss: 0.402895\tLR: 0.00036951\n",
            "Train Epoch: 6 [109056/123872 (88%)]\tLoss: 0.443324\tLR: 0.00036939\n",
            "Train Epoch: 6 [109312/123872 (88%)]\tLoss: 0.384825\tLR: 0.00036927\n",
            "Train Epoch: 6 [109568/123872 (88%)]\tLoss: 0.418461\tLR: 0.00036916\n",
            "Train Epoch: 6 [109824/123872 (89%)]\tLoss: 0.408997\tLR: 0.00036904\n",
            "Train Epoch: 6 [110080/123872 (89%)]\tLoss: 0.341322\tLR: 0.00036892\n",
            "Train Epoch: 6 [110080/123872 (89%)]\tLoss: 0.341322\n",
            "Train Epoch: 6 [110336/123872 (89%)]\tLoss: 0.395401\tLR: 0.00036881\n",
            "Train Epoch: 6 [110592/123872 (89%)]\tLoss: 0.379292\tLR: 0.00036869\n",
            "Train Epoch: 6 [110848/123872 (89%)]\tLoss: 0.442395\tLR: 0.00036857\n",
            "Train Epoch: 6 [111104/123872 (90%)]\tLoss: 0.362495\tLR: 0.00036846\n",
            "Train Epoch: 6 [111360/123872 (90%)]\tLoss: 0.372675\tLR: 0.00036834\n",
            "Train Epoch: 6 [111616/123872 (90%)]\tLoss: 0.426032\tLR: 0.00036822\n",
            "Train Epoch: 6 [111872/123872 (90%)]\tLoss: 0.331583\tLR: 0.00036811\n",
            "Train Epoch: 6 [112128/123872 (90%)]\tLoss: 0.415616\tLR: 0.00036799\n",
            "Train Epoch: 6 [112384/123872 (91%)]\tLoss: 0.335270\tLR: 0.00036787\n",
            "Train Epoch: 6 [112640/123872 (91%)]\tLoss: 0.459525\tLR: 0.00036776\n",
            "Train Epoch: 6 [112640/123872 (91%)]\tLoss: 0.459525\n",
            "Train Epoch: 6 [112896/123872 (91%)]\tLoss: 0.442334\tLR: 0.00036764\n",
            "Train Epoch: 6 [113152/123872 (91%)]\tLoss: 0.421598\tLR: 0.00036752\n",
            "Train Epoch: 6 [113408/123872 (92%)]\tLoss: 0.357340\tLR: 0.00036741\n",
            "Train Epoch: 6 [113664/123872 (92%)]\tLoss: 0.367537\tLR: 0.00036729\n",
            "Train Epoch: 6 [113920/123872 (92%)]\tLoss: 0.376522\tLR: 0.00036717\n",
            "Train Epoch: 6 [114176/123872 (92%)]\tLoss: 0.401972\tLR: 0.00036705\n",
            "Train Epoch: 6 [114432/123872 (92%)]\tLoss: 0.390407\tLR: 0.00036694\n",
            "Train Epoch: 6 [114688/123872 (93%)]\tLoss: 0.408148\tLR: 0.00036682\n",
            "Train Epoch: 6 [114944/123872 (93%)]\tLoss: 0.388972\tLR: 0.00036670\n",
            "Train Epoch: 6 [115200/123872 (93%)]\tLoss: 0.453995\tLR: 0.00036659\n",
            "Train Epoch: 6 [115200/123872 (93%)]\tLoss: 0.453995\n",
            "Train Epoch: 6 [115456/123872 (93%)]\tLoss: 0.448955\tLR: 0.00036647\n",
            "Train Epoch: 6 [115712/123872 (93%)]\tLoss: 0.451847\tLR: 0.00036635\n",
            "Train Epoch: 6 [115968/123872 (94%)]\tLoss: 0.405352\tLR: 0.00036624\n",
            "Train Epoch: 6 [116224/123872 (94%)]\tLoss: 0.409831\tLR: 0.00036612\n",
            "Train Epoch: 6 [116480/123872 (94%)]\tLoss: 0.382531\tLR: 0.00036600\n",
            "Train Epoch: 6 [116736/123872 (94%)]\tLoss: 0.445965\tLR: 0.00036589\n",
            "Train Epoch: 6 [116992/123872 (94%)]\tLoss: 0.478682\tLR: 0.00036577\n",
            "Train Epoch: 6 [117248/123872 (95%)]\tLoss: 0.376412\tLR: 0.00036565\n",
            "Train Epoch: 6 [117504/123872 (95%)]\tLoss: 0.343674\tLR: 0.00036553\n",
            "Train Epoch: 6 [117760/123872 (95%)]\tLoss: 0.373107\tLR: 0.00036542\n",
            "Train Epoch: 6 [117760/123872 (95%)]\tLoss: 0.373107\n",
            "Train Epoch: 6 [118016/123872 (95%)]\tLoss: 0.405540\tLR: 0.00036530\n",
            "Train Epoch: 6 [118272/123872 (95%)]\tLoss: 0.416036\tLR: 0.00036518\n",
            "Train Epoch: 6 [118528/123872 (96%)]\tLoss: 0.502857\tLR: 0.00036507\n",
            "Train Epoch: 6 [118784/123872 (96%)]\tLoss: 0.428934\tLR: 0.00036495\n",
            "Train Epoch: 6 [119040/123872 (96%)]\tLoss: 0.422614\tLR: 0.00036483\n",
            "Train Epoch: 6 [119296/123872 (96%)]\tLoss: 0.370678\tLR: 0.00036472\n",
            "Train Epoch: 6 [119552/123872 (96%)]\tLoss: 0.429902\tLR: 0.00036460\n",
            "Train Epoch: 6 [119808/123872 (97%)]\tLoss: 0.383733\tLR: 0.00036448\n",
            "Train Epoch: 6 [120064/123872 (97%)]\tLoss: 0.416073\tLR: 0.00036436\n",
            "Train Epoch: 6 [120320/123872 (97%)]\tLoss: 0.457104\tLR: 0.00036425\n",
            "Train Epoch: 6 [120320/123872 (97%)]\tLoss: 0.457104\n",
            "Train Epoch: 6 [120576/123872 (97%)]\tLoss: 0.391003\tLR: 0.00036413\n",
            "Train Epoch: 6 [120832/123872 (98%)]\tLoss: 0.432181\tLR: 0.00036401\n",
            "Train Epoch: 6 [121088/123872 (98%)]\tLoss: 0.366394\tLR: 0.00036390\n",
            "Train Epoch: 6 [121344/123872 (98%)]\tLoss: 0.384965\tLR: 0.00036378\n",
            "Train Epoch: 6 [121600/123872 (98%)]\tLoss: 0.318708\tLR: 0.00036366\n",
            "Train Epoch: 6 [121856/123872 (98%)]\tLoss: 0.411967\tLR: 0.00036354\n",
            "Train Epoch: 6 [122112/123872 (99%)]\tLoss: 0.443943\tLR: 0.00036343\n",
            "Train Epoch: 6 [122368/123872 (99%)]\tLoss: 0.394369\tLR: 0.00036331\n",
            "Train Epoch: 6 [122624/123872 (99%)]\tLoss: 0.369085\tLR: 0.00036319\n",
            "Train Epoch: 6 [122880/123872 (99%)]\tLoss: 0.416437\tLR: 0.00036308\n",
            "Train Epoch: 6 [122880/123872 (99%)]\tLoss: 0.416437\n",
            "Train Epoch: 6 [123136/123872 (99%)]\tLoss: 0.437447\tLR: 0.00036296\n",
            "Train Epoch: 6 [123392/123872 (100%)]\tLoss: 0.440826\tLR: 0.00036284\n",
            "Train Epoch: 6 [108192/123872 (100%)]\tLoss: 0.433367\tLR: 0.00036273\n",
            "\n",
            "Test set: Average loss: 0.0016, Accuracy: 25261/30970 (81.57%)\n",
            "\n",
            "Train Epoch: 7 [0/123872 (0%)]\tLoss: 0.370188\tLR: 0.00036261\n",
            "Train Epoch: 7 [0/123872 (0%)]\tLoss: 0.370188\n",
            "Train Epoch: 7 [256/123872 (0%)]\tLoss: 0.417142\tLR: 0.00036249\n",
            "Train Epoch: 7 [512/123872 (0%)]\tLoss: 0.417837\tLR: 0.00036237\n",
            "Train Epoch: 7 [768/123872 (1%)]\tLoss: 0.431142\tLR: 0.00036226\n",
            "Train Epoch: 7 [1024/123872 (1%)]\tLoss: 0.390957\tLR: 0.00036214\n",
            "Train Epoch: 7 [1280/123872 (1%)]\tLoss: 0.378647\tLR: 0.00036202\n",
            "Train Epoch: 7 [1536/123872 (1%)]\tLoss: 0.373596\tLR: 0.00036191\n",
            "Train Epoch: 7 [1792/123872 (1%)]\tLoss: 0.392809\tLR: 0.00036179\n",
            "Train Epoch: 7 [2048/123872 (2%)]\tLoss: 0.435190\tLR: 0.00036167\n",
            "Train Epoch: 7 [2304/123872 (2%)]\tLoss: 0.380766\tLR: 0.00036155\n",
            "Train Epoch: 7 [2560/123872 (2%)]\tLoss: 0.430775\tLR: 0.00036144\n",
            "Train Epoch: 7 [2560/123872 (2%)]\tLoss: 0.430775\n",
            "Train Epoch: 7 [2816/123872 (2%)]\tLoss: 0.372853\tLR: 0.00036132\n",
            "Train Epoch: 7 [3072/123872 (2%)]\tLoss: 0.374516\tLR: 0.00036120\n",
            "Train Epoch: 7 [3328/123872 (3%)]\tLoss: 0.434203\tLR: 0.00036108\n",
            "Train Epoch: 7 [3584/123872 (3%)]\tLoss: 0.394612\tLR: 0.00036097\n",
            "Train Epoch: 7 [3840/123872 (3%)]\tLoss: 0.418702\tLR: 0.00036085\n",
            "Train Epoch: 7 [4096/123872 (3%)]\tLoss: 0.343852\tLR: 0.00036073\n",
            "Train Epoch: 7 [4352/123872 (4%)]\tLoss: 0.418805\tLR: 0.00036062\n",
            "Train Epoch: 7 [4608/123872 (4%)]\tLoss: 0.419533\tLR: 0.00036050\n",
            "Train Epoch: 7 [4864/123872 (4%)]\tLoss: 0.436576\tLR: 0.00036038\n",
            "Train Epoch: 7 [5120/123872 (4%)]\tLoss: 0.454151\tLR: 0.00036026\n",
            "Train Epoch: 7 [5120/123872 (4%)]\tLoss: 0.454151\n",
            "Train Epoch: 7 [5376/123872 (4%)]\tLoss: 0.402888\tLR: 0.00036015\n",
            "Train Epoch: 7 [5632/123872 (5%)]\tLoss: 0.393735\tLR: 0.00036003\n",
            "Train Epoch: 7 [5888/123872 (5%)]\tLoss: 0.379077\tLR: 0.00035991\n",
            "Train Epoch: 7 [6144/123872 (5%)]\tLoss: 0.359974\tLR: 0.00035979\n",
            "Train Epoch: 7 [6400/123872 (5%)]\tLoss: 0.502089\tLR: 0.00035968\n",
            "Train Epoch: 7 [6656/123872 (5%)]\tLoss: 0.390058\tLR: 0.00035956\n",
            "Train Epoch: 7 [6912/123872 (6%)]\tLoss: 0.369319\tLR: 0.00035944\n",
            "Train Epoch: 7 [7168/123872 (6%)]\tLoss: 0.372441\tLR: 0.00035933\n",
            "Train Epoch: 7 [7424/123872 (6%)]\tLoss: 0.408079\tLR: 0.00035921\n",
            "Train Epoch: 7 [7680/123872 (6%)]\tLoss: 0.376326\tLR: 0.00035909\n",
            "Train Epoch: 7 [7680/123872 (6%)]\tLoss: 0.376326\n",
            "Train Epoch: 7 [7936/123872 (6%)]\tLoss: 0.364217\tLR: 0.00035897\n",
            "Train Epoch: 7 [8192/123872 (7%)]\tLoss: 0.414032\tLR: 0.00035886\n",
            "Train Epoch: 7 [8448/123872 (7%)]\tLoss: 0.355093\tLR: 0.00035874\n",
            "Train Epoch: 7 [8704/123872 (7%)]\tLoss: 0.376381\tLR: 0.00035862\n",
            "Train Epoch: 7 [8960/123872 (7%)]\tLoss: 0.474530\tLR: 0.00035850\n",
            "Train Epoch: 7 [9216/123872 (7%)]\tLoss: 0.362682\tLR: 0.00035839\n",
            "Train Epoch: 7 [9472/123872 (8%)]\tLoss: 0.331627\tLR: 0.00035827\n",
            "Train Epoch: 7 [9728/123872 (8%)]\tLoss: 0.394855\tLR: 0.00035815\n",
            "Train Epoch: 7 [9984/123872 (8%)]\tLoss: 0.349245\tLR: 0.00035803\n",
            "Train Epoch: 7 [10240/123872 (8%)]\tLoss: 0.390159\tLR: 0.00035792\n",
            "Train Epoch: 7 [10240/123872 (8%)]\tLoss: 0.390159\n",
            "Train Epoch: 7 [10496/123872 (8%)]\tLoss: 0.335255\tLR: 0.00035780\n",
            "Train Epoch: 7 [10752/123872 (9%)]\tLoss: 0.372159\tLR: 0.00035768\n",
            "Train Epoch: 7 [11008/123872 (9%)]\tLoss: 0.388254\tLR: 0.00035757\n",
            "Train Epoch: 7 [11264/123872 (9%)]\tLoss: 0.461614\tLR: 0.00035745\n",
            "Train Epoch: 7 [11520/123872 (9%)]\tLoss: 0.398040\tLR: 0.00035733\n",
            "Train Epoch: 7 [11776/123872 (10%)]\tLoss: 0.363331\tLR: 0.00035721\n",
            "Train Epoch: 7 [12032/123872 (10%)]\tLoss: 0.334500\tLR: 0.00035710\n",
            "Train Epoch: 7 [12288/123872 (10%)]\tLoss: 0.379176\tLR: 0.00035698\n",
            "Train Epoch: 7 [12544/123872 (10%)]\tLoss: 0.360536\tLR: 0.00035686\n",
            "Train Epoch: 7 [12800/123872 (10%)]\tLoss: 0.378915\tLR: 0.00035674\n",
            "Train Epoch: 7 [12800/123872 (10%)]\tLoss: 0.378915\n",
            "Train Epoch: 7 [13056/123872 (11%)]\tLoss: 0.359136\tLR: 0.00035663\n",
            "Train Epoch: 7 [13312/123872 (11%)]\tLoss: 0.430976\tLR: 0.00035651\n",
            "Train Epoch: 7 [13568/123872 (11%)]\tLoss: 0.452139\tLR: 0.00035639\n",
            "Train Epoch: 7 [13824/123872 (11%)]\tLoss: 0.380773\tLR: 0.00035627\n",
            "Train Epoch: 7 [14080/123872 (11%)]\tLoss: 0.373321\tLR: 0.00035616\n",
            "Train Epoch: 7 [14336/123872 (12%)]\tLoss: 0.448612\tLR: 0.00035604\n",
            "Train Epoch: 7 [14592/123872 (12%)]\tLoss: 0.453785\tLR: 0.00035592\n",
            "Train Epoch: 7 [14848/123872 (12%)]\tLoss: 0.365359\tLR: 0.00035580\n",
            "Train Epoch: 7 [15104/123872 (12%)]\tLoss: 0.414697\tLR: 0.00035569\n",
            "Train Epoch: 7 [15360/123872 (12%)]\tLoss: 0.369601\tLR: 0.00035557\n",
            "Train Epoch: 7 [15360/123872 (12%)]\tLoss: 0.369601\n",
            "Train Epoch: 7 [15616/123872 (13%)]\tLoss: 0.404112\tLR: 0.00035545\n",
            "Train Epoch: 7 [15872/123872 (13%)]\tLoss: 0.427360\tLR: 0.00035533\n",
            "Train Epoch: 7 [16128/123872 (13%)]\tLoss: 0.412079\tLR: 0.00035522\n",
            "Train Epoch: 7 [16384/123872 (13%)]\tLoss: 0.347691\tLR: 0.00035510\n",
            "Train Epoch: 7 [16640/123872 (13%)]\tLoss: 0.370669\tLR: 0.00035498\n",
            "Train Epoch: 7 [16896/123872 (14%)]\tLoss: 0.385652\tLR: 0.00035486\n",
            "Train Epoch: 7 [17152/123872 (14%)]\tLoss: 0.361047\tLR: 0.00035475\n",
            "Train Epoch: 7 [17408/123872 (14%)]\tLoss: 0.371436\tLR: 0.00035463\n",
            "Train Epoch: 7 [17664/123872 (14%)]\tLoss: 0.365888\tLR: 0.00035451\n",
            "Train Epoch: 7 [17920/123872 (14%)]\tLoss: 0.402274\tLR: 0.00035439\n",
            "Train Epoch: 7 [17920/123872 (14%)]\tLoss: 0.402274\n",
            "Train Epoch: 7 [18176/123872 (15%)]\tLoss: 0.359021\tLR: 0.00035428\n",
            "Train Epoch: 7 [18432/123872 (15%)]\tLoss: 0.430689\tLR: 0.00035416\n",
            "Train Epoch: 7 [18688/123872 (15%)]\tLoss: 0.433546\tLR: 0.00035404\n",
            "Train Epoch: 7 [18944/123872 (15%)]\tLoss: 0.369009\tLR: 0.00035392\n",
            "Train Epoch: 7 [19200/123872 (15%)]\tLoss: 0.445815\tLR: 0.00035381\n",
            "Train Epoch: 7 [19456/123872 (16%)]\tLoss: 0.407798\tLR: 0.00035369\n",
            "Train Epoch: 7 [19712/123872 (16%)]\tLoss: 0.382539\tLR: 0.00035357\n",
            "Train Epoch: 7 [19968/123872 (16%)]\tLoss: 0.390526\tLR: 0.00035345\n",
            "Train Epoch: 7 [20224/123872 (16%)]\tLoss: 0.415930\tLR: 0.00035334\n",
            "Train Epoch: 7 [20480/123872 (17%)]\tLoss: 0.361262\tLR: 0.00035322\n",
            "Train Epoch: 7 [20480/123872 (17%)]\tLoss: 0.361262\n",
            "Train Epoch: 7 [20736/123872 (17%)]\tLoss: 0.441100\tLR: 0.00035310\n",
            "Train Epoch: 7 [20992/123872 (17%)]\tLoss: 0.440901\tLR: 0.00035298\n",
            "Train Epoch: 7 [21248/123872 (17%)]\tLoss: 0.383831\tLR: 0.00035287\n",
            "Train Epoch: 7 [21504/123872 (17%)]\tLoss: 0.435210\tLR: 0.00035275\n",
            "Train Epoch: 7 [21760/123872 (18%)]\tLoss: 0.366748\tLR: 0.00035263\n",
            "Train Epoch: 7 [22016/123872 (18%)]\tLoss: 0.386505\tLR: 0.00035251\n",
            "Train Epoch: 7 [22272/123872 (18%)]\tLoss: 0.363069\tLR: 0.00035240\n",
            "Train Epoch: 7 [22528/123872 (18%)]\tLoss: 0.429188\tLR: 0.00035228\n",
            "Train Epoch: 7 [22784/123872 (18%)]\tLoss: 0.326969\tLR: 0.00035216\n",
            "Train Epoch: 7 [23040/123872 (19%)]\tLoss: 0.385282\tLR: 0.00035204\n",
            "Train Epoch: 7 [23040/123872 (19%)]\tLoss: 0.385282\n",
            "Train Epoch: 7 [23296/123872 (19%)]\tLoss: 0.450058\tLR: 0.00035193\n",
            "Train Epoch: 7 [23552/123872 (19%)]\tLoss: 0.389363\tLR: 0.00035181\n",
            "Train Epoch: 7 [23808/123872 (19%)]\tLoss: 0.379044\tLR: 0.00035169\n",
            "Train Epoch: 7 [24064/123872 (19%)]\tLoss: 0.395507\tLR: 0.00035157\n",
            "Train Epoch: 7 [24320/123872 (20%)]\tLoss: 0.394627\tLR: 0.00035145\n",
            "Train Epoch: 7 [24576/123872 (20%)]\tLoss: 0.438236\tLR: 0.00035134\n",
            "Train Epoch: 7 [24832/123872 (20%)]\tLoss: 0.377193\tLR: 0.00035122\n",
            "Train Epoch: 7 [25088/123872 (20%)]\tLoss: 0.378624\tLR: 0.00035110\n",
            "Train Epoch: 7 [25344/123872 (20%)]\tLoss: 0.370800\tLR: 0.00035098\n",
            "Train Epoch: 7 [25600/123872 (21%)]\tLoss: 0.411442\tLR: 0.00035087\n",
            "Train Epoch: 7 [25600/123872 (21%)]\tLoss: 0.411442\n",
            "Train Epoch: 7 [25856/123872 (21%)]\tLoss: 0.355343\tLR: 0.00035075\n",
            "Train Epoch: 7 [26112/123872 (21%)]\tLoss: 0.406642\tLR: 0.00035063\n",
            "Train Epoch: 7 [26368/123872 (21%)]\tLoss: 0.414084\tLR: 0.00035051\n",
            "Train Epoch: 7 [26624/123872 (21%)]\tLoss: 0.369792\tLR: 0.00035040\n",
            "Train Epoch: 7 [26880/123872 (22%)]\tLoss: 0.461521\tLR: 0.00035028\n",
            "Train Epoch: 7 [27136/123872 (22%)]\tLoss: 0.488492\tLR: 0.00035016\n",
            "Train Epoch: 7 [27392/123872 (22%)]\tLoss: 0.437483\tLR: 0.00035004\n",
            "Train Epoch: 7 [27648/123872 (22%)]\tLoss: 0.434796\tLR: 0.00034992\n",
            "Train Epoch: 7 [27904/123872 (23%)]\tLoss: 0.388720\tLR: 0.00034981\n",
            "Train Epoch: 7 [28160/123872 (23%)]\tLoss: 0.425472\tLR: 0.00034969\n",
            "Train Epoch: 7 [28160/123872 (23%)]\tLoss: 0.425472\n",
            "Train Epoch: 7 [28416/123872 (23%)]\tLoss: 0.421678\tLR: 0.00034957\n",
            "Train Epoch: 7 [28672/123872 (23%)]\tLoss: 0.461912\tLR: 0.00034945\n",
            "Train Epoch: 7 [28928/123872 (23%)]\tLoss: 0.359740\tLR: 0.00034934\n",
            "Train Epoch: 7 [29184/123872 (24%)]\tLoss: 0.408135\tLR: 0.00034922\n",
            "Train Epoch: 7 [29440/123872 (24%)]\tLoss: 0.408689\tLR: 0.00034910\n",
            "Train Epoch: 7 [29696/123872 (24%)]\tLoss: 0.404332\tLR: 0.00034898\n",
            "Train Epoch: 7 [29952/123872 (24%)]\tLoss: 0.399317\tLR: 0.00034887\n",
            "Train Epoch: 7 [30208/123872 (24%)]\tLoss: 0.408312\tLR: 0.00034875\n",
            "Train Epoch: 7 [30464/123872 (25%)]\tLoss: 0.515443\tLR: 0.00034863\n",
            "Train Epoch: 7 [30720/123872 (25%)]\tLoss: 0.380766\tLR: 0.00034851\n",
            "Train Epoch: 7 [30720/123872 (25%)]\tLoss: 0.380766\n",
            "Train Epoch: 7 [30976/123872 (25%)]\tLoss: 0.401434\tLR: 0.00034839\n",
            "Train Epoch: 7 [31232/123872 (25%)]\tLoss: 0.441507\tLR: 0.00034828\n",
            "Train Epoch: 7 [31488/123872 (25%)]\tLoss: 0.346008\tLR: 0.00034816\n",
            "Train Epoch: 7 [31744/123872 (26%)]\tLoss: 0.359425\tLR: 0.00034804\n",
            "Train Epoch: 7 [32000/123872 (26%)]\tLoss: 0.365680\tLR: 0.00034792\n",
            "Train Epoch: 7 [32256/123872 (26%)]\tLoss: 0.396995\tLR: 0.00034781\n",
            "Train Epoch: 7 [32512/123872 (26%)]\tLoss: 0.409530\tLR: 0.00034769\n",
            "Train Epoch: 7 [32768/123872 (26%)]\tLoss: 0.402423\tLR: 0.00034757\n",
            "Train Epoch: 7 [33024/123872 (27%)]\tLoss: 0.367558\tLR: 0.00034745\n",
            "Train Epoch: 7 [33280/123872 (27%)]\tLoss: 0.375542\tLR: 0.00034734\n",
            "Train Epoch: 7 [33280/123872 (27%)]\tLoss: 0.375542\n",
            "Train Epoch: 7 [33536/123872 (27%)]\tLoss: 0.356031\tLR: 0.00034722\n",
            "Train Epoch: 7 [33792/123872 (27%)]\tLoss: 0.348605\tLR: 0.00034710\n",
            "Train Epoch: 7 [34048/123872 (27%)]\tLoss: 0.417747\tLR: 0.00034698\n",
            "Train Epoch: 7 [34304/123872 (28%)]\tLoss: 0.374898\tLR: 0.00034686\n",
            "Train Epoch: 7 [34560/123872 (28%)]\tLoss: 0.411080\tLR: 0.00034675\n",
            "Train Epoch: 7 [34816/123872 (28%)]\tLoss: 0.400477\tLR: 0.00034663\n",
            "Train Epoch: 7 [35072/123872 (28%)]\tLoss: 0.333851\tLR: 0.00034651\n",
            "Train Epoch: 7 [35328/123872 (29%)]\tLoss: 0.428647\tLR: 0.00034639\n",
            "Train Epoch: 7 [35584/123872 (29%)]\tLoss: 0.374880\tLR: 0.00034628\n",
            "Train Epoch: 7 [35840/123872 (29%)]\tLoss: 0.375493\tLR: 0.00034616\n",
            "Train Epoch: 7 [35840/123872 (29%)]\tLoss: 0.375493\n",
            "Train Epoch: 7 [36096/123872 (29%)]\tLoss: 0.349628\tLR: 0.00034604\n",
            "Train Epoch: 7 [36352/123872 (29%)]\tLoss: 0.429614\tLR: 0.00034592\n",
            "Train Epoch: 7 [36608/123872 (30%)]\tLoss: 0.369896\tLR: 0.00034580\n",
            "Train Epoch: 7 [36864/123872 (30%)]\tLoss: 0.339101\tLR: 0.00034569\n",
            "Train Epoch: 7 [37120/123872 (30%)]\tLoss: 0.411345\tLR: 0.00034557\n",
            "Train Epoch: 7 [37376/123872 (30%)]\tLoss: 0.449865\tLR: 0.00034545\n",
            "Train Epoch: 7 [37632/123872 (30%)]\tLoss: 0.454692\tLR: 0.00034533\n",
            "Train Epoch: 7 [37888/123872 (31%)]\tLoss: 0.426132\tLR: 0.00034521\n",
            "Train Epoch: 7 [38144/123872 (31%)]\tLoss: 0.381344\tLR: 0.00034510\n",
            "Train Epoch: 7 [38400/123872 (31%)]\tLoss: 0.361452\tLR: 0.00034498\n",
            "Train Epoch: 7 [38400/123872 (31%)]\tLoss: 0.361452\n",
            "Train Epoch: 7 [38656/123872 (31%)]\tLoss: 0.405106\tLR: 0.00034486\n",
            "Train Epoch: 7 [38912/123872 (31%)]\tLoss: 0.362318\tLR: 0.00034474\n",
            "Train Epoch: 7 [39168/123872 (32%)]\tLoss: 0.419653\tLR: 0.00034463\n",
            "Train Epoch: 7 [39424/123872 (32%)]\tLoss: 0.422851\tLR: 0.00034451\n",
            "Train Epoch: 7 [39680/123872 (32%)]\tLoss: 0.412228\tLR: 0.00034439\n",
            "Train Epoch: 7 [39936/123872 (32%)]\tLoss: 0.392173\tLR: 0.00034427\n",
            "Train Epoch: 7 [40192/123872 (32%)]\tLoss: 0.458326\tLR: 0.00034415\n",
            "Train Epoch: 7 [40448/123872 (33%)]\tLoss: 0.357595\tLR: 0.00034404\n",
            "Train Epoch: 7 [40704/123872 (33%)]\tLoss: 0.405812\tLR: 0.00034392\n",
            "Train Epoch: 7 [40960/123872 (33%)]\tLoss: 0.435777\tLR: 0.00034380\n",
            "Train Epoch: 7 [40960/123872 (33%)]\tLoss: 0.435777\n",
            "Train Epoch: 7 [41216/123872 (33%)]\tLoss: 0.393836\tLR: 0.00034368\n",
            "Train Epoch: 7 [41472/123872 (33%)]\tLoss: 0.346541\tLR: 0.00034357\n",
            "Train Epoch: 7 [41728/123872 (34%)]\tLoss: 0.438891\tLR: 0.00034345\n",
            "Train Epoch: 7 [41984/123872 (34%)]\tLoss: 0.357388\tLR: 0.00034333\n",
            "Train Epoch: 7 [42240/123872 (34%)]\tLoss: 0.368312\tLR: 0.00034321\n",
            "Train Epoch: 7 [42496/123872 (34%)]\tLoss: 0.393307\tLR: 0.00034309\n",
            "Train Epoch: 7 [42752/123872 (35%)]\tLoss: 0.390191\tLR: 0.00034298\n",
            "Train Epoch: 7 [43008/123872 (35%)]\tLoss: 0.356559\tLR: 0.00034286\n",
            "Train Epoch: 7 [43264/123872 (35%)]\tLoss: 0.369168\tLR: 0.00034274\n",
            "Train Epoch: 7 [43520/123872 (35%)]\tLoss: 0.459212\tLR: 0.00034262\n",
            "Train Epoch: 7 [43520/123872 (35%)]\tLoss: 0.459212\n",
            "Train Epoch: 7 [43776/123872 (35%)]\tLoss: 0.404191\tLR: 0.00034250\n",
            "Train Epoch: 7 [44032/123872 (36%)]\tLoss: 0.381286\tLR: 0.00034239\n",
            "Train Epoch: 7 [44288/123872 (36%)]\tLoss: 0.413108\tLR: 0.00034227\n",
            "Train Epoch: 7 [44544/123872 (36%)]\tLoss: 0.390294\tLR: 0.00034215\n",
            "Train Epoch: 7 [44800/123872 (36%)]\tLoss: 0.431357\tLR: 0.00034203\n",
            "Train Epoch: 7 [45056/123872 (36%)]\tLoss: 0.351590\tLR: 0.00034191\n",
            "Train Epoch: 7 [45312/123872 (37%)]\tLoss: 0.416036\tLR: 0.00034180\n",
            "Train Epoch: 7 [45568/123872 (37%)]\tLoss: 0.413179\tLR: 0.00034168\n",
            "Train Epoch: 7 [45824/123872 (37%)]\tLoss: 0.354576\tLR: 0.00034156\n",
            "Train Epoch: 7 [46080/123872 (37%)]\tLoss: 0.422775\tLR: 0.00034144\n",
            "Train Epoch: 7 [46080/123872 (37%)]\tLoss: 0.422775\n",
            "Train Epoch: 7 [46336/123872 (37%)]\tLoss: 0.412153\tLR: 0.00034133\n",
            "Train Epoch: 7 [46592/123872 (38%)]\tLoss: 0.375222\tLR: 0.00034121\n",
            "Train Epoch: 7 [46848/123872 (38%)]\tLoss: 0.338767\tLR: 0.00034109\n",
            "Train Epoch: 7 [47104/123872 (38%)]\tLoss: 0.391647\tLR: 0.00034097\n",
            "Train Epoch: 7 [47360/123872 (38%)]\tLoss: 0.372324\tLR: 0.00034085\n",
            "Train Epoch: 7 [47616/123872 (38%)]\tLoss: 0.469996\tLR: 0.00034074\n",
            "Train Epoch: 7 [47872/123872 (39%)]\tLoss: 0.435547\tLR: 0.00034062\n",
            "Train Epoch: 7 [48128/123872 (39%)]\tLoss: 0.413832\tLR: 0.00034050\n",
            "Train Epoch: 7 [48384/123872 (39%)]\tLoss: 0.385448\tLR: 0.00034038\n",
            "Train Epoch: 7 [48640/123872 (39%)]\tLoss: 0.354349\tLR: 0.00034026\n",
            "Train Epoch: 7 [48640/123872 (39%)]\tLoss: 0.354349\n",
            "Train Epoch: 7 [48896/123872 (39%)]\tLoss: 0.399487\tLR: 0.00034015\n",
            "Train Epoch: 7 [49152/123872 (40%)]\tLoss: 0.363017\tLR: 0.00034003\n",
            "Train Epoch: 7 [49408/123872 (40%)]\tLoss: 0.373256\tLR: 0.00033991\n",
            "Train Epoch: 7 [49664/123872 (40%)]\tLoss: 0.408323\tLR: 0.00033979\n",
            "Train Epoch: 7 [49920/123872 (40%)]\tLoss: 0.426756\tLR: 0.00033967\n",
            "Train Epoch: 7 [50176/123872 (40%)]\tLoss: 0.433884\tLR: 0.00033956\n",
            "Train Epoch: 7 [50432/123872 (41%)]\tLoss: 0.497753\tLR: 0.00033944\n",
            "Train Epoch: 7 [50688/123872 (41%)]\tLoss: 0.462168\tLR: 0.00033932\n",
            "Train Epoch: 7 [50944/123872 (41%)]\tLoss: 0.378763\tLR: 0.00033920\n",
            "Train Epoch: 7 [51200/123872 (41%)]\tLoss: 0.421587\tLR: 0.00033908\n",
            "Train Epoch: 7 [51200/123872 (41%)]\tLoss: 0.421587\n",
            "Train Epoch: 7 [51456/123872 (42%)]\tLoss: 0.364895\tLR: 0.00033897\n",
            "Train Epoch: 7 [51712/123872 (42%)]\tLoss: 0.449854\tLR: 0.00033885\n",
            "Train Epoch: 7 [51968/123872 (42%)]\tLoss: 0.435101\tLR: 0.00033873\n",
            "Train Epoch: 7 [52224/123872 (42%)]\tLoss: 0.430973\tLR: 0.00033861\n",
            "Train Epoch: 7 [52480/123872 (42%)]\tLoss: 0.409815\tLR: 0.00033850\n",
            "Train Epoch: 7 [52736/123872 (43%)]\tLoss: 0.385146\tLR: 0.00033838\n",
            "Train Epoch: 7 [52992/123872 (43%)]\tLoss: 0.500429\tLR: 0.00033826\n",
            "Train Epoch: 7 [53248/123872 (43%)]\tLoss: 0.446873\tLR: 0.00033814\n",
            "Train Epoch: 7 [53504/123872 (43%)]\tLoss: 0.424517\tLR: 0.00033802\n",
            "Train Epoch: 7 [53760/123872 (43%)]\tLoss: 0.469901\tLR: 0.00033791\n",
            "Train Epoch: 7 [53760/123872 (43%)]\tLoss: 0.469901\n",
            "Train Epoch: 7 [54016/123872 (44%)]\tLoss: 0.407783\tLR: 0.00033779\n",
            "Train Epoch: 7 [54272/123872 (44%)]\tLoss: 0.387763\tLR: 0.00033767\n",
            "Train Epoch: 7 [54528/123872 (44%)]\tLoss: 0.420512\tLR: 0.00033755\n",
            "Train Epoch: 7 [54784/123872 (44%)]\tLoss: 0.418181\tLR: 0.00033743\n",
            "Train Epoch: 7 [55040/123872 (44%)]\tLoss: 0.404115\tLR: 0.00033732\n",
            "Train Epoch: 7 [55296/123872 (45%)]\tLoss: 0.416394\tLR: 0.00033720\n",
            "Train Epoch: 7 [55552/123872 (45%)]\tLoss: 0.415981\tLR: 0.00033708\n",
            "Train Epoch: 7 [55808/123872 (45%)]\tLoss: 0.363549\tLR: 0.00033696\n",
            "Train Epoch: 7 [56064/123872 (45%)]\tLoss: 0.423300\tLR: 0.00033684\n",
            "Train Epoch: 7 [56320/123872 (45%)]\tLoss: 0.375299\tLR: 0.00033673\n",
            "Train Epoch: 7 [56320/123872 (45%)]\tLoss: 0.375299\n",
            "Train Epoch: 7 [56576/123872 (46%)]\tLoss: 0.370597\tLR: 0.00033661\n",
            "Train Epoch: 7 [56832/123872 (46%)]\tLoss: 0.375123\tLR: 0.00033649\n",
            "Train Epoch: 7 [57088/123872 (46%)]\tLoss: 0.356748\tLR: 0.00033637\n",
            "Train Epoch: 7 [57344/123872 (46%)]\tLoss: 0.311621\tLR: 0.00033625\n",
            "Train Epoch: 7 [57600/123872 (46%)]\tLoss: 0.465461\tLR: 0.00033614\n",
            "Train Epoch: 7 [57856/123872 (47%)]\tLoss: 0.430508\tLR: 0.00033602\n",
            "Train Epoch: 7 [58112/123872 (47%)]\tLoss: 0.359199\tLR: 0.00033590\n",
            "Train Epoch: 7 [58368/123872 (47%)]\tLoss: 0.400865\tLR: 0.00033578\n",
            "Train Epoch: 7 [58624/123872 (47%)]\tLoss: 0.410758\tLR: 0.00033566\n",
            "Train Epoch: 7 [58880/123872 (48%)]\tLoss: 0.404732\tLR: 0.00033555\n",
            "Train Epoch: 7 [58880/123872 (48%)]\tLoss: 0.404732\n",
            "Train Epoch: 7 [59136/123872 (48%)]\tLoss: 0.399907\tLR: 0.00033543\n",
            "Train Epoch: 7 [59392/123872 (48%)]\tLoss: 0.382524\tLR: 0.00033531\n",
            "Train Epoch: 7 [59648/123872 (48%)]\tLoss: 0.415003\tLR: 0.00033519\n",
            "Train Epoch: 7 [59904/123872 (48%)]\tLoss: 0.400870\tLR: 0.00033507\n",
            "Train Epoch: 7 [60160/123872 (49%)]\tLoss: 0.345761\tLR: 0.00033496\n",
            "Train Epoch: 7 [60416/123872 (49%)]\tLoss: 0.439653\tLR: 0.00033484\n",
            "Train Epoch: 7 [60672/123872 (49%)]\tLoss: 0.373510\tLR: 0.00033472\n",
            "Train Epoch: 7 [60928/123872 (49%)]\tLoss: 0.406622\tLR: 0.00033460\n",
            "Train Epoch: 7 [61184/123872 (49%)]\tLoss: 0.386674\tLR: 0.00033448\n",
            "Train Epoch: 7 [61440/123872 (50%)]\tLoss: 0.391837\tLR: 0.00033437\n",
            "Train Epoch: 7 [61440/123872 (50%)]\tLoss: 0.391837\n",
            "Train Epoch: 7 [61696/123872 (50%)]\tLoss: 0.448176\tLR: 0.00033425\n",
            "Train Epoch: 7 [61952/123872 (50%)]\tLoss: 0.344697\tLR: 0.00033413\n",
            "Train Epoch: 7 [62208/123872 (50%)]\tLoss: 0.407648\tLR: 0.00033401\n",
            "Train Epoch: 7 [62464/123872 (50%)]\tLoss: 0.446564\tLR: 0.00033389\n",
            "Train Epoch: 7 [62720/123872 (51%)]\tLoss: 0.372456\tLR: 0.00033378\n",
            "Train Epoch: 7 [62976/123872 (51%)]\tLoss: 0.370487\tLR: 0.00033366\n",
            "Train Epoch: 7 [63232/123872 (51%)]\tLoss: 0.398768\tLR: 0.00033354\n",
            "Train Epoch: 7 [63488/123872 (51%)]\tLoss: 0.323811\tLR: 0.00033342\n",
            "Train Epoch: 7 [63744/123872 (51%)]\tLoss: 0.425509\tLR: 0.00033330\n",
            "Train Epoch: 7 [64000/123872 (52%)]\tLoss: 0.447677\tLR: 0.00033319\n",
            "Train Epoch: 7 [64000/123872 (52%)]\tLoss: 0.447677\n",
            "Train Epoch: 7 [64256/123872 (52%)]\tLoss: 0.439089\tLR: 0.00033307\n",
            "Train Epoch: 7 [64512/123872 (52%)]\tLoss: 0.368550\tLR: 0.00033295\n",
            "Train Epoch: 7 [64768/123872 (52%)]\tLoss: 0.405654\tLR: 0.00033283\n",
            "Train Epoch: 7 [65024/123872 (52%)]\tLoss: 0.378768\tLR: 0.00033271\n",
            "Train Epoch: 7 [65280/123872 (53%)]\tLoss: 0.424304\tLR: 0.00033260\n",
            "Train Epoch: 7 [65536/123872 (53%)]\tLoss: 0.359261\tLR: 0.00033248\n",
            "Train Epoch: 7 [65792/123872 (53%)]\tLoss: 0.391640\tLR: 0.00033236\n",
            "Train Epoch: 7 [66048/123872 (53%)]\tLoss: 0.410888\tLR: 0.00033224\n",
            "Train Epoch: 7 [66304/123872 (54%)]\tLoss: 0.418577\tLR: 0.00033212\n",
            "Train Epoch: 7 [66560/123872 (54%)]\tLoss: 0.358141\tLR: 0.00033201\n",
            "Train Epoch: 7 [66560/123872 (54%)]\tLoss: 0.358141\n",
            "Train Epoch: 7 [66816/123872 (54%)]\tLoss: 0.383116\tLR: 0.00033189\n",
            "Train Epoch: 7 [67072/123872 (54%)]\tLoss: 0.344990\tLR: 0.00033177\n",
            "Train Epoch: 7 [67328/123872 (54%)]\tLoss: 0.418224\tLR: 0.00033165\n",
            "Train Epoch: 7 [67584/123872 (55%)]\tLoss: 0.408388\tLR: 0.00033153\n",
            "Train Epoch: 7 [67840/123872 (55%)]\tLoss: 0.379126\tLR: 0.00033142\n",
            "Train Epoch: 7 [68096/123872 (55%)]\tLoss: 0.431388\tLR: 0.00033130\n",
            "Train Epoch: 7 [68352/123872 (55%)]\tLoss: 0.362551\tLR: 0.00033118\n",
            "Train Epoch: 7 [68608/123872 (55%)]\tLoss: 0.454549\tLR: 0.00033106\n",
            "Train Epoch: 7 [68864/123872 (56%)]\tLoss: 0.408472\tLR: 0.00033094\n",
            "Train Epoch: 7 [69120/123872 (56%)]\tLoss: 0.364935\tLR: 0.00033083\n",
            "Train Epoch: 7 [69120/123872 (56%)]\tLoss: 0.364935\n",
            "Train Epoch: 7 [69376/123872 (56%)]\tLoss: 0.440836\tLR: 0.00033071\n",
            "Train Epoch: 7 [69632/123872 (56%)]\tLoss: 0.464484\tLR: 0.00033059\n",
            "Train Epoch: 7 [69888/123872 (56%)]\tLoss: 0.478437\tLR: 0.00033047\n",
            "Train Epoch: 7 [70144/123872 (57%)]\tLoss: 0.367474\tLR: 0.00033035\n",
            "Train Epoch: 7 [70400/123872 (57%)]\tLoss: 0.404835\tLR: 0.00033024\n",
            "Train Epoch: 7 [70656/123872 (57%)]\tLoss: 0.443040\tLR: 0.00033012\n",
            "Train Epoch: 7 [70912/123872 (57%)]\tLoss: 0.411640\tLR: 0.00033000\n",
            "Train Epoch: 7 [71168/123872 (57%)]\tLoss: 0.436384\tLR: 0.00032988\n",
            "Train Epoch: 7 [71424/123872 (58%)]\tLoss: 0.381092\tLR: 0.00032976\n",
            "Train Epoch: 7 [71680/123872 (58%)]\tLoss: 0.394500\tLR: 0.00032965\n",
            "Train Epoch: 7 [71680/123872 (58%)]\tLoss: 0.394500\n",
            "Train Epoch: 7 [71936/123872 (58%)]\tLoss: 0.385009\tLR: 0.00032953\n",
            "Train Epoch: 7 [72192/123872 (58%)]\tLoss: 0.442985\tLR: 0.00032941\n",
            "Train Epoch: 7 [72448/123872 (58%)]\tLoss: 0.369974\tLR: 0.00032929\n",
            "Train Epoch: 7 [72704/123872 (59%)]\tLoss: 0.308690\tLR: 0.00032917\n",
            "Train Epoch: 7 [72960/123872 (59%)]\tLoss: 0.399239\tLR: 0.00032906\n",
            "Train Epoch: 7 [73216/123872 (59%)]\tLoss: 0.378637\tLR: 0.00032894\n",
            "Train Epoch: 7 [73472/123872 (59%)]\tLoss: 0.400997\tLR: 0.00032882\n",
            "Train Epoch: 7 [73728/123872 (60%)]\tLoss: 0.436921\tLR: 0.00032870\n",
            "Train Epoch: 7 [73984/123872 (60%)]\tLoss: 0.359296\tLR: 0.00032858\n",
            "Train Epoch: 7 [74240/123872 (60%)]\tLoss: 0.413341\tLR: 0.00032847\n",
            "Train Epoch: 7 [74240/123872 (60%)]\tLoss: 0.413341\n",
            "Train Epoch: 7 [74496/123872 (60%)]\tLoss: 0.421404\tLR: 0.00032835\n",
            "Train Epoch: 7 [74752/123872 (60%)]\tLoss: 0.391294\tLR: 0.00032823\n",
            "Train Epoch: 7 [75008/123872 (61%)]\tLoss: 0.386920\tLR: 0.00032811\n",
            "Train Epoch: 7 [75264/123872 (61%)]\tLoss: 0.342629\tLR: 0.00032799\n",
            "Train Epoch: 7 [75520/123872 (61%)]\tLoss: 0.426721\tLR: 0.00032788\n",
            "Train Epoch: 7 [75776/123872 (61%)]\tLoss: 0.406355\tLR: 0.00032776\n",
            "Train Epoch: 7 [76032/123872 (61%)]\tLoss: 0.400923\tLR: 0.00032764\n",
            "Train Epoch: 7 [76288/123872 (62%)]\tLoss: 0.366118\tLR: 0.00032752\n",
            "Train Epoch: 7 [76544/123872 (62%)]\tLoss: 0.394969\tLR: 0.00032740\n",
            "Train Epoch: 7 [76800/123872 (62%)]\tLoss: 0.457598\tLR: 0.00032729\n",
            "Train Epoch: 7 [76800/123872 (62%)]\tLoss: 0.457598\n",
            "Train Epoch: 7 [77056/123872 (62%)]\tLoss: 0.426553\tLR: 0.00032717\n",
            "Train Epoch: 7 [77312/123872 (62%)]\tLoss: 0.359829\tLR: 0.00032705\n",
            "Train Epoch: 7 [77568/123872 (63%)]\tLoss: 0.430872\tLR: 0.00032693\n",
            "Train Epoch: 7 [77824/123872 (63%)]\tLoss: 0.460057\tLR: 0.00032681\n",
            "Train Epoch: 7 [78080/123872 (63%)]\tLoss: 0.393172\tLR: 0.00032670\n",
            "Train Epoch: 7 [78336/123872 (63%)]\tLoss: 0.365243\tLR: 0.00032658\n",
            "Train Epoch: 7 [78592/123872 (63%)]\tLoss: 0.403270\tLR: 0.00032646\n",
            "Train Epoch: 7 [78848/123872 (64%)]\tLoss: 0.450703\tLR: 0.00032634\n",
            "Train Epoch: 7 [79104/123872 (64%)]\tLoss: 0.411501\tLR: 0.00032622\n",
            "Train Epoch: 7 [79360/123872 (64%)]\tLoss: 0.374052\tLR: 0.00032611\n",
            "Train Epoch: 7 [79360/123872 (64%)]\tLoss: 0.374052\n",
            "Train Epoch: 7 [79616/123872 (64%)]\tLoss: 0.465113\tLR: 0.00032599\n",
            "Train Epoch: 7 [79872/123872 (64%)]\tLoss: 0.366842\tLR: 0.00032587\n",
            "Train Epoch: 7 [80128/123872 (65%)]\tLoss: 0.367694\tLR: 0.00032575\n",
            "Train Epoch: 7 [80384/123872 (65%)]\tLoss: 0.399091\tLR: 0.00032563\n",
            "Train Epoch: 7 [80640/123872 (65%)]\tLoss: 0.390902\tLR: 0.00032552\n",
            "Train Epoch: 7 [80896/123872 (65%)]\tLoss: 0.459106\tLR: 0.00032540\n",
            "Train Epoch: 7 [81152/123872 (65%)]\tLoss: 0.393917\tLR: 0.00032528\n",
            "Train Epoch: 7 [81408/123872 (66%)]\tLoss: 0.423715\tLR: 0.00032516\n",
            "Train Epoch: 7 [81664/123872 (66%)]\tLoss: 0.411724\tLR: 0.00032504\n",
            "Train Epoch: 7 [81920/123872 (66%)]\tLoss: 0.365213\tLR: 0.00032493\n",
            "Train Epoch: 7 [81920/123872 (66%)]\tLoss: 0.365213\n",
            "Train Epoch: 7 [82176/123872 (66%)]\tLoss: 0.368857\tLR: 0.00032481\n",
            "Train Epoch: 7 [82432/123872 (67%)]\tLoss: 0.382976\tLR: 0.00032469\n",
            "Train Epoch: 7 [82688/123872 (67%)]\tLoss: 0.405479\tLR: 0.00032457\n",
            "Train Epoch: 7 [82944/123872 (67%)]\tLoss: 0.424056\tLR: 0.00032445\n",
            "Train Epoch: 7 [83200/123872 (67%)]\tLoss: 0.337318\tLR: 0.00032434\n",
            "Train Epoch: 7 [83456/123872 (67%)]\tLoss: 0.420365\tLR: 0.00032422\n",
            "Train Epoch: 7 [83712/123872 (68%)]\tLoss: 0.399577\tLR: 0.00032410\n",
            "Train Epoch: 7 [83968/123872 (68%)]\tLoss: 0.423352\tLR: 0.00032398\n",
            "Train Epoch: 7 [84224/123872 (68%)]\tLoss: 0.364434\tLR: 0.00032386\n",
            "Train Epoch: 7 [84480/123872 (68%)]\tLoss: 0.370973\tLR: 0.00032375\n",
            "Train Epoch: 7 [84480/123872 (68%)]\tLoss: 0.370973\n",
            "Train Epoch: 7 [84736/123872 (68%)]\tLoss: 0.402269\tLR: 0.00032363\n",
            "Train Epoch: 7 [84992/123872 (69%)]\tLoss: 0.369481\tLR: 0.00032351\n",
            "Train Epoch: 7 [85248/123872 (69%)]\tLoss: 0.357901\tLR: 0.00032339\n",
            "Train Epoch: 7 [85504/123872 (69%)]\tLoss: 0.362173\tLR: 0.00032327\n",
            "Train Epoch: 7 [85760/123872 (69%)]\tLoss: 0.377529\tLR: 0.00032316\n",
            "Train Epoch: 7 [86016/123872 (69%)]\tLoss: 0.324599\tLR: 0.00032304\n",
            "Train Epoch: 7 [86272/123872 (70%)]\tLoss: 0.427257\tLR: 0.00032292\n",
            "Train Epoch: 7 [86528/123872 (70%)]\tLoss: 0.446730\tLR: 0.00032280\n",
            "Train Epoch: 7 [86784/123872 (70%)]\tLoss: 0.374736\tLR: 0.00032268\n",
            "Train Epoch: 7 [87040/123872 (70%)]\tLoss: 0.419215\tLR: 0.00032257\n",
            "Train Epoch: 7 [87040/123872 (70%)]\tLoss: 0.419215\n",
            "Train Epoch: 7 [87296/123872 (70%)]\tLoss: 0.395698\tLR: 0.00032245\n",
            "Train Epoch: 7 [87552/123872 (71%)]\tLoss: 0.367532\tLR: 0.00032233\n",
            "Train Epoch: 7 [87808/123872 (71%)]\tLoss: 0.391376\tLR: 0.00032221\n",
            "Train Epoch: 7 [88064/123872 (71%)]\tLoss: 0.475279\tLR: 0.00032209\n",
            "Train Epoch: 7 [88320/123872 (71%)]\tLoss: 0.448215\tLR: 0.00032198\n",
            "Train Epoch: 7 [88576/123872 (71%)]\tLoss: 0.362767\tLR: 0.00032186\n",
            "Train Epoch: 7 [88832/123872 (72%)]\tLoss: 0.363997\tLR: 0.00032174\n",
            "Train Epoch: 7 [89088/123872 (72%)]\tLoss: 0.424762\tLR: 0.00032162\n",
            "Train Epoch: 7 [89344/123872 (72%)]\tLoss: 0.390699\tLR: 0.00032150\n",
            "Train Epoch: 7 [89600/123872 (72%)]\tLoss: 0.412997\tLR: 0.00032139\n",
            "Train Epoch: 7 [89600/123872 (72%)]\tLoss: 0.412997\n",
            "Train Epoch: 7 [89856/123872 (73%)]\tLoss: 0.390551\tLR: 0.00032127\n",
            "Train Epoch: 7 [90112/123872 (73%)]\tLoss: 0.415096\tLR: 0.00032115\n",
            "Train Epoch: 7 [90368/123872 (73%)]\tLoss: 0.353032\tLR: 0.00032103\n",
            "Train Epoch: 7 [90624/123872 (73%)]\tLoss: 0.372480\tLR: 0.00032092\n",
            "Train Epoch: 7 [90880/123872 (73%)]\tLoss: 0.421740\tLR: 0.00032080\n",
            "Train Epoch: 7 [91136/123872 (74%)]\tLoss: 0.343913\tLR: 0.00032068\n",
            "Train Epoch: 7 [91392/123872 (74%)]\tLoss: 0.439659\tLR: 0.00032056\n",
            "Train Epoch: 7 [91648/123872 (74%)]\tLoss: 0.355890\tLR: 0.00032044\n",
            "Train Epoch: 7 [91904/123872 (74%)]\tLoss: 0.406207\tLR: 0.00032033\n",
            "Train Epoch: 7 [92160/123872 (74%)]\tLoss: 0.365955\tLR: 0.00032021\n",
            "Train Epoch: 7 [92160/123872 (74%)]\tLoss: 0.365955\n",
            "Train Epoch: 7 [92416/123872 (75%)]\tLoss: 0.437007\tLR: 0.00032009\n",
            "Train Epoch: 7 [92672/123872 (75%)]\tLoss: 0.369472\tLR: 0.00031997\n",
            "Train Epoch: 7 [92928/123872 (75%)]\tLoss: 0.357493\tLR: 0.00031985\n",
            "Train Epoch: 7 [93184/123872 (75%)]\tLoss: 0.388222\tLR: 0.00031974\n",
            "Train Epoch: 7 [93440/123872 (75%)]\tLoss: 0.392899\tLR: 0.00031962\n",
            "Train Epoch: 7 [93696/123872 (76%)]\tLoss: 0.409873\tLR: 0.00031950\n",
            "Train Epoch: 7 [93952/123872 (76%)]\tLoss: 0.437521\tLR: 0.00031938\n",
            "Train Epoch: 7 [94208/123872 (76%)]\tLoss: 0.415710\tLR: 0.00031926\n",
            "Train Epoch: 7 [94464/123872 (76%)]\tLoss: 0.371527\tLR: 0.00031915\n",
            "Train Epoch: 7 [94720/123872 (76%)]\tLoss: 0.423091\tLR: 0.00031903\n",
            "Train Epoch: 7 [94720/123872 (76%)]\tLoss: 0.423091\n",
            "Train Epoch: 7 [94976/123872 (77%)]\tLoss: 0.438364\tLR: 0.00031891\n",
            "Train Epoch: 7 [95232/123872 (77%)]\tLoss: 0.382607\tLR: 0.00031879\n",
            "Train Epoch: 7 [95488/123872 (77%)]\tLoss: 0.398892\tLR: 0.00031867\n",
            "Train Epoch: 7 [95744/123872 (77%)]\tLoss: 0.361381\tLR: 0.00031856\n",
            "Train Epoch: 7 [96000/123872 (77%)]\tLoss: 0.372765\tLR: 0.00031844\n",
            "Train Epoch: 7 [96256/123872 (78%)]\tLoss: 0.403787\tLR: 0.00031832\n",
            "Train Epoch: 7 [96512/123872 (78%)]\tLoss: 0.437959\tLR: 0.00031820\n",
            "Train Epoch: 7 [96768/123872 (78%)]\tLoss: 0.361893\tLR: 0.00031809\n",
            "Train Epoch: 7 [97024/123872 (78%)]\tLoss: 0.389516\tLR: 0.00031797\n",
            "Train Epoch: 7 [97280/123872 (79%)]\tLoss: 0.381779\tLR: 0.00031785\n",
            "Train Epoch: 7 [97280/123872 (79%)]\tLoss: 0.381779\n",
            "Train Epoch: 7 [97536/123872 (79%)]\tLoss: 0.397884\tLR: 0.00031773\n",
            "Train Epoch: 7 [97792/123872 (79%)]\tLoss: 0.386276\tLR: 0.00031761\n",
            "Train Epoch: 7 [98048/123872 (79%)]\tLoss: 0.412908\tLR: 0.00031750\n",
            "Train Epoch: 7 [98304/123872 (79%)]\tLoss: 0.368960\tLR: 0.00031738\n",
            "Train Epoch: 7 [98560/123872 (80%)]\tLoss: 0.360447\tLR: 0.00031726\n",
            "Train Epoch: 7 [98816/123872 (80%)]\tLoss: 0.420091\tLR: 0.00031714\n",
            "Train Epoch: 7 [99072/123872 (80%)]\tLoss: 0.412517\tLR: 0.00031702\n",
            "Train Epoch: 7 [99328/123872 (80%)]\tLoss: 0.379646\tLR: 0.00031691\n",
            "Train Epoch: 7 [99584/123872 (80%)]\tLoss: 0.344249\tLR: 0.00031679\n",
            "Train Epoch: 7 [99840/123872 (81%)]\tLoss: 0.443533\tLR: 0.00031667\n",
            "Train Epoch: 7 [99840/123872 (81%)]\tLoss: 0.443533\n",
            "Train Epoch: 7 [100096/123872 (81%)]\tLoss: 0.392006\tLR: 0.00031655\n",
            "Train Epoch: 7 [100352/123872 (81%)]\tLoss: 0.436554\tLR: 0.00031643\n",
            "Train Epoch: 7 [100608/123872 (81%)]\tLoss: 0.440497\tLR: 0.00031632\n",
            "Train Epoch: 7 [100864/123872 (81%)]\tLoss: 0.358161\tLR: 0.00031620\n",
            "Train Epoch: 7 [101120/123872 (82%)]\tLoss: 0.390240\tLR: 0.00031608\n",
            "Train Epoch: 7 [101376/123872 (82%)]\tLoss: 0.390838\tLR: 0.00031596\n",
            "Train Epoch: 7 [101632/123872 (82%)]\tLoss: 0.376175\tLR: 0.00031585\n",
            "Train Epoch: 7 [101888/123872 (82%)]\tLoss: 0.402113\tLR: 0.00031573\n",
            "Train Epoch: 7 [102144/123872 (82%)]\tLoss: 0.478611\tLR: 0.00031561\n",
            "Train Epoch: 7 [102400/123872 (83%)]\tLoss: 0.396455\tLR: 0.00031549\n",
            "Train Epoch: 7 [102400/123872 (83%)]\tLoss: 0.396455\n",
            "Train Epoch: 7 [102656/123872 (83%)]\tLoss: 0.344998\tLR: 0.00031537\n",
            "Train Epoch: 7 [102912/123872 (83%)]\tLoss: 0.448510\tLR: 0.00031526\n",
            "Train Epoch: 7 [103168/123872 (83%)]\tLoss: 0.408802\tLR: 0.00031514\n",
            "Train Epoch: 7 [103424/123872 (83%)]\tLoss: 0.388750\tLR: 0.00031502\n",
            "Train Epoch: 7 [103680/123872 (84%)]\tLoss: 0.435368\tLR: 0.00031490\n",
            "Train Epoch: 7 [103936/123872 (84%)]\tLoss: 0.351535\tLR: 0.00031479\n",
            "Train Epoch: 7 [104192/123872 (84%)]\tLoss: 0.417876\tLR: 0.00031467\n",
            "Train Epoch: 7 [104448/123872 (84%)]\tLoss: 0.388863\tLR: 0.00031455\n",
            "Train Epoch: 7 [104704/123872 (85%)]\tLoss: 0.428391\tLR: 0.00031443\n",
            "Train Epoch: 7 [104960/123872 (85%)]\tLoss: 0.382143\tLR: 0.00031431\n",
            "Train Epoch: 7 [104960/123872 (85%)]\tLoss: 0.382143\n",
            "Train Epoch: 7 [105216/123872 (85%)]\tLoss: 0.404465\tLR: 0.00031420\n",
            "Train Epoch: 7 [105472/123872 (85%)]\tLoss: 0.367506\tLR: 0.00031408\n",
            "Train Epoch: 7 [105728/123872 (85%)]\tLoss: 0.386683\tLR: 0.00031396\n",
            "Train Epoch: 7 [105984/123872 (86%)]\tLoss: 0.390349\tLR: 0.00031384\n",
            "Train Epoch: 7 [106240/123872 (86%)]\tLoss: 0.405463\tLR: 0.00031372\n",
            "Train Epoch: 7 [106496/123872 (86%)]\tLoss: 0.436565\tLR: 0.00031361\n",
            "Train Epoch: 7 [106752/123872 (86%)]\tLoss: 0.424799\tLR: 0.00031349\n",
            "Train Epoch: 7 [107008/123872 (86%)]\tLoss: 0.442949\tLR: 0.00031337\n",
            "Train Epoch: 7 [107264/123872 (87%)]\tLoss: 0.428611\tLR: 0.00031325\n",
            "Train Epoch: 7 [107520/123872 (87%)]\tLoss: 0.387010\tLR: 0.00031314\n",
            "Train Epoch: 7 [107520/123872 (87%)]\tLoss: 0.387010\n",
            "Train Epoch: 7 [107776/123872 (87%)]\tLoss: 0.444267\tLR: 0.00031302\n",
            "Train Epoch: 7 [108032/123872 (87%)]\tLoss: 0.390587\tLR: 0.00031290\n",
            "Train Epoch: 7 [108288/123872 (87%)]\tLoss: 0.403828\tLR: 0.00031278\n",
            "Train Epoch: 7 [108544/123872 (88%)]\tLoss: 0.364652\tLR: 0.00031266\n",
            "Train Epoch: 7 [108800/123872 (88%)]\tLoss: 0.410770\tLR: 0.00031255\n",
            "Train Epoch: 7 [109056/123872 (88%)]\tLoss: 0.381611\tLR: 0.00031243\n",
            "Train Epoch: 7 [109312/123872 (88%)]\tLoss: 0.355213\tLR: 0.00031231\n",
            "Train Epoch: 7 [109568/123872 (88%)]\tLoss: 0.425901\tLR: 0.00031219\n",
            "Train Epoch: 7 [109824/123872 (89%)]\tLoss: 0.354916\tLR: 0.00031208\n",
            "Train Epoch: 7 [110080/123872 (89%)]\tLoss: 0.347264\tLR: 0.00031196\n",
            "Train Epoch: 7 [110080/123872 (89%)]\tLoss: 0.347264\n",
            "Train Epoch: 7 [110336/123872 (89%)]\tLoss: 0.379399\tLR: 0.00031184\n",
            "Train Epoch: 7 [110592/123872 (89%)]\tLoss: 0.476374\tLR: 0.00031172\n",
            "Train Epoch: 7 [110848/123872 (89%)]\tLoss: 0.444409\tLR: 0.00031161\n",
            "Train Epoch: 7 [111104/123872 (90%)]\tLoss: 0.403217\tLR: 0.00031149\n",
            "Train Epoch: 7 [111360/123872 (90%)]\tLoss: 0.330730\tLR: 0.00031137\n",
            "Train Epoch: 7 [111616/123872 (90%)]\tLoss: 0.356019\tLR: 0.00031125\n",
            "Train Epoch: 7 [111872/123872 (90%)]\tLoss: 0.382374\tLR: 0.00031113\n",
            "Train Epoch: 7 [112128/123872 (90%)]\tLoss: 0.403809\tLR: 0.00031102\n",
            "Train Epoch: 7 [112384/123872 (91%)]\tLoss: 0.407876\tLR: 0.00031090\n",
            "Train Epoch: 7 [112640/123872 (91%)]\tLoss: 0.384428\tLR: 0.00031078\n",
            "Train Epoch: 7 [112640/123872 (91%)]\tLoss: 0.384428\n",
            "Train Epoch: 7 [112896/123872 (91%)]\tLoss: 0.361124\tLR: 0.00031066\n",
            "Train Epoch: 7 [113152/123872 (91%)]\tLoss: 0.433369\tLR: 0.00031055\n",
            "Train Epoch: 7 [113408/123872 (92%)]\tLoss: 0.447101\tLR: 0.00031043\n",
            "Train Epoch: 7 [113664/123872 (92%)]\tLoss: 0.424947\tLR: 0.00031031\n",
            "Train Epoch: 7 [113920/123872 (92%)]\tLoss: 0.427060\tLR: 0.00031019\n",
            "Train Epoch: 7 [114176/123872 (92%)]\tLoss: 0.342632\tLR: 0.00031008\n",
            "Train Epoch: 7 [114432/123872 (92%)]\tLoss: 0.348081\tLR: 0.00030996\n",
            "Train Epoch: 7 [114688/123872 (93%)]\tLoss: 0.382830\tLR: 0.00030984\n",
            "Train Epoch: 7 [114944/123872 (93%)]\tLoss: 0.418066\tLR: 0.00030972\n",
            "Train Epoch: 7 [115200/123872 (93%)]\tLoss: 0.364996\tLR: 0.00030960\n",
            "Train Epoch: 7 [115200/123872 (93%)]\tLoss: 0.364996\n",
            "Train Epoch: 7 [115456/123872 (93%)]\tLoss: 0.398669\tLR: 0.00030949\n",
            "Train Epoch: 7 [115712/123872 (93%)]\tLoss: 0.351137\tLR: 0.00030937\n",
            "Train Epoch: 7 [115968/123872 (94%)]\tLoss: 0.451563\tLR: 0.00030925\n",
            "Train Epoch: 7 [116224/123872 (94%)]\tLoss: 0.376609\tLR: 0.00030913\n",
            "Train Epoch: 7 [116480/123872 (94%)]\tLoss: 0.401100\tLR: 0.00030902\n",
            "Train Epoch: 7 [116736/123872 (94%)]\tLoss: 0.324471\tLR: 0.00030890\n",
            "Train Epoch: 7 [116992/123872 (94%)]\tLoss: 0.381763\tLR: 0.00030878\n",
            "Train Epoch: 7 [117248/123872 (95%)]\tLoss: 0.374376\tLR: 0.00030866\n",
            "Train Epoch: 7 [117504/123872 (95%)]\tLoss: 0.359000\tLR: 0.00030855\n",
            "Train Epoch: 7 [117760/123872 (95%)]\tLoss: 0.418507\tLR: 0.00030843\n",
            "Train Epoch: 7 [117760/123872 (95%)]\tLoss: 0.418507\n",
            "Train Epoch: 7 [118016/123872 (95%)]\tLoss: 0.434353\tLR: 0.00030831\n",
            "Train Epoch: 7 [118272/123872 (95%)]\tLoss: 0.414164\tLR: 0.00030819\n",
            "Train Epoch: 7 [118528/123872 (96%)]\tLoss: 0.418510\tLR: 0.00030807\n",
            "Train Epoch: 7 [118784/123872 (96%)]\tLoss: 0.373289\tLR: 0.00030796\n",
            "Train Epoch: 7 [119040/123872 (96%)]\tLoss: 0.431336\tLR: 0.00030784\n",
            "Train Epoch: 7 [119296/123872 (96%)]\tLoss: 0.430844\tLR: 0.00030772\n",
            "Train Epoch: 7 [119552/123872 (96%)]\tLoss: 0.392202\tLR: 0.00030760\n",
            "Train Epoch: 7 [119808/123872 (97%)]\tLoss: 0.391779\tLR: 0.00030749\n",
            "Train Epoch: 7 [120064/123872 (97%)]\tLoss: 0.319927\tLR: 0.00030737\n",
            "Train Epoch: 7 [120320/123872 (97%)]\tLoss: 0.376491\tLR: 0.00030725\n",
            "Train Epoch: 7 [120320/123872 (97%)]\tLoss: 0.376491\n",
            "Train Epoch: 7 [120576/123872 (97%)]\tLoss: 0.377678\tLR: 0.00030713\n",
            "Train Epoch: 7 [120832/123872 (98%)]\tLoss: 0.357096\tLR: 0.00030702\n",
            "Train Epoch: 7 [121088/123872 (98%)]\tLoss: 0.452698\tLR: 0.00030690\n",
            "Train Epoch: 7 [121344/123872 (98%)]\tLoss: 0.338758\tLR: 0.00030678\n",
            "Train Epoch: 7 [121600/123872 (98%)]\tLoss: 0.338242\tLR: 0.00030666\n",
            "Train Epoch: 7 [121856/123872 (98%)]\tLoss: 0.414631\tLR: 0.00030655\n",
            "Train Epoch: 7 [122112/123872 (99%)]\tLoss: 0.399211\tLR: 0.00030643\n",
            "Train Epoch: 7 [122368/123872 (99%)]\tLoss: 0.375003\tLR: 0.00030631\n",
            "Train Epoch: 7 [122624/123872 (99%)]\tLoss: 0.437383\tLR: 0.00030619\n",
            "Train Epoch: 7 [122880/123872 (99%)]\tLoss: 0.428731\tLR: 0.00030608\n",
            "Train Epoch: 7 [122880/123872 (99%)]\tLoss: 0.428731\n",
            "Train Epoch: 7 [123136/123872 (99%)]\tLoss: 0.365109\tLR: 0.00030596\n",
            "Train Epoch: 7 [123392/123872 (100%)]\tLoss: 0.404221\tLR: 0.00030584\n",
            "Train Epoch: 7 [108192/123872 (100%)]\tLoss: 0.417391\tLR: 0.00030572\n",
            "\n",
            "Test set: Average loss: 0.0016, Accuracy: 25295/30970 (81.68%)\n",
            "\n",
            "Train Epoch: 8 [0/123872 (0%)]\tLoss: 0.403254\tLR: 0.00030561\n",
            "Train Epoch: 8 [0/123872 (0%)]\tLoss: 0.403254\n",
            "Train Epoch: 8 [256/123872 (0%)]\tLoss: 0.451516\tLR: 0.00030549\n",
            "Train Epoch: 8 [512/123872 (0%)]\tLoss: 0.430722\tLR: 0.00030537\n",
            "Train Epoch: 8 [768/123872 (1%)]\tLoss: 0.433947\tLR: 0.00030525\n",
            "Train Epoch: 8 [1024/123872 (1%)]\tLoss: 0.371917\tLR: 0.00030514\n",
            "Train Epoch: 8 [1280/123872 (1%)]\tLoss: 0.467354\tLR: 0.00030502\n",
            "Train Epoch: 8 [1536/123872 (1%)]\tLoss: 0.413088\tLR: 0.00030490\n",
            "Train Epoch: 8 [1792/123872 (1%)]\tLoss: 0.455931\tLR: 0.00030478\n",
            "Train Epoch: 8 [2048/123872 (2%)]\tLoss: 0.422066\tLR: 0.00030467\n",
            "Train Epoch: 8 [2304/123872 (2%)]\tLoss: 0.404670\tLR: 0.00030455\n",
            "Train Epoch: 8 [2560/123872 (2%)]\tLoss: 0.418370\tLR: 0.00030443\n",
            "Train Epoch: 8 [2560/123872 (2%)]\tLoss: 0.418370\n",
            "Train Epoch: 8 [2816/123872 (2%)]\tLoss: 0.408113\tLR: 0.00030431\n",
            "Train Epoch: 8 [3072/123872 (2%)]\tLoss: 0.379391\tLR: 0.00030420\n",
            "Train Epoch: 8 [3328/123872 (3%)]\tLoss: 0.418191\tLR: 0.00030408\n",
            "Train Epoch: 8 [3584/123872 (3%)]\tLoss: 0.344241\tLR: 0.00030396\n",
            "Train Epoch: 8 [3840/123872 (3%)]\tLoss: 0.362457\tLR: 0.00030384\n",
            "Train Epoch: 8 [4096/123872 (3%)]\tLoss: 0.404026\tLR: 0.00030373\n",
            "Train Epoch: 8 [4352/123872 (4%)]\tLoss: 0.339993\tLR: 0.00030361\n",
            "Train Epoch: 8 [4608/123872 (4%)]\tLoss: 0.353585\tLR: 0.00030349\n",
            "Train Epoch: 8 [4864/123872 (4%)]\tLoss: 0.464727\tLR: 0.00030337\n",
            "Train Epoch: 8 [5120/123872 (4%)]\tLoss: 0.372582\tLR: 0.00030326\n",
            "Train Epoch: 8 [5120/123872 (4%)]\tLoss: 0.372582\n",
            "Train Epoch: 8 [5376/123872 (4%)]\tLoss: 0.434123\tLR: 0.00030314\n",
            "Train Epoch: 8 [5632/123872 (5%)]\tLoss: 0.418645\tLR: 0.00030302\n",
            "Train Epoch: 8 [5888/123872 (5%)]\tLoss: 0.411858\tLR: 0.00030290\n",
            "Train Epoch: 8 [6144/123872 (5%)]\tLoss: 0.389239\tLR: 0.00030279\n",
            "Train Epoch: 8 [6400/123872 (5%)]\tLoss: 0.386479\tLR: 0.00030267\n",
            "Train Epoch: 8 [6656/123872 (5%)]\tLoss: 0.348176\tLR: 0.00030255\n",
            "Train Epoch: 8 [6912/123872 (6%)]\tLoss: 0.397949\tLR: 0.00030243\n",
            "Train Epoch: 8 [7168/123872 (6%)]\tLoss: 0.417535\tLR: 0.00030232\n",
            "Train Epoch: 8 [7424/123872 (6%)]\tLoss: 0.393715\tLR: 0.00030220\n",
            "Train Epoch: 8 [7680/123872 (6%)]\tLoss: 0.465270\tLR: 0.00030208\n",
            "Train Epoch: 8 [7680/123872 (6%)]\tLoss: 0.465270\n",
            "Train Epoch: 8 [7936/123872 (6%)]\tLoss: 0.415578\tLR: 0.00030197\n",
            "Train Epoch: 8 [8192/123872 (7%)]\tLoss: 0.427643\tLR: 0.00030185\n",
            "Train Epoch: 8 [8448/123872 (7%)]\tLoss: 0.376130\tLR: 0.00030173\n",
            "Train Epoch: 8 [8704/123872 (7%)]\tLoss: 0.332611\tLR: 0.00030161\n",
            "Train Epoch: 8 [8960/123872 (7%)]\tLoss: 0.416410\tLR: 0.00030150\n",
            "Train Epoch: 8 [9216/123872 (7%)]\tLoss: 0.371287\tLR: 0.00030138\n",
            "Train Epoch: 8 [9472/123872 (8%)]\tLoss: 0.346824\tLR: 0.00030126\n",
            "Train Epoch: 8 [9728/123872 (8%)]\tLoss: 0.473982\tLR: 0.00030114\n",
            "Train Epoch: 8 [9984/123872 (8%)]\tLoss: 0.388225\tLR: 0.00030103\n",
            "Train Epoch: 8 [10240/123872 (8%)]\tLoss: 0.410601\tLR: 0.00030091\n",
            "Train Epoch: 8 [10240/123872 (8%)]\tLoss: 0.410601\n",
            "Train Epoch: 8 [10496/123872 (8%)]\tLoss: 0.386704\tLR: 0.00030079\n",
            "Train Epoch: 8 [10752/123872 (9%)]\tLoss: 0.349814\tLR: 0.00030067\n",
            "Train Epoch: 8 [11008/123872 (9%)]\tLoss: 0.422083\tLR: 0.00030056\n",
            "Train Epoch: 8 [11264/123872 (9%)]\tLoss: 0.365882\tLR: 0.00030044\n",
            "Train Epoch: 8 [11520/123872 (9%)]\tLoss: 0.458263\tLR: 0.00030032\n",
            "Train Epoch: 8 [11776/123872 (10%)]\tLoss: 0.359375\tLR: 0.00030021\n",
            "Train Epoch: 8 [12032/123872 (10%)]\tLoss: 0.405518\tLR: 0.00030009\n",
            "Train Epoch: 8 [12288/123872 (10%)]\tLoss: 0.372557\tLR: 0.00029997\n",
            "Train Epoch: 8 [12544/123872 (10%)]\tLoss: 0.325760\tLR: 0.00029985\n",
            "Train Epoch: 8 [12800/123872 (10%)]\tLoss: 0.386753\tLR: 0.00029974\n",
            "Train Epoch: 8 [12800/123872 (10%)]\tLoss: 0.386753\n",
            "Train Epoch: 8 [13056/123872 (11%)]\tLoss: 0.409049\tLR: 0.00029962\n",
            "Train Epoch: 8 [13312/123872 (11%)]\tLoss: 0.410981\tLR: 0.00029950\n",
            "Train Epoch: 8 [13568/123872 (11%)]\tLoss: 0.349930\tLR: 0.00029938\n",
            "Train Epoch: 8 [13824/123872 (11%)]\tLoss: 0.425057\tLR: 0.00029927\n",
            "Train Epoch: 8 [14080/123872 (11%)]\tLoss: 0.391415\tLR: 0.00029915\n",
            "Train Epoch: 8 [14336/123872 (12%)]\tLoss: 0.393806\tLR: 0.00029903\n",
            "Train Epoch: 8 [14592/123872 (12%)]\tLoss: 0.430332\tLR: 0.00029892\n",
            "Train Epoch: 8 [14848/123872 (12%)]\tLoss: 0.454730\tLR: 0.00029880\n",
            "Train Epoch: 8 [15104/123872 (12%)]\tLoss: 0.372973\tLR: 0.00029868\n",
            "Train Epoch: 8 [15360/123872 (12%)]\tLoss: 0.390924\tLR: 0.00029856\n",
            "Train Epoch: 8 [15360/123872 (12%)]\tLoss: 0.390924\n",
            "Train Epoch: 8 [15616/123872 (13%)]\tLoss: 0.342056\tLR: 0.00029845\n",
            "Train Epoch: 8 [15872/123872 (13%)]\tLoss: 0.385163\tLR: 0.00029833\n",
            "Train Epoch: 8 [16128/123872 (13%)]\tLoss: 0.433726\tLR: 0.00029821\n",
            "Train Epoch: 8 [16384/123872 (13%)]\tLoss: 0.361006\tLR: 0.00029809\n",
            "Train Epoch: 8 [16640/123872 (13%)]\tLoss: 0.413956\tLR: 0.00029798\n",
            "Train Epoch: 8 [16896/123872 (14%)]\tLoss: 0.395553\tLR: 0.00029786\n",
            "Train Epoch: 8 [17152/123872 (14%)]\tLoss: 0.322243\tLR: 0.00029774\n",
            "Train Epoch: 8 [17408/123872 (14%)]\tLoss: 0.396263\tLR: 0.00029763\n",
            "Train Epoch: 8 [17664/123872 (14%)]\tLoss: 0.388197\tLR: 0.00029751\n",
            "Train Epoch: 8 [17920/123872 (14%)]\tLoss: 0.329971\tLR: 0.00029739\n",
            "Train Epoch: 8 [17920/123872 (14%)]\tLoss: 0.329971\n",
            "Train Epoch: 8 [18176/123872 (15%)]\tLoss: 0.376675\tLR: 0.00029727\n",
            "Train Epoch: 8 [18432/123872 (15%)]\tLoss: 0.372043\tLR: 0.00029716\n",
            "Train Epoch: 8 [18688/123872 (15%)]\tLoss: 0.461298\tLR: 0.00029704\n",
            "Train Epoch: 8 [18944/123872 (15%)]\tLoss: 0.409058\tLR: 0.00029692\n",
            "Train Epoch: 8 [19200/123872 (15%)]\tLoss: 0.378974\tLR: 0.00029681\n",
            "Train Epoch: 8 [19456/123872 (16%)]\tLoss: 0.401880\tLR: 0.00029669\n",
            "Train Epoch: 8 [19712/123872 (16%)]\tLoss: 0.414817\tLR: 0.00029657\n",
            "Train Epoch: 8 [19968/123872 (16%)]\tLoss: 0.395436\tLR: 0.00029646\n",
            "Train Epoch: 8 [20224/123872 (16%)]\tLoss: 0.380383\tLR: 0.00029634\n",
            "Train Epoch: 8 [20480/123872 (17%)]\tLoss: 0.355909\tLR: 0.00029622\n",
            "Train Epoch: 8 [20480/123872 (17%)]\tLoss: 0.355909\n",
            "Train Epoch: 8 [20736/123872 (17%)]\tLoss: 0.355908\tLR: 0.00029610\n",
            "Train Epoch: 8 [20992/123872 (17%)]\tLoss: 0.344099\tLR: 0.00029599\n",
            "Train Epoch: 8 [21248/123872 (17%)]\tLoss: 0.369301\tLR: 0.00029587\n",
            "Train Epoch: 8 [21504/123872 (17%)]\tLoss: 0.425171\tLR: 0.00029575\n",
            "Train Epoch: 8 [21760/123872 (18%)]\tLoss: 0.349000\tLR: 0.00029564\n",
            "Train Epoch: 8 [22016/123872 (18%)]\tLoss: 0.416134\tLR: 0.00029552\n",
            "Train Epoch: 8 [22272/123872 (18%)]\tLoss: 0.344740\tLR: 0.00029540\n",
            "Train Epoch: 8 [22528/123872 (18%)]\tLoss: 0.384625\tLR: 0.00029528\n",
            "Train Epoch: 8 [22784/123872 (18%)]\tLoss: 0.386322\tLR: 0.00029517\n",
            "Train Epoch: 8 [23040/123872 (19%)]\tLoss: 0.491852\tLR: 0.00029505\n",
            "Train Epoch: 8 [23040/123872 (19%)]\tLoss: 0.491852\n",
            "Train Epoch: 8 [23296/123872 (19%)]\tLoss: 0.397168\tLR: 0.00029493\n",
            "Train Epoch: 8 [23552/123872 (19%)]\tLoss: 0.388878\tLR: 0.00029482\n",
            "Train Epoch: 8 [23808/123872 (19%)]\tLoss: 0.441525\tLR: 0.00029470\n",
            "Train Epoch: 8 [24064/123872 (19%)]\tLoss: 0.369345\tLR: 0.00029458\n",
            "Train Epoch: 8 [24320/123872 (20%)]\tLoss: 0.411907\tLR: 0.00029447\n",
            "Train Epoch: 8 [24576/123872 (20%)]\tLoss: 0.404770\tLR: 0.00029435\n",
            "Train Epoch: 8 [24832/123872 (20%)]\tLoss: 0.434767\tLR: 0.00029423\n",
            "Train Epoch: 8 [25088/123872 (20%)]\tLoss: 0.441659\tLR: 0.00029411\n",
            "Train Epoch: 8 [25344/123872 (20%)]\tLoss: 0.403772\tLR: 0.00029400\n",
            "Train Epoch: 8 [25600/123872 (21%)]\tLoss: 0.402487\tLR: 0.00029388\n",
            "Train Epoch: 8 [25600/123872 (21%)]\tLoss: 0.402487\n",
            "Train Epoch: 8 [25856/123872 (21%)]\tLoss: 0.403717\tLR: 0.00029376\n",
            "Train Epoch: 8 [26112/123872 (21%)]\tLoss: 0.358351\tLR: 0.00029365\n",
            "Train Epoch: 8 [26368/123872 (21%)]\tLoss: 0.403691\tLR: 0.00029353\n",
            "Train Epoch: 8 [26624/123872 (21%)]\tLoss: 0.359611\tLR: 0.00029341\n",
            "Train Epoch: 8 [26880/123872 (22%)]\tLoss: 0.343475\tLR: 0.00029330\n",
            "Train Epoch: 8 [27136/123872 (22%)]\tLoss: 0.319302\tLR: 0.00029318\n",
            "Train Epoch: 8 [27392/123872 (22%)]\tLoss: 0.417653\tLR: 0.00029306\n",
            "Train Epoch: 8 [27648/123872 (22%)]\tLoss: 0.388669\tLR: 0.00029295\n",
            "Train Epoch: 8 [27904/123872 (23%)]\tLoss: 0.365788\tLR: 0.00029283\n",
            "Train Epoch: 8 [28160/123872 (23%)]\tLoss: 0.342481\tLR: 0.00029271\n",
            "Train Epoch: 8 [28160/123872 (23%)]\tLoss: 0.342481\n",
            "Train Epoch: 8 [28416/123872 (23%)]\tLoss: 0.395484\tLR: 0.00029259\n",
            "Train Epoch: 8 [28672/123872 (23%)]\tLoss: 0.388685\tLR: 0.00029248\n",
            "Train Epoch: 8 [28928/123872 (23%)]\tLoss: 0.393182\tLR: 0.00029236\n",
            "Train Epoch: 8 [29184/123872 (24%)]\tLoss: 0.397172\tLR: 0.00029224\n",
            "Train Epoch: 8 [29440/123872 (24%)]\tLoss: 0.403299\tLR: 0.00029213\n",
            "Train Epoch: 8 [29696/123872 (24%)]\tLoss: 0.450400\tLR: 0.00029201\n",
            "Train Epoch: 8 [29952/123872 (24%)]\tLoss: 0.323407\tLR: 0.00029189\n",
            "Train Epoch: 8 [30208/123872 (24%)]\tLoss: 0.337355\tLR: 0.00029178\n",
            "Train Epoch: 8 [30464/123872 (25%)]\tLoss: 0.406645\tLR: 0.00029166\n",
            "Train Epoch: 8 [30720/123872 (25%)]\tLoss: 0.390077\tLR: 0.00029154\n",
            "Train Epoch: 8 [30720/123872 (25%)]\tLoss: 0.390077\n",
            "Train Epoch: 8 [30976/123872 (25%)]\tLoss: 0.363097\tLR: 0.00029143\n",
            "Train Epoch: 8 [31232/123872 (25%)]\tLoss: 0.366858\tLR: 0.00029131\n",
            "Train Epoch: 8 [31488/123872 (25%)]\tLoss: 0.382259\tLR: 0.00029119\n",
            "Train Epoch: 8 [31744/123872 (26%)]\tLoss: 0.383863\tLR: 0.00029108\n",
            "Train Epoch: 8 [32000/123872 (26%)]\tLoss: 0.330512\tLR: 0.00029096\n",
            "Train Epoch: 8 [32256/123872 (26%)]\tLoss: 0.328130\tLR: 0.00029084\n",
            "Train Epoch: 8 [32512/123872 (26%)]\tLoss: 0.331699\tLR: 0.00029073\n",
            "Train Epoch: 8 [32768/123872 (26%)]\tLoss: 0.365202\tLR: 0.00029061\n",
            "Train Epoch: 8 [33024/123872 (27%)]\tLoss: 0.418015\tLR: 0.00029049\n",
            "Train Epoch: 8 [33280/123872 (27%)]\tLoss: 0.482251\tLR: 0.00029038\n",
            "Train Epoch: 8 [33280/123872 (27%)]\tLoss: 0.482251\n",
            "Train Epoch: 8 [33536/123872 (27%)]\tLoss: 0.386061\tLR: 0.00029026\n",
            "Train Epoch: 8 [33792/123872 (27%)]\tLoss: 0.421970\tLR: 0.00029014\n",
            "Train Epoch: 8 [34048/123872 (27%)]\tLoss: 0.351833\tLR: 0.00029003\n",
            "Train Epoch: 8 [34304/123872 (28%)]\tLoss: 0.402806\tLR: 0.00028991\n",
            "Train Epoch: 8 [34560/123872 (28%)]\tLoss: 0.411091\tLR: 0.00028979\n",
            "Train Epoch: 8 [34816/123872 (28%)]\tLoss: 0.419352\tLR: 0.00028968\n",
            "Train Epoch: 8 [35072/123872 (28%)]\tLoss: 0.376554\tLR: 0.00028956\n",
            "Train Epoch: 8 [35328/123872 (29%)]\tLoss: 0.416201\tLR: 0.00028944\n",
            "Train Epoch: 8 [35584/123872 (29%)]\tLoss: 0.406966\tLR: 0.00028933\n",
            "Train Epoch: 8 [35840/123872 (29%)]\tLoss: 0.341133\tLR: 0.00028921\n",
            "Train Epoch: 8 [35840/123872 (29%)]\tLoss: 0.341133\n",
            "Train Epoch: 8 [36096/123872 (29%)]\tLoss: 0.330337\tLR: 0.00028909\n",
            "Train Epoch: 8 [36352/123872 (29%)]\tLoss: 0.337048\tLR: 0.00028898\n",
            "Train Epoch: 8 [36608/123872 (30%)]\tLoss: 0.400709\tLR: 0.00028886\n",
            "Train Epoch: 8 [36864/123872 (30%)]\tLoss: 0.473084\tLR: 0.00028874\n",
            "Train Epoch: 8 [37120/123872 (30%)]\tLoss: 0.425622\tLR: 0.00028863\n",
            "Train Epoch: 8 [37376/123872 (30%)]\tLoss: 0.449156\tLR: 0.00028851\n",
            "Train Epoch: 8 [37632/123872 (30%)]\tLoss: 0.365697\tLR: 0.00028839\n",
            "Train Epoch: 8 [37888/123872 (31%)]\tLoss: 0.425498\tLR: 0.00028828\n",
            "Train Epoch: 8 [38144/123872 (31%)]\tLoss: 0.376396\tLR: 0.00028816\n",
            "Train Epoch: 8 [38400/123872 (31%)]\tLoss: 0.389689\tLR: 0.00028804\n",
            "Train Epoch: 8 [38400/123872 (31%)]\tLoss: 0.389689\n",
            "Train Epoch: 8 [38656/123872 (31%)]\tLoss: 0.419712\tLR: 0.00028793\n",
            "Train Epoch: 8 [38912/123872 (31%)]\tLoss: 0.343869\tLR: 0.00028781\n",
            "Train Epoch: 8 [39168/123872 (32%)]\tLoss: 0.467129\tLR: 0.00028769\n",
            "Train Epoch: 8 [39424/123872 (32%)]\tLoss: 0.346080\tLR: 0.00028758\n",
            "Train Epoch: 8 [39680/123872 (32%)]\tLoss: 0.375936\tLR: 0.00028746\n",
            "Train Epoch: 8 [39936/123872 (32%)]\tLoss: 0.396096\tLR: 0.00028734\n",
            "Train Epoch: 8 [40192/123872 (32%)]\tLoss: 0.413802\tLR: 0.00028723\n",
            "Train Epoch: 8 [40448/123872 (33%)]\tLoss: 0.393249\tLR: 0.00028711\n",
            "Train Epoch: 8 [40704/123872 (33%)]\tLoss: 0.451351\tLR: 0.00028699\n",
            "Train Epoch: 8 [40960/123872 (33%)]\tLoss: 0.340151\tLR: 0.00028688\n",
            "Train Epoch: 8 [40960/123872 (33%)]\tLoss: 0.340151\n",
            "Train Epoch: 8 [41216/123872 (33%)]\tLoss: 0.361817\tLR: 0.00028676\n",
            "Train Epoch: 8 [41472/123872 (33%)]\tLoss: 0.358271\tLR: 0.00028664\n",
            "Train Epoch: 8 [41728/123872 (34%)]\tLoss: 0.391144\tLR: 0.00028653\n",
            "Train Epoch: 8 [41984/123872 (34%)]\tLoss: 0.378941\tLR: 0.00028641\n",
            "Train Epoch: 8 [42240/123872 (34%)]\tLoss: 0.313333\tLR: 0.00028629\n",
            "Train Epoch: 8 [42496/123872 (34%)]\tLoss: 0.392985\tLR: 0.00028618\n",
            "Train Epoch: 8 [42752/123872 (35%)]\tLoss: 0.393658\tLR: 0.00028606\n",
            "Train Epoch: 8 [43008/123872 (35%)]\tLoss: 0.455478\tLR: 0.00028595\n",
            "Train Epoch: 8 [43264/123872 (35%)]\tLoss: 0.374860\tLR: 0.00028583\n",
            "Train Epoch: 8 [43520/123872 (35%)]\tLoss: 0.381721\tLR: 0.00028571\n",
            "Train Epoch: 8 [43520/123872 (35%)]\tLoss: 0.381721\n",
            "Train Epoch: 8 [43776/123872 (35%)]\tLoss: 0.402017\tLR: 0.00028560\n",
            "Train Epoch: 8 [44032/123872 (36%)]\tLoss: 0.365798\tLR: 0.00028548\n",
            "Train Epoch: 8 [44288/123872 (36%)]\tLoss: 0.362930\tLR: 0.00028536\n",
            "Train Epoch: 8 [44544/123872 (36%)]\tLoss: 0.400593\tLR: 0.00028525\n",
            "Train Epoch: 8 [44800/123872 (36%)]\tLoss: 0.373396\tLR: 0.00028513\n",
            "Train Epoch: 8 [45056/123872 (36%)]\tLoss: 0.357201\tLR: 0.00028501\n",
            "Train Epoch: 8 [45312/123872 (37%)]\tLoss: 0.360868\tLR: 0.00028490\n",
            "Train Epoch: 8 [45568/123872 (37%)]\tLoss: 0.320873\tLR: 0.00028478\n",
            "Train Epoch: 8 [45824/123872 (37%)]\tLoss: 0.414346\tLR: 0.00028467\n",
            "Train Epoch: 8 [46080/123872 (37%)]\tLoss: 0.403559\tLR: 0.00028455\n",
            "Train Epoch: 8 [46080/123872 (37%)]\tLoss: 0.403559\n",
            "Train Epoch: 8 [46336/123872 (37%)]\tLoss: 0.381121\tLR: 0.00028443\n",
            "Train Epoch: 8 [46592/123872 (38%)]\tLoss: 0.426213\tLR: 0.00028432\n",
            "Train Epoch: 8 [46848/123872 (38%)]\tLoss: 0.402536\tLR: 0.00028420\n",
            "Train Epoch: 8 [47104/123872 (38%)]\tLoss: 0.414126\tLR: 0.00028408\n",
            "Train Epoch: 8 [47360/123872 (38%)]\tLoss: 0.410017\tLR: 0.00028397\n",
            "Train Epoch: 8 [47616/123872 (38%)]\tLoss: 0.369793\tLR: 0.00028385\n",
            "Train Epoch: 8 [47872/123872 (39%)]\tLoss: 0.335049\tLR: 0.00028373\n",
            "Train Epoch: 8 [48128/123872 (39%)]\tLoss: 0.410574\tLR: 0.00028362\n",
            "Train Epoch: 8 [48384/123872 (39%)]\tLoss: 0.431881\tLR: 0.00028350\n",
            "Train Epoch: 8 [48640/123872 (39%)]\tLoss: 0.334827\tLR: 0.00028339\n",
            "Train Epoch: 8 [48640/123872 (39%)]\tLoss: 0.334827\n",
            "Train Epoch: 8 [48896/123872 (39%)]\tLoss: 0.392578\tLR: 0.00028327\n",
            "Train Epoch: 8 [49152/123872 (40%)]\tLoss: 0.421688\tLR: 0.00028315\n",
            "Train Epoch: 8 [49408/123872 (40%)]\tLoss: 0.359020\tLR: 0.00028304\n",
            "Train Epoch: 8 [49664/123872 (40%)]\tLoss: 0.374426\tLR: 0.00028292\n",
            "Train Epoch: 8 [49920/123872 (40%)]\tLoss: 0.328656\tLR: 0.00028281\n",
            "Train Epoch: 8 [50176/123872 (40%)]\tLoss: 0.403116\tLR: 0.00028269\n",
            "Train Epoch: 8 [50432/123872 (41%)]\tLoss: 0.365026\tLR: 0.00028257\n",
            "Train Epoch: 8 [50688/123872 (41%)]\tLoss: 0.384941\tLR: 0.00028246\n",
            "Train Epoch: 8 [50944/123872 (41%)]\tLoss: 0.431710\tLR: 0.00028234\n",
            "Train Epoch: 8 [51200/123872 (41%)]\tLoss: 0.375927\tLR: 0.00028222\n",
            "Train Epoch: 8 [51200/123872 (41%)]\tLoss: 0.375927\n",
            "Train Epoch: 8 [51456/123872 (42%)]\tLoss: 0.373165\tLR: 0.00028211\n",
            "Train Epoch: 8 [51712/123872 (42%)]\tLoss: 0.426224\tLR: 0.00028199\n",
            "Train Epoch: 8 [51968/123872 (42%)]\tLoss: 0.424571\tLR: 0.00028188\n",
            "Train Epoch: 8 [52224/123872 (42%)]\tLoss: 0.428422\tLR: 0.00028176\n",
            "Train Epoch: 8 [52480/123872 (42%)]\tLoss: 0.390613\tLR: 0.00028164\n",
            "Train Epoch: 8 [52736/123872 (43%)]\tLoss: 0.424464\tLR: 0.00028153\n",
            "Train Epoch: 8 [52992/123872 (43%)]\tLoss: 0.367933\tLR: 0.00028141\n",
            "Train Epoch: 8 [53248/123872 (43%)]\tLoss: 0.375871\tLR: 0.00028130\n",
            "Train Epoch: 8 [53504/123872 (43%)]\tLoss: 0.414205\tLR: 0.00028118\n",
            "Train Epoch: 8 [53760/123872 (43%)]\tLoss: 0.428213\tLR: 0.00028106\n",
            "Train Epoch: 8 [53760/123872 (43%)]\tLoss: 0.428213\n",
            "Train Epoch: 8 [54016/123872 (44%)]\tLoss: 0.417014\tLR: 0.00028095\n",
            "Train Epoch: 8 [54272/123872 (44%)]\tLoss: 0.392466\tLR: 0.00028083\n",
            "Train Epoch: 8 [54528/123872 (44%)]\tLoss: 0.447814\tLR: 0.00028072\n",
            "Train Epoch: 8 [54784/123872 (44%)]\tLoss: 0.384718\tLR: 0.00028060\n",
            "Train Epoch: 8 [55040/123872 (44%)]\tLoss: 0.419270\tLR: 0.00028048\n",
            "Train Epoch: 8 [55296/123872 (45%)]\tLoss: 0.406044\tLR: 0.00028037\n",
            "Train Epoch: 8 [55552/123872 (45%)]\tLoss: 0.405535\tLR: 0.00028025\n",
            "Train Epoch: 8 [55808/123872 (45%)]\tLoss: 0.418921\tLR: 0.00028014\n",
            "Train Epoch: 8 [56064/123872 (45%)]\tLoss: 0.413390\tLR: 0.00028002\n",
            "Train Epoch: 8 [56320/123872 (45%)]\tLoss: 0.419920\tLR: 0.00027990\n",
            "Train Epoch: 8 [56320/123872 (45%)]\tLoss: 0.419920\n",
            "Train Epoch: 8 [56576/123872 (46%)]\tLoss: 0.418870\tLR: 0.00027979\n",
            "Train Epoch: 8 [56832/123872 (46%)]\tLoss: 0.397390\tLR: 0.00027967\n",
            "Train Epoch: 8 [57088/123872 (46%)]\tLoss: 0.321095\tLR: 0.00027956\n",
            "Train Epoch: 8 [57344/123872 (46%)]\tLoss: 0.355989\tLR: 0.00027944\n",
            "Train Epoch: 8 [57600/123872 (46%)]\tLoss: 0.391594\tLR: 0.00027932\n",
            "Train Epoch: 8 [57856/123872 (47%)]\tLoss: 0.381397\tLR: 0.00027921\n",
            "Train Epoch: 8 [58112/123872 (47%)]\tLoss: 0.358711\tLR: 0.00027909\n",
            "Train Epoch: 8 [58368/123872 (47%)]\tLoss: 0.443152\tLR: 0.00027898\n",
            "Train Epoch: 8 [58624/123872 (47%)]\tLoss: 0.348000\tLR: 0.00027886\n",
            "Train Epoch: 8 [58880/123872 (48%)]\tLoss: 0.327218\tLR: 0.00027874\n",
            "Train Epoch: 8 [58880/123872 (48%)]\tLoss: 0.327218\n",
            "Train Epoch: 8 [59136/123872 (48%)]\tLoss: 0.396214\tLR: 0.00027863\n",
            "Train Epoch: 8 [59392/123872 (48%)]\tLoss: 0.427249\tLR: 0.00027851\n",
            "Train Epoch: 8 [59648/123872 (48%)]\tLoss: 0.343095\tLR: 0.00027840\n",
            "Train Epoch: 8 [59904/123872 (48%)]\tLoss: 0.367162\tLR: 0.00027828\n",
            "Train Epoch: 8 [60160/123872 (49%)]\tLoss: 0.381060\tLR: 0.00027816\n",
            "Train Epoch: 8 [60416/123872 (49%)]\tLoss: 0.422107\tLR: 0.00027805\n",
            "Train Epoch: 8 [60672/123872 (49%)]\tLoss: 0.407728\tLR: 0.00027793\n",
            "Train Epoch: 8 [60928/123872 (49%)]\tLoss: 0.369654\tLR: 0.00027782\n",
            "Train Epoch: 8 [61184/123872 (49%)]\tLoss: 0.421917\tLR: 0.00027770\n",
            "Train Epoch: 8 [61440/123872 (50%)]\tLoss: 0.403535\tLR: 0.00027759\n",
            "Train Epoch: 8 [61440/123872 (50%)]\tLoss: 0.403535\n",
            "Train Epoch: 8 [61696/123872 (50%)]\tLoss: 0.401449\tLR: 0.00027747\n",
            "Train Epoch: 8 [61952/123872 (50%)]\tLoss: 0.455163\tLR: 0.00027735\n",
            "Train Epoch: 8 [62208/123872 (50%)]\tLoss: 0.370746\tLR: 0.00027724\n",
            "Train Epoch: 8 [62464/123872 (50%)]\tLoss: 0.417001\tLR: 0.00027712\n",
            "Train Epoch: 8 [62720/123872 (51%)]\tLoss: 0.392094\tLR: 0.00027701\n",
            "Train Epoch: 8 [62976/123872 (51%)]\tLoss: 0.481164\tLR: 0.00027689\n",
            "Train Epoch: 8 [63232/123872 (51%)]\tLoss: 0.436058\tLR: 0.00027678\n",
            "Train Epoch: 8 [63488/123872 (51%)]\tLoss: 0.370841\tLR: 0.00027666\n",
            "Train Epoch: 8 [63744/123872 (51%)]\tLoss: 0.382306\tLR: 0.00027654\n",
            "Train Epoch: 8 [64000/123872 (52%)]\tLoss: 0.387101\tLR: 0.00027643\n",
            "Train Epoch: 8 [64000/123872 (52%)]\tLoss: 0.387101\n",
            "Train Epoch: 8 [64256/123872 (52%)]\tLoss: 0.380656\tLR: 0.00027631\n",
            "Train Epoch: 8 [64512/123872 (52%)]\tLoss: 0.389984\tLR: 0.00027620\n",
            "Train Epoch: 8 [64768/123872 (52%)]\tLoss: 0.437956\tLR: 0.00027608\n",
            "Train Epoch: 8 [65024/123872 (52%)]\tLoss: 0.444657\tLR: 0.00027597\n",
            "Train Epoch: 8 [65280/123872 (53%)]\tLoss: 0.344690\tLR: 0.00027585\n",
            "Train Epoch: 8 [65536/123872 (53%)]\tLoss: 0.376353\tLR: 0.00027574\n",
            "Train Epoch: 8 [65792/123872 (53%)]\tLoss: 0.389655\tLR: 0.00027562\n",
            "Train Epoch: 8 [66048/123872 (53%)]\tLoss: 0.380090\tLR: 0.00027550\n",
            "Train Epoch: 8 [66304/123872 (54%)]\tLoss: 0.410074\tLR: 0.00027539\n",
            "Train Epoch: 8 [66560/123872 (54%)]\tLoss: 0.440121\tLR: 0.00027527\n",
            "Train Epoch: 8 [66560/123872 (54%)]\tLoss: 0.440121\n",
            "Train Epoch: 8 [66816/123872 (54%)]\tLoss: 0.366338\tLR: 0.00027516\n",
            "Train Epoch: 8 [67072/123872 (54%)]\tLoss: 0.394879\tLR: 0.00027504\n",
            "Train Epoch: 8 [67328/123872 (54%)]\tLoss: 0.361323\tLR: 0.00027493\n",
            "Train Epoch: 8 [67584/123872 (55%)]\tLoss: 0.387519\tLR: 0.00027481\n",
            "Train Epoch: 8 [67840/123872 (55%)]\tLoss: 0.389676\tLR: 0.00027470\n",
            "Train Epoch: 8 [68096/123872 (55%)]\tLoss: 0.480395\tLR: 0.00027458\n",
            "Train Epoch: 8 [68352/123872 (55%)]\tLoss: 0.380871\tLR: 0.00027446\n",
            "Train Epoch: 8 [68608/123872 (55%)]\tLoss: 0.390381\tLR: 0.00027435\n",
            "Train Epoch: 8 [68864/123872 (56%)]\tLoss: 0.385075\tLR: 0.00027423\n",
            "Train Epoch: 8 [69120/123872 (56%)]\tLoss: 0.389789\tLR: 0.00027412\n",
            "Train Epoch: 8 [69120/123872 (56%)]\tLoss: 0.389789\n",
            "Train Epoch: 8 [69376/123872 (56%)]\tLoss: 0.391530\tLR: 0.00027400\n",
            "Train Epoch: 8 [69632/123872 (56%)]\tLoss: 0.381083\tLR: 0.00027389\n",
            "Train Epoch: 8 [69888/123872 (56%)]\tLoss: 0.410283\tLR: 0.00027377\n",
            "Train Epoch: 8 [70144/123872 (57%)]\tLoss: 0.356836\tLR: 0.00027366\n",
            "Train Epoch: 8 [70400/123872 (57%)]\tLoss: 0.375504\tLR: 0.00027354\n",
            "Train Epoch: 8 [70656/123872 (57%)]\tLoss: 0.389182\tLR: 0.00027343\n",
            "Train Epoch: 8 [70912/123872 (57%)]\tLoss: 0.461804\tLR: 0.00027331\n",
            "Train Epoch: 8 [71168/123872 (57%)]\tLoss: 0.380602\tLR: 0.00027319\n",
            "Train Epoch: 8 [71424/123872 (58%)]\tLoss: 0.323858\tLR: 0.00027308\n",
            "Train Epoch: 8 [71680/123872 (58%)]\tLoss: 0.376952\tLR: 0.00027296\n",
            "Train Epoch: 8 [71680/123872 (58%)]\tLoss: 0.376952\n",
            "Train Epoch: 8 [71936/123872 (58%)]\tLoss: 0.447045\tLR: 0.00027285\n",
            "Train Epoch: 8 [72192/123872 (58%)]\tLoss: 0.342387\tLR: 0.00027273\n",
            "Train Epoch: 8 [72448/123872 (58%)]\tLoss: 0.352547\tLR: 0.00027262\n",
            "Train Epoch: 8 [72704/123872 (59%)]\tLoss: 0.375206\tLR: 0.00027250\n",
            "Train Epoch: 8 [72960/123872 (59%)]\tLoss: 0.331185\tLR: 0.00027239\n",
            "Train Epoch: 8 [73216/123872 (59%)]\tLoss: 0.383378\tLR: 0.00027227\n",
            "Train Epoch: 8 [73472/123872 (59%)]\tLoss: 0.455680\tLR: 0.00027216\n",
            "Train Epoch: 8 [73728/123872 (60%)]\tLoss: 0.400917\tLR: 0.00027204\n",
            "Train Epoch: 8 [73984/123872 (60%)]\tLoss: 0.356470\tLR: 0.00027193\n",
            "Train Epoch: 8 [74240/123872 (60%)]\tLoss: 0.427002\tLR: 0.00027181\n",
            "Train Epoch: 8 [74240/123872 (60%)]\tLoss: 0.427002\n",
            "Train Epoch: 8 [74496/123872 (60%)]\tLoss: 0.363035\tLR: 0.00027170\n",
            "Train Epoch: 8 [74752/123872 (60%)]\tLoss: 0.365026\tLR: 0.00027158\n",
            "Train Epoch: 8 [75008/123872 (61%)]\tLoss: 0.371562\tLR: 0.00027147\n",
            "Train Epoch: 8 [75264/123872 (61%)]\tLoss: 0.374951\tLR: 0.00027135\n",
            "Train Epoch: 8 [75520/123872 (61%)]\tLoss: 0.363962\tLR: 0.00027123\n",
            "Train Epoch: 8 [75776/123872 (61%)]\tLoss: 0.399094\tLR: 0.00027112\n",
            "Train Epoch: 8 [76032/123872 (61%)]\tLoss: 0.318724\tLR: 0.00027100\n",
            "Train Epoch: 8 [76288/123872 (62%)]\tLoss: 0.383650\tLR: 0.00027089\n",
            "Train Epoch: 8 [76544/123872 (62%)]\tLoss: 0.394328\tLR: 0.00027077\n",
            "Train Epoch: 8 [76800/123872 (62%)]\tLoss: 0.392909\tLR: 0.00027066\n",
            "Train Epoch: 8 [76800/123872 (62%)]\tLoss: 0.392909\n",
            "Train Epoch: 8 [77056/123872 (62%)]\tLoss: 0.462869\tLR: 0.00027054\n",
            "Train Epoch: 8 [77312/123872 (62%)]\tLoss: 0.329046\tLR: 0.00027043\n",
            "Train Epoch: 8 [77568/123872 (63%)]\tLoss: 0.376088\tLR: 0.00027031\n",
            "Train Epoch: 8 [77824/123872 (63%)]\tLoss: 0.374826\tLR: 0.00027020\n",
            "Train Epoch: 8 [78080/123872 (63%)]\tLoss: 0.374817\tLR: 0.00027008\n",
            "Train Epoch: 8 [78336/123872 (63%)]\tLoss: 0.372057\tLR: 0.00026997\n",
            "Train Epoch: 8 [78592/123872 (63%)]\tLoss: 0.405709\tLR: 0.00026985\n",
            "Train Epoch: 8 [78848/123872 (64%)]\tLoss: 0.398872\tLR: 0.00026974\n",
            "Train Epoch: 8 [79104/123872 (64%)]\tLoss: 0.362478\tLR: 0.00026962\n",
            "Train Epoch: 8 [79360/123872 (64%)]\tLoss: 0.398558\tLR: 0.00026951\n",
            "Train Epoch: 8 [79360/123872 (64%)]\tLoss: 0.398558\n",
            "Train Epoch: 8 [79616/123872 (64%)]\tLoss: 0.341709\tLR: 0.00026939\n",
            "Train Epoch: 8 [79872/123872 (64%)]\tLoss: 0.383964\tLR: 0.00026928\n",
            "Train Epoch: 8 [80128/123872 (65%)]\tLoss: 0.358385\tLR: 0.00026916\n",
            "Train Epoch: 8 [80384/123872 (65%)]\tLoss: 0.334887\tLR: 0.00026905\n",
            "Train Epoch: 8 [80640/123872 (65%)]\tLoss: 0.372896\tLR: 0.00026893\n",
            "Train Epoch: 8 [80896/123872 (65%)]\tLoss: 0.377747\tLR: 0.00026882\n",
            "Train Epoch: 8 [81152/123872 (65%)]\tLoss: 0.300588\tLR: 0.00026870\n",
            "Train Epoch: 8 [81408/123872 (66%)]\tLoss: 0.407788\tLR: 0.00026859\n",
            "Train Epoch: 8 [81664/123872 (66%)]\tLoss: 0.411015\tLR: 0.00026847\n",
            "Train Epoch: 8 [81920/123872 (66%)]\tLoss: 0.337103\tLR: 0.00026836\n",
            "Train Epoch: 8 [81920/123872 (66%)]\tLoss: 0.337103\n",
            "Train Epoch: 8 [82176/123872 (66%)]\tLoss: 0.457205\tLR: 0.00026824\n",
            "Train Epoch: 8 [82432/123872 (67%)]\tLoss: 0.362741\tLR: 0.00026813\n",
            "Train Epoch: 8 [82688/123872 (67%)]\tLoss: 0.341563\tLR: 0.00026801\n",
            "Train Epoch: 8 [82944/123872 (67%)]\tLoss: 0.363306\tLR: 0.00026790\n",
            "Train Epoch: 8 [83200/123872 (67%)]\tLoss: 0.337607\tLR: 0.00026778\n",
            "Train Epoch: 8 [83456/123872 (67%)]\tLoss: 0.350788\tLR: 0.00026767\n",
            "Train Epoch: 8 [83712/123872 (68%)]\tLoss: 0.397088\tLR: 0.00026756\n",
            "Train Epoch: 8 [83968/123872 (68%)]\tLoss: 0.313580\tLR: 0.00026744\n",
            "Train Epoch: 8 [84224/123872 (68%)]\tLoss: 0.380749\tLR: 0.00026733\n",
            "Train Epoch: 8 [84480/123872 (68%)]\tLoss: 0.365691\tLR: 0.00026721\n",
            "Train Epoch: 8 [84480/123872 (68%)]\tLoss: 0.365691\n",
            "Train Epoch: 8 [84736/123872 (68%)]\tLoss: 0.321940\tLR: 0.00026710\n",
            "Train Epoch: 8 [84992/123872 (69%)]\tLoss: 0.415311\tLR: 0.00026698\n",
            "Train Epoch: 8 [85248/123872 (69%)]\tLoss: 0.353685\tLR: 0.00026687\n",
            "Train Epoch: 8 [85504/123872 (69%)]\tLoss: 0.403909\tLR: 0.00026675\n",
            "Train Epoch: 8 [85760/123872 (69%)]\tLoss: 0.422425\tLR: 0.00026664\n",
            "Train Epoch: 8 [86016/123872 (69%)]\tLoss: 0.448239\tLR: 0.00026652\n",
            "Train Epoch: 8 [86272/123872 (70%)]\tLoss: 0.345912\tLR: 0.00026641\n",
            "Train Epoch: 8 [86528/123872 (70%)]\tLoss: 0.321323\tLR: 0.00026629\n",
            "Train Epoch: 8 [86784/123872 (70%)]\tLoss: 0.405093\tLR: 0.00026618\n",
            "Train Epoch: 8 [87040/123872 (70%)]\tLoss: 0.357466\tLR: 0.00026606\n",
            "Train Epoch: 8 [87040/123872 (70%)]\tLoss: 0.357466\n",
            "Train Epoch: 8 [87296/123872 (70%)]\tLoss: 0.411936\tLR: 0.00026595\n",
            "Train Epoch: 8 [87552/123872 (71%)]\tLoss: 0.357379\tLR: 0.00026583\n",
            "Train Epoch: 8 [87808/123872 (71%)]\tLoss: 0.460118\tLR: 0.00026572\n",
            "Train Epoch: 8 [88064/123872 (71%)]\tLoss: 0.353710\tLR: 0.00026561\n",
            "Train Epoch: 8 [88320/123872 (71%)]\tLoss: 0.438346\tLR: 0.00026549\n",
            "Train Epoch: 8 [88576/123872 (71%)]\tLoss: 0.453049\tLR: 0.00026538\n",
            "Train Epoch: 8 [88832/123872 (72%)]\tLoss: 0.356392\tLR: 0.00026526\n",
            "Train Epoch: 8 [89088/123872 (72%)]\tLoss: 0.354324\tLR: 0.00026515\n",
            "Train Epoch: 8 [89344/123872 (72%)]\tLoss: 0.399703\tLR: 0.00026503\n",
            "Train Epoch: 8 [89600/123872 (72%)]\tLoss: 0.384153\tLR: 0.00026492\n",
            "Train Epoch: 8 [89600/123872 (72%)]\tLoss: 0.384153\n",
            "Train Epoch: 8 [89856/123872 (73%)]\tLoss: 0.364822\tLR: 0.00026480\n",
            "Train Epoch: 8 [90112/123872 (73%)]\tLoss: 0.385209\tLR: 0.00026469\n",
            "Train Epoch: 8 [90368/123872 (73%)]\tLoss: 0.413129\tLR: 0.00026457\n",
            "Train Epoch: 8 [90624/123872 (73%)]\tLoss: 0.353146\tLR: 0.00026446\n",
            "Train Epoch: 8 [90880/123872 (73%)]\tLoss: 0.394756\tLR: 0.00026435\n",
            "Train Epoch: 8 [91136/123872 (74%)]\tLoss: 0.344045\tLR: 0.00026423\n",
            "Train Epoch: 8 [91392/123872 (74%)]\tLoss: 0.404897\tLR: 0.00026412\n",
            "Train Epoch: 8 [91648/123872 (74%)]\tLoss: 0.376310\tLR: 0.00026400\n",
            "Train Epoch: 8 [91904/123872 (74%)]\tLoss: 0.336899\tLR: 0.00026389\n",
            "Train Epoch: 8 [92160/123872 (74%)]\tLoss: 0.421891\tLR: 0.00026377\n",
            "Train Epoch: 8 [92160/123872 (74%)]\tLoss: 0.421891\n",
            "Train Epoch: 8 [92416/123872 (75%)]\tLoss: 0.349829\tLR: 0.00026366\n",
            "Train Epoch: 8 [92672/123872 (75%)]\tLoss: 0.295392\tLR: 0.00026354\n",
            "Train Epoch: 8 [92928/123872 (75%)]\tLoss: 0.395329\tLR: 0.00026343\n",
            "Train Epoch: 8 [93184/123872 (75%)]\tLoss: 0.372802\tLR: 0.00026332\n",
            "Train Epoch: 8 [93440/123872 (75%)]\tLoss: 0.376611\tLR: 0.00026320\n",
            "Train Epoch: 8 [93696/123872 (76%)]\tLoss: 0.369956\tLR: 0.00026309\n",
            "Train Epoch: 8 [93952/123872 (76%)]\tLoss: 0.358042\tLR: 0.00026297\n",
            "Train Epoch: 8 [94208/123872 (76%)]\tLoss: 0.388472\tLR: 0.00026286\n",
            "Train Epoch: 8 [94464/123872 (76%)]\tLoss: 0.370619\tLR: 0.00026274\n",
            "Train Epoch: 8 [94720/123872 (76%)]\tLoss: 0.311764\tLR: 0.00026263\n",
            "Train Epoch: 8 [94720/123872 (76%)]\tLoss: 0.311764\n",
            "Train Epoch: 8 [94976/123872 (77%)]\tLoss: 0.392290\tLR: 0.00026252\n",
            "Train Epoch: 8 [95232/123872 (77%)]\tLoss: 0.350821\tLR: 0.00026240\n",
            "Train Epoch: 8 [95488/123872 (77%)]\tLoss: 0.370222\tLR: 0.00026229\n",
            "Train Epoch: 8 [95744/123872 (77%)]\tLoss: 0.367049\tLR: 0.00026217\n",
            "Train Epoch: 8 [96000/123872 (77%)]\tLoss: 0.367946\tLR: 0.00026206\n",
            "Train Epoch: 8 [96256/123872 (78%)]\tLoss: 0.406874\tLR: 0.00026194\n",
            "Train Epoch: 8 [96512/123872 (78%)]\tLoss: 0.438586\tLR: 0.00026183\n",
            "Train Epoch: 8 [96768/123872 (78%)]\tLoss: 0.388656\tLR: 0.00026172\n",
            "Train Epoch: 8 [97024/123872 (78%)]\tLoss: 0.371591\tLR: 0.00026160\n",
            "Train Epoch: 8 [97280/123872 (79%)]\tLoss: 0.411873\tLR: 0.00026149\n",
            "Train Epoch: 8 [97280/123872 (79%)]\tLoss: 0.411873\n",
            "Train Epoch: 8 [97536/123872 (79%)]\tLoss: 0.369371\tLR: 0.00026137\n",
            "Train Epoch: 8 [97792/123872 (79%)]\tLoss: 0.417045\tLR: 0.00026126\n",
            "Train Epoch: 8 [98048/123872 (79%)]\tLoss: 0.385527\tLR: 0.00026115\n",
            "Train Epoch: 8 [98304/123872 (79%)]\tLoss: 0.364824\tLR: 0.00026103\n",
            "Train Epoch: 8 [98560/123872 (80%)]\tLoss: 0.370151\tLR: 0.00026092\n",
            "Train Epoch: 8 [98816/123872 (80%)]\tLoss: 0.403498\tLR: 0.00026080\n",
            "Train Epoch: 8 [99072/123872 (80%)]\tLoss: 0.373814\tLR: 0.00026069\n",
            "Train Epoch: 8 [99328/123872 (80%)]\tLoss: 0.402228\tLR: 0.00026057\n",
            "Train Epoch: 8 [99584/123872 (80%)]\tLoss: 0.388265\tLR: 0.00026046\n",
            "Train Epoch: 8 [99840/123872 (81%)]\tLoss: 0.349211\tLR: 0.00026035\n",
            "Train Epoch: 8 [99840/123872 (81%)]\tLoss: 0.349211\n",
            "Train Epoch: 8 [100096/123872 (81%)]\tLoss: 0.378450\tLR: 0.00026023\n",
            "Train Epoch: 8 [100352/123872 (81%)]\tLoss: 0.352143\tLR: 0.00026012\n",
            "Train Epoch: 8 [100608/123872 (81%)]\tLoss: 0.418280\tLR: 0.00026000\n",
            "Train Epoch: 8 [100864/123872 (81%)]\tLoss: 0.392994\tLR: 0.00025989\n",
            "Train Epoch: 8 [101120/123872 (82%)]\tLoss: 0.412685\tLR: 0.00025978\n",
            "Train Epoch: 8 [101376/123872 (82%)]\tLoss: 0.373687\tLR: 0.00025966\n",
            "Train Epoch: 8 [101632/123872 (82%)]\tLoss: 0.390560\tLR: 0.00025955\n",
            "Train Epoch: 8 [101888/123872 (82%)]\tLoss: 0.403050\tLR: 0.00025944\n",
            "Train Epoch: 8 [102144/123872 (82%)]\tLoss: 0.450357\tLR: 0.00025932\n",
            "Train Epoch: 8 [102400/123872 (83%)]\tLoss: 0.413848\tLR: 0.00025921\n",
            "Train Epoch: 8 [102400/123872 (83%)]\tLoss: 0.413848\n",
            "Train Epoch: 8 [102656/123872 (83%)]\tLoss: 0.385503\tLR: 0.00025909\n",
            "Train Epoch: 8 [102912/123872 (83%)]\tLoss: 0.433959\tLR: 0.00025898\n",
            "Train Epoch: 8 [103168/123872 (83%)]\tLoss: 0.319345\tLR: 0.00025887\n",
            "Train Epoch: 8 [103424/123872 (83%)]\tLoss: 0.333294\tLR: 0.00025875\n",
            "Train Epoch: 8 [103680/123872 (84%)]\tLoss: 0.443295\tLR: 0.00025864\n",
            "Train Epoch: 8 [103936/123872 (84%)]\tLoss: 0.359534\tLR: 0.00025852\n",
            "Train Epoch: 8 [104192/123872 (84%)]\tLoss: 0.408836\tLR: 0.00025841\n",
            "Train Epoch: 8 [104448/123872 (84%)]\tLoss: 0.429832\tLR: 0.00025830\n",
            "Train Epoch: 8 [104704/123872 (85%)]\tLoss: 0.435847\tLR: 0.00025818\n",
            "Train Epoch: 8 [104960/123872 (85%)]\tLoss: 0.367151\tLR: 0.00025807\n",
            "Train Epoch: 8 [104960/123872 (85%)]\tLoss: 0.367151\n",
            "Train Epoch: 8 [105216/123872 (85%)]\tLoss: 0.360454\tLR: 0.00025796\n",
            "Train Epoch: 8 [105472/123872 (85%)]\tLoss: 0.469361\tLR: 0.00025784\n",
            "Train Epoch: 8 [105728/123872 (85%)]\tLoss: 0.390921\tLR: 0.00025773\n",
            "Train Epoch: 8 [105984/123872 (86%)]\tLoss: 0.394846\tLR: 0.00025761\n",
            "Train Epoch: 8 [106240/123872 (86%)]\tLoss: 0.428055\tLR: 0.00025750\n",
            "Train Epoch: 8 [106496/123872 (86%)]\tLoss: 0.419528\tLR: 0.00025739\n",
            "Train Epoch: 8 [106752/123872 (86%)]\tLoss: 0.405303\tLR: 0.00025727\n",
            "Train Epoch: 8 [107008/123872 (86%)]\tLoss: 0.391344\tLR: 0.00025716\n",
            "Train Epoch: 8 [107264/123872 (87%)]\tLoss: 0.401311\tLR: 0.00025705\n",
            "Train Epoch: 8 [107520/123872 (87%)]\tLoss: 0.378262\tLR: 0.00025693\n",
            "Train Epoch: 8 [107520/123872 (87%)]\tLoss: 0.378262\n",
            "Train Epoch: 8 [107776/123872 (87%)]\tLoss: 0.321356\tLR: 0.00025682\n",
            "Train Epoch: 8 [108032/123872 (87%)]\tLoss: 0.385086\tLR: 0.00025671\n",
            "Train Epoch: 8 [108288/123872 (87%)]\tLoss: 0.394789\tLR: 0.00025659\n",
            "Train Epoch: 8 [108544/123872 (88%)]\tLoss: 0.367752\tLR: 0.00025648\n",
            "Train Epoch: 8 [108800/123872 (88%)]\tLoss: 0.340237\tLR: 0.00025636\n",
            "Train Epoch: 8 [109056/123872 (88%)]\tLoss: 0.427781\tLR: 0.00025625\n",
            "Train Epoch: 8 [109312/123872 (88%)]\tLoss: 0.378403\tLR: 0.00025614\n",
            "Train Epoch: 8 [109568/123872 (88%)]\tLoss: 0.379650\tLR: 0.00025602\n",
            "Train Epoch: 8 [109824/123872 (89%)]\tLoss: 0.406847\tLR: 0.00025591\n",
            "Train Epoch: 8 [110080/123872 (89%)]\tLoss: 0.375844\tLR: 0.00025580\n",
            "Train Epoch: 8 [110080/123872 (89%)]\tLoss: 0.375844\n",
            "Train Epoch: 8 [110336/123872 (89%)]\tLoss: 0.362255\tLR: 0.00025568\n",
            "Train Epoch: 8 [110592/123872 (89%)]\tLoss: 0.407098\tLR: 0.00025557\n",
            "Train Epoch: 8 [110848/123872 (89%)]\tLoss: 0.366167\tLR: 0.00025546\n",
            "Train Epoch: 8 [111104/123872 (90%)]\tLoss: 0.358086\tLR: 0.00025534\n",
            "Train Epoch: 8 [111360/123872 (90%)]\tLoss: 0.441519\tLR: 0.00025523\n",
            "Train Epoch: 8 [111616/123872 (90%)]\tLoss: 0.368543\tLR: 0.00025512\n",
            "Train Epoch: 8 [111872/123872 (90%)]\tLoss: 0.355285\tLR: 0.00025500\n",
            "Train Epoch: 8 [112128/123872 (90%)]\tLoss: 0.365983\tLR: 0.00025489\n",
            "Train Epoch: 8 [112384/123872 (91%)]\tLoss: 0.394484\tLR: 0.00025478\n",
            "Train Epoch: 8 [112640/123872 (91%)]\tLoss: 0.374662\tLR: 0.00025466\n",
            "Train Epoch: 8 [112640/123872 (91%)]\tLoss: 0.374662\n",
            "Train Epoch: 8 [112896/123872 (91%)]\tLoss: 0.369519\tLR: 0.00025455\n",
            "Train Epoch: 8 [113152/123872 (91%)]\tLoss: 0.421639\tLR: 0.00025444\n",
            "Train Epoch: 8 [113408/123872 (92%)]\tLoss: 0.430675\tLR: 0.00025432\n",
            "Train Epoch: 8 [113664/123872 (92%)]\tLoss: 0.299424\tLR: 0.00025421\n",
            "Train Epoch: 8 [113920/123872 (92%)]\tLoss: 0.412848\tLR: 0.00025410\n",
            "Train Epoch: 8 [114176/123872 (92%)]\tLoss: 0.368606\tLR: 0.00025398\n",
            "Train Epoch: 8 [114432/123872 (92%)]\tLoss: 0.395877\tLR: 0.00025387\n",
            "Train Epoch: 8 [114688/123872 (93%)]\tLoss: 0.371459\tLR: 0.00025376\n",
            "Train Epoch: 8 [114944/123872 (93%)]\tLoss: 0.394705\tLR: 0.00025364\n",
            "Train Epoch: 8 [115200/123872 (93%)]\tLoss: 0.426239\tLR: 0.00025353\n",
            "Train Epoch: 8 [115200/123872 (93%)]\tLoss: 0.426239\n",
            "Train Epoch: 8 [115456/123872 (93%)]\tLoss: 0.422860\tLR: 0.00025342\n",
            "Train Epoch: 8 [115712/123872 (93%)]\tLoss: 0.398232\tLR: 0.00025330\n",
            "Train Epoch: 8 [115968/123872 (94%)]\tLoss: 0.392778\tLR: 0.00025319\n",
            "Train Epoch: 8 [116224/123872 (94%)]\tLoss: 0.384401\tLR: 0.00025308\n",
            "Train Epoch: 8 [116480/123872 (94%)]\tLoss: 0.329932\tLR: 0.00025297\n",
            "Train Epoch: 8 [116736/123872 (94%)]\tLoss: 0.464114\tLR: 0.00025285\n",
            "Train Epoch: 8 [116992/123872 (94%)]\tLoss: 0.373613\tLR: 0.00025274\n",
            "Train Epoch: 8 [117248/123872 (95%)]\tLoss: 0.411769\tLR: 0.00025263\n",
            "Train Epoch: 8 [117504/123872 (95%)]\tLoss: 0.359507\tLR: 0.00025251\n",
            "Train Epoch: 8 [117760/123872 (95%)]\tLoss: 0.362868\tLR: 0.00025240\n",
            "Train Epoch: 8 [117760/123872 (95%)]\tLoss: 0.362868\n",
            "Train Epoch: 8 [118016/123872 (95%)]\tLoss: 0.384084\tLR: 0.00025229\n",
            "Train Epoch: 8 [118272/123872 (95%)]\tLoss: 0.388111\tLR: 0.00025217\n",
            "Train Epoch: 8 [118528/123872 (96%)]\tLoss: 0.428981\tLR: 0.00025206\n",
            "Train Epoch: 8 [118784/123872 (96%)]\tLoss: 0.410696\tLR: 0.00025195\n",
            "Train Epoch: 8 [119040/123872 (96%)]\tLoss: 0.355041\tLR: 0.00025183\n",
            "Train Epoch: 8 [119296/123872 (96%)]\tLoss: 0.365322\tLR: 0.00025172\n",
            "Train Epoch: 8 [119552/123872 (96%)]\tLoss: 0.398900\tLR: 0.00025161\n",
            "Train Epoch: 8 [119808/123872 (97%)]\tLoss: 0.388666\tLR: 0.00025150\n",
            "Train Epoch: 8 [120064/123872 (97%)]\tLoss: 0.408356\tLR: 0.00025138\n",
            "Train Epoch: 8 [120320/123872 (97%)]\tLoss: 0.306224\tLR: 0.00025127\n",
            "Train Epoch: 8 [120320/123872 (97%)]\tLoss: 0.306224\n",
            "Train Epoch: 8 [120576/123872 (97%)]\tLoss: 0.413068\tLR: 0.00025116\n",
            "Train Epoch: 8 [120832/123872 (98%)]\tLoss: 0.359594\tLR: 0.00025104\n",
            "Train Epoch: 8 [121088/123872 (98%)]\tLoss: 0.417614\tLR: 0.00025093\n",
            "Train Epoch: 8 [121344/123872 (98%)]\tLoss: 0.370740\tLR: 0.00025082\n",
            "Train Epoch: 8 [121600/123872 (98%)]\tLoss: 0.374488\tLR: 0.00025071\n",
            "Train Epoch: 8 [121856/123872 (98%)]\tLoss: 0.319515\tLR: 0.00025059\n",
            "Train Epoch: 8 [122112/123872 (99%)]\tLoss: 0.354882\tLR: 0.00025048\n",
            "Train Epoch: 8 [122368/123872 (99%)]\tLoss: 0.350647\tLR: 0.00025037\n",
            "Train Epoch: 8 [122624/123872 (99%)]\tLoss: 0.405838\tLR: 0.00025026\n",
            "Train Epoch: 8 [122880/123872 (99%)]\tLoss: 0.286938\tLR: 0.00025014\n",
            "Train Epoch: 8 [122880/123872 (99%)]\tLoss: 0.286938\n",
            "Train Epoch: 8 [123136/123872 (99%)]\tLoss: 0.371652\tLR: 0.00025003\n",
            "Train Epoch: 8 [123392/123872 (100%)]\tLoss: 0.450002\tLR: 0.00024992\n",
            "Train Epoch: 8 [108192/123872 (100%)]\tLoss: 0.339642\tLR: 0.00024980\n",
            "\n",
            "Test set: Average loss: 0.0015, Accuracy: 25650/30970 (82.82%)\n",
            "\n",
            "Train Epoch: 9 [0/123872 (0%)]\tLoss: 0.415813\tLR: 0.00024969\n",
            "Train Epoch: 9 [0/123872 (0%)]\tLoss: 0.415813\n",
            "Train Epoch: 9 [256/123872 (0%)]\tLoss: 0.388822\tLR: 0.00024958\n",
            "Train Epoch: 9 [512/123872 (0%)]\tLoss: 0.422655\tLR: 0.00024947\n",
            "Train Epoch: 9 [768/123872 (1%)]\tLoss: 0.393254\tLR: 0.00024935\n",
            "Train Epoch: 9 [1024/123872 (1%)]\tLoss: 0.400874\tLR: 0.00024924\n",
            "Train Epoch: 9 [1280/123872 (1%)]\tLoss: 0.365349\tLR: 0.00024913\n",
            "Train Epoch: 9 [1536/123872 (1%)]\tLoss: 0.367678\tLR: 0.00024902\n",
            "Train Epoch: 9 [1792/123872 (1%)]\tLoss: 0.295353\tLR: 0.00024890\n",
            "Train Epoch: 9 [2048/123872 (2%)]\tLoss: 0.356062\tLR: 0.00024879\n",
            "Train Epoch: 9 [2304/123872 (2%)]\tLoss: 0.496863\tLR: 0.00024868\n",
            "Train Epoch: 9 [2560/123872 (2%)]\tLoss: 0.358928\tLR: 0.00024857\n",
            "Train Epoch: 9 [2560/123872 (2%)]\tLoss: 0.358928\n",
            "Train Epoch: 9 [2816/123872 (2%)]\tLoss: 0.431168\tLR: 0.00024845\n",
            "Train Epoch: 9 [3072/123872 (2%)]\tLoss: 0.362208\tLR: 0.00024834\n",
            "Train Epoch: 9 [3328/123872 (3%)]\tLoss: 0.379859\tLR: 0.00024823\n",
            "Train Epoch: 9 [3584/123872 (3%)]\tLoss: 0.358037\tLR: 0.00024812\n",
            "Train Epoch: 9 [3840/123872 (3%)]\tLoss: 0.395307\tLR: 0.00024800\n",
            "Train Epoch: 9 [4096/123872 (3%)]\tLoss: 0.441496\tLR: 0.00024789\n",
            "Train Epoch: 9 [4352/123872 (4%)]\tLoss: 0.423509\tLR: 0.00024778\n",
            "Train Epoch: 9 [4608/123872 (4%)]\tLoss: 0.416507\tLR: 0.00024767\n",
            "Train Epoch: 9 [4864/123872 (4%)]\tLoss: 0.381801\tLR: 0.00024755\n",
            "Train Epoch: 9 [5120/123872 (4%)]\tLoss: 0.397660\tLR: 0.00024744\n",
            "Train Epoch: 9 [5120/123872 (4%)]\tLoss: 0.397660\n",
            "Train Epoch: 9 [5376/123872 (4%)]\tLoss: 0.364097\tLR: 0.00024733\n",
            "Train Epoch: 9 [5632/123872 (5%)]\tLoss: 0.390758\tLR: 0.00024722\n",
            "Train Epoch: 9 [5888/123872 (5%)]\tLoss: 0.347181\tLR: 0.00024710\n",
            "Train Epoch: 9 [6144/123872 (5%)]\tLoss: 0.379308\tLR: 0.00024699\n",
            "Train Epoch: 9 [6400/123872 (5%)]\tLoss: 0.318676\tLR: 0.00024688\n",
            "Train Epoch: 9 [6656/123872 (5%)]\tLoss: 0.343537\tLR: 0.00024677\n",
            "Train Epoch: 9 [6912/123872 (6%)]\tLoss: 0.383897\tLR: 0.00024666\n",
            "Train Epoch: 9 [7168/123872 (6%)]\tLoss: 0.336482\tLR: 0.00024654\n",
            "Train Epoch: 9 [7424/123872 (6%)]\tLoss: 0.378647\tLR: 0.00024643\n",
            "Train Epoch: 9 [7680/123872 (6%)]\tLoss: 0.314331\tLR: 0.00024632\n",
            "Train Epoch: 9 [7680/123872 (6%)]\tLoss: 0.314331\n",
            "Train Epoch: 9 [7936/123872 (6%)]\tLoss: 0.274346\tLR: 0.00024621\n",
            "Train Epoch: 9 [8192/123872 (7%)]\tLoss: 0.376636\tLR: 0.00024609\n",
            "Train Epoch: 9 [8448/123872 (7%)]\tLoss: 0.419314\tLR: 0.00024598\n",
            "Train Epoch: 9 [8704/123872 (7%)]\tLoss: 0.462688\tLR: 0.00024587\n",
            "Train Epoch: 9 [8960/123872 (7%)]\tLoss: 0.362866\tLR: 0.00024576\n",
            "Train Epoch: 9 [9216/123872 (7%)]\tLoss: 0.421468\tLR: 0.00024565\n",
            "Train Epoch: 9 [9472/123872 (8%)]\tLoss: 0.354694\tLR: 0.00024553\n",
            "Train Epoch: 9 [9728/123872 (8%)]\tLoss: 0.375092\tLR: 0.00024542\n",
            "Train Epoch: 9 [9984/123872 (8%)]\tLoss: 0.370164\tLR: 0.00024531\n",
            "Train Epoch: 9 [10240/123872 (8%)]\tLoss: 0.336320\tLR: 0.00024520\n",
            "Train Epoch: 9 [10240/123872 (8%)]\tLoss: 0.336320\n",
            "Train Epoch: 9 [10496/123872 (8%)]\tLoss: 0.357711\tLR: 0.00024509\n",
            "Train Epoch: 9 [10752/123872 (9%)]\tLoss: 0.390806\tLR: 0.00024497\n",
            "Train Epoch: 9 [11008/123872 (9%)]\tLoss: 0.350811\tLR: 0.00024486\n",
            "Train Epoch: 9 [11264/123872 (9%)]\tLoss: 0.386867\tLR: 0.00024475\n",
            "Train Epoch: 9 [11520/123872 (9%)]\tLoss: 0.335913\tLR: 0.00024464\n",
            "Train Epoch: 9 [11776/123872 (10%)]\tLoss: 0.398737\tLR: 0.00024453\n",
            "Train Epoch: 9 [12032/123872 (10%)]\tLoss: 0.460765\tLR: 0.00024441\n",
            "Train Epoch: 9 [12288/123872 (10%)]\tLoss: 0.384398\tLR: 0.00024430\n",
            "Train Epoch: 9 [12544/123872 (10%)]\tLoss: 0.470000\tLR: 0.00024419\n",
            "Train Epoch: 9 [12800/123872 (10%)]\tLoss: 0.416281\tLR: 0.00024408\n",
            "Train Epoch: 9 [12800/123872 (10%)]\tLoss: 0.416281\n",
            "Train Epoch: 9 [13056/123872 (11%)]\tLoss: 0.389056\tLR: 0.00024397\n",
            "Train Epoch: 9 [13312/123872 (11%)]\tLoss: 0.432751\tLR: 0.00024385\n",
            "Train Epoch: 9 [13568/123872 (11%)]\tLoss: 0.375701\tLR: 0.00024374\n",
            "Train Epoch: 9 [13824/123872 (11%)]\tLoss: 0.342238\tLR: 0.00024363\n",
            "Train Epoch: 9 [14080/123872 (11%)]\tLoss: 0.408740\tLR: 0.00024352\n",
            "Train Epoch: 9 [14336/123872 (12%)]\tLoss: 0.460984\tLR: 0.00024341\n",
            "Train Epoch: 9 [14592/123872 (12%)]\tLoss: 0.388191\tLR: 0.00024330\n",
            "Train Epoch: 9 [14848/123872 (12%)]\tLoss: 0.373234\tLR: 0.00024318\n",
            "Train Epoch: 9 [15104/123872 (12%)]\tLoss: 0.362975\tLR: 0.00024307\n",
            "Train Epoch: 9 [15360/123872 (12%)]\tLoss: 0.369883\tLR: 0.00024296\n",
            "Train Epoch: 9 [15360/123872 (12%)]\tLoss: 0.369883\n",
            "Train Epoch: 9 [15616/123872 (13%)]\tLoss: 0.336317\tLR: 0.00024285\n",
            "Train Epoch: 9 [15872/123872 (13%)]\tLoss: 0.408518\tLR: 0.00024274\n",
            "Train Epoch: 9 [16128/123872 (13%)]\tLoss: 0.419036\tLR: 0.00024262\n",
            "Train Epoch: 9 [16384/123872 (13%)]\tLoss: 0.339450\tLR: 0.00024251\n",
            "Train Epoch: 9 [16640/123872 (13%)]\tLoss: 0.366808\tLR: 0.00024240\n",
            "Train Epoch: 9 [16896/123872 (14%)]\tLoss: 0.398486\tLR: 0.00024229\n",
            "Train Epoch: 9 [17152/123872 (14%)]\tLoss: 0.441040\tLR: 0.00024218\n",
            "Train Epoch: 9 [17408/123872 (14%)]\tLoss: 0.372144\tLR: 0.00024207\n",
            "Train Epoch: 9 [17664/123872 (14%)]\tLoss: 0.348002\tLR: 0.00024196\n",
            "Train Epoch: 9 [17920/123872 (14%)]\tLoss: 0.371546\tLR: 0.00024184\n",
            "Train Epoch: 9 [17920/123872 (14%)]\tLoss: 0.371546\n",
            "Train Epoch: 9 [18176/123872 (15%)]\tLoss: 0.395727\tLR: 0.00024173\n",
            "Train Epoch: 9 [18432/123872 (15%)]\tLoss: 0.373292\tLR: 0.00024162\n",
            "Train Epoch: 9 [18688/123872 (15%)]\tLoss: 0.378024\tLR: 0.00024151\n",
            "Train Epoch: 9 [18944/123872 (15%)]\tLoss: 0.347238\tLR: 0.00024140\n",
            "Train Epoch: 9 [19200/123872 (15%)]\tLoss: 0.395143\tLR: 0.00024129\n",
            "Train Epoch: 9 [19456/123872 (16%)]\tLoss: 0.308332\tLR: 0.00024117\n",
            "Train Epoch: 9 [19712/123872 (16%)]\tLoss: 0.368503\tLR: 0.00024106\n",
            "Train Epoch: 9 [19968/123872 (16%)]\tLoss: 0.396272\tLR: 0.00024095\n",
            "Train Epoch: 9 [20224/123872 (16%)]\tLoss: 0.364082\tLR: 0.00024084\n",
            "Train Epoch: 9 [20480/123872 (17%)]\tLoss: 0.367814\tLR: 0.00024073\n",
            "Train Epoch: 9 [20480/123872 (17%)]\tLoss: 0.367814\n",
            "Train Epoch: 9 [20736/123872 (17%)]\tLoss: 0.351469\tLR: 0.00024062\n",
            "Train Epoch: 9 [20992/123872 (17%)]\tLoss: 0.363906\tLR: 0.00024051\n",
            "Train Epoch: 9 [21248/123872 (17%)]\tLoss: 0.387291\tLR: 0.00024040\n",
            "Train Epoch: 9 [21504/123872 (17%)]\tLoss: 0.391981\tLR: 0.00024028\n",
            "Train Epoch: 9 [21760/123872 (18%)]\tLoss: 0.394147\tLR: 0.00024017\n",
            "Train Epoch: 9 [22016/123872 (18%)]\tLoss: 0.450520\tLR: 0.00024006\n",
            "Train Epoch: 9 [22272/123872 (18%)]\tLoss: 0.437114\tLR: 0.00023995\n",
            "Train Epoch: 9 [22528/123872 (18%)]\tLoss: 0.390648\tLR: 0.00023984\n",
            "Train Epoch: 9 [22784/123872 (18%)]\tLoss: 0.366828\tLR: 0.00023973\n",
            "Train Epoch: 9 [23040/123872 (19%)]\tLoss: 0.395950\tLR: 0.00023962\n",
            "Train Epoch: 9 [23040/123872 (19%)]\tLoss: 0.395950\n",
            "Train Epoch: 9 [23296/123872 (19%)]\tLoss: 0.413764\tLR: 0.00023951\n",
            "Train Epoch: 9 [23552/123872 (19%)]\tLoss: 0.388299\tLR: 0.00023939\n",
            "Train Epoch: 9 [23808/123872 (19%)]\tLoss: 0.362150\tLR: 0.00023928\n",
            "Train Epoch: 9 [24064/123872 (19%)]\tLoss: 0.356172\tLR: 0.00023917\n",
            "Train Epoch: 9 [24320/123872 (20%)]\tLoss: 0.362299\tLR: 0.00023906\n",
            "Train Epoch: 9 [24576/123872 (20%)]\tLoss: 0.336886\tLR: 0.00023895\n",
            "Train Epoch: 9 [24832/123872 (20%)]\tLoss: 0.360914\tLR: 0.00023884\n",
            "Train Epoch: 9 [25088/123872 (20%)]\tLoss: 0.397261\tLR: 0.00023873\n",
            "Train Epoch: 9 [25344/123872 (20%)]\tLoss: 0.437081\tLR: 0.00023862\n",
            "Train Epoch: 9 [25600/123872 (21%)]\tLoss: 0.420308\tLR: 0.00023851\n",
            "Train Epoch: 9 [25600/123872 (21%)]\tLoss: 0.420308\n",
            "Train Epoch: 9 [25856/123872 (21%)]\tLoss: 0.378685\tLR: 0.00023839\n",
            "Train Epoch: 9 [26112/123872 (21%)]\tLoss: 0.351763\tLR: 0.00023828\n",
            "Train Epoch: 9 [26368/123872 (21%)]\tLoss: 0.412134\tLR: 0.00023817\n",
            "Train Epoch: 9 [26624/123872 (21%)]\tLoss: 0.328988\tLR: 0.00023806\n",
            "Train Epoch: 9 [26880/123872 (22%)]\tLoss: 0.334560\tLR: 0.00023795\n",
            "Train Epoch: 9 [27136/123872 (22%)]\tLoss: 0.353640\tLR: 0.00023784\n",
            "Train Epoch: 9 [27392/123872 (22%)]\tLoss: 0.437315\tLR: 0.00023773\n",
            "Train Epoch: 9 [27648/123872 (22%)]\tLoss: 0.367327\tLR: 0.00023762\n",
            "Train Epoch: 9 [27904/123872 (23%)]\tLoss: 0.398732\tLR: 0.00023751\n",
            "Train Epoch: 9 [28160/123872 (23%)]\tLoss: 0.369257\tLR: 0.00023740\n",
            "Train Epoch: 9 [28160/123872 (23%)]\tLoss: 0.369257\n",
            "Train Epoch: 9 [28416/123872 (23%)]\tLoss: 0.329937\tLR: 0.00023729\n",
            "Train Epoch: 9 [28672/123872 (23%)]\tLoss: 0.354483\tLR: 0.00023717\n",
            "Train Epoch: 9 [28928/123872 (23%)]\tLoss: 0.349072\tLR: 0.00023706\n",
            "Train Epoch: 9 [29184/123872 (24%)]\tLoss: 0.377354\tLR: 0.00023695\n",
            "Train Epoch: 9 [29440/123872 (24%)]\tLoss: 0.346782\tLR: 0.00023684\n",
            "Train Epoch: 9 [29696/123872 (24%)]\tLoss: 0.341931\tLR: 0.00023673\n",
            "Train Epoch: 9 [29952/123872 (24%)]\tLoss: 0.392052\tLR: 0.00023662\n",
            "Train Epoch: 9 [30208/123872 (24%)]\tLoss: 0.335724\tLR: 0.00023651\n",
            "Train Epoch: 9 [30464/123872 (25%)]\tLoss: 0.317532\tLR: 0.00023640\n",
            "Train Epoch: 9 [30720/123872 (25%)]\tLoss: 0.350184\tLR: 0.00023629\n",
            "Train Epoch: 9 [30720/123872 (25%)]\tLoss: 0.350184\n",
            "Train Epoch: 9 [30976/123872 (25%)]\tLoss: 0.331336\tLR: 0.00023618\n",
            "Train Epoch: 9 [31232/123872 (25%)]\tLoss: 0.396502\tLR: 0.00023607\n",
            "Train Epoch: 9 [31488/123872 (25%)]\tLoss: 0.397439\tLR: 0.00023596\n",
            "Train Epoch: 9 [31744/123872 (26%)]\tLoss: 0.366754\tLR: 0.00023585\n",
            "Train Epoch: 9 [32000/123872 (26%)]\tLoss: 0.361773\tLR: 0.00023574\n",
            "Train Epoch: 9 [32256/123872 (26%)]\tLoss: 0.397273\tLR: 0.00023562\n",
            "Train Epoch: 9 [32512/123872 (26%)]\tLoss: 0.365481\tLR: 0.00023551\n",
            "Train Epoch: 9 [32768/123872 (26%)]\tLoss: 0.410993\tLR: 0.00023540\n",
            "Train Epoch: 9 [33024/123872 (27%)]\tLoss: 0.378612\tLR: 0.00023529\n",
            "Train Epoch: 9 [33280/123872 (27%)]\tLoss: 0.339416\tLR: 0.00023518\n",
            "Train Epoch: 9 [33280/123872 (27%)]\tLoss: 0.339416\n",
            "Train Epoch: 9 [33536/123872 (27%)]\tLoss: 0.328967\tLR: 0.00023507\n",
            "Train Epoch: 9 [33792/123872 (27%)]\tLoss: 0.349674\tLR: 0.00023496\n",
            "Train Epoch: 9 [34048/123872 (27%)]\tLoss: 0.343675\tLR: 0.00023485\n",
            "Train Epoch: 9 [34304/123872 (28%)]\tLoss: 0.344838\tLR: 0.00023474\n",
            "Train Epoch: 9 [34560/123872 (28%)]\tLoss: 0.434249\tLR: 0.00023463\n",
            "Train Epoch: 9 [34816/123872 (28%)]\tLoss: 0.394445\tLR: 0.00023452\n",
            "Train Epoch: 9 [35072/123872 (28%)]\tLoss: 0.339426\tLR: 0.00023441\n",
            "Train Epoch: 9 [35328/123872 (29%)]\tLoss: 0.378612\tLR: 0.00023430\n",
            "Train Epoch: 9 [35584/123872 (29%)]\tLoss: 0.406256\tLR: 0.00023419\n",
            "Train Epoch: 9 [35840/123872 (29%)]\tLoss: 0.374237\tLR: 0.00023408\n",
            "Train Epoch: 9 [35840/123872 (29%)]\tLoss: 0.374237\n",
            "Train Epoch: 9 [36096/123872 (29%)]\tLoss: 0.396015\tLR: 0.00023397\n",
            "Train Epoch: 9 [36352/123872 (29%)]\tLoss: 0.357001\tLR: 0.00023386\n",
            "Train Epoch: 9 [36608/123872 (30%)]\tLoss: 0.419336\tLR: 0.00023375\n",
            "Train Epoch: 9 [36864/123872 (30%)]\tLoss: 0.398981\tLR: 0.00023364\n",
            "Train Epoch: 9 [37120/123872 (30%)]\tLoss: 0.379538\tLR: 0.00023353\n",
            "Train Epoch: 9 [37376/123872 (30%)]\tLoss: 0.371528\tLR: 0.00023342\n",
            "Train Epoch: 9 [37632/123872 (30%)]\tLoss: 0.359469\tLR: 0.00023331\n",
            "Train Epoch: 9 [37888/123872 (31%)]\tLoss: 0.348970\tLR: 0.00023320\n",
            "Train Epoch: 9 [38144/123872 (31%)]\tLoss: 0.358299\tLR: 0.00023309\n",
            "Train Epoch: 9 [38400/123872 (31%)]\tLoss: 0.403233\tLR: 0.00023298\n",
            "Train Epoch: 9 [38400/123872 (31%)]\tLoss: 0.403233\n",
            "Train Epoch: 9 [38656/123872 (31%)]\tLoss: 0.334132\tLR: 0.00023287\n",
            "Train Epoch: 9 [38912/123872 (31%)]\tLoss: 0.404888\tLR: 0.00023276\n",
            "Train Epoch: 9 [39168/123872 (32%)]\tLoss: 0.400639\tLR: 0.00023265\n",
            "Train Epoch: 9 [39424/123872 (32%)]\tLoss: 0.344496\tLR: 0.00023254\n",
            "Train Epoch: 9 [39680/123872 (32%)]\tLoss: 0.424326\tLR: 0.00023243\n",
            "Train Epoch: 9 [39936/123872 (32%)]\tLoss: 0.375773\tLR: 0.00023232\n",
            "Train Epoch: 9 [40192/123872 (32%)]\tLoss: 0.436678\tLR: 0.00023221\n",
            "Train Epoch: 9 [40448/123872 (33%)]\tLoss: 0.344380\tLR: 0.00023210\n",
            "Train Epoch: 9 [40704/123872 (33%)]\tLoss: 0.378844\tLR: 0.00023199\n",
            "Train Epoch: 9 [40960/123872 (33%)]\tLoss: 0.329055\tLR: 0.00023188\n",
            "Train Epoch: 9 [40960/123872 (33%)]\tLoss: 0.329055\n",
            "Train Epoch: 9 [41216/123872 (33%)]\tLoss: 0.381235\tLR: 0.00023177\n",
            "Train Epoch: 9 [41472/123872 (33%)]\tLoss: 0.422917\tLR: 0.00023166\n",
            "Train Epoch: 9 [41728/123872 (34%)]\tLoss: 0.405165\tLR: 0.00023155\n",
            "Train Epoch: 9 [41984/123872 (34%)]\tLoss: 0.375585\tLR: 0.00023144\n",
            "Train Epoch: 9 [42240/123872 (34%)]\tLoss: 0.407564\tLR: 0.00023133\n",
            "Train Epoch: 9 [42496/123872 (34%)]\tLoss: 0.422392\tLR: 0.00023122\n",
            "Train Epoch: 9 [42752/123872 (35%)]\tLoss: 0.397407\tLR: 0.00023111\n",
            "Train Epoch: 9 [43008/123872 (35%)]\tLoss: 0.372023\tLR: 0.00023100\n",
            "Train Epoch: 9 [43264/123872 (35%)]\tLoss: 0.379435\tLR: 0.00023089\n",
            "Train Epoch: 9 [43520/123872 (35%)]\tLoss: 0.346323\tLR: 0.00023078\n",
            "Train Epoch: 9 [43520/123872 (35%)]\tLoss: 0.346323\n",
            "Train Epoch: 9 [43776/123872 (35%)]\tLoss: 0.382167\tLR: 0.00023067\n",
            "Train Epoch: 9 [44032/123872 (36%)]\tLoss: 0.394201\tLR: 0.00023056\n",
            "Train Epoch: 9 [44288/123872 (36%)]\tLoss: 0.377847\tLR: 0.00023045\n",
            "Train Epoch: 9 [44544/123872 (36%)]\tLoss: 0.387968\tLR: 0.00023034\n",
            "Train Epoch: 9 [44800/123872 (36%)]\tLoss: 0.391315\tLR: 0.00023023\n",
            "Train Epoch: 9 [45056/123872 (36%)]\tLoss: 0.446938\tLR: 0.00023012\n",
            "Train Epoch: 9 [45312/123872 (37%)]\tLoss: 0.410164\tLR: 0.00023001\n",
            "Train Epoch: 9 [45568/123872 (37%)]\tLoss: 0.401311\tLR: 0.00022990\n",
            "Train Epoch: 9 [45824/123872 (37%)]\tLoss: 0.350324\tLR: 0.00022979\n",
            "Train Epoch: 9 [46080/123872 (37%)]\tLoss: 0.329121\tLR: 0.00022968\n",
            "Train Epoch: 9 [46080/123872 (37%)]\tLoss: 0.329121\n",
            "Train Epoch: 9 [46336/123872 (37%)]\tLoss: 0.388586\tLR: 0.00022957\n",
            "Train Epoch: 9 [46592/123872 (38%)]\tLoss: 0.362429\tLR: 0.00022946\n",
            "Train Epoch: 9 [46848/123872 (38%)]\tLoss: 0.338579\tLR: 0.00022935\n",
            "Train Epoch: 9 [47104/123872 (38%)]\tLoss: 0.364894\tLR: 0.00022924\n",
            "Train Epoch: 9 [47360/123872 (38%)]\tLoss: 0.451417\tLR: 0.00022913\n",
            "Train Epoch: 9 [47616/123872 (38%)]\tLoss: 0.322084\tLR: 0.00022902\n",
            "Train Epoch: 9 [47872/123872 (39%)]\tLoss: 0.367270\tLR: 0.00022891\n",
            "Train Epoch: 9 [48128/123872 (39%)]\tLoss: 0.351823\tLR: 0.00022881\n",
            "Train Epoch: 9 [48384/123872 (39%)]\tLoss: 0.420823\tLR: 0.00022870\n",
            "Train Epoch: 9 [48640/123872 (39%)]\tLoss: 0.413113\tLR: 0.00022859\n",
            "Train Epoch: 9 [48640/123872 (39%)]\tLoss: 0.413113\n",
            "Train Epoch: 9 [48896/123872 (39%)]\tLoss: 0.362531\tLR: 0.00022848\n",
            "Train Epoch: 9 [49152/123872 (40%)]\tLoss: 0.369503\tLR: 0.00022837\n",
            "Train Epoch: 9 [49408/123872 (40%)]\tLoss: 0.382098\tLR: 0.00022826\n",
            "Train Epoch: 9 [49664/123872 (40%)]\tLoss: 0.325252\tLR: 0.00022815\n",
            "Train Epoch: 9 [49920/123872 (40%)]\tLoss: 0.368011\tLR: 0.00022804\n",
            "Train Epoch: 9 [50176/123872 (40%)]\tLoss: 0.375064\tLR: 0.00022793\n",
            "Train Epoch: 9 [50432/123872 (41%)]\tLoss: 0.414917\tLR: 0.00022782\n",
            "Train Epoch: 9 [50688/123872 (41%)]\tLoss: 0.358708\tLR: 0.00022771\n",
            "Train Epoch: 9 [50944/123872 (41%)]\tLoss: 0.338910\tLR: 0.00022760\n",
            "Train Epoch: 9 [51200/123872 (41%)]\tLoss: 0.368357\tLR: 0.00022749\n",
            "Train Epoch: 9 [51200/123872 (41%)]\tLoss: 0.368357\n",
            "Train Epoch: 9 [51456/123872 (42%)]\tLoss: 0.395170\tLR: 0.00022738\n",
            "Train Epoch: 9 [51712/123872 (42%)]\tLoss: 0.389944\tLR: 0.00022728\n",
            "Train Epoch: 9 [51968/123872 (42%)]\tLoss: 0.338270\tLR: 0.00022717\n",
            "Train Epoch: 9 [52224/123872 (42%)]\tLoss: 0.381995\tLR: 0.00022706\n",
            "Train Epoch: 9 [52480/123872 (42%)]\tLoss: 0.414519\tLR: 0.00022695\n",
            "Train Epoch: 9 [52736/123872 (43%)]\tLoss: 0.420450\tLR: 0.00022684\n",
            "Train Epoch: 9 [52992/123872 (43%)]\tLoss: 0.328838\tLR: 0.00022673\n",
            "Train Epoch: 9 [53248/123872 (43%)]\tLoss: 0.385296\tLR: 0.00022662\n",
            "Train Epoch: 9 [53504/123872 (43%)]\tLoss: 0.392657\tLR: 0.00022651\n",
            "Train Epoch: 9 [53760/123872 (43%)]\tLoss: 0.374582\tLR: 0.00022640\n",
            "Train Epoch: 9 [53760/123872 (43%)]\tLoss: 0.374582\n",
            "Train Epoch: 9 [54016/123872 (44%)]\tLoss: 0.392828\tLR: 0.00022629\n",
            "Train Epoch: 9 [54272/123872 (44%)]\tLoss: 0.411770\tLR: 0.00022619\n",
            "Train Epoch: 9 [54528/123872 (44%)]\tLoss: 0.413861\tLR: 0.00022608\n",
            "Train Epoch: 9 [54784/123872 (44%)]\tLoss: 0.358813\tLR: 0.00022597\n",
            "Train Epoch: 9 [55040/123872 (44%)]\tLoss: 0.391825\tLR: 0.00022586\n",
            "Train Epoch: 9 [55296/123872 (45%)]\tLoss: 0.360790\tLR: 0.00022575\n",
            "Train Epoch: 9 [55552/123872 (45%)]\tLoss: 0.396022\tLR: 0.00022564\n",
            "Train Epoch: 9 [55808/123872 (45%)]\tLoss: 0.427631\tLR: 0.00022553\n",
            "Train Epoch: 9 [56064/123872 (45%)]\tLoss: 0.385838\tLR: 0.00022542\n",
            "Train Epoch: 9 [56320/123872 (45%)]\tLoss: 0.411886\tLR: 0.00022531\n",
            "Train Epoch: 9 [56320/123872 (45%)]\tLoss: 0.411886\n",
            "Train Epoch: 9 [56576/123872 (46%)]\tLoss: 0.431202\tLR: 0.00022521\n",
            "Train Epoch: 9 [56832/123872 (46%)]\tLoss: 0.342792\tLR: 0.00022510\n",
            "Train Epoch: 9 [57088/123872 (46%)]\tLoss: 0.393097\tLR: 0.00022499\n",
            "Train Epoch: 9 [57344/123872 (46%)]\tLoss: 0.404900\tLR: 0.00022488\n",
            "Train Epoch: 9 [57600/123872 (46%)]\tLoss: 0.348138\tLR: 0.00022477\n",
            "Train Epoch: 9 [57856/123872 (47%)]\tLoss: 0.322918\tLR: 0.00022466\n",
            "Train Epoch: 9 [58112/123872 (47%)]\tLoss: 0.394889\tLR: 0.00022455\n",
            "Train Epoch: 9 [58368/123872 (47%)]\tLoss: 0.393295\tLR: 0.00022444\n",
            "Train Epoch: 9 [58624/123872 (47%)]\tLoss: 0.382734\tLR: 0.00022434\n",
            "Train Epoch: 9 [58880/123872 (48%)]\tLoss: 0.318700\tLR: 0.00022423\n",
            "Train Epoch: 9 [58880/123872 (48%)]\tLoss: 0.318700\n",
            "Train Epoch: 9 [59136/123872 (48%)]\tLoss: 0.435198\tLR: 0.00022412\n",
            "Train Epoch: 9 [59392/123872 (48%)]\tLoss: 0.339367\tLR: 0.00022401\n",
            "Train Epoch: 9 [59648/123872 (48%)]\tLoss: 0.426775\tLR: 0.00022390\n",
            "Train Epoch: 9 [59904/123872 (48%)]\tLoss: 0.409258\tLR: 0.00022379\n",
            "Train Epoch: 9 [60160/123872 (49%)]\tLoss: 0.443646\tLR: 0.00022368\n",
            "Train Epoch: 9 [60416/123872 (49%)]\tLoss: 0.349008\tLR: 0.00022358\n",
            "Train Epoch: 9 [60672/123872 (49%)]\tLoss: 0.387868\tLR: 0.00022347\n",
            "Train Epoch: 9 [60928/123872 (49%)]\tLoss: 0.356561\tLR: 0.00022336\n",
            "Train Epoch: 9 [61184/123872 (49%)]\tLoss: 0.357895\tLR: 0.00022325\n",
            "Train Epoch: 9 [61440/123872 (50%)]\tLoss: 0.406177\tLR: 0.00022314\n",
            "Train Epoch: 9 [61440/123872 (50%)]\tLoss: 0.406177\n",
            "Train Epoch: 9 [61696/123872 (50%)]\tLoss: 0.363118\tLR: 0.00022303\n",
            "Train Epoch: 9 [61952/123872 (50%)]\tLoss: 0.402935\tLR: 0.00022293\n",
            "Train Epoch: 9 [62208/123872 (50%)]\tLoss: 0.354312\tLR: 0.00022282\n",
            "Train Epoch: 9 [62464/123872 (50%)]\tLoss: 0.371295\tLR: 0.00022271\n",
            "Train Epoch: 9 [62720/123872 (51%)]\tLoss: 0.394579\tLR: 0.00022260\n",
            "Train Epoch: 9 [62976/123872 (51%)]\tLoss: 0.459607\tLR: 0.00022249\n",
            "Train Epoch: 9 [63232/123872 (51%)]\tLoss: 0.314828\tLR: 0.00022238\n",
            "Train Epoch: 9 [63488/123872 (51%)]\tLoss: 0.432085\tLR: 0.00022228\n",
            "Train Epoch: 9 [63744/123872 (51%)]\tLoss: 0.415926\tLR: 0.00022217\n",
            "Train Epoch: 9 [64000/123872 (52%)]\tLoss: 0.414327\tLR: 0.00022206\n",
            "Train Epoch: 9 [64000/123872 (52%)]\tLoss: 0.414327\n",
            "Train Epoch: 9 [64256/123872 (52%)]\tLoss: 0.330339\tLR: 0.00022195\n",
            "Train Epoch: 9 [64512/123872 (52%)]\tLoss: 0.435492\tLR: 0.00022184\n",
            "Train Epoch: 9 [64768/123872 (52%)]\tLoss: 0.308763\tLR: 0.00022174\n",
            "Train Epoch: 9 [65024/123872 (52%)]\tLoss: 0.395925\tLR: 0.00022163\n",
            "Train Epoch: 9 [65280/123872 (53%)]\tLoss: 0.319473\tLR: 0.00022152\n",
            "Train Epoch: 9 [65536/123872 (53%)]\tLoss: 0.410813\tLR: 0.00022141\n",
            "Train Epoch: 9 [65792/123872 (53%)]\tLoss: 0.324598\tLR: 0.00022130\n",
            "Train Epoch: 9 [66048/123872 (53%)]\tLoss: 0.379923\tLR: 0.00022120\n",
            "Train Epoch: 9 [66304/123872 (54%)]\tLoss: 0.390078\tLR: 0.00022109\n",
            "Train Epoch: 9 [66560/123872 (54%)]\tLoss: 0.376817\tLR: 0.00022098\n",
            "Train Epoch: 9 [66560/123872 (54%)]\tLoss: 0.376817\n",
            "Train Epoch: 9 [66816/123872 (54%)]\tLoss: 0.379270\tLR: 0.00022087\n",
            "Train Epoch: 9 [67072/123872 (54%)]\tLoss: 0.434297\tLR: 0.00022076\n",
            "Train Epoch: 9 [67328/123872 (54%)]\tLoss: 0.411954\tLR: 0.00022066\n",
            "Train Epoch: 9 [67584/123872 (55%)]\tLoss: 0.361196\tLR: 0.00022055\n",
            "Train Epoch: 9 [67840/123872 (55%)]\tLoss: 0.372292\tLR: 0.00022044\n",
            "Train Epoch: 9 [68096/123872 (55%)]\tLoss: 0.385003\tLR: 0.00022033\n",
            "Train Epoch: 9 [68352/123872 (55%)]\tLoss: 0.377611\tLR: 0.00022022\n",
            "Train Epoch: 9 [68608/123872 (55%)]\tLoss: 0.348870\tLR: 0.00022012\n",
            "Train Epoch: 9 [68864/123872 (56%)]\tLoss: 0.377393\tLR: 0.00022001\n",
            "Train Epoch: 9 [69120/123872 (56%)]\tLoss: 0.371675\tLR: 0.00021990\n",
            "Train Epoch: 9 [69120/123872 (56%)]\tLoss: 0.371675\n",
            "Train Epoch: 9 [69376/123872 (56%)]\tLoss: 0.429125\tLR: 0.00021979\n",
            "Train Epoch: 9 [69632/123872 (56%)]\tLoss: 0.357141\tLR: 0.00021969\n",
            "Train Epoch: 9 [69888/123872 (56%)]\tLoss: 0.318530\tLR: 0.00021958\n",
            "Train Epoch: 9 [70144/123872 (57%)]\tLoss: 0.378095\tLR: 0.00021947\n",
            "Train Epoch: 9 [70400/123872 (57%)]\tLoss: 0.441302\tLR: 0.00021936\n",
            "Train Epoch: 9 [70656/123872 (57%)]\tLoss: 0.387202\tLR: 0.00021925\n",
            "Train Epoch: 9 [70912/123872 (57%)]\tLoss: 0.328169\tLR: 0.00021915\n",
            "Train Epoch: 9 [71168/123872 (57%)]\tLoss: 0.332853\tLR: 0.00021904\n",
            "Train Epoch: 9 [71424/123872 (58%)]\tLoss: 0.365528\tLR: 0.00021893\n",
            "Train Epoch: 9 [71680/123872 (58%)]\tLoss: 0.426811\tLR: 0.00021882\n",
            "Train Epoch: 9 [71680/123872 (58%)]\tLoss: 0.426811\n",
            "Train Epoch: 9 [71936/123872 (58%)]\tLoss: 0.332828\tLR: 0.00021872\n",
            "Train Epoch: 9 [72192/123872 (58%)]\tLoss: 0.377736\tLR: 0.00021861\n",
            "Train Epoch: 9 [72448/123872 (58%)]\tLoss: 0.364177\tLR: 0.00021850\n",
            "Train Epoch: 9 [72704/123872 (59%)]\tLoss: 0.365186\tLR: 0.00021839\n",
            "Train Epoch: 9 [72960/123872 (59%)]\tLoss: 0.378429\tLR: 0.00021829\n",
            "Train Epoch: 9 [73216/123872 (59%)]\tLoss: 0.336379\tLR: 0.00021818\n",
            "Train Epoch: 9 [73472/123872 (59%)]\tLoss: 0.401448\tLR: 0.00021807\n",
            "Train Epoch: 9 [73728/123872 (60%)]\tLoss: 0.357292\tLR: 0.00021796\n",
            "Train Epoch: 9 [73984/123872 (60%)]\tLoss: 0.360475\tLR: 0.00021786\n",
            "Train Epoch: 9 [74240/123872 (60%)]\tLoss: 0.382431\tLR: 0.00021775\n",
            "Train Epoch: 9 [74240/123872 (60%)]\tLoss: 0.382431\n",
            "Train Epoch: 9 [74496/123872 (60%)]\tLoss: 0.405667\tLR: 0.00021764\n",
            "Train Epoch: 9 [74752/123872 (60%)]\tLoss: 0.349284\tLR: 0.00021754\n",
            "Train Epoch: 9 [75008/123872 (61%)]\tLoss: 0.401190\tLR: 0.00021743\n",
            "Train Epoch: 9 [75264/123872 (61%)]\tLoss: 0.378392\tLR: 0.00021732\n",
            "Train Epoch: 9 [75520/123872 (61%)]\tLoss: 0.309780\tLR: 0.00021721\n",
            "Train Epoch: 9 [75776/123872 (61%)]\tLoss: 0.421979\tLR: 0.00021711\n",
            "Train Epoch: 9 [76032/123872 (61%)]\tLoss: 0.321678\tLR: 0.00021700\n",
            "Train Epoch: 9 [76288/123872 (62%)]\tLoss: 0.339226\tLR: 0.00021689\n",
            "Train Epoch: 9 [76544/123872 (62%)]\tLoss: 0.350994\tLR: 0.00021679\n",
            "Train Epoch: 9 [76800/123872 (62%)]\tLoss: 0.474360\tLR: 0.00021668\n",
            "Train Epoch: 9 [76800/123872 (62%)]\tLoss: 0.474360\n",
            "Train Epoch: 9 [77056/123872 (62%)]\tLoss: 0.414880\tLR: 0.00021657\n",
            "Train Epoch: 9 [77312/123872 (62%)]\tLoss: 0.361272\tLR: 0.00021646\n",
            "Train Epoch: 9 [77568/123872 (63%)]\tLoss: 0.430743\tLR: 0.00021636\n",
            "Train Epoch: 9 [77824/123872 (63%)]\tLoss: 0.351062\tLR: 0.00021625\n",
            "Train Epoch: 9 [78080/123872 (63%)]\tLoss: 0.379516\tLR: 0.00021614\n",
            "Train Epoch: 9 [78336/123872 (63%)]\tLoss: 0.349022\tLR: 0.00021604\n",
            "Train Epoch: 9 [78592/123872 (63%)]\tLoss: 0.351927\tLR: 0.00021593\n",
            "Train Epoch: 9 [78848/123872 (64%)]\tLoss: 0.310256\tLR: 0.00021582\n",
            "Train Epoch: 9 [79104/123872 (64%)]\tLoss: 0.372751\tLR: 0.00021571\n",
            "Train Epoch: 9 [79360/123872 (64%)]\tLoss: 0.401861\tLR: 0.00021561\n",
            "Train Epoch: 9 [79360/123872 (64%)]\tLoss: 0.401861\n",
            "Train Epoch: 9 [79616/123872 (64%)]\tLoss: 0.354859\tLR: 0.00021550\n",
            "Train Epoch: 9 [79872/123872 (64%)]\tLoss: 0.349170\tLR: 0.00021539\n",
            "Train Epoch: 9 [80128/123872 (65%)]\tLoss: 0.296921\tLR: 0.00021529\n",
            "Train Epoch: 9 [80384/123872 (65%)]\tLoss: 0.389288\tLR: 0.00021518\n",
            "Train Epoch: 9 [80640/123872 (65%)]\tLoss: 0.358893\tLR: 0.00021507\n",
            "Train Epoch: 9 [80896/123872 (65%)]\tLoss: 0.421970\tLR: 0.00021497\n",
            "Train Epoch: 9 [81152/123872 (65%)]\tLoss: 0.316270\tLR: 0.00021486\n",
            "Train Epoch: 9 [81408/123872 (66%)]\tLoss: 0.339888\tLR: 0.00021475\n",
            "Train Epoch: 9 [81664/123872 (66%)]\tLoss: 0.405462\tLR: 0.00021465\n",
            "Train Epoch: 9 [81920/123872 (66%)]\tLoss: 0.398542\tLR: 0.00021454\n",
            "Train Epoch: 9 [81920/123872 (66%)]\tLoss: 0.398542\n",
            "Train Epoch: 9 [82176/123872 (66%)]\tLoss: 0.360697\tLR: 0.00021443\n",
            "Train Epoch: 9 [82432/123872 (67%)]\tLoss: 0.312678\tLR: 0.00021433\n",
            "Train Epoch: 9 [82688/123872 (67%)]\tLoss: 0.396383\tLR: 0.00021422\n",
            "Train Epoch: 9 [82944/123872 (67%)]\tLoss: 0.445934\tLR: 0.00021411\n",
            "Train Epoch: 9 [83200/123872 (67%)]\tLoss: 0.372411\tLR: 0.00021401\n",
            "Train Epoch: 9 [83456/123872 (67%)]\tLoss: 0.371832\tLR: 0.00021390\n",
            "Train Epoch: 9 [83712/123872 (68%)]\tLoss: 0.438264\tLR: 0.00021379\n",
            "Train Epoch: 9 [83968/123872 (68%)]\tLoss: 0.332057\tLR: 0.00021369\n",
            "Train Epoch: 9 [84224/123872 (68%)]\tLoss: 0.391759\tLR: 0.00021358\n",
            "Train Epoch: 9 [84480/123872 (68%)]\tLoss: 0.381426\tLR: 0.00021347\n",
            "Train Epoch: 9 [84480/123872 (68%)]\tLoss: 0.381426\n",
            "Train Epoch: 9 [84736/123872 (68%)]\tLoss: 0.356096\tLR: 0.00021337\n",
            "Train Epoch: 9 [84992/123872 (69%)]\tLoss: 0.396900\tLR: 0.00021326\n",
            "Train Epoch: 9 [85248/123872 (69%)]\tLoss: 0.376260\tLR: 0.00021316\n",
            "Train Epoch: 9 [85504/123872 (69%)]\tLoss: 0.353290\tLR: 0.00021305\n",
            "Train Epoch: 9 [85760/123872 (69%)]\tLoss: 0.317220\tLR: 0.00021294\n",
            "Train Epoch: 9 [86016/123872 (69%)]\tLoss: 0.377222\tLR: 0.00021284\n",
            "Train Epoch: 9 [86272/123872 (70%)]\tLoss: 0.291469\tLR: 0.00021273\n",
            "Train Epoch: 9 [86528/123872 (70%)]\tLoss: 0.410416\tLR: 0.00021262\n",
            "Train Epoch: 9 [86784/123872 (70%)]\tLoss: 0.318462\tLR: 0.00021252\n",
            "Train Epoch: 9 [87040/123872 (70%)]\tLoss: 0.313886\tLR: 0.00021241\n",
            "Train Epoch: 9 [87040/123872 (70%)]\tLoss: 0.313886\n",
            "Train Epoch: 9 [87296/123872 (70%)]\tLoss: 0.402733\tLR: 0.00021230\n",
            "Train Epoch: 9 [87552/123872 (71%)]\tLoss: 0.372332\tLR: 0.00021220\n",
            "Train Epoch: 9 [87808/123872 (71%)]\tLoss: 0.358814\tLR: 0.00021209\n",
            "Train Epoch: 9 [88064/123872 (71%)]\tLoss: 0.348974\tLR: 0.00021199\n",
            "Train Epoch: 9 [88320/123872 (71%)]\tLoss: 0.358854\tLR: 0.00021188\n",
            "Train Epoch: 9 [88576/123872 (71%)]\tLoss: 0.358775\tLR: 0.00021177\n",
            "Train Epoch: 9 [88832/123872 (72%)]\tLoss: 0.373206\tLR: 0.00021167\n",
            "Train Epoch: 9 [89088/123872 (72%)]\tLoss: 0.439791\tLR: 0.00021156\n",
            "Train Epoch: 9 [89344/123872 (72%)]\tLoss: 0.415933\tLR: 0.00021146\n",
            "Train Epoch: 9 [89600/123872 (72%)]\tLoss: 0.364293\tLR: 0.00021135\n",
            "Train Epoch: 9 [89600/123872 (72%)]\tLoss: 0.364293\n",
            "Train Epoch: 9 [89856/123872 (73%)]\tLoss: 0.367344\tLR: 0.00021124\n",
            "Train Epoch: 9 [90112/123872 (73%)]\tLoss: 0.394130\tLR: 0.00021114\n",
            "Train Epoch: 9 [90368/123872 (73%)]\tLoss: 0.384650\tLR: 0.00021103\n",
            "Train Epoch: 9 [90624/123872 (73%)]\tLoss: 0.373841\tLR: 0.00021093\n",
            "Train Epoch: 9 [90880/123872 (73%)]\tLoss: 0.411744\tLR: 0.00021082\n",
            "Train Epoch: 9 [91136/123872 (74%)]\tLoss: 0.351763\tLR: 0.00021071\n",
            "Train Epoch: 9 [91392/123872 (74%)]\tLoss: 0.360518\tLR: 0.00021061\n",
            "Train Epoch: 9 [91648/123872 (74%)]\tLoss: 0.358837\tLR: 0.00021050\n",
            "Train Epoch: 9 [91904/123872 (74%)]\tLoss: 0.416891\tLR: 0.00021040\n",
            "Train Epoch: 9 [92160/123872 (74%)]\tLoss: 0.378101\tLR: 0.00021029\n",
            "Train Epoch: 9 [92160/123872 (74%)]\tLoss: 0.378101\n",
            "Train Epoch: 9 [92416/123872 (75%)]\tLoss: 0.370122\tLR: 0.00021019\n",
            "Train Epoch: 9 [92672/123872 (75%)]\tLoss: 0.389418\tLR: 0.00021008\n",
            "Train Epoch: 9 [92928/123872 (75%)]\tLoss: 0.379146\tLR: 0.00020997\n",
            "Train Epoch: 9 [93184/123872 (75%)]\tLoss: 0.383944\tLR: 0.00020987\n",
            "Train Epoch: 9 [93440/123872 (75%)]\tLoss: 0.367119\tLR: 0.00020976\n",
            "Train Epoch: 9 [93696/123872 (76%)]\tLoss: 0.365215\tLR: 0.00020966\n",
            "Train Epoch: 9 [93952/123872 (76%)]\tLoss: 0.334052\tLR: 0.00020955\n",
            "Train Epoch: 9 [94208/123872 (76%)]\tLoss: 0.390097\tLR: 0.00020945\n",
            "Train Epoch: 9 [94464/123872 (76%)]\tLoss: 0.379789\tLR: 0.00020934\n",
            "Train Epoch: 9 [94720/123872 (76%)]\tLoss: 0.381596\tLR: 0.00020923\n",
            "Train Epoch: 9 [94720/123872 (76%)]\tLoss: 0.381596\n",
            "Train Epoch: 9 [94976/123872 (77%)]\tLoss: 0.427322\tLR: 0.00020913\n",
            "Train Epoch: 9 [95232/123872 (77%)]\tLoss: 0.397293\tLR: 0.00020902\n",
            "Train Epoch: 9 [95488/123872 (77%)]\tLoss: 0.392028\tLR: 0.00020892\n",
            "Train Epoch: 9 [95744/123872 (77%)]\tLoss: 0.366454\tLR: 0.00020881\n",
            "Train Epoch: 9 [96000/123872 (77%)]\tLoss: 0.408431\tLR: 0.00020871\n",
            "Train Epoch: 9 [96256/123872 (78%)]\tLoss: 0.394291\tLR: 0.00020860\n",
            "Train Epoch: 9 [96512/123872 (78%)]\tLoss: 0.367429\tLR: 0.00020850\n",
            "Train Epoch: 9 [96768/123872 (78%)]\tLoss: 0.400074\tLR: 0.00020839\n",
            "Train Epoch: 9 [97024/123872 (78%)]\tLoss: 0.383232\tLR: 0.00020829\n",
            "Train Epoch: 9 [97280/123872 (79%)]\tLoss: 0.362549\tLR: 0.00020818\n",
            "Train Epoch: 9 [97280/123872 (79%)]\tLoss: 0.362549\n",
            "Train Epoch: 9 [97536/123872 (79%)]\tLoss: 0.366381\tLR: 0.00020807\n",
            "Train Epoch: 9 [97792/123872 (79%)]\tLoss: 0.275409\tLR: 0.00020797\n",
            "Train Epoch: 9 [98048/123872 (79%)]\tLoss: 0.416909\tLR: 0.00020786\n",
            "Train Epoch: 9 [98304/123872 (79%)]\tLoss: 0.437840\tLR: 0.00020776\n",
            "Train Epoch: 9 [98560/123872 (80%)]\tLoss: 0.379124\tLR: 0.00020765\n",
            "Train Epoch: 9 [98816/123872 (80%)]\tLoss: 0.361301\tLR: 0.00020755\n",
            "Train Epoch: 9 [99072/123872 (80%)]\tLoss: 0.384354\tLR: 0.00020744\n",
            "Train Epoch: 9 [99328/123872 (80%)]\tLoss: 0.394916\tLR: 0.00020734\n",
            "Train Epoch: 9 [99584/123872 (80%)]\tLoss: 0.376909\tLR: 0.00020723\n",
            "Train Epoch: 9 [99840/123872 (81%)]\tLoss: 0.359116\tLR: 0.00020713\n",
            "Train Epoch: 9 [99840/123872 (81%)]\tLoss: 0.359116\n",
            "Train Epoch: 9 [100096/123872 (81%)]\tLoss: 0.424932\tLR: 0.00020702\n",
            "Train Epoch: 9 [100352/123872 (81%)]\tLoss: 0.369907\tLR: 0.00020692\n",
            "Train Epoch: 9 [100608/123872 (81%)]\tLoss: 0.432651\tLR: 0.00020681\n",
            "Train Epoch: 9 [100864/123872 (81%)]\tLoss: 0.381856\tLR: 0.00020671\n",
            "Train Epoch: 9 [101120/123872 (82%)]\tLoss: 0.322664\tLR: 0.00020660\n",
            "Train Epoch: 9 [101376/123872 (82%)]\tLoss: 0.318121\tLR: 0.00020650\n",
            "Train Epoch: 9 [101632/123872 (82%)]\tLoss: 0.345508\tLR: 0.00020639\n",
            "Train Epoch: 9 [101888/123872 (82%)]\tLoss: 0.445094\tLR: 0.00020629\n",
            "Train Epoch: 9 [102144/123872 (82%)]\tLoss: 0.368309\tLR: 0.00020618\n",
            "Train Epoch: 9 [102400/123872 (83%)]\tLoss: 0.386075\tLR: 0.00020608\n",
            "Train Epoch: 9 [102400/123872 (83%)]\tLoss: 0.386075\n",
            "Train Epoch: 9 [102656/123872 (83%)]\tLoss: 0.351531\tLR: 0.00020597\n",
            "Train Epoch: 9 [102912/123872 (83%)]\tLoss: 0.360838\tLR: 0.00020587\n",
            "Train Epoch: 9 [103168/123872 (83%)]\tLoss: 0.385006\tLR: 0.00020576\n",
            "Train Epoch: 9 [103424/123872 (83%)]\tLoss: 0.376277\tLR: 0.00020566\n",
            "Train Epoch: 9 [103680/123872 (84%)]\tLoss: 0.376554\tLR: 0.00020555\n",
            "Train Epoch: 9 [103936/123872 (84%)]\tLoss: 0.348074\tLR: 0.00020545\n",
            "Train Epoch: 9 [104192/123872 (84%)]\tLoss: 0.341382\tLR: 0.00020535\n",
            "Train Epoch: 9 [104448/123872 (84%)]\tLoss: 0.414030\tLR: 0.00020524\n",
            "Train Epoch: 9 [104704/123872 (85%)]\tLoss: 0.362906\tLR: 0.00020514\n",
            "Train Epoch: 9 [104960/123872 (85%)]\tLoss: 0.302069\tLR: 0.00020503\n",
            "Train Epoch: 9 [104960/123872 (85%)]\tLoss: 0.302069\n",
            "Train Epoch: 9 [105216/123872 (85%)]\tLoss: 0.350064\tLR: 0.00020493\n",
            "Train Epoch: 9 [105472/123872 (85%)]\tLoss: 0.360274\tLR: 0.00020482\n",
            "Train Epoch: 9 [105728/123872 (85%)]\tLoss: 0.431223\tLR: 0.00020472\n",
            "Train Epoch: 9 [105984/123872 (86%)]\tLoss: 0.351360\tLR: 0.00020461\n",
            "Train Epoch: 9 [106240/123872 (86%)]\tLoss: 0.367498\tLR: 0.00020451\n",
            "Train Epoch: 9 [106496/123872 (86%)]\tLoss: 0.416535\tLR: 0.00020440\n",
            "Train Epoch: 9 [106752/123872 (86%)]\tLoss: 0.415740\tLR: 0.00020430\n",
            "Train Epoch: 9 [107008/123872 (86%)]\tLoss: 0.381114\tLR: 0.00020420\n",
            "Train Epoch: 9 [107264/123872 (87%)]\tLoss: 0.371625\tLR: 0.00020409\n",
            "Train Epoch: 9 [107520/123872 (87%)]\tLoss: 0.402006\tLR: 0.00020399\n",
            "Train Epoch: 9 [107520/123872 (87%)]\tLoss: 0.402006\n",
            "Train Epoch: 9 [107776/123872 (87%)]\tLoss: 0.401511\tLR: 0.00020388\n",
            "Train Epoch: 9 [108032/123872 (87%)]\tLoss: 0.363315\tLR: 0.00020378\n",
            "Train Epoch: 9 [108288/123872 (87%)]\tLoss: 0.376589\tLR: 0.00020367\n",
            "Train Epoch: 9 [108544/123872 (88%)]\tLoss: 0.341141\tLR: 0.00020357\n",
            "Train Epoch: 9 [108800/123872 (88%)]\tLoss: 0.456240\tLR: 0.00020347\n",
            "Train Epoch: 9 [109056/123872 (88%)]\tLoss: 0.328717\tLR: 0.00020336\n",
            "Train Epoch: 9 [109312/123872 (88%)]\tLoss: 0.387753\tLR: 0.00020326\n",
            "Train Epoch: 9 [109568/123872 (88%)]\tLoss: 0.371566\tLR: 0.00020315\n",
            "Train Epoch: 9 [109824/123872 (89%)]\tLoss: 0.362506\tLR: 0.00020305\n",
            "Train Epoch: 9 [110080/123872 (89%)]\tLoss: 0.397001\tLR: 0.00020294\n",
            "Train Epoch: 9 [110080/123872 (89%)]\tLoss: 0.397001\n",
            "Train Epoch: 9 [110336/123872 (89%)]\tLoss: 0.396811\tLR: 0.00020284\n",
            "Train Epoch: 9 [110592/123872 (89%)]\tLoss: 0.410597\tLR: 0.00020274\n",
            "Train Epoch: 9 [110848/123872 (89%)]\tLoss: 0.371338\tLR: 0.00020263\n",
            "Train Epoch: 9 [111104/123872 (90%)]\tLoss: 0.403171\tLR: 0.00020253\n",
            "Train Epoch: 9 [111360/123872 (90%)]\tLoss: 0.394598\tLR: 0.00020242\n",
            "Train Epoch: 9 [111616/123872 (90%)]\tLoss: 0.346632\tLR: 0.00020232\n",
            "Train Epoch: 9 [111872/123872 (90%)]\tLoss: 0.343472\tLR: 0.00020222\n",
            "Train Epoch: 9 [112128/123872 (90%)]\tLoss: 0.347912\tLR: 0.00020211\n",
            "Train Epoch: 9 [112384/123872 (91%)]\tLoss: 0.391326\tLR: 0.00020201\n",
            "Train Epoch: 9 [112640/123872 (91%)]\tLoss: 0.366768\tLR: 0.00020190\n",
            "Train Epoch: 9 [112640/123872 (91%)]\tLoss: 0.366768\n",
            "Train Epoch: 9 [112896/123872 (91%)]\tLoss: 0.342190\tLR: 0.00020180\n",
            "Train Epoch: 9 [113152/123872 (91%)]\tLoss: 0.323075\tLR: 0.00020170\n",
            "Train Epoch: 9 [113408/123872 (92%)]\tLoss: 0.359489\tLR: 0.00020159\n",
            "Train Epoch: 9 [113664/123872 (92%)]\tLoss: 0.341213\tLR: 0.00020149\n",
            "Train Epoch: 9 [113920/123872 (92%)]\tLoss: 0.393270\tLR: 0.00020138\n",
            "Train Epoch: 9 [114176/123872 (92%)]\tLoss: 0.352954\tLR: 0.00020128\n",
            "Train Epoch: 9 [114432/123872 (92%)]\tLoss: 0.356449\tLR: 0.00020118\n",
            "Train Epoch: 9 [114688/123872 (93%)]\tLoss: 0.354691\tLR: 0.00020107\n",
            "Train Epoch: 9 [114944/123872 (93%)]\tLoss: 0.397609\tLR: 0.00020097\n",
            "Train Epoch: 9 [115200/123872 (93%)]\tLoss: 0.360671\tLR: 0.00020087\n",
            "Train Epoch: 9 [115200/123872 (93%)]\tLoss: 0.360671\n",
            "Train Epoch: 9 [115456/123872 (93%)]\tLoss: 0.295768\tLR: 0.00020076\n",
            "Train Epoch: 9 [115712/123872 (93%)]\tLoss: 0.402054\tLR: 0.00020066\n",
            "Train Epoch: 9 [115968/123872 (94%)]\tLoss: 0.345395\tLR: 0.00020056\n",
            "Train Epoch: 9 [116224/123872 (94%)]\tLoss: 0.370893\tLR: 0.00020045\n",
            "Train Epoch: 9 [116480/123872 (94%)]\tLoss: 0.357550\tLR: 0.00020035\n",
            "Train Epoch: 9 [116736/123872 (94%)]\tLoss: 0.360161\tLR: 0.00020025\n",
            "Train Epoch: 9 [116992/123872 (94%)]\tLoss: 0.366993\tLR: 0.00020014\n",
            "Train Epoch: 9 [117248/123872 (95%)]\tLoss: 0.366484\tLR: 0.00020004\n",
            "Train Epoch: 9 [117504/123872 (95%)]\tLoss: 0.323339\tLR: 0.00019993\n",
            "Train Epoch: 9 [117760/123872 (95%)]\tLoss: 0.418310\tLR: 0.00019983\n",
            "Train Epoch: 9 [117760/123872 (95%)]\tLoss: 0.418310\n",
            "Train Epoch: 9 [118016/123872 (95%)]\tLoss: 0.369286\tLR: 0.00019973\n",
            "Train Epoch: 9 [118272/123872 (95%)]\tLoss: 0.404477\tLR: 0.00019962\n",
            "Train Epoch: 9 [118528/123872 (96%)]\tLoss: 0.334783\tLR: 0.00019952\n",
            "Train Epoch: 9 [118784/123872 (96%)]\tLoss: 0.380303\tLR: 0.00019942\n",
            "Train Epoch: 9 [119040/123872 (96%)]\tLoss: 0.345707\tLR: 0.00019931\n",
            "Train Epoch: 9 [119296/123872 (96%)]\tLoss: 0.339301\tLR: 0.00019921\n",
            "Train Epoch: 9 [119552/123872 (96%)]\tLoss: 0.337801\tLR: 0.00019911\n",
            "Train Epoch: 9 [119808/123872 (97%)]\tLoss: 0.325321\tLR: 0.00019901\n",
            "Train Epoch: 9 [120064/123872 (97%)]\tLoss: 0.375312\tLR: 0.00019890\n",
            "Train Epoch: 9 [120320/123872 (97%)]\tLoss: 0.375780\tLR: 0.00019880\n",
            "Train Epoch: 9 [120320/123872 (97%)]\tLoss: 0.375780\n",
            "Train Epoch: 9 [120576/123872 (97%)]\tLoss: 0.393788\tLR: 0.00019870\n",
            "Train Epoch: 9 [120832/123872 (98%)]\tLoss: 0.359548\tLR: 0.00019859\n",
            "Train Epoch: 9 [121088/123872 (98%)]\tLoss: 0.360655\tLR: 0.00019849\n",
            "Train Epoch: 9 [121344/123872 (98%)]\tLoss: 0.299785\tLR: 0.00019839\n",
            "Train Epoch: 9 [121600/123872 (98%)]\tLoss: 0.320699\tLR: 0.00019828\n",
            "Train Epoch: 9 [121856/123872 (98%)]\tLoss: 0.349227\tLR: 0.00019818\n",
            "Train Epoch: 9 [122112/123872 (99%)]\tLoss: 0.395002\tLR: 0.00019808\n",
            "Train Epoch: 9 [122368/123872 (99%)]\tLoss: 0.322226\tLR: 0.00019797\n",
            "Train Epoch: 9 [122624/123872 (99%)]\tLoss: 0.393399\tLR: 0.00019787\n",
            "Train Epoch: 9 [122880/123872 (99%)]\tLoss: 0.287904\tLR: 0.00019777\n",
            "Train Epoch: 9 [122880/123872 (99%)]\tLoss: 0.287904\n",
            "Train Epoch: 9 [123136/123872 (99%)]\tLoss: 0.363480\tLR: 0.00019767\n",
            "Train Epoch: 9 [123392/123872 (100%)]\tLoss: 0.366618\tLR: 0.00019756\n",
            "Train Epoch: 9 [108192/123872 (100%)]\tLoss: 0.361056\tLR: 0.00019746\n",
            "\n",
            "Test set: Average loss: 0.0015, Accuracy: 25764/30970 (83.19%)\n",
            "\n",
            "Train Epoch: 10 [0/123872 (0%)]\tLoss: 0.389030\tLR: 0.00019736\n",
            "Train Epoch: 10 [0/123872 (0%)]\tLoss: 0.389030\n",
            "Train Epoch: 10 [256/123872 (0%)]\tLoss: 0.399531\tLR: 0.00019725\n",
            "Train Epoch: 10 [512/123872 (0%)]\tLoss: 0.305920\tLR: 0.00019715\n",
            "Train Epoch: 10 [768/123872 (1%)]\tLoss: 0.319478\tLR: 0.00019705\n",
            "Train Epoch: 10 [1024/123872 (1%)]\tLoss: 0.401130\tLR: 0.00019695\n",
            "Train Epoch: 10 [1280/123872 (1%)]\tLoss: 0.372813\tLR: 0.00019684\n",
            "Train Epoch: 10 [1536/123872 (1%)]\tLoss: 0.317784\tLR: 0.00019674\n",
            "Train Epoch: 10 [1792/123872 (1%)]\tLoss: 0.342362\tLR: 0.00019664\n",
            "Train Epoch: 10 [2048/123872 (2%)]\tLoss: 0.425920\tLR: 0.00019654\n",
            "Train Epoch: 10 [2304/123872 (2%)]\tLoss: 0.316853\tLR: 0.00019643\n",
            "Train Epoch: 10 [2560/123872 (2%)]\tLoss: 0.343971\tLR: 0.00019633\n",
            "Train Epoch: 10 [2560/123872 (2%)]\tLoss: 0.343971\n",
            "Train Epoch: 10 [2816/123872 (2%)]\tLoss: 0.370258\tLR: 0.00019623\n",
            "Train Epoch: 10 [3072/123872 (2%)]\tLoss: 0.403652\tLR: 0.00019613\n",
            "Train Epoch: 10 [3328/123872 (3%)]\tLoss: 0.355080\tLR: 0.00019602\n",
            "Train Epoch: 10 [3584/123872 (3%)]\tLoss: 0.339883\tLR: 0.00019592\n",
            "Train Epoch: 10 [3840/123872 (3%)]\tLoss: 0.333818\tLR: 0.00019582\n",
            "Train Epoch: 10 [4096/123872 (3%)]\tLoss: 0.405510\tLR: 0.00019572\n",
            "Train Epoch: 10 [4352/123872 (4%)]\tLoss: 0.450773\tLR: 0.00019561\n",
            "Train Epoch: 10 [4608/123872 (4%)]\tLoss: 0.392270\tLR: 0.00019551\n",
            "Train Epoch: 10 [4864/123872 (4%)]\tLoss: 0.300856\tLR: 0.00019541\n",
            "Train Epoch: 10 [5120/123872 (4%)]\tLoss: 0.396780\tLR: 0.00019531\n",
            "Train Epoch: 10 [5120/123872 (4%)]\tLoss: 0.396780\n",
            "Train Epoch: 10 [5376/123872 (4%)]\tLoss: 0.427558\tLR: 0.00019520\n",
            "Train Epoch: 10 [5632/123872 (5%)]\tLoss: 0.402900\tLR: 0.00019510\n",
            "Train Epoch: 10 [5888/123872 (5%)]\tLoss: 0.399091\tLR: 0.00019500\n",
            "Train Epoch: 10 [6144/123872 (5%)]\tLoss: 0.362685\tLR: 0.00019490\n",
            "Train Epoch: 10 [6400/123872 (5%)]\tLoss: 0.417020\tLR: 0.00019480\n",
            "Train Epoch: 10 [6656/123872 (5%)]\tLoss: 0.363216\tLR: 0.00019469\n",
            "Train Epoch: 10 [6912/123872 (6%)]\tLoss: 0.397687\tLR: 0.00019459\n",
            "Train Epoch: 10 [7168/123872 (6%)]\tLoss: 0.410809\tLR: 0.00019449\n",
            "Train Epoch: 10 [7424/123872 (6%)]\tLoss: 0.360923\tLR: 0.00019439\n",
            "Train Epoch: 10 [7680/123872 (6%)]\tLoss: 0.335177\tLR: 0.00019429\n",
            "Train Epoch: 10 [7680/123872 (6%)]\tLoss: 0.335177\n",
            "Train Epoch: 10 [7936/123872 (6%)]\tLoss: 0.385764\tLR: 0.00019418\n",
            "Train Epoch: 10 [8192/123872 (7%)]\tLoss: 0.310228\tLR: 0.00019408\n",
            "Train Epoch: 10 [8448/123872 (7%)]\tLoss: 0.314114\tLR: 0.00019398\n",
            "Train Epoch: 10 [8704/123872 (7%)]\tLoss: 0.403055\tLR: 0.00019388\n",
            "Train Epoch: 10 [8960/123872 (7%)]\tLoss: 0.339383\tLR: 0.00019378\n",
            "Train Epoch: 10 [9216/123872 (7%)]\tLoss: 0.325841\tLR: 0.00019367\n",
            "Train Epoch: 10 [9472/123872 (8%)]\tLoss: 0.399362\tLR: 0.00019357\n",
            "Train Epoch: 10 [9728/123872 (8%)]\tLoss: 0.326108\tLR: 0.00019347\n",
            "Train Epoch: 10 [9984/123872 (8%)]\tLoss: 0.456095\tLR: 0.00019337\n",
            "Train Epoch: 10 [10240/123872 (8%)]\tLoss: 0.343914\tLR: 0.00019327\n",
            "Train Epoch: 10 [10240/123872 (8%)]\tLoss: 0.343914\n",
            "Train Epoch: 10 [10496/123872 (8%)]\tLoss: 0.344488\tLR: 0.00019316\n",
            "Train Epoch: 10 [10752/123872 (9%)]\tLoss: 0.411401\tLR: 0.00019306\n",
            "Train Epoch: 10 [11008/123872 (9%)]\tLoss: 0.399619\tLR: 0.00019296\n",
            "Train Epoch: 10 [11264/123872 (9%)]\tLoss: 0.323535\tLR: 0.00019286\n",
            "Train Epoch: 10 [11520/123872 (9%)]\tLoss: 0.330362\tLR: 0.00019276\n",
            "Train Epoch: 10 [11776/123872 (10%)]\tLoss: 0.407376\tLR: 0.00019266\n",
            "Train Epoch: 10 [12032/123872 (10%)]\tLoss: 0.381945\tLR: 0.00019255\n",
            "Train Epoch: 10 [12288/123872 (10%)]\tLoss: 0.345196\tLR: 0.00019245\n",
            "Train Epoch: 10 [12544/123872 (10%)]\tLoss: 0.419727\tLR: 0.00019235\n",
            "Train Epoch: 10 [12800/123872 (10%)]\tLoss: 0.354553\tLR: 0.00019225\n",
            "Train Epoch: 10 [12800/123872 (10%)]\tLoss: 0.354553\n",
            "Train Epoch: 10 [13056/123872 (11%)]\tLoss: 0.456305\tLR: 0.00019215\n",
            "Train Epoch: 10 [13312/123872 (11%)]\tLoss: 0.360674\tLR: 0.00019205\n",
            "Train Epoch: 10 [13568/123872 (11%)]\tLoss: 0.345925\tLR: 0.00019195\n",
            "Train Epoch: 10 [13824/123872 (11%)]\tLoss: 0.334821\tLR: 0.00019184\n",
            "Train Epoch: 10 [14080/123872 (11%)]\tLoss: 0.342500\tLR: 0.00019174\n",
            "Train Epoch: 10 [14336/123872 (12%)]\tLoss: 0.327385\tLR: 0.00019164\n",
            "Train Epoch: 10 [14592/123872 (12%)]\tLoss: 0.361062\tLR: 0.00019154\n",
            "Train Epoch: 10 [14848/123872 (12%)]\tLoss: 0.377625\tLR: 0.00019144\n",
            "Train Epoch: 10 [15104/123872 (12%)]\tLoss: 0.427088\tLR: 0.00019134\n",
            "Train Epoch: 10 [15360/123872 (12%)]\tLoss: 0.445246\tLR: 0.00019124\n",
            "Train Epoch: 10 [15360/123872 (12%)]\tLoss: 0.445246\n",
            "Train Epoch: 10 [15616/123872 (13%)]\tLoss: 0.363485\tLR: 0.00019114\n",
            "Train Epoch: 10 [15872/123872 (13%)]\tLoss: 0.350815\tLR: 0.00019103\n",
            "Train Epoch: 10 [16128/123872 (13%)]\tLoss: 0.364989\tLR: 0.00019093\n",
            "Train Epoch: 10 [16384/123872 (13%)]\tLoss: 0.383958\tLR: 0.00019083\n",
            "Train Epoch: 10 [16640/123872 (13%)]\tLoss: 0.342460\tLR: 0.00019073\n",
            "Train Epoch: 10 [16896/123872 (14%)]\tLoss: 0.388360\tLR: 0.00019063\n",
            "Train Epoch: 10 [17152/123872 (14%)]\tLoss: 0.359868\tLR: 0.00019053\n",
            "Train Epoch: 10 [17408/123872 (14%)]\tLoss: 0.344897\tLR: 0.00019043\n",
            "Train Epoch: 10 [17664/123872 (14%)]\tLoss: 0.316741\tLR: 0.00019033\n",
            "Train Epoch: 10 [17920/123872 (14%)]\tLoss: 0.327234\tLR: 0.00019023\n",
            "Train Epoch: 10 [17920/123872 (14%)]\tLoss: 0.327234\n",
            "Train Epoch: 10 [18176/123872 (15%)]\tLoss: 0.312098\tLR: 0.00019012\n",
            "Train Epoch: 10 [18432/123872 (15%)]\tLoss: 0.302404\tLR: 0.00019002\n",
            "Train Epoch: 10 [18688/123872 (15%)]\tLoss: 0.385260\tLR: 0.00018992\n",
            "Train Epoch: 10 [18944/123872 (15%)]\tLoss: 0.473565\tLR: 0.00018982\n",
            "Train Epoch: 10 [19200/123872 (15%)]\tLoss: 0.300834\tLR: 0.00018972\n",
            "Train Epoch: 10 [19456/123872 (16%)]\tLoss: 0.357671\tLR: 0.00018962\n",
            "Train Epoch: 10 [19712/123872 (16%)]\tLoss: 0.383035\tLR: 0.00018952\n",
            "Train Epoch: 10 [19968/123872 (16%)]\tLoss: 0.370611\tLR: 0.00018942\n",
            "Train Epoch: 10 [20224/123872 (16%)]\tLoss: 0.372620\tLR: 0.00018932\n",
            "Train Epoch: 10 [20480/123872 (17%)]\tLoss: 0.349681\tLR: 0.00018922\n",
            "Train Epoch: 10 [20480/123872 (17%)]\tLoss: 0.349681\n",
            "Train Epoch: 10 [20736/123872 (17%)]\tLoss: 0.333850\tLR: 0.00018912\n",
            "Train Epoch: 10 [20992/123872 (17%)]\tLoss: 0.432095\tLR: 0.00018902\n",
            "Train Epoch: 10 [21248/123872 (17%)]\tLoss: 0.362916\tLR: 0.00018892\n",
            "Train Epoch: 10 [21504/123872 (17%)]\tLoss: 0.383721\tLR: 0.00018881\n",
            "Train Epoch: 10 [21760/123872 (18%)]\tLoss: 0.361608\tLR: 0.00018871\n",
            "Train Epoch: 10 [22016/123872 (18%)]\tLoss: 0.372584\tLR: 0.00018861\n",
            "Train Epoch: 10 [22272/123872 (18%)]\tLoss: 0.355103\tLR: 0.00018851\n",
            "Train Epoch: 10 [22528/123872 (18%)]\tLoss: 0.411146\tLR: 0.00018841\n",
            "Train Epoch: 10 [22784/123872 (18%)]\tLoss: 0.374866\tLR: 0.00018831\n",
            "Train Epoch: 10 [23040/123872 (19%)]\tLoss: 0.388938\tLR: 0.00018821\n",
            "Train Epoch: 10 [23040/123872 (19%)]\tLoss: 0.388938\n",
            "Train Epoch: 10 [23296/123872 (19%)]\tLoss: 0.416845\tLR: 0.00018811\n",
            "Train Epoch: 10 [23552/123872 (19%)]\tLoss: 0.370319\tLR: 0.00018801\n",
            "Train Epoch: 10 [23808/123872 (19%)]\tLoss: 0.391966\tLR: 0.00018791\n",
            "Train Epoch: 10 [24064/123872 (19%)]\tLoss: 0.392041\tLR: 0.00018781\n",
            "Train Epoch: 10 [24320/123872 (20%)]\tLoss: 0.358069\tLR: 0.00018771\n",
            "Train Epoch: 10 [24576/123872 (20%)]\tLoss: 0.391967\tLR: 0.00018761\n",
            "Train Epoch: 10 [24832/123872 (20%)]\tLoss: 0.338266\tLR: 0.00018751\n",
            "Train Epoch: 10 [25088/123872 (20%)]\tLoss: 0.355778\tLR: 0.00018741\n",
            "Train Epoch: 10 [25344/123872 (20%)]\tLoss: 0.350674\tLR: 0.00018731\n",
            "Train Epoch: 10 [25600/123872 (21%)]\tLoss: 0.315409\tLR: 0.00018721\n",
            "Train Epoch: 10 [25600/123872 (21%)]\tLoss: 0.315409\n",
            "Train Epoch: 10 [25856/123872 (21%)]\tLoss: 0.343175\tLR: 0.00018711\n",
            "Train Epoch: 10 [26112/123872 (21%)]\tLoss: 0.403830\tLR: 0.00018701\n",
            "Train Epoch: 10 [26368/123872 (21%)]\tLoss: 0.350018\tLR: 0.00018691\n",
            "Train Epoch: 10 [26624/123872 (21%)]\tLoss: 0.275907\tLR: 0.00018681\n",
            "Train Epoch: 10 [26880/123872 (22%)]\tLoss: 0.432539\tLR: 0.00018671\n",
            "Train Epoch: 10 [27136/123872 (22%)]\tLoss: 0.403514\tLR: 0.00018661\n",
            "Train Epoch: 10 [27392/123872 (22%)]\tLoss: 0.350259\tLR: 0.00018651\n",
            "Train Epoch: 10 [27648/123872 (22%)]\tLoss: 0.319584\tLR: 0.00018641\n",
            "Train Epoch: 10 [27904/123872 (23%)]\tLoss: 0.369878\tLR: 0.00018631\n",
            "Train Epoch: 10 [28160/123872 (23%)]\tLoss: 0.377859\tLR: 0.00018621\n",
            "Train Epoch: 10 [28160/123872 (23%)]\tLoss: 0.377859\n",
            "Train Epoch: 10 [28416/123872 (23%)]\tLoss: 0.385464\tLR: 0.00018611\n",
            "Train Epoch: 10 [28672/123872 (23%)]\tLoss: 0.360265\tLR: 0.00018601\n",
            "Train Epoch: 10 [28928/123872 (23%)]\tLoss: 0.388246\tLR: 0.00018591\n",
            "Train Epoch: 10 [29184/123872 (24%)]\tLoss: 0.336689\tLR: 0.00018581\n",
            "Train Epoch: 10 [29440/123872 (24%)]\tLoss: 0.379475\tLR: 0.00018571\n",
            "Train Epoch: 10 [29696/123872 (24%)]\tLoss: 0.358864\tLR: 0.00018561\n",
            "Train Epoch: 10 [29952/123872 (24%)]\tLoss: 0.377541\tLR: 0.00018551\n",
            "Train Epoch: 10 [30208/123872 (24%)]\tLoss: 0.351836\tLR: 0.00018541\n",
            "Train Epoch: 10 [30464/123872 (25%)]\tLoss: 0.354871\tLR: 0.00018531\n",
            "Train Epoch: 10 [30720/123872 (25%)]\tLoss: 0.398373\tLR: 0.00018521\n",
            "Train Epoch: 10 [30720/123872 (25%)]\tLoss: 0.398373\n",
            "Train Epoch: 10 [30976/123872 (25%)]\tLoss: 0.376110\tLR: 0.00018511\n",
            "Train Epoch: 10 [31232/123872 (25%)]\tLoss: 0.398694\tLR: 0.00018501\n",
            "Train Epoch: 10 [31488/123872 (25%)]\tLoss: 0.320567\tLR: 0.00018491\n",
            "Train Epoch: 10 [31744/123872 (26%)]\tLoss: 0.347993\tLR: 0.00018481\n",
            "Train Epoch: 10 [32000/123872 (26%)]\tLoss: 0.387218\tLR: 0.00018471\n",
            "Train Epoch: 10 [32256/123872 (26%)]\tLoss: 0.328476\tLR: 0.00018461\n",
            "Train Epoch: 10 [32512/123872 (26%)]\tLoss: 0.341811\tLR: 0.00018451\n",
            "Train Epoch: 10 [32768/123872 (26%)]\tLoss: 0.316930\tLR: 0.00018442\n",
            "Train Epoch: 10 [33024/123872 (27%)]\tLoss: 0.396305\tLR: 0.00018432\n",
            "Train Epoch: 10 [33280/123872 (27%)]\tLoss: 0.368980\tLR: 0.00018422\n",
            "Train Epoch: 10 [33280/123872 (27%)]\tLoss: 0.368980\n",
            "Train Epoch: 10 [33536/123872 (27%)]\tLoss: 0.377406\tLR: 0.00018412\n",
            "Train Epoch: 10 [33792/123872 (27%)]\tLoss: 0.388706\tLR: 0.00018402\n",
            "Train Epoch: 10 [34048/123872 (27%)]\tLoss: 0.284011\tLR: 0.00018392\n",
            "Train Epoch: 10 [34304/123872 (28%)]\tLoss: 0.326404\tLR: 0.00018382\n",
            "Train Epoch: 10 [34560/123872 (28%)]\tLoss: 0.351923\tLR: 0.00018372\n",
            "Train Epoch: 10 [34816/123872 (28%)]\tLoss: 0.313715\tLR: 0.00018362\n",
            "Train Epoch: 10 [35072/123872 (28%)]\tLoss: 0.331085\tLR: 0.00018352\n",
            "Train Epoch: 10 [35328/123872 (29%)]\tLoss: 0.351488\tLR: 0.00018342\n",
            "Train Epoch: 10 [35584/123872 (29%)]\tLoss: 0.369745\tLR: 0.00018332\n",
            "Train Epoch: 10 [35840/123872 (29%)]\tLoss: 0.348417\tLR: 0.00018322\n",
            "Train Epoch: 10 [35840/123872 (29%)]\tLoss: 0.348417\n",
            "Train Epoch: 10 [36096/123872 (29%)]\tLoss: 0.341315\tLR: 0.00018313\n",
            "Train Epoch: 10 [36352/123872 (29%)]\tLoss: 0.402214\tLR: 0.00018303\n",
            "Train Epoch: 10 [36608/123872 (30%)]\tLoss: 0.336574\tLR: 0.00018293\n",
            "Train Epoch: 10 [36864/123872 (30%)]\tLoss: 0.426912\tLR: 0.00018283\n",
            "Train Epoch: 10 [37120/123872 (30%)]\tLoss: 0.389660\tLR: 0.00018273\n",
            "Train Epoch: 10 [37376/123872 (30%)]\tLoss: 0.324856\tLR: 0.00018263\n",
            "Train Epoch: 10 [37632/123872 (30%)]\tLoss: 0.360811\tLR: 0.00018253\n",
            "Train Epoch: 10 [37888/123872 (31%)]\tLoss: 0.305965\tLR: 0.00018243\n",
            "Train Epoch: 10 [38144/123872 (31%)]\tLoss: 0.413241\tLR: 0.00018233\n",
            "Train Epoch: 10 [38400/123872 (31%)]\tLoss: 0.338562\tLR: 0.00018224\n",
            "Train Epoch: 10 [38400/123872 (31%)]\tLoss: 0.338562\n",
            "Train Epoch: 10 [38656/123872 (31%)]\tLoss: 0.322594\tLR: 0.00018214\n",
            "Train Epoch: 10 [38912/123872 (31%)]\tLoss: 0.335367\tLR: 0.00018204\n",
            "Train Epoch: 10 [39168/123872 (32%)]\tLoss: 0.314323\tLR: 0.00018194\n",
            "Train Epoch: 10 [39424/123872 (32%)]\tLoss: 0.366236\tLR: 0.00018184\n",
            "Train Epoch: 10 [39680/123872 (32%)]\tLoss: 0.367849\tLR: 0.00018174\n",
            "Train Epoch: 10 [39936/123872 (32%)]\tLoss: 0.404515\tLR: 0.00018164\n",
            "Train Epoch: 10 [40192/123872 (32%)]\tLoss: 0.329542\tLR: 0.00018154\n",
            "Train Epoch: 10 [40448/123872 (33%)]\tLoss: 0.402128\tLR: 0.00018145\n",
            "Train Epoch: 10 [40704/123872 (33%)]\tLoss: 0.368388\tLR: 0.00018135\n",
            "Train Epoch: 10 [40960/123872 (33%)]\tLoss: 0.367437\tLR: 0.00018125\n",
            "Train Epoch: 10 [40960/123872 (33%)]\tLoss: 0.367437\n",
            "Train Epoch: 10 [41216/123872 (33%)]\tLoss: 0.395046\tLR: 0.00018115\n",
            "Train Epoch: 10 [41472/123872 (33%)]\tLoss: 0.373972\tLR: 0.00018105\n",
            "Train Epoch: 10 [41728/123872 (34%)]\tLoss: 0.295734\tLR: 0.00018095\n",
            "Train Epoch: 10 [41984/123872 (34%)]\tLoss: 0.314492\tLR: 0.00018086\n",
            "Train Epoch: 10 [42240/123872 (34%)]\tLoss: 0.335257\tLR: 0.00018076\n",
            "Train Epoch: 10 [42496/123872 (34%)]\tLoss: 0.304084\tLR: 0.00018066\n",
            "Train Epoch: 10 [42752/123872 (35%)]\tLoss: 0.339321\tLR: 0.00018056\n",
            "Train Epoch: 10 [43008/123872 (35%)]\tLoss: 0.372525\tLR: 0.00018046\n",
            "Train Epoch: 10 [43264/123872 (35%)]\tLoss: 0.355584\tLR: 0.00018036\n",
            "Train Epoch: 10 [43520/123872 (35%)]\tLoss: 0.372899\tLR: 0.00018027\n",
            "Train Epoch: 10 [43520/123872 (35%)]\tLoss: 0.372899\n",
            "Train Epoch: 10 [43776/123872 (35%)]\tLoss: 0.377493\tLR: 0.00018017\n",
            "Train Epoch: 10 [44032/123872 (36%)]\tLoss: 0.330894\tLR: 0.00018007\n",
            "Train Epoch: 10 [44288/123872 (36%)]\tLoss: 0.382374\tLR: 0.00017997\n",
            "Train Epoch: 10 [44544/123872 (36%)]\tLoss: 0.400138\tLR: 0.00017987\n",
            "Train Epoch: 10 [44800/123872 (36%)]\tLoss: 0.426710\tLR: 0.00017978\n",
            "Train Epoch: 10 [45056/123872 (36%)]\tLoss: 0.390932\tLR: 0.00017968\n",
            "Train Epoch: 10 [45312/123872 (37%)]\tLoss: 0.417865\tLR: 0.00017958\n",
            "Train Epoch: 10 [45568/123872 (37%)]\tLoss: 0.366401\tLR: 0.00017948\n",
            "Train Epoch: 10 [45824/123872 (37%)]\tLoss: 0.344258\tLR: 0.00017938\n",
            "Train Epoch: 10 [46080/123872 (37%)]\tLoss: 0.274975\tLR: 0.00017929\n",
            "Train Epoch: 10 [46080/123872 (37%)]\tLoss: 0.274975\n",
            "Train Epoch: 10 [46336/123872 (37%)]\tLoss: 0.390081\tLR: 0.00017919\n",
            "Train Epoch: 10 [46592/123872 (38%)]\tLoss: 0.339736\tLR: 0.00017909\n",
            "Train Epoch: 10 [46848/123872 (38%)]\tLoss: 0.394095\tLR: 0.00017899\n",
            "Train Epoch: 10 [47104/123872 (38%)]\tLoss: 0.357773\tLR: 0.00017889\n",
            "Train Epoch: 10 [47360/123872 (38%)]\tLoss: 0.334811\tLR: 0.00017880\n",
            "Train Epoch: 10 [47616/123872 (38%)]\tLoss: 0.368721\tLR: 0.00017870\n",
            "Train Epoch: 10 [47872/123872 (39%)]\tLoss: 0.353155\tLR: 0.00017860\n",
            "Train Epoch: 10 [48128/123872 (39%)]\tLoss: 0.354625\tLR: 0.00017850\n",
            "Train Epoch: 10 [48384/123872 (39%)]\tLoss: 0.381628\tLR: 0.00017841\n",
            "Train Epoch: 10 [48640/123872 (39%)]\tLoss: 0.353304\tLR: 0.00017831\n",
            "Train Epoch: 10 [48640/123872 (39%)]\tLoss: 0.353304\n",
            "Train Epoch: 10 [48896/123872 (39%)]\tLoss: 0.360921\tLR: 0.00017821\n",
            "Train Epoch: 10 [49152/123872 (40%)]\tLoss: 0.376677\tLR: 0.00017811\n",
            "Train Epoch: 10 [49408/123872 (40%)]\tLoss: 0.345072\tLR: 0.00017802\n",
            "Train Epoch: 10 [49664/123872 (40%)]\tLoss: 0.359812\tLR: 0.00017792\n",
            "Train Epoch: 10 [49920/123872 (40%)]\tLoss: 0.417181\tLR: 0.00017782\n",
            "Train Epoch: 10 [50176/123872 (40%)]\tLoss: 0.381615\tLR: 0.00017772\n",
            "Train Epoch: 10 [50432/123872 (41%)]\tLoss: 0.326735\tLR: 0.00017763\n",
            "Train Epoch: 10 [50688/123872 (41%)]\tLoss: 0.331816\tLR: 0.00017753\n",
            "Train Epoch: 10 [50944/123872 (41%)]\tLoss: 0.358146\tLR: 0.00017743\n",
            "Train Epoch: 10 [51200/123872 (41%)]\tLoss: 0.360151\tLR: 0.00017733\n",
            "Train Epoch: 10 [51200/123872 (41%)]\tLoss: 0.360151\n",
            "Train Epoch: 10 [51456/123872 (42%)]\tLoss: 0.411548\tLR: 0.00017724\n",
            "Train Epoch: 10 [51712/123872 (42%)]\tLoss: 0.421221\tLR: 0.00017714\n",
            "Train Epoch: 10 [51968/123872 (42%)]\tLoss: 0.375860\tLR: 0.00017704\n",
            "Train Epoch: 10 [52224/123872 (42%)]\tLoss: 0.370701\tLR: 0.00017694\n",
            "Train Epoch: 10 [52480/123872 (42%)]\tLoss: 0.416654\tLR: 0.00017685\n",
            "Train Epoch: 10 [52736/123872 (43%)]\tLoss: 0.407159\tLR: 0.00017675\n",
            "Train Epoch: 10 [52992/123872 (43%)]\tLoss: 0.332572\tLR: 0.00017665\n",
            "Train Epoch: 10 [53248/123872 (43%)]\tLoss: 0.320522\tLR: 0.00017656\n",
            "Train Epoch: 10 [53504/123872 (43%)]\tLoss: 0.294696\tLR: 0.00017646\n",
            "Train Epoch: 10 [53760/123872 (43%)]\tLoss: 0.363908\tLR: 0.00017636\n",
            "Train Epoch: 10 [53760/123872 (43%)]\tLoss: 0.363908\n",
            "Train Epoch: 10 [54016/123872 (44%)]\tLoss: 0.421969\tLR: 0.00017626\n",
            "Train Epoch: 10 [54272/123872 (44%)]\tLoss: 0.402639\tLR: 0.00017617\n",
            "Train Epoch: 10 [54528/123872 (44%)]\tLoss: 0.384283\tLR: 0.00017607\n",
            "Train Epoch: 10 [54784/123872 (44%)]\tLoss: 0.316518\tLR: 0.00017597\n",
            "Train Epoch: 10 [55040/123872 (44%)]\tLoss: 0.332293\tLR: 0.00017588\n",
            "Train Epoch: 10 [55296/123872 (45%)]\tLoss: 0.327114\tLR: 0.00017578\n",
            "Train Epoch: 10 [55552/123872 (45%)]\tLoss: 0.374793\tLR: 0.00017568\n",
            "Train Epoch: 10 [55808/123872 (45%)]\tLoss: 0.389293\tLR: 0.00017559\n",
            "Train Epoch: 10 [56064/123872 (45%)]\tLoss: 0.364980\tLR: 0.00017549\n",
            "Train Epoch: 10 [56320/123872 (45%)]\tLoss: 0.346516\tLR: 0.00017539\n",
            "Train Epoch: 10 [56320/123872 (45%)]\tLoss: 0.346516\n",
            "Train Epoch: 10 [56576/123872 (46%)]\tLoss: 0.360646\tLR: 0.00017530\n",
            "Train Epoch: 10 [56832/123872 (46%)]\tLoss: 0.360266\tLR: 0.00017520\n",
            "Train Epoch: 10 [57088/123872 (46%)]\tLoss: 0.389380\tLR: 0.00017510\n",
            "Train Epoch: 10 [57344/123872 (46%)]\tLoss: 0.283829\tLR: 0.00017501\n",
            "Train Epoch: 10 [57600/123872 (46%)]\tLoss: 0.320899\tLR: 0.00017491\n",
            "Train Epoch: 10 [57856/123872 (47%)]\tLoss: 0.396109\tLR: 0.00017481\n",
            "Train Epoch: 10 [58112/123872 (47%)]\tLoss: 0.317861\tLR: 0.00017472\n",
            "Train Epoch: 10 [58368/123872 (47%)]\tLoss: 0.379096\tLR: 0.00017462\n",
            "Train Epoch: 10 [58624/123872 (47%)]\tLoss: 0.340378\tLR: 0.00017452\n",
            "Train Epoch: 10 [58880/123872 (48%)]\tLoss: 0.348931\tLR: 0.00017443\n",
            "Train Epoch: 10 [58880/123872 (48%)]\tLoss: 0.348931\n",
            "Train Epoch: 10 [59136/123872 (48%)]\tLoss: 0.334322\tLR: 0.00017433\n",
            "Train Epoch: 10 [59392/123872 (48%)]\tLoss: 0.343473\tLR: 0.00017423\n",
            "Train Epoch: 10 [59648/123872 (48%)]\tLoss: 0.342782\tLR: 0.00017414\n",
            "Train Epoch: 10 [59904/123872 (48%)]\tLoss: 0.364417\tLR: 0.00017404\n",
            "Train Epoch: 10 [60160/123872 (49%)]\tLoss: 0.399092\tLR: 0.00017394\n",
            "Train Epoch: 10 [60416/123872 (49%)]\tLoss: 0.343756\tLR: 0.00017385\n",
            "Train Epoch: 10 [60672/123872 (49%)]\tLoss: 0.312537\tLR: 0.00017375\n",
            "Train Epoch: 10 [60928/123872 (49%)]\tLoss: 0.302732\tLR: 0.00017366\n",
            "Train Epoch: 10 [61184/123872 (49%)]\tLoss: 0.367420\tLR: 0.00017356\n",
            "Train Epoch: 10 [61440/123872 (50%)]\tLoss: 0.390925\tLR: 0.00017346\n",
            "Train Epoch: 10 [61440/123872 (50%)]\tLoss: 0.390925\n",
            "Train Epoch: 10 [61696/123872 (50%)]\tLoss: 0.349174\tLR: 0.00017337\n",
            "Train Epoch: 10 [61952/123872 (50%)]\tLoss: 0.397601\tLR: 0.00017327\n",
            "Train Epoch: 10 [62208/123872 (50%)]\tLoss: 0.362551\tLR: 0.00017317\n",
            "Train Epoch: 10 [62464/123872 (50%)]\tLoss: 0.358239\tLR: 0.00017308\n",
            "Train Epoch: 10 [62720/123872 (51%)]\tLoss: 0.361745\tLR: 0.00017298\n",
            "Train Epoch: 10 [62976/123872 (51%)]\tLoss: 0.406272\tLR: 0.00017289\n",
            "Train Epoch: 10 [63232/123872 (51%)]\tLoss: 0.326689\tLR: 0.00017279\n",
            "Train Epoch: 10 [63488/123872 (51%)]\tLoss: 0.338246\tLR: 0.00017270\n",
            "Train Epoch: 10 [63744/123872 (51%)]\tLoss: 0.376392\tLR: 0.00017260\n",
            "Train Epoch: 10 [64000/123872 (52%)]\tLoss: 0.343462\tLR: 0.00017250\n",
            "Train Epoch: 10 [64000/123872 (52%)]\tLoss: 0.343462\n",
            "Train Epoch: 10 [64256/123872 (52%)]\tLoss: 0.358096\tLR: 0.00017241\n",
            "Train Epoch: 10 [64512/123872 (52%)]\tLoss: 0.344620\tLR: 0.00017231\n",
            "Train Epoch: 10 [64768/123872 (52%)]\tLoss: 0.375659\tLR: 0.00017222\n",
            "Train Epoch: 10 [65024/123872 (52%)]\tLoss: 0.355963\tLR: 0.00017212\n",
            "Train Epoch: 10 [65280/123872 (53%)]\tLoss: 0.364635\tLR: 0.00017202\n",
            "Train Epoch: 10 [65536/123872 (53%)]\tLoss: 0.289354\tLR: 0.00017193\n",
            "Train Epoch: 10 [65792/123872 (53%)]\tLoss: 0.353882\tLR: 0.00017183\n",
            "Train Epoch: 10 [66048/123872 (53%)]\tLoss: 0.349126\tLR: 0.00017174\n",
            "Train Epoch: 10 [66304/123872 (54%)]\tLoss: 0.373843\tLR: 0.00017164\n",
            "Train Epoch: 10 [66560/123872 (54%)]\tLoss: 0.362146\tLR: 0.00017155\n",
            "Train Epoch: 10 [66560/123872 (54%)]\tLoss: 0.362146\n",
            "Train Epoch: 10 [66816/123872 (54%)]\tLoss: 0.415000\tLR: 0.00017145\n",
            "Train Epoch: 10 [67072/123872 (54%)]\tLoss: 0.316362\tLR: 0.00017136\n",
            "Train Epoch: 10 [67328/123872 (54%)]\tLoss: 0.404703\tLR: 0.00017126\n",
            "Train Epoch: 10 [67584/123872 (55%)]\tLoss: 0.388922\tLR: 0.00017116\n",
            "Train Epoch: 10 [67840/123872 (55%)]\tLoss: 0.373069\tLR: 0.00017107\n",
            "Train Epoch: 10 [68096/123872 (55%)]\tLoss: 0.372576\tLR: 0.00017097\n",
            "Train Epoch: 10 [68352/123872 (55%)]\tLoss: 0.423566\tLR: 0.00017088\n",
            "Train Epoch: 10 [68608/123872 (55%)]\tLoss: 0.388134\tLR: 0.00017078\n",
            "Train Epoch: 10 [68864/123872 (56%)]\tLoss: 0.377239\tLR: 0.00017069\n",
            "Train Epoch: 10 [69120/123872 (56%)]\tLoss: 0.373481\tLR: 0.00017059\n",
            "Train Epoch: 10 [69120/123872 (56%)]\tLoss: 0.373481\n",
            "Train Epoch: 10 [69376/123872 (56%)]\tLoss: 0.386426\tLR: 0.00017050\n",
            "Train Epoch: 10 [69632/123872 (56%)]\tLoss: 0.397593\tLR: 0.00017040\n",
            "Train Epoch: 10 [69888/123872 (56%)]\tLoss: 0.356039\tLR: 0.00017031\n",
            "Train Epoch: 10 [70144/123872 (57%)]\tLoss: 0.399204\tLR: 0.00017021\n",
            "Train Epoch: 10 [70400/123872 (57%)]\tLoss: 0.415238\tLR: 0.00017012\n",
            "Train Epoch: 10 [70656/123872 (57%)]\tLoss: 0.356322\tLR: 0.00017002\n",
            "Train Epoch: 10 [70912/123872 (57%)]\tLoss: 0.383771\tLR: 0.00016993\n",
            "Train Epoch: 10 [71168/123872 (57%)]\tLoss: 0.360598\tLR: 0.00016983\n",
            "Train Epoch: 10 [71424/123872 (58%)]\tLoss: 0.336200\tLR: 0.00016974\n",
            "Train Epoch: 10 [71680/123872 (58%)]\tLoss: 0.362764\tLR: 0.00016964\n",
            "Train Epoch: 10 [71680/123872 (58%)]\tLoss: 0.362764\n",
            "Train Epoch: 10 [71936/123872 (58%)]\tLoss: 0.402382\tLR: 0.00016955\n",
            "Train Epoch: 10 [72192/123872 (58%)]\tLoss: 0.397345\tLR: 0.00016945\n",
            "Train Epoch: 10 [72448/123872 (58%)]\tLoss: 0.363478\tLR: 0.00016936\n",
            "Train Epoch: 10 [72704/123872 (59%)]\tLoss: 0.341278\tLR: 0.00016926\n",
            "Train Epoch: 10 [72960/123872 (59%)]\tLoss: 0.357075\tLR: 0.00016917\n",
            "Train Epoch: 10 [73216/123872 (59%)]\tLoss: 0.344449\tLR: 0.00016907\n",
            "Train Epoch: 10 [73472/123872 (59%)]\tLoss: 0.358362\tLR: 0.00016898\n",
            "Train Epoch: 10 [73728/123872 (60%)]\tLoss: 0.331540\tLR: 0.00016888\n",
            "Train Epoch: 10 [73984/123872 (60%)]\tLoss: 0.355561\tLR: 0.00016879\n",
            "Train Epoch: 10 [74240/123872 (60%)]\tLoss: 0.342899\tLR: 0.00016869\n",
            "Train Epoch: 10 [74240/123872 (60%)]\tLoss: 0.342899\n",
            "Train Epoch: 10 [74496/123872 (60%)]\tLoss: 0.387614\tLR: 0.00016860\n",
            "Train Epoch: 10 [74752/123872 (60%)]\tLoss: 0.353879\tLR: 0.00016850\n",
            "Train Epoch: 10 [75008/123872 (61%)]\tLoss: 0.375354\tLR: 0.00016841\n",
            "Train Epoch: 10 [75264/123872 (61%)]\tLoss: 0.334796\tLR: 0.00016832\n",
            "Train Epoch: 10 [75520/123872 (61%)]\tLoss: 0.319260\tLR: 0.00016822\n",
            "Train Epoch: 10 [75776/123872 (61%)]\tLoss: 0.359747\tLR: 0.00016813\n",
            "Train Epoch: 10 [76032/123872 (61%)]\tLoss: 0.373745\tLR: 0.00016803\n",
            "Train Epoch: 10 [76288/123872 (62%)]\tLoss: 0.425538\tLR: 0.00016794\n",
            "Train Epoch: 10 [76544/123872 (62%)]\tLoss: 0.385686\tLR: 0.00016784\n",
            "Train Epoch: 10 [76800/123872 (62%)]\tLoss: 0.297338\tLR: 0.00016775\n",
            "Train Epoch: 10 [76800/123872 (62%)]\tLoss: 0.297338\n",
            "Train Epoch: 10 [77056/123872 (62%)]\tLoss: 0.341978\tLR: 0.00016765\n",
            "Train Epoch: 10 [77312/123872 (62%)]\tLoss: 0.329018\tLR: 0.00016756\n",
            "Train Epoch: 10 [77568/123872 (63%)]\tLoss: 0.312087\tLR: 0.00016747\n",
            "Train Epoch: 10 [77824/123872 (63%)]\tLoss: 0.323133\tLR: 0.00016737\n",
            "Train Epoch: 10 [78080/123872 (63%)]\tLoss: 0.429060\tLR: 0.00016728\n",
            "Train Epoch: 10 [78336/123872 (63%)]\tLoss: 0.361257\tLR: 0.00016718\n",
            "Train Epoch: 10 [78592/123872 (63%)]\tLoss: 0.357296\tLR: 0.00016709\n",
            "Train Epoch: 10 [78848/123872 (64%)]\tLoss: 0.327479\tLR: 0.00016700\n",
            "Train Epoch: 10 [79104/123872 (64%)]\tLoss: 0.317061\tLR: 0.00016690\n",
            "Train Epoch: 10 [79360/123872 (64%)]\tLoss: 0.358790\tLR: 0.00016681\n",
            "Train Epoch: 10 [79360/123872 (64%)]\tLoss: 0.358790\n",
            "Train Epoch: 10 [79616/123872 (64%)]\tLoss: 0.392596\tLR: 0.00016671\n",
            "Train Epoch: 10 [79872/123872 (64%)]\tLoss: 0.362558\tLR: 0.00016662\n",
            "Train Epoch: 10 [80128/123872 (65%)]\tLoss: 0.379851\tLR: 0.00016653\n",
            "Train Epoch: 10 [80384/123872 (65%)]\tLoss: 0.373298\tLR: 0.00016643\n",
            "Train Epoch: 10 [80640/123872 (65%)]\tLoss: 0.355469\tLR: 0.00016634\n",
            "Train Epoch: 10 [80896/123872 (65%)]\tLoss: 0.376931\tLR: 0.00016624\n",
            "Train Epoch: 10 [81152/123872 (65%)]\tLoss: 0.348658\tLR: 0.00016615\n",
            "Train Epoch: 10 [81408/123872 (66%)]\tLoss: 0.440084\tLR: 0.00016606\n",
            "Train Epoch: 10 [81664/123872 (66%)]\tLoss: 0.394757\tLR: 0.00016596\n",
            "Train Epoch: 10 [81920/123872 (66%)]\tLoss: 0.410314\tLR: 0.00016587\n",
            "Train Epoch: 10 [81920/123872 (66%)]\tLoss: 0.410314\n",
            "Train Epoch: 10 [82176/123872 (66%)]\tLoss: 0.327316\tLR: 0.00016577\n",
            "Train Epoch: 10 [82432/123872 (67%)]\tLoss: 0.420111\tLR: 0.00016568\n",
            "Train Epoch: 10 [82688/123872 (67%)]\tLoss: 0.371652\tLR: 0.00016559\n",
            "Train Epoch: 10 [82944/123872 (67%)]\tLoss: 0.366117\tLR: 0.00016549\n",
            "Train Epoch: 10 [83200/123872 (67%)]\tLoss: 0.349137\tLR: 0.00016540\n",
            "Train Epoch: 10 [83456/123872 (67%)]\tLoss: 0.353673\tLR: 0.00016531\n",
            "Train Epoch: 10 [83712/123872 (68%)]\tLoss: 0.321587\tLR: 0.00016521\n",
            "Train Epoch: 10 [83968/123872 (68%)]\tLoss: 0.338393\tLR: 0.00016512\n",
            "Train Epoch: 10 [84224/123872 (68%)]\tLoss: 0.319343\tLR: 0.00016503\n",
            "Train Epoch: 10 [84480/123872 (68%)]\tLoss: 0.323787\tLR: 0.00016493\n",
            "Train Epoch: 10 [84480/123872 (68%)]\tLoss: 0.323787\n",
            "Train Epoch: 10 [84736/123872 (68%)]\tLoss: 0.328006\tLR: 0.00016484\n",
            "Train Epoch: 10 [84992/123872 (69%)]\tLoss: 0.378725\tLR: 0.00016475\n",
            "Train Epoch: 10 [85248/123872 (69%)]\tLoss: 0.349620\tLR: 0.00016465\n",
            "Train Epoch: 10 [85504/123872 (69%)]\tLoss: 0.399580\tLR: 0.00016456\n",
            "Train Epoch: 10 [85760/123872 (69%)]\tLoss: 0.382811\tLR: 0.00016447\n",
            "Train Epoch: 10 [86016/123872 (69%)]\tLoss: 0.361185\tLR: 0.00016437\n",
            "Train Epoch: 10 [86272/123872 (70%)]\tLoss: 0.378791\tLR: 0.00016428\n",
            "Train Epoch: 10 [86528/123872 (70%)]\tLoss: 0.374137\tLR: 0.00016419\n",
            "Train Epoch: 10 [86784/123872 (70%)]\tLoss: 0.353689\tLR: 0.00016409\n",
            "Train Epoch: 10 [87040/123872 (70%)]\tLoss: 0.312118\tLR: 0.00016400\n",
            "Train Epoch: 10 [87040/123872 (70%)]\tLoss: 0.312118\n",
            "Train Epoch: 10 [87296/123872 (70%)]\tLoss: 0.350574\tLR: 0.00016391\n",
            "Train Epoch: 10 [87552/123872 (71%)]\tLoss: 0.376268\tLR: 0.00016381\n",
            "Train Epoch: 10 [87808/123872 (71%)]\tLoss: 0.371481\tLR: 0.00016372\n",
            "Train Epoch: 10 [88064/123872 (71%)]\tLoss: 0.340021\tLR: 0.00016363\n",
            "Train Epoch: 10 [88320/123872 (71%)]\tLoss: 0.363765\tLR: 0.00016354\n",
            "Train Epoch: 10 [88576/123872 (71%)]\tLoss: 0.324245\tLR: 0.00016344\n",
            "Train Epoch: 10 [88832/123872 (72%)]\tLoss: 0.364346\tLR: 0.00016335\n",
            "Train Epoch: 10 [89088/123872 (72%)]\tLoss: 0.421048\tLR: 0.00016326\n",
            "Train Epoch: 10 [89344/123872 (72%)]\tLoss: 0.372128\tLR: 0.00016316\n",
            "Train Epoch: 10 [89600/123872 (72%)]\tLoss: 0.361912\tLR: 0.00016307\n",
            "Train Epoch: 10 [89600/123872 (72%)]\tLoss: 0.361912\n",
            "Train Epoch: 10 [89856/123872 (73%)]\tLoss: 0.401523\tLR: 0.00016298\n",
            "Train Epoch: 10 [90112/123872 (73%)]\tLoss: 0.294406\tLR: 0.00016289\n",
            "Train Epoch: 10 [90368/123872 (73%)]\tLoss: 0.391319\tLR: 0.00016279\n",
            "Train Epoch: 10 [90624/123872 (73%)]\tLoss: 0.340736\tLR: 0.00016270\n",
            "Train Epoch: 10 [90880/123872 (73%)]\tLoss: 0.403511\tLR: 0.00016261\n",
            "Train Epoch: 10 [91136/123872 (74%)]\tLoss: 0.339555\tLR: 0.00016252\n",
            "Train Epoch: 10 [91392/123872 (74%)]\tLoss: 0.357060\tLR: 0.00016242\n",
            "Train Epoch: 10 [91648/123872 (74%)]\tLoss: 0.397164\tLR: 0.00016233\n",
            "Train Epoch: 10 [91904/123872 (74%)]\tLoss: 0.326028\tLR: 0.00016224\n",
            "Train Epoch: 10 [92160/123872 (74%)]\tLoss: 0.351776\tLR: 0.00016215\n",
            "Train Epoch: 10 [92160/123872 (74%)]\tLoss: 0.351776\n",
            "Train Epoch: 10 [92416/123872 (75%)]\tLoss: 0.321374\tLR: 0.00016205\n",
            "Train Epoch: 10 [92672/123872 (75%)]\tLoss: 0.396580\tLR: 0.00016196\n",
            "Train Epoch: 10 [92928/123872 (75%)]\tLoss: 0.437544\tLR: 0.00016187\n",
            "Train Epoch: 10 [93184/123872 (75%)]\tLoss: 0.411145\tLR: 0.00016178\n",
            "Train Epoch: 10 [93440/123872 (75%)]\tLoss: 0.388126\tLR: 0.00016168\n",
            "Train Epoch: 10 [93696/123872 (76%)]\tLoss: 0.412391\tLR: 0.00016159\n",
            "Train Epoch: 10 [93952/123872 (76%)]\tLoss: 0.319398\tLR: 0.00016150\n",
            "Train Epoch: 10 [94208/123872 (76%)]\tLoss: 0.363625\tLR: 0.00016141\n",
            "Train Epoch: 10 [94464/123872 (76%)]\tLoss: 0.404213\tLR: 0.00016132\n",
            "Train Epoch: 10 [94720/123872 (76%)]\tLoss: 0.405425\tLR: 0.00016122\n",
            "Train Epoch: 10 [94720/123872 (76%)]\tLoss: 0.405425\n",
            "Train Epoch: 10 [94976/123872 (77%)]\tLoss: 0.344667\tLR: 0.00016113\n",
            "Train Epoch: 10 [95232/123872 (77%)]\tLoss: 0.361232\tLR: 0.00016104\n",
            "Train Epoch: 10 [95488/123872 (77%)]\tLoss: 0.365540\tLR: 0.00016095\n",
            "Train Epoch: 10 [95744/123872 (77%)]\tLoss: 0.362982\tLR: 0.00016085\n",
            "Train Epoch: 10 [96000/123872 (77%)]\tLoss: 0.376952\tLR: 0.00016076\n",
            "Train Epoch: 10 [96256/123872 (78%)]\tLoss: 0.350857\tLR: 0.00016067\n",
            "Train Epoch: 10 [96512/123872 (78%)]\tLoss: 0.398457\tLR: 0.00016058\n",
            "Train Epoch: 10 [96768/123872 (78%)]\tLoss: 0.342918\tLR: 0.00016049\n",
            "Train Epoch: 10 [97024/123872 (78%)]\tLoss: 0.355614\tLR: 0.00016040\n",
            "Train Epoch: 10 [97280/123872 (79%)]\tLoss: 0.336704\tLR: 0.00016030\n",
            "Train Epoch: 10 [97280/123872 (79%)]\tLoss: 0.336704\n",
            "Train Epoch: 10 [97536/123872 (79%)]\tLoss: 0.352578\tLR: 0.00016021\n",
            "Train Epoch: 10 [97792/123872 (79%)]\tLoss: 0.364361\tLR: 0.00016012\n",
            "Train Epoch: 10 [98048/123872 (79%)]\tLoss: 0.375301\tLR: 0.00016003\n",
            "Train Epoch: 10 [98304/123872 (79%)]\tLoss: 0.356730\tLR: 0.00015994\n",
            "Train Epoch: 10 [98560/123872 (80%)]\tLoss: 0.416284\tLR: 0.00015985\n",
            "Train Epoch: 10 [98816/123872 (80%)]\tLoss: 0.379755\tLR: 0.00015975\n",
            "Train Epoch: 10 [99072/123872 (80%)]\tLoss: 0.351967\tLR: 0.00015966\n",
            "Train Epoch: 10 [99328/123872 (80%)]\tLoss: 0.395190\tLR: 0.00015957\n",
            "Train Epoch: 10 [99584/123872 (80%)]\tLoss: 0.360825\tLR: 0.00015948\n",
            "Train Epoch: 10 [99840/123872 (81%)]\tLoss: 0.361934\tLR: 0.00015939\n",
            "Train Epoch: 10 [99840/123872 (81%)]\tLoss: 0.361934\n",
            "Train Epoch: 10 [100096/123872 (81%)]\tLoss: 0.349041\tLR: 0.00015930\n",
            "Train Epoch: 10 [100352/123872 (81%)]\tLoss: 0.365179\tLR: 0.00015920\n",
            "Train Epoch: 10 [100608/123872 (81%)]\tLoss: 0.340077\tLR: 0.00015911\n",
            "Train Epoch: 10 [100864/123872 (81%)]\tLoss: 0.396221\tLR: 0.00015902\n",
            "Train Epoch: 10 [101120/123872 (82%)]\tLoss: 0.396949\tLR: 0.00015893\n",
            "Train Epoch: 10 [101376/123872 (82%)]\tLoss: 0.345994\tLR: 0.00015884\n",
            "Train Epoch: 10 [101632/123872 (82%)]\tLoss: 0.322226\tLR: 0.00015875\n",
            "Train Epoch: 10 [101888/123872 (82%)]\tLoss: 0.331030\tLR: 0.00015866\n",
            "Train Epoch: 10 [102144/123872 (82%)]\tLoss: 0.374164\tLR: 0.00015857\n",
            "Train Epoch: 10 [102400/123872 (83%)]\tLoss: 0.343555\tLR: 0.00015847\n",
            "Train Epoch: 10 [102400/123872 (83%)]\tLoss: 0.343555\n",
            "Train Epoch: 10 [102656/123872 (83%)]\tLoss: 0.343108\tLR: 0.00015838\n",
            "Train Epoch: 10 [102912/123872 (83%)]\tLoss: 0.356902\tLR: 0.00015829\n",
            "Train Epoch: 10 [103168/123872 (83%)]\tLoss: 0.367991\tLR: 0.00015820\n",
            "Train Epoch: 10 [103424/123872 (83%)]\tLoss: 0.380824\tLR: 0.00015811\n",
            "Train Epoch: 10 [103680/123872 (84%)]\tLoss: 0.352888\tLR: 0.00015802\n",
            "Train Epoch: 10 [103936/123872 (84%)]\tLoss: 0.391693\tLR: 0.00015793\n",
            "Train Epoch: 10 [104192/123872 (84%)]\tLoss: 0.369369\tLR: 0.00015784\n",
            "Train Epoch: 10 [104448/123872 (84%)]\tLoss: 0.407929\tLR: 0.00015775\n",
            "Train Epoch: 10 [104704/123872 (85%)]\tLoss: 0.335967\tLR: 0.00015766\n",
            "Train Epoch: 10 [104960/123872 (85%)]\tLoss: 0.354986\tLR: 0.00015756\n",
            "Train Epoch: 10 [104960/123872 (85%)]\tLoss: 0.354986\n",
            "Train Epoch: 10 [105216/123872 (85%)]\tLoss: 0.389475\tLR: 0.00015747\n",
            "Train Epoch: 10 [105472/123872 (85%)]\tLoss: 0.321281\tLR: 0.00015738\n",
            "Train Epoch: 10 [105728/123872 (85%)]\tLoss: 0.329332\tLR: 0.00015729\n",
            "Train Epoch: 10 [105984/123872 (86%)]\tLoss: 0.375506\tLR: 0.00015720\n",
            "Train Epoch: 10 [106240/123872 (86%)]\tLoss: 0.392871\tLR: 0.00015711\n",
            "Train Epoch: 10 [106496/123872 (86%)]\tLoss: 0.375429\tLR: 0.00015702\n",
            "Train Epoch: 10 [106752/123872 (86%)]\tLoss: 0.345044\tLR: 0.00015693\n",
            "Train Epoch: 10 [107008/123872 (86%)]\tLoss: 0.331578\tLR: 0.00015684\n",
            "Train Epoch: 10 [107264/123872 (87%)]\tLoss: 0.368436\tLR: 0.00015675\n",
            "Train Epoch: 10 [107520/123872 (87%)]\tLoss: 0.318319\tLR: 0.00015666\n",
            "Train Epoch: 10 [107520/123872 (87%)]\tLoss: 0.318319\n",
            "Train Epoch: 10 [107776/123872 (87%)]\tLoss: 0.354286\tLR: 0.00015657\n",
            "Train Epoch: 10 [108032/123872 (87%)]\tLoss: 0.306400\tLR: 0.00015648\n",
            "Train Epoch: 10 [108288/123872 (87%)]\tLoss: 0.391942\tLR: 0.00015639\n",
            "Train Epoch: 10 [108544/123872 (88%)]\tLoss: 0.401506\tLR: 0.00015630\n",
            "Train Epoch: 10 [108800/123872 (88%)]\tLoss: 0.417969\tLR: 0.00015621\n",
            "Train Epoch: 10 [109056/123872 (88%)]\tLoss: 0.383086\tLR: 0.00015612\n",
            "Train Epoch: 10 [109312/123872 (88%)]\tLoss: 0.319128\tLR: 0.00015603\n",
            "Train Epoch: 10 [109568/123872 (88%)]\tLoss: 0.371239\tLR: 0.00015594\n",
            "Train Epoch: 10 [109824/123872 (89%)]\tLoss: 0.330702\tLR: 0.00015585\n",
            "Train Epoch: 10 [110080/123872 (89%)]\tLoss: 0.379050\tLR: 0.00015576\n",
            "Train Epoch: 10 [110080/123872 (89%)]\tLoss: 0.379050\n",
            "Train Epoch: 10 [110336/123872 (89%)]\tLoss: 0.314729\tLR: 0.00015567\n",
            "Train Epoch: 10 [110592/123872 (89%)]\tLoss: 0.355893\tLR: 0.00015558\n",
            "Train Epoch: 10 [110848/123872 (89%)]\tLoss: 0.440503\tLR: 0.00015548\n",
            "Train Epoch: 10 [111104/123872 (90%)]\tLoss: 0.385583\tLR: 0.00015539\n",
            "Train Epoch: 10 [111360/123872 (90%)]\tLoss: 0.294633\tLR: 0.00015530\n",
            "Train Epoch: 10 [111616/123872 (90%)]\tLoss: 0.346322\tLR: 0.00015522\n",
            "Train Epoch: 10 [111872/123872 (90%)]\tLoss: 0.343409\tLR: 0.00015513\n",
            "Train Epoch: 10 [112128/123872 (90%)]\tLoss: 0.378868\tLR: 0.00015504\n",
            "Train Epoch: 10 [112384/123872 (91%)]\tLoss: 0.333100\tLR: 0.00015495\n",
            "Train Epoch: 10 [112640/123872 (91%)]\tLoss: 0.386277\tLR: 0.00015486\n",
            "Train Epoch: 10 [112640/123872 (91%)]\tLoss: 0.386277\n",
            "Train Epoch: 10 [112896/123872 (91%)]\tLoss: 0.412657\tLR: 0.00015477\n",
            "Train Epoch: 10 [113152/123872 (91%)]\tLoss: 0.401530\tLR: 0.00015468\n",
            "Train Epoch: 10 [113408/123872 (92%)]\tLoss: 0.392090\tLR: 0.00015459\n",
            "Train Epoch: 10 [113664/123872 (92%)]\tLoss: 0.342748\tLR: 0.00015450\n",
            "Train Epoch: 10 [113920/123872 (92%)]\tLoss: 0.310335\tLR: 0.00015441\n",
            "Train Epoch: 10 [114176/123872 (92%)]\tLoss: 0.423833\tLR: 0.00015432\n",
            "Train Epoch: 10 [114432/123872 (92%)]\tLoss: 0.330667\tLR: 0.00015423\n",
            "Train Epoch: 10 [114688/123872 (93%)]\tLoss: 0.343235\tLR: 0.00015414\n",
            "Train Epoch: 10 [114944/123872 (93%)]\tLoss: 0.346598\tLR: 0.00015405\n",
            "Train Epoch: 10 [115200/123872 (93%)]\tLoss: 0.414293\tLR: 0.00015396\n",
            "Train Epoch: 10 [115200/123872 (93%)]\tLoss: 0.414293\n",
            "Train Epoch: 10 [115456/123872 (93%)]\tLoss: 0.353449\tLR: 0.00015387\n",
            "Train Epoch: 10 [115712/123872 (93%)]\tLoss: 0.405327\tLR: 0.00015378\n",
            "Train Epoch: 10 [115968/123872 (94%)]\tLoss: 0.402203\tLR: 0.00015369\n",
            "Train Epoch: 10 [116224/123872 (94%)]\tLoss: 0.360922\tLR: 0.00015360\n",
            "Train Epoch: 10 [116480/123872 (94%)]\tLoss: 0.359973\tLR: 0.00015351\n",
            "Train Epoch: 10 [116736/123872 (94%)]\tLoss: 0.388010\tLR: 0.00015342\n",
            "Train Epoch: 10 [116992/123872 (94%)]\tLoss: 0.416606\tLR: 0.00015333\n",
            "Train Epoch: 10 [117248/123872 (95%)]\tLoss: 0.334756\tLR: 0.00015324\n",
            "Train Epoch: 10 [117504/123872 (95%)]\tLoss: 0.436428\tLR: 0.00015316\n",
            "Train Epoch: 10 [117760/123872 (95%)]\tLoss: 0.359713\tLR: 0.00015307\n",
            "Train Epoch: 10 [117760/123872 (95%)]\tLoss: 0.359713\n",
            "Train Epoch: 10 [118016/123872 (95%)]\tLoss: 0.453554\tLR: 0.00015298\n",
            "Train Epoch: 10 [118272/123872 (95%)]\tLoss: 0.391172\tLR: 0.00015289\n",
            "Train Epoch: 10 [118528/123872 (96%)]\tLoss: 0.367945\tLR: 0.00015280\n",
            "Train Epoch: 10 [118784/123872 (96%)]\tLoss: 0.310700\tLR: 0.00015271\n",
            "Train Epoch: 10 [119040/123872 (96%)]\tLoss: 0.321371\tLR: 0.00015262\n",
            "Train Epoch: 10 [119296/123872 (96%)]\tLoss: 0.330791\tLR: 0.00015253\n",
            "Train Epoch: 10 [119552/123872 (96%)]\tLoss: 0.341832\tLR: 0.00015244\n",
            "Train Epoch: 10 [119808/123872 (97%)]\tLoss: 0.352462\tLR: 0.00015235\n",
            "Train Epoch: 10 [120064/123872 (97%)]\tLoss: 0.363876\tLR: 0.00015227\n",
            "Train Epoch: 10 [120320/123872 (97%)]\tLoss: 0.332533\tLR: 0.00015218\n",
            "Train Epoch: 10 [120320/123872 (97%)]\tLoss: 0.332533\n",
            "Train Epoch: 10 [120576/123872 (97%)]\tLoss: 0.394893\tLR: 0.00015209\n",
            "Train Epoch: 10 [120832/123872 (98%)]\tLoss: 0.382089\tLR: 0.00015200\n",
            "Train Epoch: 10 [121088/123872 (98%)]\tLoss: 0.382507\tLR: 0.00015191\n",
            "Train Epoch: 10 [121344/123872 (98%)]\tLoss: 0.355221\tLR: 0.00015182\n",
            "Train Epoch: 10 [121600/123872 (98%)]\tLoss: 0.375041\tLR: 0.00015173\n",
            "Train Epoch: 10 [121856/123872 (98%)]\tLoss: 0.383190\tLR: 0.00015164\n",
            "Train Epoch: 10 [122112/123872 (99%)]\tLoss: 0.360816\tLR: 0.00015156\n",
            "Train Epoch: 10 [122368/123872 (99%)]\tLoss: 0.399019\tLR: 0.00015147\n",
            "Train Epoch: 10 [122624/123872 (99%)]\tLoss: 0.268439\tLR: 0.00015138\n",
            "Train Epoch: 10 [122880/123872 (99%)]\tLoss: 0.371014\tLR: 0.00015129\n",
            "Train Epoch: 10 [122880/123872 (99%)]\tLoss: 0.371014\n",
            "Train Epoch: 10 [123136/123872 (99%)]\tLoss: 0.420859\tLR: 0.00015120\n",
            "Train Epoch: 10 [123392/123872 (100%)]\tLoss: 0.339584\tLR: 0.00015111\n",
            "Train Epoch: 10 [108192/123872 (100%)]\tLoss: 0.389589\tLR: 0.00015102\n",
            "\n",
            "Test set: Average loss: 0.0015, Accuracy: 25806/30970 (83.33%)\n",
            "\n",
            "Train Epoch: 11 [0/123872 (0%)]\tLoss: 0.336059\tLR: 0.00015094\n",
            "Train Epoch: 11 [0/123872 (0%)]\tLoss: 0.336059\n",
            "Train Epoch: 11 [256/123872 (0%)]\tLoss: 0.333752\tLR: 0.00015085\n",
            "Train Epoch: 11 [512/123872 (0%)]\tLoss: 0.318654\tLR: 0.00015076\n",
            "Train Epoch: 11 [768/123872 (1%)]\tLoss: 0.376914\tLR: 0.00015067\n",
            "Train Epoch: 11 [1024/123872 (1%)]\tLoss: 0.296981\tLR: 0.00015058\n",
            "Train Epoch: 11 [1280/123872 (1%)]\tLoss: 0.383216\tLR: 0.00015050\n",
            "Train Epoch: 11 [1536/123872 (1%)]\tLoss: 0.347698\tLR: 0.00015041\n",
            "Train Epoch: 11 [1792/123872 (1%)]\tLoss: 0.327379\tLR: 0.00015032\n",
            "Train Epoch: 11 [2048/123872 (2%)]\tLoss: 0.335279\tLR: 0.00015023\n",
            "Train Epoch: 11 [2304/123872 (2%)]\tLoss: 0.307623\tLR: 0.00015014\n",
            "Train Epoch: 11 [2560/123872 (2%)]\tLoss: 0.334474\tLR: 0.00015006\n",
            "Train Epoch: 11 [2560/123872 (2%)]\tLoss: 0.334474\n",
            "Train Epoch: 11 [2816/123872 (2%)]\tLoss: 0.349829\tLR: 0.00014997\n",
            "Train Epoch: 11 [3072/123872 (2%)]\tLoss: 0.297810\tLR: 0.00014988\n",
            "Train Epoch: 11 [3328/123872 (3%)]\tLoss: 0.436647\tLR: 0.00014979\n",
            "Train Epoch: 11 [3584/123872 (3%)]\tLoss: 0.246029\tLR: 0.00014970\n",
            "Train Epoch: 11 [3840/123872 (3%)]\tLoss: 0.373595\tLR: 0.00014962\n",
            "Train Epoch: 11 [4096/123872 (3%)]\tLoss: 0.391726\tLR: 0.00014953\n",
            "Train Epoch: 11 [4352/123872 (4%)]\tLoss: 0.356641\tLR: 0.00014944\n",
            "Train Epoch: 11 [4608/123872 (4%)]\tLoss: 0.389337\tLR: 0.00014935\n",
            "Train Epoch: 11 [4864/123872 (4%)]\tLoss: 0.304096\tLR: 0.00014926\n",
            "Train Epoch: 11 [5120/123872 (4%)]\tLoss: 0.374821\tLR: 0.00014918\n",
            "Train Epoch: 11 [5120/123872 (4%)]\tLoss: 0.374821\n",
            "Train Epoch: 11 [5376/123872 (4%)]\tLoss: 0.427793\tLR: 0.00014909\n",
            "Train Epoch: 11 [5632/123872 (5%)]\tLoss: 0.421526\tLR: 0.00014900\n",
            "Train Epoch: 11 [5888/123872 (5%)]\tLoss: 0.359004\tLR: 0.00014891\n",
            "Train Epoch: 11 [6144/123872 (5%)]\tLoss: 0.326093\tLR: 0.00014883\n",
            "Train Epoch: 11 [6400/123872 (5%)]\tLoss: 0.349548\tLR: 0.00014874\n",
            "Train Epoch: 11 [6656/123872 (5%)]\tLoss: 0.356159\tLR: 0.00014865\n",
            "Train Epoch: 11 [6912/123872 (6%)]\tLoss: 0.367319\tLR: 0.00014856\n",
            "Train Epoch: 11 [7168/123872 (6%)]\tLoss: 0.387820\tLR: 0.00014848\n",
            "Train Epoch: 11 [7424/123872 (6%)]\tLoss: 0.361390\tLR: 0.00014839\n",
            "Train Epoch: 11 [7680/123872 (6%)]\tLoss: 0.318097\tLR: 0.00014830\n",
            "Train Epoch: 11 [7680/123872 (6%)]\tLoss: 0.318097\n",
            "Train Epoch: 11 [7936/123872 (6%)]\tLoss: 0.333821\tLR: 0.00014822\n",
            "Train Epoch: 11 [8192/123872 (7%)]\tLoss: 0.375916\tLR: 0.00014813\n",
            "Train Epoch: 11 [8448/123872 (7%)]\tLoss: 0.445067\tLR: 0.00014804\n",
            "Train Epoch: 11 [8704/123872 (7%)]\tLoss: 0.308252\tLR: 0.00014795\n",
            "Train Epoch: 11 [8960/123872 (7%)]\tLoss: 0.439599\tLR: 0.00014787\n",
            "Train Epoch: 11 [9216/123872 (7%)]\tLoss: 0.413697\tLR: 0.00014778\n",
            "Train Epoch: 11 [9472/123872 (8%)]\tLoss: 0.325244\tLR: 0.00014769\n",
            "Train Epoch: 11 [9728/123872 (8%)]\tLoss: 0.429583\tLR: 0.00014761\n",
            "Train Epoch: 11 [9984/123872 (8%)]\tLoss: 0.335545\tLR: 0.00014752\n",
            "Train Epoch: 11 [10240/123872 (8%)]\tLoss: 0.305283\tLR: 0.00014743\n",
            "Train Epoch: 11 [10240/123872 (8%)]\tLoss: 0.305283\n",
            "Train Epoch: 11 [10496/123872 (8%)]\tLoss: 0.315242\tLR: 0.00014734\n",
            "Train Epoch: 11 [10752/123872 (9%)]\tLoss: 0.361585\tLR: 0.00014726\n",
            "Train Epoch: 11 [11008/123872 (9%)]\tLoss: 0.366219\tLR: 0.00014717\n",
            "Train Epoch: 11 [11264/123872 (9%)]\tLoss: 0.322580\tLR: 0.00014708\n",
            "Train Epoch: 11 [11520/123872 (9%)]\tLoss: 0.311999\tLR: 0.00014700\n",
            "Train Epoch: 11 [11776/123872 (10%)]\tLoss: 0.328260\tLR: 0.00014691\n",
            "Train Epoch: 11 [12032/123872 (10%)]\tLoss: 0.331215\tLR: 0.00014682\n",
            "Train Epoch: 11 [12288/123872 (10%)]\tLoss: 0.293169\tLR: 0.00014674\n",
            "Train Epoch: 11 [12544/123872 (10%)]\tLoss: 0.385353\tLR: 0.00014665\n",
            "Train Epoch: 11 [12800/123872 (10%)]\tLoss: 0.348353\tLR: 0.00014656\n",
            "Train Epoch: 11 [12800/123872 (10%)]\tLoss: 0.348353\n",
            "Train Epoch: 11 [13056/123872 (11%)]\tLoss: 0.413463\tLR: 0.00014648\n",
            "Train Epoch: 11 [13312/123872 (11%)]\tLoss: 0.370775\tLR: 0.00014639\n",
            "Train Epoch: 11 [13568/123872 (11%)]\tLoss: 0.345943\tLR: 0.00014630\n",
            "Train Epoch: 11 [13824/123872 (11%)]\tLoss: 0.289087\tLR: 0.00014622\n",
            "Train Epoch: 11 [14080/123872 (11%)]\tLoss: 0.401350\tLR: 0.00014613\n",
            "Train Epoch: 11 [14336/123872 (12%)]\tLoss: 0.354814\tLR: 0.00014604\n",
            "Train Epoch: 11 [14592/123872 (12%)]\tLoss: 0.395108\tLR: 0.00014596\n",
            "Train Epoch: 11 [14848/123872 (12%)]\tLoss: 0.356871\tLR: 0.00014587\n",
            "Train Epoch: 11 [15104/123872 (12%)]\tLoss: 0.365343\tLR: 0.00014579\n",
            "Train Epoch: 11 [15360/123872 (12%)]\tLoss: 0.384429\tLR: 0.00014570\n",
            "Train Epoch: 11 [15360/123872 (12%)]\tLoss: 0.384429\n",
            "Train Epoch: 11 [15616/123872 (13%)]\tLoss: 0.355056\tLR: 0.00014561\n",
            "Train Epoch: 11 [15872/123872 (13%)]\tLoss: 0.377373\tLR: 0.00014553\n",
            "Train Epoch: 11 [16128/123872 (13%)]\tLoss: 0.341185\tLR: 0.00014544\n",
            "Train Epoch: 11 [16384/123872 (13%)]\tLoss: 0.400076\tLR: 0.00014535\n",
            "Train Epoch: 11 [16640/123872 (13%)]\tLoss: 0.334218\tLR: 0.00014527\n",
            "Train Epoch: 11 [16896/123872 (14%)]\tLoss: 0.378443\tLR: 0.00014518\n",
            "Train Epoch: 11 [17152/123872 (14%)]\tLoss: 0.328479\tLR: 0.00014510\n",
            "Train Epoch: 11 [17408/123872 (14%)]\tLoss: 0.320907\tLR: 0.00014501\n",
            "Train Epoch: 11 [17664/123872 (14%)]\tLoss: 0.337897\tLR: 0.00014492\n",
            "Train Epoch: 11 [17920/123872 (14%)]\tLoss: 0.338490\tLR: 0.00014484\n",
            "Train Epoch: 11 [17920/123872 (14%)]\tLoss: 0.338490\n",
            "Train Epoch: 11 [18176/123872 (15%)]\tLoss: 0.348664\tLR: 0.00014475\n",
            "Train Epoch: 11 [18432/123872 (15%)]\tLoss: 0.396621\tLR: 0.00014467\n",
            "Train Epoch: 11 [18688/123872 (15%)]\tLoss: 0.344311\tLR: 0.00014458\n",
            "Train Epoch: 11 [18944/123872 (15%)]\tLoss: 0.338259\tLR: 0.00014450\n",
            "Train Epoch: 11 [19200/123872 (15%)]\tLoss: 0.290225\tLR: 0.00014441\n",
            "Train Epoch: 11 [19456/123872 (16%)]\tLoss: 0.385110\tLR: 0.00014432\n",
            "Train Epoch: 11 [19712/123872 (16%)]\tLoss: 0.391961\tLR: 0.00014424\n",
            "Train Epoch: 11 [19968/123872 (16%)]\tLoss: 0.285343\tLR: 0.00014415\n",
            "Train Epoch: 11 [20224/123872 (16%)]\tLoss: 0.357654\tLR: 0.00014407\n",
            "Train Epoch: 11 [20480/123872 (17%)]\tLoss: 0.312520\tLR: 0.00014398\n",
            "Train Epoch: 11 [20480/123872 (17%)]\tLoss: 0.312520\n",
            "Train Epoch: 11 [20736/123872 (17%)]\tLoss: 0.413991\tLR: 0.00014390\n",
            "Train Epoch: 11 [20992/123872 (17%)]\tLoss: 0.371448\tLR: 0.00014381\n",
            "Train Epoch: 11 [21248/123872 (17%)]\tLoss: 0.339984\tLR: 0.00014373\n",
            "Train Epoch: 11 [21504/123872 (17%)]\tLoss: 0.301587\tLR: 0.00014364\n",
            "Train Epoch: 11 [21760/123872 (18%)]\tLoss: 0.393825\tLR: 0.00014355\n",
            "Train Epoch: 11 [22016/123872 (18%)]\tLoss: 0.377804\tLR: 0.00014347\n",
            "Train Epoch: 11 [22272/123872 (18%)]\tLoss: 0.338142\tLR: 0.00014338\n",
            "Train Epoch: 11 [22528/123872 (18%)]\tLoss: 0.369583\tLR: 0.00014330\n",
            "Train Epoch: 11 [22784/123872 (18%)]\tLoss: 0.372768\tLR: 0.00014321\n",
            "Train Epoch: 11 [23040/123872 (19%)]\tLoss: 0.362081\tLR: 0.00014313\n",
            "Train Epoch: 11 [23040/123872 (19%)]\tLoss: 0.362081\n",
            "Train Epoch: 11 [23296/123872 (19%)]\tLoss: 0.350919\tLR: 0.00014304\n",
            "Train Epoch: 11 [23552/123872 (19%)]\tLoss: 0.392063\tLR: 0.00014296\n",
            "Train Epoch: 11 [23808/123872 (19%)]\tLoss: 0.346986\tLR: 0.00014287\n",
            "Train Epoch: 11 [24064/123872 (19%)]\tLoss: 0.347471\tLR: 0.00014279\n",
            "Train Epoch: 11 [24320/123872 (20%)]\tLoss: 0.352720\tLR: 0.00014270\n",
            "Train Epoch: 11 [24576/123872 (20%)]\tLoss: 0.352768\tLR: 0.00014262\n",
            "Train Epoch: 11 [24832/123872 (20%)]\tLoss: 0.313177\tLR: 0.00014253\n",
            "Train Epoch: 11 [25088/123872 (20%)]\tLoss: 0.362359\tLR: 0.00014245\n",
            "Train Epoch: 11 [25344/123872 (20%)]\tLoss: 0.352564\tLR: 0.00014236\n",
            "Train Epoch: 11 [25600/123872 (21%)]\tLoss: 0.390123\tLR: 0.00014228\n",
            "Train Epoch: 11 [25600/123872 (21%)]\tLoss: 0.390123\n",
            "Train Epoch: 11 [25856/123872 (21%)]\tLoss: 0.381902\tLR: 0.00014219\n",
            "Train Epoch: 11 [26112/123872 (21%)]\tLoss: 0.366912\tLR: 0.00014211\n",
            "Train Epoch: 11 [26368/123872 (21%)]\tLoss: 0.322328\tLR: 0.00014202\n",
            "Train Epoch: 11 [26624/123872 (21%)]\tLoss: 0.324049\tLR: 0.00014194\n",
            "Train Epoch: 11 [26880/123872 (22%)]\tLoss: 0.395013\tLR: 0.00014185\n",
            "Train Epoch: 11 [27136/123872 (22%)]\tLoss: 0.384099\tLR: 0.00014177\n",
            "Train Epoch: 11 [27392/123872 (22%)]\tLoss: 0.312762\tLR: 0.00014169\n",
            "Train Epoch: 11 [27648/123872 (22%)]\tLoss: 0.333067\tLR: 0.00014160\n",
            "Train Epoch: 11 [27904/123872 (23%)]\tLoss: 0.381170\tLR: 0.00014152\n",
            "Train Epoch: 11 [28160/123872 (23%)]\tLoss: 0.404543\tLR: 0.00014143\n",
            "Train Epoch: 11 [28160/123872 (23%)]\tLoss: 0.404543\n",
            "Train Epoch: 11 [28416/123872 (23%)]\tLoss: 0.356061\tLR: 0.00014135\n",
            "Train Epoch: 11 [28672/123872 (23%)]\tLoss: 0.382307\tLR: 0.00014126\n",
            "Train Epoch: 11 [28928/123872 (23%)]\tLoss: 0.384919\tLR: 0.00014118\n",
            "Train Epoch: 11 [29184/123872 (24%)]\tLoss: 0.381370\tLR: 0.00014109\n",
            "Train Epoch: 11 [29440/123872 (24%)]\tLoss: 0.394429\tLR: 0.00014101\n",
            "Train Epoch: 11 [29696/123872 (24%)]\tLoss: 0.337432\tLR: 0.00014093\n",
            "Train Epoch: 11 [29952/123872 (24%)]\tLoss: 0.401819\tLR: 0.00014084\n",
            "Train Epoch: 11 [30208/123872 (24%)]\tLoss: 0.380205\tLR: 0.00014076\n",
            "Train Epoch: 11 [30464/123872 (25%)]\tLoss: 0.418676\tLR: 0.00014067\n",
            "Train Epoch: 11 [30720/123872 (25%)]\tLoss: 0.328419\tLR: 0.00014059\n",
            "Train Epoch: 11 [30720/123872 (25%)]\tLoss: 0.328419\n",
            "Train Epoch: 11 [30976/123872 (25%)]\tLoss: 0.363765\tLR: 0.00014050\n",
            "Train Epoch: 11 [31232/123872 (25%)]\tLoss: 0.356137\tLR: 0.00014042\n",
            "Train Epoch: 11 [31488/123872 (25%)]\tLoss: 0.378317\tLR: 0.00014034\n",
            "Train Epoch: 11 [31744/123872 (26%)]\tLoss: 0.352099\tLR: 0.00014025\n",
            "Train Epoch: 11 [32000/123872 (26%)]\tLoss: 0.348873\tLR: 0.00014017\n",
            "Train Epoch: 11 [32256/123872 (26%)]\tLoss: 0.356184\tLR: 0.00014009\n",
            "Train Epoch: 11 [32512/123872 (26%)]\tLoss: 0.371804\tLR: 0.00014000\n",
            "Train Epoch: 11 [32768/123872 (26%)]\tLoss: 0.389871\tLR: 0.00013992\n",
            "Train Epoch: 11 [33024/123872 (27%)]\tLoss: 0.316466\tLR: 0.00013983\n",
            "Train Epoch: 11 [33280/123872 (27%)]\tLoss: 0.346427\tLR: 0.00013975\n",
            "Train Epoch: 11 [33280/123872 (27%)]\tLoss: 0.346427\n",
            "Train Epoch: 11 [33536/123872 (27%)]\tLoss: 0.357358\tLR: 0.00013967\n",
            "Train Epoch: 11 [33792/123872 (27%)]\tLoss: 0.297920\tLR: 0.00013958\n",
            "Train Epoch: 11 [34048/123872 (27%)]\tLoss: 0.296944\tLR: 0.00013950\n",
            "Train Epoch: 11 [34304/123872 (28%)]\tLoss: 0.427306\tLR: 0.00013942\n",
            "Train Epoch: 11 [34560/123872 (28%)]\tLoss: 0.321083\tLR: 0.00013933\n",
            "Train Epoch: 11 [34816/123872 (28%)]\tLoss: 0.289370\tLR: 0.00013925\n",
            "Train Epoch: 11 [35072/123872 (28%)]\tLoss: 0.315838\tLR: 0.00013916\n",
            "Train Epoch: 11 [35328/123872 (29%)]\tLoss: 0.408500\tLR: 0.00013908\n",
            "Train Epoch: 11 [35584/123872 (29%)]\tLoss: 0.309009\tLR: 0.00013900\n",
            "Train Epoch: 11 [35840/123872 (29%)]\tLoss: 0.367932\tLR: 0.00013891\n",
            "Train Epoch: 11 [35840/123872 (29%)]\tLoss: 0.367932\n",
            "Train Epoch: 11 [36096/123872 (29%)]\tLoss: 0.356397\tLR: 0.00013883\n",
            "Train Epoch: 11 [36352/123872 (29%)]\tLoss: 0.419621\tLR: 0.00013875\n",
            "Train Epoch: 11 [36608/123872 (30%)]\tLoss: 0.369302\tLR: 0.00013866\n",
            "Train Epoch: 11 [36864/123872 (30%)]\tLoss: 0.400220\tLR: 0.00013858\n",
            "Train Epoch: 11 [37120/123872 (30%)]\tLoss: 0.328503\tLR: 0.00013850\n",
            "Train Epoch: 11 [37376/123872 (30%)]\tLoss: 0.425926\tLR: 0.00013841\n",
            "Train Epoch: 11 [37632/123872 (30%)]\tLoss: 0.320007\tLR: 0.00013833\n",
            "Train Epoch: 11 [37888/123872 (31%)]\tLoss: 0.339547\tLR: 0.00013825\n",
            "Train Epoch: 11 [38144/123872 (31%)]\tLoss: 0.377715\tLR: 0.00013817\n",
            "Train Epoch: 11 [38400/123872 (31%)]\tLoss: 0.380198\tLR: 0.00013808\n",
            "Train Epoch: 11 [38400/123872 (31%)]\tLoss: 0.380198\n",
            "Train Epoch: 11 [38656/123872 (31%)]\tLoss: 0.253921\tLR: 0.00013800\n",
            "Train Epoch: 11 [38912/123872 (31%)]\tLoss: 0.377814\tLR: 0.00013792\n",
            "Train Epoch: 11 [39168/123872 (32%)]\tLoss: 0.333010\tLR: 0.00013783\n",
            "Train Epoch: 11 [39424/123872 (32%)]\tLoss: 0.370299\tLR: 0.00013775\n",
            "Train Epoch: 11 [39680/123872 (32%)]\tLoss: 0.322471\tLR: 0.00013767\n",
            "Train Epoch: 11 [39936/123872 (32%)]\tLoss: 0.399574\tLR: 0.00013759\n",
            "Train Epoch: 11 [40192/123872 (32%)]\tLoss: 0.351622\tLR: 0.00013750\n",
            "Train Epoch: 11 [40448/123872 (33%)]\tLoss: 0.355370\tLR: 0.00013742\n",
            "Train Epoch: 11 [40704/123872 (33%)]\tLoss: 0.334789\tLR: 0.00013734\n",
            "Train Epoch: 11 [40960/123872 (33%)]\tLoss: 0.366761\tLR: 0.00013725\n",
            "Train Epoch: 11 [40960/123872 (33%)]\tLoss: 0.366761\n",
            "Train Epoch: 11 [41216/123872 (33%)]\tLoss: 0.363048\tLR: 0.00013717\n",
            "Train Epoch: 11 [41472/123872 (33%)]\tLoss: 0.310240\tLR: 0.00013709\n",
            "Train Epoch: 11 [41728/123872 (34%)]\tLoss: 0.351433\tLR: 0.00013701\n",
            "Train Epoch: 11 [41984/123872 (34%)]\tLoss: 0.367728\tLR: 0.00013692\n",
            "Train Epoch: 11 [42240/123872 (34%)]\tLoss: 0.359056\tLR: 0.00013684\n",
            "Train Epoch: 11 [42496/123872 (34%)]\tLoss: 0.350503\tLR: 0.00013676\n",
            "Train Epoch: 11 [42752/123872 (35%)]\tLoss: 0.328918\tLR: 0.00013668\n",
            "Train Epoch: 11 [43008/123872 (35%)]\tLoss: 0.403460\tLR: 0.00013659\n",
            "Train Epoch: 11 [43264/123872 (35%)]\tLoss: 0.305613\tLR: 0.00013651\n",
            "Train Epoch: 11 [43520/123872 (35%)]\tLoss: 0.392028\tLR: 0.00013643\n",
            "Train Epoch: 11 [43520/123872 (35%)]\tLoss: 0.392028\n",
            "Train Epoch: 11 [43776/123872 (35%)]\tLoss: 0.391969\tLR: 0.00013635\n",
            "Train Epoch: 11 [44032/123872 (36%)]\tLoss: 0.309439\tLR: 0.00013627\n",
            "Train Epoch: 11 [44288/123872 (36%)]\tLoss: 0.345619\tLR: 0.00013618\n",
            "Train Epoch: 11 [44544/123872 (36%)]\tLoss: 0.400765\tLR: 0.00013610\n",
            "Train Epoch: 11 [44800/123872 (36%)]\tLoss: 0.344419\tLR: 0.00013602\n",
            "Train Epoch: 11 [45056/123872 (36%)]\tLoss: 0.422531\tLR: 0.00013594\n",
            "Train Epoch: 11 [45312/123872 (37%)]\tLoss: 0.359281\tLR: 0.00013585\n",
            "Train Epoch: 11 [45568/123872 (37%)]\tLoss: 0.304419\tLR: 0.00013577\n",
            "Train Epoch: 11 [45824/123872 (37%)]\tLoss: 0.390739\tLR: 0.00013569\n",
            "Train Epoch: 11 [46080/123872 (37%)]\tLoss: 0.385686\tLR: 0.00013561\n",
            "Train Epoch: 11 [46080/123872 (37%)]\tLoss: 0.385686\n",
            "Train Epoch: 11 [46336/123872 (37%)]\tLoss: 0.334885\tLR: 0.00013553\n",
            "Train Epoch: 11 [46592/123872 (38%)]\tLoss: 0.325213\tLR: 0.00013545\n",
            "Train Epoch: 11 [46848/123872 (38%)]\tLoss: 0.374633\tLR: 0.00013536\n",
            "Train Epoch: 11 [47104/123872 (38%)]\tLoss: 0.295642\tLR: 0.00013528\n",
            "Train Epoch: 11 [47360/123872 (38%)]\tLoss: 0.340008\tLR: 0.00013520\n",
            "Train Epoch: 11 [47616/123872 (38%)]\tLoss: 0.375542\tLR: 0.00013512\n",
            "Train Epoch: 11 [47872/123872 (39%)]\tLoss: 0.347120\tLR: 0.00013504\n",
            "Train Epoch: 11 [48128/123872 (39%)]\tLoss: 0.359430\tLR: 0.00013495\n",
            "Train Epoch: 11 [48384/123872 (39%)]\tLoss: 0.367570\tLR: 0.00013487\n",
            "Train Epoch: 11 [48640/123872 (39%)]\tLoss: 0.331595\tLR: 0.00013479\n",
            "Train Epoch: 11 [48640/123872 (39%)]\tLoss: 0.331595\n",
            "Train Epoch: 11 [48896/123872 (39%)]\tLoss: 0.395964\tLR: 0.00013471\n",
            "Train Epoch: 11 [49152/123872 (40%)]\tLoss: 0.310262\tLR: 0.00013463\n",
            "Train Epoch: 11 [49408/123872 (40%)]\tLoss: 0.382147\tLR: 0.00013455\n",
            "Train Epoch: 11 [49664/123872 (40%)]\tLoss: 0.302832\tLR: 0.00013447\n",
            "Train Epoch: 11 [49920/123872 (40%)]\tLoss: 0.366071\tLR: 0.00013438\n",
            "Train Epoch: 11 [50176/123872 (40%)]\tLoss: 0.335654\tLR: 0.00013430\n",
            "Train Epoch: 11 [50432/123872 (41%)]\tLoss: 0.325052\tLR: 0.00013422\n",
            "Train Epoch: 11 [50688/123872 (41%)]\tLoss: 0.342485\tLR: 0.00013414\n",
            "Train Epoch: 11 [50944/123872 (41%)]\tLoss: 0.418388\tLR: 0.00013406\n",
            "Train Epoch: 11 [51200/123872 (41%)]\tLoss: 0.431573\tLR: 0.00013398\n",
            "Train Epoch: 11 [51200/123872 (41%)]\tLoss: 0.431573\n",
            "Train Epoch: 11 [51456/123872 (42%)]\tLoss: 0.346089\tLR: 0.00013390\n",
            "Train Epoch: 11 [51712/123872 (42%)]\tLoss: 0.310616\tLR: 0.00013382\n",
            "Train Epoch: 11 [51968/123872 (42%)]\tLoss: 0.325417\tLR: 0.00013374\n",
            "Train Epoch: 11 [52224/123872 (42%)]\tLoss: 0.357172\tLR: 0.00013365\n",
            "Train Epoch: 11 [52480/123872 (42%)]\tLoss: 0.376370\tLR: 0.00013357\n",
            "Train Epoch: 11 [52736/123872 (43%)]\tLoss: 0.375550\tLR: 0.00013349\n",
            "Train Epoch: 11 [52992/123872 (43%)]\tLoss: 0.342814\tLR: 0.00013341\n",
            "Train Epoch: 11 [53248/123872 (43%)]\tLoss: 0.365313\tLR: 0.00013333\n",
            "Train Epoch: 11 [53504/123872 (43%)]\tLoss: 0.326467\tLR: 0.00013325\n",
            "Train Epoch: 11 [53760/123872 (43%)]\tLoss: 0.301757\tLR: 0.00013317\n",
            "Train Epoch: 11 [53760/123872 (43%)]\tLoss: 0.301757\n",
            "Train Epoch: 11 [54016/123872 (44%)]\tLoss: 0.314960\tLR: 0.00013309\n",
            "Train Epoch: 11 [54272/123872 (44%)]\tLoss: 0.309321\tLR: 0.00013301\n",
            "Train Epoch: 11 [54528/123872 (44%)]\tLoss: 0.378221\tLR: 0.00013293\n",
            "Train Epoch: 11 [54784/123872 (44%)]\tLoss: 0.433538\tLR: 0.00013285\n",
            "Train Epoch: 11 [55040/123872 (44%)]\tLoss: 0.324749\tLR: 0.00013277\n",
            "Train Epoch: 11 [55296/123872 (45%)]\tLoss: 0.349838\tLR: 0.00013268\n",
            "Train Epoch: 11 [55552/123872 (45%)]\tLoss: 0.366254\tLR: 0.00013260\n",
            "Train Epoch: 11 [55808/123872 (45%)]\tLoss: 0.337071\tLR: 0.00013252\n",
            "Train Epoch: 11 [56064/123872 (45%)]\tLoss: 0.334429\tLR: 0.00013244\n",
            "Train Epoch: 11 [56320/123872 (45%)]\tLoss: 0.359963\tLR: 0.00013236\n",
            "Train Epoch: 11 [56320/123872 (45%)]\tLoss: 0.359963\n",
            "Train Epoch: 11 [56576/123872 (46%)]\tLoss: 0.347043\tLR: 0.00013228\n",
            "Train Epoch: 11 [56832/123872 (46%)]\tLoss: 0.382182\tLR: 0.00013220\n",
            "Train Epoch: 11 [57088/123872 (46%)]\tLoss: 0.323025\tLR: 0.00013212\n",
            "Train Epoch: 11 [57344/123872 (46%)]\tLoss: 0.309895\tLR: 0.00013204\n",
            "Train Epoch: 11 [57600/123872 (46%)]\tLoss: 0.430823\tLR: 0.00013196\n",
            "Train Epoch: 11 [57856/123872 (47%)]\tLoss: 0.355119\tLR: 0.00013188\n",
            "Train Epoch: 11 [58112/123872 (47%)]\tLoss: 0.312826\tLR: 0.00013180\n",
            "Train Epoch: 11 [58368/123872 (47%)]\tLoss: 0.400357\tLR: 0.00013172\n",
            "Train Epoch: 11 [58624/123872 (47%)]\tLoss: 0.316541\tLR: 0.00013164\n",
            "Train Epoch: 11 [58880/123872 (48%)]\tLoss: 0.349970\tLR: 0.00013156\n",
            "Train Epoch: 11 [58880/123872 (48%)]\tLoss: 0.349970\n",
            "Train Epoch: 11 [59136/123872 (48%)]\tLoss: 0.350524\tLR: 0.00013148\n",
            "Train Epoch: 11 [59392/123872 (48%)]\tLoss: 0.341329\tLR: 0.00013140\n",
            "Train Epoch: 11 [59648/123872 (48%)]\tLoss: 0.372824\tLR: 0.00013132\n",
            "Train Epoch: 11 [59904/123872 (48%)]\tLoss: 0.384157\tLR: 0.00013124\n",
            "Train Epoch: 11 [60160/123872 (49%)]\tLoss: 0.371733\tLR: 0.00013116\n",
            "Train Epoch: 11 [60416/123872 (49%)]\tLoss: 0.345512\tLR: 0.00013108\n",
            "Train Epoch: 11 [60672/123872 (49%)]\tLoss: 0.329517\tLR: 0.00013100\n",
            "Train Epoch: 11 [60928/123872 (49%)]\tLoss: 0.361662\tLR: 0.00013092\n",
            "Train Epoch: 11 [61184/123872 (49%)]\tLoss: 0.373580\tLR: 0.00013084\n",
            "Train Epoch: 11 [61440/123872 (50%)]\tLoss: 0.366144\tLR: 0.00013076\n",
            "Train Epoch: 11 [61440/123872 (50%)]\tLoss: 0.366144\n",
            "Train Epoch: 11 [61696/123872 (50%)]\tLoss: 0.310825\tLR: 0.00013068\n",
            "Train Epoch: 11 [61952/123872 (50%)]\tLoss: 0.373991\tLR: 0.00013060\n",
            "Train Epoch: 11 [62208/123872 (50%)]\tLoss: 0.349498\tLR: 0.00013052\n",
            "Train Epoch: 11 [62464/123872 (50%)]\tLoss: 0.382985\tLR: 0.00013044\n",
            "Train Epoch: 11 [62720/123872 (51%)]\tLoss: 0.361957\tLR: 0.00013036\n",
            "Train Epoch: 11 [62976/123872 (51%)]\tLoss: 0.341191\tLR: 0.00013029\n",
            "Train Epoch: 11 [63232/123872 (51%)]\tLoss: 0.363648\tLR: 0.00013021\n",
            "Train Epoch: 11 [63488/123872 (51%)]\tLoss: 0.318370\tLR: 0.00013013\n",
            "Train Epoch: 11 [63744/123872 (51%)]\tLoss: 0.406419\tLR: 0.00013005\n",
            "Train Epoch: 11 [64000/123872 (52%)]\tLoss: 0.346988\tLR: 0.00012997\n",
            "Train Epoch: 11 [64000/123872 (52%)]\tLoss: 0.346988\n",
            "Train Epoch: 11 [64256/123872 (52%)]\tLoss: 0.407207\tLR: 0.00012989\n",
            "Train Epoch: 11 [64512/123872 (52%)]\tLoss: 0.379013\tLR: 0.00012981\n",
            "Train Epoch: 11 [64768/123872 (52%)]\tLoss: 0.309888\tLR: 0.00012973\n",
            "Train Epoch: 11 [65024/123872 (52%)]\tLoss: 0.310830\tLR: 0.00012965\n",
            "Train Epoch: 11 [65280/123872 (53%)]\tLoss: 0.349782\tLR: 0.00012957\n",
            "Train Epoch: 11 [65536/123872 (53%)]\tLoss: 0.397675\tLR: 0.00012949\n",
            "Train Epoch: 11 [65792/123872 (53%)]\tLoss: 0.363523\tLR: 0.00012941\n",
            "Train Epoch: 11 [66048/123872 (53%)]\tLoss: 0.433275\tLR: 0.00012934\n",
            "Train Epoch: 11 [66304/123872 (54%)]\tLoss: 0.294196\tLR: 0.00012926\n",
            "Train Epoch: 11 [66560/123872 (54%)]\tLoss: 0.317813\tLR: 0.00012918\n",
            "Train Epoch: 11 [66560/123872 (54%)]\tLoss: 0.317813\n",
            "Train Epoch: 11 [66816/123872 (54%)]\tLoss: 0.314630\tLR: 0.00012910\n",
            "Train Epoch: 11 [67072/123872 (54%)]\tLoss: 0.399431\tLR: 0.00012902\n",
            "Train Epoch: 11 [67328/123872 (54%)]\tLoss: 0.398456\tLR: 0.00012894\n",
            "Train Epoch: 11 [67584/123872 (55%)]\tLoss: 0.358492\tLR: 0.00012886\n",
            "Train Epoch: 11 [67840/123872 (55%)]\tLoss: 0.345234\tLR: 0.00012878\n",
            "Train Epoch: 11 [68096/123872 (55%)]\tLoss: 0.285646\tLR: 0.00012870\n",
            "Train Epoch: 11 [68352/123872 (55%)]\tLoss: 0.359359\tLR: 0.00012863\n",
            "Train Epoch: 11 [68608/123872 (55%)]\tLoss: 0.336907\tLR: 0.00012855\n",
            "Train Epoch: 11 [68864/123872 (56%)]\tLoss: 0.316626\tLR: 0.00012847\n",
            "Train Epoch: 11 [69120/123872 (56%)]\tLoss: 0.394562\tLR: 0.00012839\n",
            "Train Epoch: 11 [69120/123872 (56%)]\tLoss: 0.394562\n",
            "Train Epoch: 11 [69376/123872 (56%)]\tLoss: 0.337500\tLR: 0.00012831\n",
            "Train Epoch: 11 [69632/123872 (56%)]\tLoss: 0.334448\tLR: 0.00012823\n",
            "Train Epoch: 11 [69888/123872 (56%)]\tLoss: 0.432074\tLR: 0.00012816\n",
            "Train Epoch: 11 [70144/123872 (57%)]\tLoss: 0.354150\tLR: 0.00012808\n",
            "Train Epoch: 11 [70400/123872 (57%)]\tLoss: 0.379212\tLR: 0.00012800\n",
            "Train Epoch: 11 [70656/123872 (57%)]\tLoss: 0.369372\tLR: 0.00012792\n",
            "Train Epoch: 11 [70912/123872 (57%)]\tLoss: 0.346723\tLR: 0.00012784\n",
            "Train Epoch: 11 [71168/123872 (57%)]\tLoss: 0.317568\tLR: 0.00012776\n",
            "Train Epoch: 11 [71424/123872 (58%)]\tLoss: 0.364656\tLR: 0.00012769\n",
            "Train Epoch: 11 [71680/123872 (58%)]\tLoss: 0.329801\tLR: 0.00012761\n",
            "Train Epoch: 11 [71680/123872 (58%)]\tLoss: 0.329801\n",
            "Train Epoch: 11 [71936/123872 (58%)]\tLoss: 0.410168\tLR: 0.00012753\n",
            "Train Epoch: 11 [72192/123872 (58%)]\tLoss: 0.353050\tLR: 0.00012745\n",
            "Train Epoch: 11 [72448/123872 (58%)]\tLoss: 0.324568\tLR: 0.00012737\n",
            "Train Epoch: 11 [72704/123872 (59%)]\tLoss: 0.398145\tLR: 0.00012730\n",
            "Train Epoch: 11 [72960/123872 (59%)]\tLoss: 0.314736\tLR: 0.00012722\n",
            "Train Epoch: 11 [73216/123872 (59%)]\tLoss: 0.362428\tLR: 0.00012714\n",
            "Train Epoch: 11 [73472/123872 (59%)]\tLoss: 0.309539\tLR: 0.00012706\n",
            "Train Epoch: 11 [73728/123872 (60%)]\tLoss: 0.335617\tLR: 0.00012698\n",
            "Train Epoch: 11 [73984/123872 (60%)]\tLoss: 0.294225\tLR: 0.00012691\n",
            "Train Epoch: 11 [74240/123872 (60%)]\tLoss: 0.324279\tLR: 0.00012683\n",
            "Train Epoch: 11 [74240/123872 (60%)]\tLoss: 0.324279\n",
            "Train Epoch: 11 [74496/123872 (60%)]\tLoss: 0.410586\tLR: 0.00012675\n",
            "Train Epoch: 11 [74752/123872 (60%)]\tLoss: 0.365282\tLR: 0.00012667\n",
            "Train Epoch: 11 [75008/123872 (61%)]\tLoss: 0.333459\tLR: 0.00012660\n",
            "Train Epoch: 11 [75264/123872 (61%)]\tLoss: 0.347353\tLR: 0.00012652\n",
            "Train Epoch: 11 [75520/123872 (61%)]\tLoss: 0.329124\tLR: 0.00012644\n",
            "Train Epoch: 11 [75776/123872 (61%)]\tLoss: 0.420512\tLR: 0.00012636\n",
            "Train Epoch: 11 [76032/123872 (61%)]\tLoss: 0.332571\tLR: 0.00012629\n",
            "Train Epoch: 11 [76288/123872 (62%)]\tLoss: 0.283803\tLR: 0.00012621\n",
            "Train Epoch: 11 [76544/123872 (62%)]\tLoss: 0.350121\tLR: 0.00012613\n",
            "Train Epoch: 11 [76800/123872 (62%)]\tLoss: 0.355134\tLR: 0.00012605\n",
            "Train Epoch: 11 [76800/123872 (62%)]\tLoss: 0.355134\n",
            "Train Epoch: 11 [77056/123872 (62%)]\tLoss: 0.358428\tLR: 0.00012598\n",
            "Train Epoch: 11 [77312/123872 (62%)]\tLoss: 0.338211\tLR: 0.00012590\n",
            "Train Epoch: 11 [77568/123872 (63%)]\tLoss: 0.355982\tLR: 0.00012582\n",
            "Train Epoch: 11 [77824/123872 (63%)]\tLoss: 0.340224\tLR: 0.00012574\n",
            "Train Epoch: 11 [78080/123872 (63%)]\tLoss: 0.325767\tLR: 0.00012567\n",
            "Train Epoch: 11 [78336/123872 (63%)]\tLoss: 0.403778\tLR: 0.00012559\n",
            "Train Epoch: 11 [78592/123872 (63%)]\tLoss: 0.309937\tLR: 0.00012551\n",
            "Train Epoch: 11 [78848/123872 (64%)]\tLoss: 0.314120\tLR: 0.00012544\n",
            "Train Epoch: 11 [79104/123872 (64%)]\tLoss: 0.358401\tLR: 0.00012536\n",
            "Train Epoch: 11 [79360/123872 (64%)]\tLoss: 0.320549\tLR: 0.00012528\n",
            "Train Epoch: 11 [79360/123872 (64%)]\tLoss: 0.320549\n",
            "Train Epoch: 11 [79616/123872 (64%)]\tLoss: 0.366252\tLR: 0.00012520\n",
            "Train Epoch: 11 [79872/123872 (64%)]\tLoss: 0.367521\tLR: 0.00012513\n",
            "Train Epoch: 11 [80128/123872 (65%)]\tLoss: 0.333404\tLR: 0.00012505\n",
            "Train Epoch: 11 [80384/123872 (65%)]\tLoss: 0.397337\tLR: 0.00012497\n",
            "Train Epoch: 11 [80640/123872 (65%)]\tLoss: 0.283710\tLR: 0.00012490\n",
            "Train Epoch: 11 [80896/123872 (65%)]\tLoss: 0.347991\tLR: 0.00012482\n",
            "Train Epoch: 11 [81152/123872 (65%)]\tLoss: 0.315924\tLR: 0.00012474\n",
            "Train Epoch: 11 [81408/123872 (66%)]\tLoss: 0.360316\tLR: 0.00012467\n",
            "Train Epoch: 11 [81664/123872 (66%)]\tLoss: 0.374651\tLR: 0.00012459\n",
            "Train Epoch: 11 [81920/123872 (66%)]\tLoss: 0.325609\tLR: 0.00012451\n",
            "Train Epoch: 11 [81920/123872 (66%)]\tLoss: 0.325609\n",
            "Train Epoch: 11 [82176/123872 (66%)]\tLoss: 0.342789\tLR: 0.00012444\n",
            "Train Epoch: 11 [82432/123872 (67%)]\tLoss: 0.348643\tLR: 0.00012436\n",
            "Train Epoch: 11 [82688/123872 (67%)]\tLoss: 0.363722\tLR: 0.00012428\n",
            "Train Epoch: 11 [82944/123872 (67%)]\tLoss: 0.384779\tLR: 0.00012421\n",
            "Train Epoch: 11 [83200/123872 (67%)]\tLoss: 0.387507\tLR: 0.00012413\n",
            "Train Epoch: 11 [83456/123872 (67%)]\tLoss: 0.350857\tLR: 0.00012406\n",
            "Train Epoch: 11 [83712/123872 (68%)]\tLoss: 0.305626\tLR: 0.00012398\n",
            "Train Epoch: 11 [83968/123872 (68%)]\tLoss: 0.442615\tLR: 0.00012390\n",
            "Train Epoch: 11 [84224/123872 (68%)]\tLoss: 0.389388\tLR: 0.00012383\n",
            "Train Epoch: 11 [84480/123872 (68%)]\tLoss: 0.286568\tLR: 0.00012375\n",
            "Train Epoch: 11 [84480/123872 (68%)]\tLoss: 0.286568\n",
            "Train Epoch: 11 [84736/123872 (68%)]\tLoss: 0.334613\tLR: 0.00012367\n",
            "Train Epoch: 11 [84992/123872 (69%)]\tLoss: 0.376159\tLR: 0.00012360\n",
            "Train Epoch: 11 [85248/123872 (69%)]\tLoss: 0.303173\tLR: 0.00012352\n",
            "Train Epoch: 11 [85504/123872 (69%)]\tLoss: 0.361576\tLR: 0.00012345\n",
            "Train Epoch: 11 [85760/123872 (69%)]\tLoss: 0.351292\tLR: 0.00012337\n",
            "Train Epoch: 11 [86016/123872 (69%)]\tLoss: 0.344202\tLR: 0.00012329\n",
            "Train Epoch: 11 [86272/123872 (70%)]\tLoss: 0.357790\tLR: 0.00012322\n",
            "Train Epoch: 11 [86528/123872 (70%)]\tLoss: 0.399974\tLR: 0.00012314\n",
            "Train Epoch: 11 [86784/123872 (70%)]\tLoss: 0.343777\tLR: 0.00012307\n",
            "Train Epoch: 11 [87040/123872 (70%)]\tLoss: 0.384649\tLR: 0.00012299\n",
            "Train Epoch: 11 [87040/123872 (70%)]\tLoss: 0.384649\n",
            "Train Epoch: 11 [87296/123872 (70%)]\tLoss: 0.330419\tLR: 0.00012292\n",
            "Train Epoch: 11 [87552/123872 (71%)]\tLoss: 0.366848\tLR: 0.00012284\n",
            "Train Epoch: 11 [87808/123872 (71%)]\tLoss: 0.332749\tLR: 0.00012276\n",
            "Train Epoch: 11 [88064/123872 (71%)]\tLoss: 0.344352\tLR: 0.00012269\n",
            "Train Epoch: 11 [88320/123872 (71%)]\tLoss: 0.333802\tLR: 0.00012261\n",
            "Train Epoch: 11 [88576/123872 (71%)]\tLoss: 0.346950\tLR: 0.00012254\n",
            "Train Epoch: 11 [88832/123872 (72%)]\tLoss: 0.351375\tLR: 0.00012246\n",
            "Train Epoch: 11 [89088/123872 (72%)]\tLoss: 0.349290\tLR: 0.00012239\n",
            "Train Epoch: 11 [89344/123872 (72%)]\tLoss: 0.331255\tLR: 0.00012231\n",
            "Train Epoch: 11 [89600/123872 (72%)]\tLoss: 0.320618\tLR: 0.00012224\n",
            "Train Epoch: 11 [89600/123872 (72%)]\tLoss: 0.320618\n",
            "Train Epoch: 11 [89856/123872 (73%)]\tLoss: 0.340691\tLR: 0.00012216\n",
            "Train Epoch: 11 [90112/123872 (73%)]\tLoss: 0.334832\tLR: 0.00012208\n",
            "Train Epoch: 11 [90368/123872 (73%)]\tLoss: 0.316928\tLR: 0.00012201\n",
            "Train Epoch: 11 [90624/123872 (73%)]\tLoss: 0.405360\tLR: 0.00012193\n",
            "Train Epoch: 11 [90880/123872 (73%)]\tLoss: 0.301554\tLR: 0.00012186\n",
            "Train Epoch: 11 [91136/123872 (74%)]\tLoss: 0.358020\tLR: 0.00012178\n",
            "Train Epoch: 11 [91392/123872 (74%)]\tLoss: 0.300288\tLR: 0.00012171\n",
            "Train Epoch: 11 [91648/123872 (74%)]\tLoss: 0.283481\tLR: 0.00012163\n",
            "Train Epoch: 11 [91904/123872 (74%)]\tLoss: 0.285039\tLR: 0.00012156\n",
            "Train Epoch: 11 [92160/123872 (74%)]\tLoss: 0.320992\tLR: 0.00012148\n",
            "Train Epoch: 11 [92160/123872 (74%)]\tLoss: 0.320992\n",
            "Train Epoch: 11 [92416/123872 (75%)]\tLoss: 0.336721\tLR: 0.00012141\n",
            "Train Epoch: 11 [92672/123872 (75%)]\tLoss: 0.329828\tLR: 0.00012133\n",
            "Train Epoch: 11 [92928/123872 (75%)]\tLoss: 0.416103\tLR: 0.00012126\n",
            "Train Epoch: 11 [93184/123872 (75%)]\tLoss: 0.311820\tLR: 0.00012118\n",
            "Train Epoch: 11 [93440/123872 (75%)]\tLoss: 0.411766\tLR: 0.00012111\n",
            "Train Epoch: 11 [93696/123872 (76%)]\tLoss: 0.367692\tLR: 0.00012103\n",
            "Train Epoch: 11 [93952/123872 (76%)]\tLoss: 0.391218\tLR: 0.00012096\n",
            "Train Epoch: 11 [94208/123872 (76%)]\tLoss: 0.339244\tLR: 0.00012089\n",
            "Train Epoch: 11 [94464/123872 (76%)]\tLoss: 0.319071\tLR: 0.00012081\n",
            "Train Epoch: 11 [94720/123872 (76%)]\tLoss: 0.317378\tLR: 0.00012074\n",
            "Train Epoch: 11 [94720/123872 (76%)]\tLoss: 0.317378\n",
            "Train Epoch: 11 [94976/123872 (77%)]\tLoss: 0.384630\tLR: 0.00012066\n",
            "Train Epoch: 11 [95232/123872 (77%)]\tLoss: 0.352248\tLR: 0.00012059\n",
            "Train Epoch: 11 [95488/123872 (77%)]\tLoss: 0.370974\tLR: 0.00012051\n",
            "Train Epoch: 11 [95744/123872 (77%)]\tLoss: 0.347062\tLR: 0.00012044\n",
            "Train Epoch: 11 [96000/123872 (77%)]\tLoss: 0.362603\tLR: 0.00012036\n",
            "Train Epoch: 11 [96256/123872 (78%)]\tLoss: 0.389729\tLR: 0.00012029\n",
            "Train Epoch: 11 [96512/123872 (78%)]\tLoss: 0.369989\tLR: 0.00012022\n",
            "Train Epoch: 11 [96768/123872 (78%)]\tLoss: 0.355844\tLR: 0.00012014\n",
            "Train Epoch: 11 [97024/123872 (78%)]\tLoss: 0.337673\tLR: 0.00012007\n",
            "Train Epoch: 11 [97280/123872 (79%)]\tLoss: 0.322209\tLR: 0.00011999\n",
            "Train Epoch: 11 [97280/123872 (79%)]\tLoss: 0.322209\n",
            "Train Epoch: 11 [97536/123872 (79%)]\tLoss: 0.466904\tLR: 0.00011992\n",
            "Train Epoch: 11 [97792/123872 (79%)]\tLoss: 0.314491\tLR: 0.00011984\n",
            "Train Epoch: 11 [98048/123872 (79%)]\tLoss: 0.347439\tLR: 0.00011977\n",
            "Train Epoch: 11 [98304/123872 (79%)]\tLoss: 0.364871\tLR: 0.00011970\n",
            "Train Epoch: 11 [98560/123872 (80%)]\tLoss: 0.348050\tLR: 0.00011962\n",
            "Train Epoch: 11 [98816/123872 (80%)]\tLoss: 0.316139\tLR: 0.00011955\n",
            "Train Epoch: 11 [99072/123872 (80%)]\tLoss: 0.382246\tLR: 0.00011947\n",
            "Train Epoch: 11 [99328/123872 (80%)]\tLoss: 0.348885\tLR: 0.00011940\n",
            "Train Epoch: 11 [99584/123872 (80%)]\tLoss: 0.364661\tLR: 0.00011933\n",
            "Train Epoch: 11 [99840/123872 (81%)]\tLoss: 0.358641\tLR: 0.00011925\n",
            "Train Epoch: 11 [99840/123872 (81%)]\tLoss: 0.358641\n",
            "Train Epoch: 11 [100096/123872 (81%)]\tLoss: 0.322613\tLR: 0.00011918\n",
            "Train Epoch: 11 [100352/123872 (81%)]\tLoss: 0.356448\tLR: 0.00011911\n",
            "Train Epoch: 11 [100608/123872 (81%)]\tLoss: 0.341641\tLR: 0.00011903\n",
            "Train Epoch: 11 [100864/123872 (81%)]\tLoss: 0.373881\tLR: 0.00011896\n",
            "Train Epoch: 11 [101120/123872 (82%)]\tLoss: 0.383004\tLR: 0.00011888\n",
            "Train Epoch: 11 [101376/123872 (82%)]\tLoss: 0.337077\tLR: 0.00011881\n",
            "Train Epoch: 11 [101632/123872 (82%)]\tLoss: 0.362145\tLR: 0.00011874\n",
            "Train Epoch: 11 [101888/123872 (82%)]\tLoss: 0.373277\tLR: 0.00011866\n",
            "Train Epoch: 11 [102144/123872 (82%)]\tLoss: 0.417302\tLR: 0.00011859\n",
            "Train Epoch: 11 [102400/123872 (83%)]\tLoss: 0.335111\tLR: 0.00011852\n",
            "Train Epoch: 11 [102400/123872 (83%)]\tLoss: 0.335111\n",
            "Train Epoch: 11 [102656/123872 (83%)]\tLoss: 0.336599\tLR: 0.00011844\n",
            "Train Epoch: 11 [102912/123872 (83%)]\tLoss: 0.365412\tLR: 0.00011837\n",
            "Train Epoch: 11 [103168/123872 (83%)]\tLoss: 0.308134\tLR: 0.00011830\n",
            "Train Epoch: 11 [103424/123872 (83%)]\tLoss: 0.318126\tLR: 0.00011822\n",
            "Train Epoch: 11 [103680/123872 (84%)]\tLoss: 0.408898\tLR: 0.00011815\n",
            "Train Epoch: 11 [103936/123872 (84%)]\tLoss: 0.338928\tLR: 0.00011808\n",
            "Train Epoch: 11 [104192/123872 (84%)]\tLoss: 0.277330\tLR: 0.00011800\n",
            "Train Epoch: 11 [104448/123872 (84%)]\tLoss: 0.305073\tLR: 0.00011793\n",
            "Train Epoch: 11 [104704/123872 (85%)]\tLoss: 0.358370\tLR: 0.00011786\n",
            "Train Epoch: 11 [104960/123872 (85%)]\tLoss: 0.360450\tLR: 0.00011779\n",
            "Train Epoch: 11 [104960/123872 (85%)]\tLoss: 0.360450\n",
            "Train Epoch: 11 [105216/123872 (85%)]\tLoss: 0.438800\tLR: 0.00011771\n",
            "Train Epoch: 11 [105472/123872 (85%)]\tLoss: 0.360830\tLR: 0.00011764\n",
            "Train Epoch: 11 [105728/123872 (85%)]\tLoss: 0.368579\tLR: 0.00011757\n",
            "Train Epoch: 11 [105984/123872 (86%)]\tLoss: 0.335997\tLR: 0.00011749\n",
            "Train Epoch: 11 [106240/123872 (86%)]\tLoss: 0.397205\tLR: 0.00011742\n",
            "Train Epoch: 11 [106496/123872 (86%)]\tLoss: 0.352243\tLR: 0.00011735\n",
            "Train Epoch: 11 [106752/123872 (86%)]\tLoss: 0.331111\tLR: 0.00011728\n",
            "Train Epoch: 11 [107008/123872 (86%)]\tLoss: 0.257294\tLR: 0.00011720\n",
            "Train Epoch: 11 [107264/123872 (87%)]\tLoss: 0.358660\tLR: 0.00011713\n",
            "Train Epoch: 11 [107520/123872 (87%)]\tLoss: 0.342755\tLR: 0.00011706\n",
            "Train Epoch: 11 [107520/123872 (87%)]\tLoss: 0.342755\n",
            "Train Epoch: 11 [107776/123872 (87%)]\tLoss: 0.379272\tLR: 0.00011699\n",
            "Train Epoch: 11 [108032/123872 (87%)]\tLoss: 0.367420\tLR: 0.00011691\n",
            "Train Epoch: 11 [108288/123872 (87%)]\tLoss: 0.311973\tLR: 0.00011684\n",
            "Train Epoch: 11 [108544/123872 (88%)]\tLoss: 0.319945\tLR: 0.00011677\n",
            "Train Epoch: 11 [108800/123872 (88%)]\tLoss: 0.319639\tLR: 0.00011670\n",
            "Train Epoch: 11 [109056/123872 (88%)]\tLoss: 0.311622\tLR: 0.00011662\n",
            "Train Epoch: 11 [109312/123872 (88%)]\tLoss: 0.347598\tLR: 0.00011655\n",
            "Train Epoch: 11 [109568/123872 (88%)]\tLoss: 0.319505\tLR: 0.00011648\n",
            "Train Epoch: 11 [109824/123872 (89%)]\tLoss: 0.312934\tLR: 0.00011641\n",
            "Train Epoch: 11 [110080/123872 (89%)]\tLoss: 0.411083\tLR: 0.00011633\n",
            "Train Epoch: 11 [110080/123872 (89%)]\tLoss: 0.411083\n",
            "Train Epoch: 11 [110336/123872 (89%)]\tLoss: 0.325181\tLR: 0.00011626\n",
            "Train Epoch: 11 [110592/123872 (89%)]\tLoss: 0.349365\tLR: 0.00011619\n",
            "Train Epoch: 11 [110848/123872 (89%)]\tLoss: 0.354090\tLR: 0.00011612\n",
            "Train Epoch: 11 [111104/123872 (90%)]\tLoss: 0.375143\tLR: 0.00011605\n",
            "Train Epoch: 11 [111360/123872 (90%)]\tLoss: 0.345652\tLR: 0.00011597\n",
            "Train Epoch: 11 [111616/123872 (90%)]\tLoss: 0.346439\tLR: 0.00011590\n",
            "Train Epoch: 11 [111872/123872 (90%)]\tLoss: 0.330625\tLR: 0.00011583\n",
            "Train Epoch: 11 [112128/123872 (90%)]\tLoss: 0.414621\tLR: 0.00011576\n",
            "Train Epoch: 11 [112384/123872 (91%)]\tLoss: 0.329731\tLR: 0.00011569\n",
            "Train Epoch: 11 [112640/123872 (91%)]\tLoss: 0.339195\tLR: 0.00011562\n",
            "Train Epoch: 11 [112640/123872 (91%)]\tLoss: 0.339195\n",
            "Train Epoch: 11 [112896/123872 (91%)]\tLoss: 0.378895\tLR: 0.00011554\n",
            "Train Epoch: 11 [113152/123872 (91%)]\tLoss: 0.348793\tLR: 0.00011547\n",
            "Train Epoch: 11 [113408/123872 (92%)]\tLoss: 0.323849\tLR: 0.00011540\n",
            "Train Epoch: 11 [113664/123872 (92%)]\tLoss: 0.388495\tLR: 0.00011533\n",
            "Train Epoch: 11 [113920/123872 (92%)]\tLoss: 0.345580\tLR: 0.00011526\n",
            "Train Epoch: 11 [114176/123872 (92%)]\tLoss: 0.358760\tLR: 0.00011519\n",
            "Train Epoch: 11 [114432/123872 (92%)]\tLoss: 0.356411\tLR: 0.00011511\n",
            "Train Epoch: 11 [114688/123872 (93%)]\tLoss: 0.302310\tLR: 0.00011504\n",
            "Train Epoch: 11 [114944/123872 (93%)]\tLoss: 0.404631\tLR: 0.00011497\n",
            "Train Epoch: 11 [115200/123872 (93%)]\tLoss: 0.353379\tLR: 0.00011490\n",
            "Train Epoch: 11 [115200/123872 (93%)]\tLoss: 0.353379\n",
            "Train Epoch: 11 [115456/123872 (93%)]\tLoss: 0.374286\tLR: 0.00011483\n",
            "Train Epoch: 11 [115712/123872 (93%)]\tLoss: 0.321521\tLR: 0.00011476\n",
            "Train Epoch: 11 [115968/123872 (94%)]\tLoss: 0.365643\tLR: 0.00011469\n",
            "Train Epoch: 11 [116224/123872 (94%)]\tLoss: 0.363500\tLR: 0.00011461\n",
            "Train Epoch: 11 [116480/123872 (94%)]\tLoss: 0.299795\tLR: 0.00011454\n",
            "Train Epoch: 11 [116736/123872 (94%)]\tLoss: 0.326755\tLR: 0.00011447\n",
            "Train Epoch: 11 [116992/123872 (94%)]\tLoss: 0.375042\tLR: 0.00011440\n",
            "Train Epoch: 11 [117248/123872 (95%)]\tLoss: 0.392735\tLR: 0.00011433\n",
            "Train Epoch: 11 [117504/123872 (95%)]\tLoss: 0.370399\tLR: 0.00011426\n",
            "Train Epoch: 11 [117760/123872 (95%)]\tLoss: 0.328754\tLR: 0.00011419\n",
            "Train Epoch: 11 [117760/123872 (95%)]\tLoss: 0.328754\n",
            "Train Epoch: 11 [118016/123872 (95%)]\tLoss: 0.357798\tLR: 0.00011412\n",
            "Train Epoch: 11 [118272/123872 (95%)]\tLoss: 0.368098\tLR: 0.00011405\n",
            "Train Epoch: 11 [118528/123872 (96%)]\tLoss: 0.369901\tLR: 0.00011398\n",
            "Train Epoch: 11 [118784/123872 (96%)]\tLoss: 0.321660\tLR: 0.00011391\n",
            "Train Epoch: 11 [119040/123872 (96%)]\tLoss: 0.325077\tLR: 0.00011383\n",
            "Train Epoch: 11 [119296/123872 (96%)]\tLoss: 0.392353\tLR: 0.00011376\n",
            "Train Epoch: 11 [119552/123872 (96%)]\tLoss: 0.318840\tLR: 0.00011369\n",
            "Train Epoch: 11 [119808/123872 (97%)]\tLoss: 0.338418\tLR: 0.00011362\n",
            "Train Epoch: 11 [120064/123872 (97%)]\tLoss: 0.371575\tLR: 0.00011355\n",
            "Train Epoch: 11 [120320/123872 (97%)]\tLoss: 0.359135\tLR: 0.00011348\n",
            "Train Epoch: 11 [120320/123872 (97%)]\tLoss: 0.359135\n",
            "Train Epoch: 11 [120576/123872 (97%)]\tLoss: 0.321923\tLR: 0.00011341\n",
            "Train Epoch: 11 [120832/123872 (98%)]\tLoss: 0.343313\tLR: 0.00011334\n",
            "Train Epoch: 11 [121088/123872 (98%)]\tLoss: 0.355414\tLR: 0.00011327\n",
            "Train Epoch: 11 [121344/123872 (98%)]\tLoss: 0.347756\tLR: 0.00011320\n",
            "Train Epoch: 11 [121600/123872 (98%)]\tLoss: 0.400422\tLR: 0.00011313\n",
            "Train Epoch: 11 [121856/123872 (98%)]\tLoss: 0.391617\tLR: 0.00011306\n",
            "Train Epoch: 11 [122112/123872 (99%)]\tLoss: 0.359791\tLR: 0.00011299\n",
            "Train Epoch: 11 [122368/123872 (99%)]\tLoss: 0.309272\tLR: 0.00011292\n",
            "Train Epoch: 11 [122624/123872 (99%)]\tLoss: 0.381796\tLR: 0.00011285\n",
            "Train Epoch: 11 [122880/123872 (99%)]\tLoss: 0.366211\tLR: 0.00011278\n",
            "Train Epoch: 11 [122880/123872 (99%)]\tLoss: 0.366211\n",
            "Train Epoch: 11 [123136/123872 (99%)]\tLoss: 0.303631\tLR: 0.00011271\n",
            "Train Epoch: 11 [123392/123872 (100%)]\tLoss: 0.354957\tLR: 0.00011264\n",
            "Train Epoch: 11 [108192/123872 (100%)]\tLoss: 0.338246\tLR: 0.00011257\n",
            "\n",
            "Test set: Average loss: 0.0015, Accuracy: 25656/30970 (82.84%)\n",
            "\n",
            "Train Epoch: 12 [0/123872 (0%)]\tLoss: 0.344788\tLR: 0.00011250\n",
            "Train Epoch: 12 [0/123872 (0%)]\tLoss: 0.344788\n",
            "Train Epoch: 12 [256/123872 (0%)]\tLoss: 0.344464\tLR: 0.00011243\n",
            "Train Epoch: 12 [512/123872 (0%)]\tLoss: 0.374204\tLR: 0.00011236\n",
            "Train Epoch: 12 [768/123872 (1%)]\tLoss: 0.367849\tLR: 0.00011229\n",
            "Train Epoch: 12 [1024/123872 (1%)]\tLoss: 0.339641\tLR: 0.00011222\n",
            "Train Epoch: 12 [1280/123872 (1%)]\tLoss: 0.396469\tLR: 0.00011215\n",
            "Train Epoch: 12 [1536/123872 (1%)]\tLoss: 0.314637\tLR: 0.00011208\n",
            "Train Epoch: 12 [1792/123872 (1%)]\tLoss: 0.331303\tLR: 0.00011201\n",
            "Train Epoch: 12 [2048/123872 (2%)]\tLoss: 0.347049\tLR: 0.00011194\n",
            "Train Epoch: 12 [2304/123872 (2%)]\tLoss: 0.328203\tLR: 0.00011187\n",
            "Train Epoch: 12 [2560/123872 (2%)]\tLoss: 0.339961\tLR: 0.00011180\n",
            "Train Epoch: 12 [2560/123872 (2%)]\tLoss: 0.339961\n",
            "Train Epoch: 12 [2816/123872 (2%)]\tLoss: 0.363003\tLR: 0.00011173\n",
            "Train Epoch: 12 [3072/123872 (2%)]\tLoss: 0.327128\tLR: 0.00011166\n",
            "Train Epoch: 12 [3328/123872 (3%)]\tLoss: 0.283055\tLR: 0.00011159\n",
            "Train Epoch: 12 [3584/123872 (3%)]\tLoss: 0.397681\tLR: 0.00011152\n",
            "Train Epoch: 12 [3840/123872 (3%)]\tLoss: 0.376633\tLR: 0.00011145\n",
            "Train Epoch: 12 [4096/123872 (3%)]\tLoss: 0.380531\tLR: 0.00011139\n",
            "Train Epoch: 12 [4352/123872 (4%)]\tLoss: 0.404037\tLR: 0.00011132\n",
            "Train Epoch: 12 [4608/123872 (4%)]\tLoss: 0.376799\tLR: 0.00011125\n",
            "Train Epoch: 12 [4864/123872 (4%)]\tLoss: 0.326073\tLR: 0.00011118\n",
            "Train Epoch: 12 [5120/123872 (4%)]\tLoss: 0.386651\tLR: 0.00011111\n",
            "Train Epoch: 12 [5120/123872 (4%)]\tLoss: 0.386651\n",
            "Train Epoch: 12 [5376/123872 (4%)]\tLoss: 0.363995\tLR: 0.00011104\n",
            "Train Epoch: 12 [5632/123872 (5%)]\tLoss: 0.355680\tLR: 0.00011097\n",
            "Train Epoch: 12 [5888/123872 (5%)]\tLoss: 0.375422\tLR: 0.00011090\n",
            "Train Epoch: 12 [6144/123872 (5%)]\tLoss: 0.367988\tLR: 0.00011083\n",
            "Train Epoch: 12 [6400/123872 (5%)]\tLoss: 0.316416\tLR: 0.00011076\n",
            "Train Epoch: 12 [6656/123872 (5%)]\tLoss: 0.320823\tLR: 0.00011069\n",
            "Train Epoch: 12 [6912/123872 (6%)]\tLoss: 0.361640\tLR: 0.00011063\n",
            "Train Epoch: 12 [7168/123872 (6%)]\tLoss: 0.322443\tLR: 0.00011056\n",
            "Train Epoch: 12 [7424/123872 (6%)]\tLoss: 0.338411\tLR: 0.00011049\n",
            "Train Epoch: 12 [7680/123872 (6%)]\tLoss: 0.385321\tLR: 0.00011042\n",
            "Train Epoch: 12 [7680/123872 (6%)]\tLoss: 0.385321\n",
            "Train Epoch: 12 [7936/123872 (6%)]\tLoss: 0.294263\tLR: 0.00011035\n",
            "Train Epoch: 12 [8192/123872 (7%)]\tLoss: 0.410990\tLR: 0.00011028\n",
            "Train Epoch: 12 [8448/123872 (7%)]\tLoss: 0.362118\tLR: 0.00011021\n",
            "Train Epoch: 12 [8704/123872 (7%)]\tLoss: 0.367171\tLR: 0.00011015\n",
            "Train Epoch: 12 [8960/123872 (7%)]\tLoss: 0.365980\tLR: 0.00011008\n",
            "Train Epoch: 12 [9216/123872 (7%)]\tLoss: 0.336577\tLR: 0.00011001\n",
            "Train Epoch: 12 [9472/123872 (8%)]\tLoss: 0.351729\tLR: 0.00010994\n",
            "Train Epoch: 12 [9728/123872 (8%)]\tLoss: 0.294117\tLR: 0.00010987\n",
            "Train Epoch: 12 [9984/123872 (8%)]\tLoss: 0.306445\tLR: 0.00010980\n",
            "Train Epoch: 12 [10240/123872 (8%)]\tLoss: 0.381088\tLR: 0.00010974\n",
            "Train Epoch: 12 [10240/123872 (8%)]\tLoss: 0.381088\n",
            "Train Epoch: 12 [10496/123872 (8%)]\tLoss: 0.335888\tLR: 0.00010967\n",
            "Train Epoch: 12 [10752/123872 (9%)]\tLoss: 0.352183\tLR: 0.00010960\n",
            "Train Epoch: 12 [11008/123872 (9%)]\tLoss: 0.307349\tLR: 0.00010953\n",
            "Train Epoch: 12 [11264/123872 (9%)]\tLoss: 0.398269\tLR: 0.00010946\n",
            "Train Epoch: 12 [11520/123872 (9%)]\tLoss: 0.291663\tLR: 0.00010939\n",
            "Train Epoch: 12 [11776/123872 (10%)]\tLoss: 0.389047\tLR: 0.00010933\n",
            "Train Epoch: 12 [12032/123872 (10%)]\tLoss: 0.312701\tLR: 0.00010926\n",
            "Train Epoch: 12 [12288/123872 (10%)]\tLoss: 0.359868\tLR: 0.00010919\n",
            "Train Epoch: 12 [12544/123872 (10%)]\tLoss: 0.296117\tLR: 0.00010912\n",
            "Train Epoch: 12 [12800/123872 (10%)]\tLoss: 0.311301\tLR: 0.00010905\n",
            "Train Epoch: 12 [12800/123872 (10%)]\tLoss: 0.311301\n",
            "Train Epoch: 12 [13056/123872 (11%)]\tLoss: 0.351368\tLR: 0.00010899\n",
            "Train Epoch: 12 [13312/123872 (11%)]\tLoss: 0.270819\tLR: 0.00010892\n",
            "Train Epoch: 12 [13568/123872 (11%)]\tLoss: 0.358740\tLR: 0.00010885\n",
            "Train Epoch: 12 [13824/123872 (11%)]\tLoss: 0.315496\tLR: 0.00010878\n",
            "Train Epoch: 12 [14080/123872 (11%)]\tLoss: 0.398301\tLR: 0.00010872\n",
            "Train Epoch: 12 [14336/123872 (12%)]\tLoss: 0.419428\tLR: 0.00010865\n",
            "Train Epoch: 12 [14592/123872 (12%)]\tLoss: 0.325828\tLR: 0.00010858\n",
            "Train Epoch: 12 [14848/123872 (12%)]\tLoss: 0.378544\tLR: 0.00010851\n",
            "Train Epoch: 12 [15104/123872 (12%)]\tLoss: 0.345284\tLR: 0.00010845\n",
            "Train Epoch: 12 [15360/123872 (12%)]\tLoss: 0.320318\tLR: 0.00010838\n",
            "Train Epoch: 12 [15360/123872 (12%)]\tLoss: 0.320318\n",
            "Train Epoch: 12 [15616/123872 (13%)]\tLoss: 0.321751\tLR: 0.00010831\n",
            "Train Epoch: 12 [15872/123872 (13%)]\tLoss: 0.342094\tLR: 0.00010824\n",
            "Train Epoch: 12 [16128/123872 (13%)]\tLoss: 0.404777\tLR: 0.00010818\n",
            "Train Epoch: 12 [16384/123872 (13%)]\tLoss: 0.298836\tLR: 0.00010811\n",
            "Train Epoch: 12 [16640/123872 (13%)]\tLoss: 0.358143\tLR: 0.00010804\n",
            "Train Epoch: 12 [16896/123872 (14%)]\tLoss: 0.349501\tLR: 0.00010797\n",
            "Train Epoch: 12 [17152/123872 (14%)]\tLoss: 0.349930\tLR: 0.00010791\n",
            "Train Epoch: 12 [17408/123872 (14%)]\tLoss: 0.318589\tLR: 0.00010784\n",
            "Train Epoch: 12 [17664/123872 (14%)]\tLoss: 0.384973\tLR: 0.00010777\n",
            "Train Epoch: 12 [17920/123872 (14%)]\tLoss: 0.320163\tLR: 0.00010771\n",
            "Train Epoch: 12 [17920/123872 (14%)]\tLoss: 0.320163\n",
            "Train Epoch: 12 [18176/123872 (15%)]\tLoss: 0.290764\tLR: 0.00010764\n",
            "Train Epoch: 12 [18432/123872 (15%)]\tLoss: 0.335470\tLR: 0.00010757\n",
            "Train Epoch: 12 [18688/123872 (15%)]\tLoss: 0.367786\tLR: 0.00010751\n",
            "Train Epoch: 12 [18944/123872 (15%)]\tLoss: 0.344716\tLR: 0.00010744\n",
            "Train Epoch: 12 [19200/123872 (15%)]\tLoss: 0.331547\tLR: 0.00010737\n",
            "Train Epoch: 12 [19456/123872 (16%)]\tLoss: 0.405823\tLR: 0.00010731\n",
            "Train Epoch: 12 [19712/123872 (16%)]\tLoss: 0.409927\tLR: 0.00010724\n",
            "Train Epoch: 12 [19968/123872 (16%)]\tLoss: 0.319748\tLR: 0.00010717\n",
            "Train Epoch: 12 [20224/123872 (16%)]\tLoss: 0.346260\tLR: 0.00010711\n",
            "Train Epoch: 12 [20480/123872 (17%)]\tLoss: 0.328716\tLR: 0.00010704\n",
            "Train Epoch: 12 [20480/123872 (17%)]\tLoss: 0.328716\n",
            "Train Epoch: 12 [20736/123872 (17%)]\tLoss: 0.348686\tLR: 0.00010697\n",
            "Train Epoch: 12 [20992/123872 (17%)]\tLoss: 0.369849\tLR: 0.00010691\n",
            "Train Epoch: 12 [21248/123872 (17%)]\tLoss: 0.282311\tLR: 0.00010684\n",
            "Train Epoch: 12 [21504/123872 (17%)]\tLoss: 0.336773\tLR: 0.00010677\n",
            "Train Epoch: 12 [21760/123872 (18%)]\tLoss: 0.350079\tLR: 0.00010671\n",
            "Train Epoch: 12 [22016/123872 (18%)]\tLoss: 0.386645\tLR: 0.00010664\n",
            "Train Epoch: 12 [22272/123872 (18%)]\tLoss: 0.390473\tLR: 0.00010657\n",
            "Train Epoch: 12 [22528/123872 (18%)]\tLoss: 0.342733\tLR: 0.00010651\n",
            "Train Epoch: 12 [22784/123872 (18%)]\tLoss: 0.335814\tLR: 0.00010644\n",
            "Train Epoch: 12 [23040/123872 (19%)]\tLoss: 0.376990\tLR: 0.00010638\n",
            "Train Epoch: 12 [23040/123872 (19%)]\tLoss: 0.376990\n",
            "Train Epoch: 12 [23296/123872 (19%)]\tLoss: 0.326902\tLR: 0.00010631\n",
            "Train Epoch: 12 [23552/123872 (19%)]\tLoss: 0.331795\tLR: 0.00010624\n",
            "Train Epoch: 12 [23808/123872 (19%)]\tLoss: 0.308465\tLR: 0.00010618\n",
            "Train Epoch: 12 [24064/123872 (19%)]\tLoss: 0.453966\tLR: 0.00010611\n",
            "Train Epoch: 12 [24320/123872 (20%)]\tLoss: 0.357045\tLR: 0.00010605\n",
            "Train Epoch: 12 [24576/123872 (20%)]\tLoss: 0.355729\tLR: 0.00010598\n",
            "Train Epoch: 12 [24832/123872 (20%)]\tLoss: 0.384539\tLR: 0.00010591\n",
            "Train Epoch: 12 [25088/123872 (20%)]\tLoss: 0.334983\tLR: 0.00010585\n",
            "Train Epoch: 12 [25344/123872 (20%)]\tLoss: 0.350230\tLR: 0.00010578\n",
            "Train Epoch: 12 [25600/123872 (21%)]\tLoss: 0.329938\tLR: 0.00010572\n",
            "Train Epoch: 12 [25600/123872 (21%)]\tLoss: 0.329938\n",
            "Train Epoch: 12 [25856/123872 (21%)]\tLoss: 0.358121\tLR: 0.00010565\n",
            "Train Epoch: 12 [26112/123872 (21%)]\tLoss: 0.341316\tLR: 0.00010559\n",
            "Train Epoch: 12 [26368/123872 (21%)]\tLoss: 0.392588\tLR: 0.00010552\n",
            "Train Epoch: 12 [26624/123872 (21%)]\tLoss: 0.319943\tLR: 0.00010545\n",
            "Train Epoch: 12 [26880/123872 (22%)]\tLoss: 0.365395\tLR: 0.00010539\n",
            "Train Epoch: 12 [27136/123872 (22%)]\tLoss: 0.300289\tLR: 0.00010532\n",
            "Train Epoch: 12 [27392/123872 (22%)]\tLoss: 0.359390\tLR: 0.00010526\n",
            "Train Epoch: 12 [27648/123872 (22%)]\tLoss: 0.354857\tLR: 0.00010519\n",
            "Train Epoch: 12 [27904/123872 (23%)]\tLoss: 0.345670\tLR: 0.00010513\n",
            "Train Epoch: 12 [28160/123872 (23%)]\tLoss: 0.287031\tLR: 0.00010506\n",
            "Train Epoch: 12 [28160/123872 (23%)]\tLoss: 0.287031\n",
            "Train Epoch: 12 [28416/123872 (23%)]\tLoss: 0.317782\tLR: 0.00010500\n",
            "Train Epoch: 12 [28672/123872 (23%)]\tLoss: 0.359536\tLR: 0.00010493\n",
            "Train Epoch: 12 [28928/123872 (23%)]\tLoss: 0.395900\tLR: 0.00010487\n",
            "Train Epoch: 12 [29184/123872 (24%)]\tLoss: 0.305463\tLR: 0.00010480\n",
            "Train Epoch: 12 [29440/123872 (24%)]\tLoss: 0.337469\tLR: 0.00010474\n",
            "Train Epoch: 12 [29696/123872 (24%)]\tLoss: 0.326398\tLR: 0.00010467\n",
            "Train Epoch: 12 [29952/123872 (24%)]\tLoss: 0.401201\tLR: 0.00010461\n",
            "Train Epoch: 12 [30208/123872 (24%)]\tLoss: 0.379283\tLR: 0.00010454\n",
            "Train Epoch: 12 [30464/123872 (25%)]\tLoss: 0.357422\tLR: 0.00010448\n",
            "Train Epoch: 12 [30720/123872 (25%)]\tLoss: 0.364606\tLR: 0.00010441\n",
            "Train Epoch: 12 [30720/123872 (25%)]\tLoss: 0.364606\n",
            "Train Epoch: 12 [30976/123872 (25%)]\tLoss: 0.321496\tLR: 0.00010435\n",
            "Train Epoch: 12 [31232/123872 (25%)]\tLoss: 0.274336\tLR: 0.00010428\n",
            "Train Epoch: 12 [31488/123872 (25%)]\tLoss: 0.337936\tLR: 0.00010422\n",
            "Train Epoch: 12 [31744/123872 (26%)]\tLoss: 0.329224\tLR: 0.00010415\n",
            "Train Epoch: 12 [32000/123872 (26%)]\tLoss: 0.322389\tLR: 0.00010409\n",
            "Train Epoch: 12 [32256/123872 (26%)]\tLoss: 0.261552\tLR: 0.00010402\n",
            "Train Epoch: 12 [32512/123872 (26%)]\tLoss: 0.323683\tLR: 0.00010396\n",
            "Train Epoch: 12 [32768/123872 (26%)]\tLoss: 0.370312\tLR: 0.00010389\n",
            "Train Epoch: 12 [33024/123872 (27%)]\tLoss: 0.340099\tLR: 0.00010383\n",
            "Train Epoch: 12 [33280/123872 (27%)]\tLoss: 0.318330\tLR: 0.00010376\n",
            "Train Epoch: 12 [33280/123872 (27%)]\tLoss: 0.318330\n",
            "Train Epoch: 12 [33536/123872 (27%)]\tLoss: 0.352789\tLR: 0.00010370\n",
            "Train Epoch: 12 [33792/123872 (27%)]\tLoss: 0.295290\tLR: 0.00010364\n",
            "Train Epoch: 12 [34048/123872 (27%)]\tLoss: 0.320822\tLR: 0.00010357\n",
            "Train Epoch: 12 [34304/123872 (28%)]\tLoss: 0.291904\tLR: 0.00010351\n",
            "Train Epoch: 12 [34560/123872 (28%)]\tLoss: 0.319675\tLR: 0.00010344\n",
            "Train Epoch: 12 [34816/123872 (28%)]\tLoss: 0.338333\tLR: 0.00010338\n",
            "Train Epoch: 12 [35072/123872 (28%)]\tLoss: 0.355547\tLR: 0.00010332\n",
            "Train Epoch: 12 [35328/123872 (29%)]\tLoss: 0.351596\tLR: 0.00010325\n",
            "Train Epoch: 12 [35584/123872 (29%)]\tLoss: 0.293398\tLR: 0.00010319\n",
            "Train Epoch: 12 [35840/123872 (29%)]\tLoss: 0.353240\tLR: 0.00010312\n",
            "Train Epoch: 12 [35840/123872 (29%)]\tLoss: 0.353240\n",
            "Train Epoch: 12 [36096/123872 (29%)]\tLoss: 0.295398\tLR: 0.00010306\n",
            "Train Epoch: 12 [36352/123872 (29%)]\tLoss: 0.317025\tLR: 0.00010300\n",
            "Train Epoch: 12 [36608/123872 (30%)]\tLoss: 0.324419\tLR: 0.00010293\n",
            "Train Epoch: 12 [36864/123872 (30%)]\tLoss: 0.296684\tLR: 0.00010287\n",
            "Train Epoch: 12 [37120/123872 (30%)]\tLoss: 0.415167\tLR: 0.00010280\n",
            "Train Epoch: 12 [37376/123872 (30%)]\tLoss: 0.407865\tLR: 0.00010274\n",
            "Train Epoch: 12 [37632/123872 (30%)]\tLoss: 0.303501\tLR: 0.00010268\n",
            "Train Epoch: 12 [37888/123872 (31%)]\tLoss: 0.340591\tLR: 0.00010261\n",
            "Train Epoch: 12 [38144/123872 (31%)]\tLoss: 0.356881\tLR: 0.00010255\n",
            "Train Epoch: 12 [38400/123872 (31%)]\tLoss: 0.358364\tLR: 0.00010249\n",
            "Train Epoch: 12 [38400/123872 (31%)]\tLoss: 0.358364\n",
            "Train Epoch: 12 [38656/123872 (31%)]\tLoss: 0.352835\tLR: 0.00010242\n",
            "Train Epoch: 12 [38912/123872 (31%)]\tLoss: 0.371997\tLR: 0.00010236\n",
            "Train Epoch: 12 [39168/123872 (32%)]\tLoss: 0.336341\tLR: 0.00010229\n",
            "Train Epoch: 12 [39424/123872 (32%)]\tLoss: 0.362349\tLR: 0.00010223\n",
            "Train Epoch: 12 [39680/123872 (32%)]\tLoss: 0.399904\tLR: 0.00010217\n",
            "Train Epoch: 12 [39936/123872 (32%)]\tLoss: 0.308858\tLR: 0.00010210\n",
            "Train Epoch: 12 [40192/123872 (32%)]\tLoss: 0.388362\tLR: 0.00010204\n",
            "Train Epoch: 12 [40448/123872 (33%)]\tLoss: 0.282200\tLR: 0.00010198\n",
            "Train Epoch: 12 [40704/123872 (33%)]\tLoss: 0.363429\tLR: 0.00010192\n",
            "Train Epoch: 12 [40960/123872 (33%)]\tLoss: 0.405108\tLR: 0.00010185\n",
            "Train Epoch: 12 [40960/123872 (33%)]\tLoss: 0.405108\n",
            "Train Epoch: 12 [41216/123872 (33%)]\tLoss: 0.330173\tLR: 0.00010179\n",
            "Train Epoch: 12 [41472/123872 (33%)]\tLoss: 0.271474\tLR: 0.00010173\n",
            "Train Epoch: 12 [41728/123872 (34%)]\tLoss: 0.317666\tLR: 0.00010166\n",
            "Train Epoch: 12 [41984/123872 (34%)]\tLoss: 0.390335\tLR: 0.00010160\n",
            "Train Epoch: 12 [42240/123872 (34%)]\tLoss: 0.390755\tLR: 0.00010154\n",
            "Train Epoch: 12 [42496/123872 (34%)]\tLoss: 0.406223\tLR: 0.00010147\n",
            "Train Epoch: 12 [42752/123872 (35%)]\tLoss: 0.340284\tLR: 0.00010141\n",
            "Train Epoch: 12 [43008/123872 (35%)]\tLoss: 0.376448\tLR: 0.00010135\n",
            "Train Epoch: 12 [43264/123872 (35%)]\tLoss: 0.376349\tLR: 0.00010129\n",
            "Train Epoch: 12 [43520/123872 (35%)]\tLoss: 0.368376\tLR: 0.00010122\n",
            "Train Epoch: 12 [43520/123872 (35%)]\tLoss: 0.368376\n",
            "Train Epoch: 12 [43776/123872 (35%)]\tLoss: 0.359409\tLR: 0.00010116\n",
            "Train Epoch: 12 [44032/123872 (36%)]\tLoss: 0.320182\tLR: 0.00010110\n",
            "Train Epoch: 12 [44288/123872 (36%)]\tLoss: 0.331370\tLR: 0.00010104\n",
            "Train Epoch: 12 [44544/123872 (36%)]\tLoss: 0.364301\tLR: 0.00010097\n",
            "Train Epoch: 12 [44800/123872 (36%)]\tLoss: 0.369414\tLR: 0.00010091\n",
            "Train Epoch: 12 [45056/123872 (36%)]\tLoss: 0.316902\tLR: 0.00010085\n",
            "Train Epoch: 12 [45312/123872 (37%)]\tLoss: 0.287704\tLR: 0.00010079\n",
            "Train Epoch: 12 [45568/123872 (37%)]\tLoss: 0.309279\tLR: 0.00010072\n",
            "Train Epoch: 12 [45824/123872 (37%)]\tLoss: 0.370143\tLR: 0.00010066\n",
            "Train Epoch: 12 [46080/123872 (37%)]\tLoss: 0.343690\tLR: 0.00010060\n",
            "Train Epoch: 12 [46080/123872 (37%)]\tLoss: 0.343690\n",
            "Train Epoch: 12 [46336/123872 (37%)]\tLoss: 0.372331\tLR: 0.00010054\n",
            "Train Epoch: 12 [46592/123872 (38%)]\tLoss: 0.317384\tLR: 0.00010047\n",
            "Train Epoch: 12 [46848/123872 (38%)]\tLoss: 0.377321\tLR: 0.00010041\n",
            "Train Epoch: 12 [47104/123872 (38%)]\tLoss: 0.329624\tLR: 0.00010035\n",
            "Train Epoch: 12 [47360/123872 (38%)]\tLoss: 0.314897\tLR: 0.00010029\n",
            "Train Epoch: 12 [47616/123872 (38%)]\tLoss: 0.310016\tLR: 0.00010023\n",
            "Train Epoch: 12 [47872/123872 (39%)]\tLoss: 0.325255\tLR: 0.00010016\n",
            "Train Epoch: 12 [48128/123872 (39%)]\tLoss: 0.309419\tLR: 0.00010010\n",
            "Train Epoch: 12 [48384/123872 (39%)]\tLoss: 0.394533\tLR: 0.00010004\n",
            "Train Epoch: 12 [48640/123872 (39%)]\tLoss: 0.375569\tLR: 0.00009998\n",
            "Train Epoch: 12 [48640/123872 (39%)]\tLoss: 0.375569\n",
            "Train Epoch: 12 [48896/123872 (39%)]\tLoss: 0.369192\tLR: 0.00009992\n",
            "Train Epoch: 12 [49152/123872 (40%)]\tLoss: 0.326785\tLR: 0.00009986\n",
            "Train Epoch: 12 [49408/123872 (40%)]\tLoss: 0.387855\tLR: 0.00009979\n",
            "Train Epoch: 12 [49664/123872 (40%)]\tLoss: 0.321254\tLR: 0.00009973\n",
            "Train Epoch: 12 [49920/123872 (40%)]\tLoss: 0.382008\tLR: 0.00009967\n",
            "Train Epoch: 12 [50176/123872 (40%)]\tLoss: 0.374976\tLR: 0.00009961\n",
            "Train Epoch: 12 [50432/123872 (41%)]\tLoss: 0.426959\tLR: 0.00009955\n",
            "Train Epoch: 12 [50688/123872 (41%)]\tLoss: 0.347585\tLR: 0.00009949\n",
            "Train Epoch: 12 [50944/123872 (41%)]\tLoss: 0.343456\tLR: 0.00009942\n",
            "Train Epoch: 12 [51200/123872 (41%)]\tLoss: 0.321125\tLR: 0.00009936\n",
            "Train Epoch: 12 [51200/123872 (41%)]\tLoss: 0.321125\n",
            "Train Epoch: 12 [51456/123872 (42%)]\tLoss: 0.383453\tLR: 0.00009930\n",
            "Train Epoch: 12 [51712/123872 (42%)]\tLoss: 0.352471\tLR: 0.00009924\n",
            "Train Epoch: 12 [51968/123872 (42%)]\tLoss: 0.295597\tLR: 0.00009918\n",
            "Train Epoch: 12 [52224/123872 (42%)]\tLoss: 0.295386\tLR: 0.00009912\n",
            "Train Epoch: 12 [52480/123872 (42%)]\tLoss: 0.328988\tLR: 0.00009906\n",
            "Train Epoch: 12 [52736/123872 (43%)]\tLoss: 0.338055\tLR: 0.00009900\n",
            "Train Epoch: 12 [52992/123872 (43%)]\tLoss: 0.335100\tLR: 0.00009893\n",
            "Train Epoch: 12 [53248/123872 (43%)]\tLoss: 0.352261\tLR: 0.00009887\n",
            "Train Epoch: 12 [53504/123872 (43%)]\tLoss: 0.368855\tLR: 0.00009881\n",
            "Train Epoch: 12 [53760/123872 (43%)]\tLoss: 0.328419\tLR: 0.00009875\n",
            "Train Epoch: 12 [53760/123872 (43%)]\tLoss: 0.328419\n",
            "Train Epoch: 12 [54016/123872 (44%)]\tLoss: 0.349974\tLR: 0.00009869\n",
            "Train Epoch: 12 [54272/123872 (44%)]\tLoss: 0.347192\tLR: 0.00009863\n",
            "Train Epoch: 12 [54528/123872 (44%)]\tLoss: 0.400104\tLR: 0.00009857\n",
            "Train Epoch: 12 [54784/123872 (44%)]\tLoss: 0.366947\tLR: 0.00009851\n",
            "Train Epoch: 12 [55040/123872 (44%)]\tLoss: 0.258994\tLR: 0.00009845\n",
            "Train Epoch: 12 [55296/123872 (45%)]\tLoss: 0.333996\tLR: 0.00009839\n",
            "Train Epoch: 12 [55552/123872 (45%)]\tLoss: 0.392507\tLR: 0.00009833\n",
            "Train Epoch: 12 [55808/123872 (45%)]\tLoss: 0.325119\tLR: 0.00009827\n",
            "Train Epoch: 12 [56064/123872 (45%)]\tLoss: 0.335620\tLR: 0.00009820\n",
            "Train Epoch: 12 [56320/123872 (45%)]\tLoss: 0.359814\tLR: 0.00009814\n",
            "Train Epoch: 12 [56320/123872 (45%)]\tLoss: 0.359814\n",
            "Train Epoch: 12 [56576/123872 (46%)]\tLoss: 0.379533\tLR: 0.00009808\n",
            "Train Epoch: 12 [56832/123872 (46%)]\tLoss: 0.382999\tLR: 0.00009802\n",
            "Train Epoch: 12 [57088/123872 (46%)]\tLoss: 0.332875\tLR: 0.00009796\n",
            "Train Epoch: 12 [57344/123872 (46%)]\tLoss: 0.383504\tLR: 0.00009790\n",
            "Train Epoch: 12 [57600/123872 (46%)]\tLoss: 0.367315\tLR: 0.00009784\n",
            "Train Epoch: 12 [57856/123872 (47%)]\tLoss: 0.320392\tLR: 0.00009778\n",
            "Train Epoch: 12 [58112/123872 (47%)]\tLoss: 0.394622\tLR: 0.00009772\n",
            "Train Epoch: 12 [58368/123872 (47%)]\tLoss: 0.292881\tLR: 0.00009766\n",
            "Train Epoch: 12 [58624/123872 (47%)]\tLoss: 0.391268\tLR: 0.00009760\n",
            "Train Epoch: 12 [58880/123872 (48%)]\tLoss: 0.319824\tLR: 0.00009754\n",
            "Train Epoch: 12 [58880/123872 (48%)]\tLoss: 0.319824\n",
            "Train Epoch: 12 [59136/123872 (48%)]\tLoss: 0.311424\tLR: 0.00009748\n",
            "Train Epoch: 12 [59392/123872 (48%)]\tLoss: 0.355884\tLR: 0.00009742\n",
            "Train Epoch: 12 [59648/123872 (48%)]\tLoss: 0.281638\tLR: 0.00009736\n",
            "Train Epoch: 12 [59904/123872 (48%)]\tLoss: 0.351892\tLR: 0.00009730\n",
            "Train Epoch: 12 [60160/123872 (49%)]\tLoss: 0.336205\tLR: 0.00009724\n",
            "Train Epoch: 12 [60416/123872 (49%)]\tLoss: 0.315229\tLR: 0.00009718\n",
            "Train Epoch: 12 [60672/123872 (49%)]\tLoss: 0.347014\tLR: 0.00009712\n",
            "Train Epoch: 12 [60928/123872 (49%)]\tLoss: 0.320443\tLR: 0.00009706\n",
            "Train Epoch: 12 [61184/123872 (49%)]\tLoss: 0.422812\tLR: 0.00009700\n",
            "Train Epoch: 12 [61440/123872 (50%)]\tLoss: 0.290455\tLR: 0.00009694\n",
            "Train Epoch: 12 [61440/123872 (50%)]\tLoss: 0.290455\n",
            "Train Epoch: 12 [61696/123872 (50%)]\tLoss: 0.317076\tLR: 0.00009688\n",
            "Train Epoch: 12 [61952/123872 (50%)]\tLoss: 0.372400\tLR: 0.00009682\n",
            "Train Epoch: 12 [62208/123872 (50%)]\tLoss: 0.346885\tLR: 0.00009677\n",
            "Train Epoch: 12 [62464/123872 (50%)]\tLoss: 0.367159\tLR: 0.00009671\n",
            "Train Epoch: 12 [62720/123872 (51%)]\tLoss: 0.298448\tLR: 0.00009665\n",
            "Train Epoch: 12 [62976/123872 (51%)]\tLoss: 0.396679\tLR: 0.00009659\n",
            "Train Epoch: 12 [63232/123872 (51%)]\tLoss: 0.311942\tLR: 0.00009653\n",
            "Train Epoch: 12 [63488/123872 (51%)]\tLoss: 0.398757\tLR: 0.00009647\n",
            "Train Epoch: 12 [63744/123872 (51%)]\tLoss: 0.377195\tLR: 0.00009641\n",
            "Train Epoch: 12 [64000/123872 (52%)]\tLoss: 0.364776\tLR: 0.00009635\n",
            "Train Epoch: 12 [64000/123872 (52%)]\tLoss: 0.364776\n",
            "Train Epoch: 12 [64256/123872 (52%)]\tLoss: 0.422385\tLR: 0.00009629\n",
            "Train Epoch: 12 [64512/123872 (52%)]\tLoss: 0.336685\tLR: 0.00009623\n",
            "Train Epoch: 12 [64768/123872 (52%)]\tLoss: 0.325441\tLR: 0.00009617\n",
            "Train Epoch: 12 [65024/123872 (52%)]\tLoss: 0.344262\tLR: 0.00009611\n",
            "Train Epoch: 12 [65280/123872 (53%)]\tLoss: 0.367817\tLR: 0.00009606\n",
            "Train Epoch: 12 [65536/123872 (53%)]\tLoss: 0.367445\tLR: 0.00009600\n",
            "Train Epoch: 12 [65792/123872 (53%)]\tLoss: 0.331601\tLR: 0.00009594\n",
            "Train Epoch: 12 [66048/123872 (53%)]\tLoss: 0.306763\tLR: 0.00009588\n",
            "Train Epoch: 12 [66304/123872 (54%)]\tLoss: 0.336898\tLR: 0.00009582\n",
            "Train Epoch: 12 [66560/123872 (54%)]\tLoss: 0.279499\tLR: 0.00009576\n",
            "Train Epoch: 12 [66560/123872 (54%)]\tLoss: 0.279499\n",
            "Train Epoch: 12 [66816/123872 (54%)]\tLoss: 0.353775\tLR: 0.00009570\n",
            "Train Epoch: 12 [67072/123872 (54%)]\tLoss: 0.259841\tLR: 0.00009564\n",
            "Train Epoch: 12 [67328/123872 (54%)]\tLoss: 0.310509\tLR: 0.00009559\n",
            "Train Epoch: 12 [67584/123872 (55%)]\tLoss: 0.336038\tLR: 0.00009553\n",
            "Train Epoch: 12 [67840/123872 (55%)]\tLoss: 0.287049\tLR: 0.00009547\n",
            "Train Epoch: 12 [68096/123872 (55%)]\tLoss: 0.342747\tLR: 0.00009541\n",
            "Train Epoch: 12 [68352/123872 (55%)]\tLoss: 0.325126\tLR: 0.00009535\n",
            "Train Epoch: 12 [68608/123872 (55%)]\tLoss: 0.314615\tLR: 0.00009529\n",
            "Train Epoch: 12 [68864/123872 (56%)]\tLoss: 0.348063\tLR: 0.00009523\n",
            "Train Epoch: 12 [69120/123872 (56%)]\tLoss: 0.371242\tLR: 0.00009518\n",
            "Train Epoch: 12 [69120/123872 (56%)]\tLoss: 0.371242\n",
            "Train Epoch: 12 [69376/123872 (56%)]\tLoss: 0.295114\tLR: 0.00009512\n",
            "Train Epoch: 12 [69632/123872 (56%)]\tLoss: 0.308316\tLR: 0.00009506\n",
            "Train Epoch: 12 [69888/123872 (56%)]\tLoss: 0.349078\tLR: 0.00009500\n",
            "Train Epoch: 12 [70144/123872 (57%)]\tLoss: 0.346861\tLR: 0.00009494\n",
            "Train Epoch: 12 [70400/123872 (57%)]\tLoss: 0.332917\tLR: 0.00009489\n",
            "Train Epoch: 12 [70656/123872 (57%)]\tLoss: 0.286648\tLR: 0.00009483\n",
            "Train Epoch: 12 [70912/123872 (57%)]\tLoss: 0.329302\tLR: 0.00009477\n",
            "Train Epoch: 12 [71168/123872 (57%)]\tLoss: 0.276627\tLR: 0.00009471\n",
            "Train Epoch: 12 [71424/123872 (58%)]\tLoss: 0.334591\tLR: 0.00009465\n",
            "Train Epoch: 12 [71680/123872 (58%)]\tLoss: 0.357125\tLR: 0.00009460\n",
            "Train Epoch: 12 [71680/123872 (58%)]\tLoss: 0.357125\n",
            "Train Epoch: 12 [71936/123872 (58%)]\tLoss: 0.337236\tLR: 0.00009454\n",
            "Train Epoch: 12 [72192/123872 (58%)]\tLoss: 0.304729\tLR: 0.00009448\n",
            "Train Epoch: 12 [72448/123872 (58%)]\tLoss: 0.348651\tLR: 0.00009442\n",
            "Train Epoch: 12 [72704/123872 (59%)]\tLoss: 0.316761\tLR: 0.00009437\n",
            "Train Epoch: 12 [72960/123872 (59%)]\tLoss: 0.280407\tLR: 0.00009431\n",
            "Train Epoch: 12 [73216/123872 (59%)]\tLoss: 0.351483\tLR: 0.00009425\n",
            "Train Epoch: 12 [73472/123872 (59%)]\tLoss: 0.334738\tLR: 0.00009419\n",
            "Train Epoch: 12 [73728/123872 (60%)]\tLoss: 0.301182\tLR: 0.00009414\n",
            "Train Epoch: 12 [73984/123872 (60%)]\tLoss: 0.404038\tLR: 0.00009408\n",
            "Train Epoch: 12 [74240/123872 (60%)]\tLoss: 0.401537\tLR: 0.00009402\n",
            "Train Epoch: 12 [74240/123872 (60%)]\tLoss: 0.401537\n",
            "Train Epoch: 12 [74496/123872 (60%)]\tLoss: 0.331348\tLR: 0.00009396\n",
            "Train Epoch: 12 [74752/123872 (60%)]\tLoss: 0.348437\tLR: 0.00009391\n",
            "Train Epoch: 12 [75008/123872 (61%)]\tLoss: 0.340147\tLR: 0.00009385\n",
            "Train Epoch: 12 [75264/123872 (61%)]\tLoss: 0.357961\tLR: 0.00009379\n",
            "Train Epoch: 12 [75520/123872 (61%)]\tLoss: 0.286979\tLR: 0.00009373\n",
            "Train Epoch: 12 [75776/123872 (61%)]\tLoss: 0.331552\tLR: 0.00009368\n",
            "Train Epoch: 12 [76032/123872 (61%)]\tLoss: 0.342897\tLR: 0.00009362\n",
            "Train Epoch: 12 [76288/123872 (62%)]\tLoss: 0.365463\tLR: 0.00009356\n",
            "Train Epoch: 12 [76544/123872 (62%)]\tLoss: 0.354730\tLR: 0.00009351\n",
            "Train Epoch: 12 [76800/123872 (62%)]\tLoss: 0.297016\tLR: 0.00009345\n",
            "Train Epoch: 12 [76800/123872 (62%)]\tLoss: 0.297016\n",
            "Train Epoch: 12 [77056/123872 (62%)]\tLoss: 0.382599\tLR: 0.00009339\n",
            "Train Epoch: 12 [77312/123872 (62%)]\tLoss: 0.323278\tLR: 0.00009334\n",
            "Train Epoch: 12 [77568/123872 (63%)]\tLoss: 0.365362\tLR: 0.00009328\n",
            "Train Epoch: 12 [77824/123872 (63%)]\tLoss: 0.358369\tLR: 0.00009322\n",
            "Train Epoch: 12 [78080/123872 (63%)]\tLoss: 0.325631\tLR: 0.00009317\n",
            "Train Epoch: 12 [78336/123872 (63%)]\tLoss: 0.337340\tLR: 0.00009311\n",
            "Train Epoch: 12 [78592/123872 (63%)]\tLoss: 0.337140\tLR: 0.00009305\n",
            "Train Epoch: 12 [78848/123872 (64%)]\tLoss: 0.300617\tLR: 0.00009300\n",
            "Train Epoch: 12 [79104/123872 (64%)]\tLoss: 0.325935\tLR: 0.00009294\n",
            "Train Epoch: 12 [79360/123872 (64%)]\tLoss: 0.356808\tLR: 0.00009288\n",
            "Train Epoch: 12 [79360/123872 (64%)]\tLoss: 0.356808\n",
            "Train Epoch: 12 [79616/123872 (64%)]\tLoss: 0.372433\tLR: 0.00009283\n",
            "Train Epoch: 12 [79872/123872 (64%)]\tLoss: 0.357797\tLR: 0.00009277\n",
            "Train Epoch: 12 [80128/123872 (65%)]\tLoss: 0.283367\tLR: 0.00009271\n",
            "Train Epoch: 12 [80384/123872 (65%)]\tLoss: 0.325849\tLR: 0.00009266\n",
            "Train Epoch: 12 [80640/123872 (65%)]\tLoss: 0.327676\tLR: 0.00009260\n",
            "Train Epoch: 12 [80896/123872 (65%)]\tLoss: 0.370542\tLR: 0.00009255\n",
            "Train Epoch: 12 [81152/123872 (65%)]\tLoss: 0.395357\tLR: 0.00009249\n",
            "Train Epoch: 12 [81408/123872 (66%)]\tLoss: 0.312409\tLR: 0.00009243\n",
            "Train Epoch: 12 [81664/123872 (66%)]\tLoss: 0.359941\tLR: 0.00009238\n",
            "Train Epoch: 12 [81920/123872 (66%)]\tLoss: 0.354916\tLR: 0.00009232\n",
            "Train Epoch: 12 [81920/123872 (66%)]\tLoss: 0.354916\n",
            "Train Epoch: 12 [82176/123872 (66%)]\tLoss: 0.320660\tLR: 0.00009226\n",
            "Train Epoch: 12 [82432/123872 (67%)]\tLoss: 0.365847\tLR: 0.00009221\n",
            "Train Epoch: 12 [82688/123872 (67%)]\tLoss: 0.330804\tLR: 0.00009215\n",
            "Train Epoch: 12 [82944/123872 (67%)]\tLoss: 0.427932\tLR: 0.00009210\n",
            "Train Epoch: 12 [83200/123872 (67%)]\tLoss: 0.362092\tLR: 0.00009204\n",
            "Train Epoch: 12 [83456/123872 (67%)]\tLoss: 0.413175\tLR: 0.00009199\n",
            "Train Epoch: 12 [83712/123872 (68%)]\tLoss: 0.355061\tLR: 0.00009193\n",
            "Train Epoch: 12 [83968/123872 (68%)]\tLoss: 0.331508\tLR: 0.00009187\n",
            "Train Epoch: 12 [84224/123872 (68%)]\tLoss: 0.325388\tLR: 0.00009182\n",
            "Train Epoch: 12 [84480/123872 (68%)]\tLoss: 0.416081\tLR: 0.00009176\n",
            "Train Epoch: 12 [84480/123872 (68%)]\tLoss: 0.416081\n",
            "Train Epoch: 12 [84736/123872 (68%)]\tLoss: 0.373257\tLR: 0.00009171\n",
            "Train Epoch: 12 [84992/123872 (69%)]\tLoss: 0.405510\tLR: 0.00009165\n",
            "Train Epoch: 12 [85248/123872 (69%)]\tLoss: 0.359569\tLR: 0.00009160\n",
            "Train Epoch: 12 [85504/123872 (69%)]\tLoss: 0.331296\tLR: 0.00009154\n",
            "Train Epoch: 12 [85760/123872 (69%)]\tLoss: 0.289424\tLR: 0.00009149\n",
            "Train Epoch: 12 [86016/123872 (69%)]\tLoss: 0.320255\tLR: 0.00009143\n",
            "Train Epoch: 12 [86272/123872 (70%)]\tLoss: 0.318437\tLR: 0.00009138\n",
            "Train Epoch: 12 [86528/123872 (70%)]\tLoss: 0.379130\tLR: 0.00009132\n",
            "Train Epoch: 12 [86784/123872 (70%)]\tLoss: 0.335256\tLR: 0.00009127\n",
            "Train Epoch: 12 [87040/123872 (70%)]\tLoss: 0.356862\tLR: 0.00009121\n",
            "Train Epoch: 12 [87040/123872 (70%)]\tLoss: 0.356862\n",
            "Train Epoch: 12 [87296/123872 (70%)]\tLoss: 0.363545\tLR: 0.00009116\n",
            "Train Epoch: 12 [87552/123872 (71%)]\tLoss: 0.352026\tLR: 0.00009110\n",
            "Train Epoch: 12 [87808/123872 (71%)]\tLoss: 0.290762\tLR: 0.00009105\n",
            "Train Epoch: 12 [88064/123872 (71%)]\tLoss: 0.299571\tLR: 0.00009099\n",
            "Train Epoch: 12 [88320/123872 (71%)]\tLoss: 0.297532\tLR: 0.00009094\n",
            "Train Epoch: 12 [88576/123872 (71%)]\tLoss: 0.378375\tLR: 0.00009088\n",
            "Train Epoch: 12 [88832/123872 (72%)]\tLoss: 0.331988\tLR: 0.00009083\n",
            "Train Epoch: 12 [89088/123872 (72%)]\tLoss: 0.376090\tLR: 0.00009077\n",
            "Train Epoch: 12 [89344/123872 (72%)]\tLoss: 0.377524\tLR: 0.00009072\n",
            "Train Epoch: 12 [89600/123872 (72%)]\tLoss: 0.341151\tLR: 0.00009066\n",
            "Train Epoch: 12 [89600/123872 (72%)]\tLoss: 0.341151\n",
            "Train Epoch: 12 [89856/123872 (73%)]\tLoss: 0.353454\tLR: 0.00009061\n",
            "Train Epoch: 12 [90112/123872 (73%)]\tLoss: 0.316956\tLR: 0.00009055\n",
            "Train Epoch: 12 [90368/123872 (73%)]\tLoss: 0.316587\tLR: 0.00009050\n",
            "Train Epoch: 12 [90624/123872 (73%)]\tLoss: 0.347170\tLR: 0.00009044\n",
            "Train Epoch: 12 [90880/123872 (73%)]\tLoss: 0.383995\tLR: 0.00009039\n",
            "Train Epoch: 12 [91136/123872 (74%)]\tLoss: 0.385110\tLR: 0.00009033\n",
            "Train Epoch: 12 [91392/123872 (74%)]\tLoss: 0.287716\tLR: 0.00009028\n",
            "Train Epoch: 12 [91648/123872 (74%)]\tLoss: 0.328763\tLR: 0.00009023\n",
            "Train Epoch: 12 [91904/123872 (74%)]\tLoss: 0.342719\tLR: 0.00009017\n",
            "Train Epoch: 12 [92160/123872 (74%)]\tLoss: 0.348562\tLR: 0.00009012\n",
            "Train Epoch: 12 [92160/123872 (74%)]\tLoss: 0.348562\n",
            "Train Epoch: 12 [92416/123872 (75%)]\tLoss: 0.371914\tLR: 0.00009006\n",
            "Train Epoch: 12 [92672/123872 (75%)]\tLoss: 0.326808\tLR: 0.00009001\n",
            "Train Epoch: 12 [92928/123872 (75%)]\tLoss: 0.354971\tLR: 0.00008996\n",
            "Train Epoch: 12 [93184/123872 (75%)]\tLoss: 0.336179\tLR: 0.00008990\n",
            "Train Epoch: 12 [93440/123872 (75%)]\tLoss: 0.317897\tLR: 0.00008985\n",
            "Train Epoch: 12 [93696/123872 (76%)]\tLoss: 0.370456\tLR: 0.00008979\n",
            "Train Epoch: 12 [93952/123872 (76%)]\tLoss: 0.385143\tLR: 0.00008974\n",
            "Train Epoch: 12 [94208/123872 (76%)]\tLoss: 0.340027\tLR: 0.00008969\n",
            "Train Epoch: 12 [94464/123872 (76%)]\tLoss: 0.338005\tLR: 0.00008963\n",
            "Train Epoch: 12 [94720/123872 (76%)]\tLoss: 0.298479\tLR: 0.00008958\n",
            "Train Epoch: 12 [94720/123872 (76%)]\tLoss: 0.298479\n",
            "Train Epoch: 12 [94976/123872 (77%)]\tLoss: 0.340591\tLR: 0.00008952\n",
            "Train Epoch: 12 [95232/123872 (77%)]\tLoss: 0.290822\tLR: 0.00008947\n",
            "Train Epoch: 12 [95488/123872 (77%)]\tLoss: 0.381132\tLR: 0.00008942\n",
            "Train Epoch: 12 [95744/123872 (77%)]\tLoss: 0.401208\tLR: 0.00008936\n",
            "Train Epoch: 12 [96000/123872 (77%)]\tLoss: 0.341874\tLR: 0.00008931\n",
            "Train Epoch: 12 [96256/123872 (78%)]\tLoss: 0.321125\tLR: 0.00008926\n",
            "Train Epoch: 12 [96512/123872 (78%)]\tLoss: 0.367461\tLR: 0.00008920\n",
            "Train Epoch: 12 [96768/123872 (78%)]\tLoss: 0.308586\tLR: 0.00008915\n",
            "Train Epoch: 12 [97024/123872 (78%)]\tLoss: 0.356323\tLR: 0.00008910\n",
            "Train Epoch: 12 [97280/123872 (79%)]\tLoss: 0.306200\tLR: 0.00008904\n",
            "Train Epoch: 12 [97280/123872 (79%)]\tLoss: 0.306200\n",
            "Train Epoch: 12 [97536/123872 (79%)]\tLoss: 0.326482\tLR: 0.00008899\n",
            "Train Epoch: 12 [97792/123872 (79%)]\tLoss: 0.340996\tLR: 0.00008894\n",
            "Train Epoch: 12 [98048/123872 (79%)]\tLoss: 0.280430\tLR: 0.00008888\n",
            "Train Epoch: 12 [98304/123872 (79%)]\tLoss: 0.343131\tLR: 0.00008883\n",
            "Train Epoch: 12 [98560/123872 (80%)]\tLoss: 0.340000\tLR: 0.00008878\n",
            "Train Epoch: 12 [98816/123872 (80%)]\tLoss: 0.350155\tLR: 0.00008873\n",
            "Train Epoch: 12 [99072/123872 (80%)]\tLoss: 0.316083\tLR: 0.00008867\n",
            "Train Epoch: 12 [99328/123872 (80%)]\tLoss: 0.317835\tLR: 0.00008862\n",
            "Train Epoch: 12 [99584/123872 (80%)]\tLoss: 0.339377\tLR: 0.00008857\n",
            "Train Epoch: 12 [99840/123872 (81%)]\tLoss: 0.375281\tLR: 0.00008851\n",
            "Train Epoch: 12 [99840/123872 (81%)]\tLoss: 0.375281\n",
            "Train Epoch: 12 [100096/123872 (81%)]\tLoss: 0.280200\tLR: 0.00008846\n",
            "Train Epoch: 12 [100352/123872 (81%)]\tLoss: 0.354900\tLR: 0.00008841\n",
            "Train Epoch: 12 [100608/123872 (81%)]\tLoss: 0.331458\tLR: 0.00008836\n",
            "Train Epoch: 12 [100864/123872 (81%)]\tLoss: 0.272367\tLR: 0.00008830\n",
            "Train Epoch: 12 [101120/123872 (82%)]\tLoss: 0.317480\tLR: 0.00008825\n",
            "Train Epoch: 12 [101376/123872 (82%)]\tLoss: 0.331939\tLR: 0.00008820\n",
            "Train Epoch: 12 [101632/123872 (82%)]\tLoss: 0.322538\tLR: 0.00008815\n",
            "Train Epoch: 12 [101888/123872 (82%)]\tLoss: 0.330401\tLR: 0.00008809\n",
            "Train Epoch: 12 [102144/123872 (82%)]\tLoss: 0.285918\tLR: 0.00008804\n",
            "Train Epoch: 12 [102400/123872 (83%)]\tLoss: 0.328576\tLR: 0.00008799\n",
            "Train Epoch: 12 [102400/123872 (83%)]\tLoss: 0.328576\n",
            "Train Epoch: 12 [102656/123872 (83%)]\tLoss: 0.320264\tLR: 0.00008794\n",
            "Train Epoch: 12 [102912/123872 (83%)]\tLoss: 0.352285\tLR: 0.00008788\n",
            "Train Epoch: 12 [103168/123872 (83%)]\tLoss: 0.295930\tLR: 0.00008783\n",
            "Train Epoch: 12 [103424/123872 (83%)]\tLoss: 0.321312\tLR: 0.00008778\n",
            "Train Epoch: 12 [103680/123872 (84%)]\tLoss: 0.343437\tLR: 0.00008773\n",
            "Train Epoch: 12 [103936/123872 (84%)]\tLoss: 0.351362\tLR: 0.00008767\n",
            "Train Epoch: 12 [104192/123872 (84%)]\tLoss: 0.385205\tLR: 0.00008762\n",
            "Train Epoch: 12 [104448/123872 (84%)]\tLoss: 0.298014\tLR: 0.00008757\n",
            "Train Epoch: 12 [104704/123872 (85%)]\tLoss: 0.350077\tLR: 0.00008752\n",
            "Train Epoch: 12 [104960/123872 (85%)]\tLoss: 0.342379\tLR: 0.00008747\n",
            "Train Epoch: 12 [104960/123872 (85%)]\tLoss: 0.342379\n",
            "Train Epoch: 12 [105216/123872 (85%)]\tLoss: 0.359127\tLR: 0.00008742\n",
            "Train Epoch: 12 [105472/123872 (85%)]\tLoss: 0.324553\tLR: 0.00008736\n",
            "Train Epoch: 12 [105728/123872 (85%)]\tLoss: 0.379028\tLR: 0.00008731\n",
            "Train Epoch: 12 [105984/123872 (86%)]\tLoss: 0.332396\tLR: 0.00008726\n",
            "Train Epoch: 12 [106240/123872 (86%)]\tLoss: 0.357379\tLR: 0.00008721\n",
            "Train Epoch: 12 [106496/123872 (86%)]\tLoss: 0.375325\tLR: 0.00008716\n",
            "Train Epoch: 12 [106752/123872 (86%)]\tLoss: 0.401082\tLR: 0.00008711\n",
            "Train Epoch: 12 [107008/123872 (86%)]\tLoss: 0.450767\tLR: 0.00008705\n",
            "Train Epoch: 12 [107264/123872 (87%)]\tLoss: 0.312126\tLR: 0.00008700\n",
            "Train Epoch: 12 [107520/123872 (87%)]\tLoss: 0.363839\tLR: 0.00008695\n",
            "Train Epoch: 12 [107520/123872 (87%)]\tLoss: 0.363839\n",
            "Train Epoch: 12 [107776/123872 (87%)]\tLoss: 0.315825\tLR: 0.00008690\n",
            "Train Epoch: 12 [108032/123872 (87%)]\tLoss: 0.351182\tLR: 0.00008685\n",
            "Train Epoch: 12 [108288/123872 (87%)]\tLoss: 0.442029\tLR: 0.00008680\n",
            "Train Epoch: 12 [108544/123872 (88%)]\tLoss: 0.247619\tLR: 0.00008675\n",
            "Train Epoch: 12 [108800/123872 (88%)]\tLoss: 0.328130\tLR: 0.00008669\n",
            "Train Epoch: 12 [109056/123872 (88%)]\tLoss: 0.301626\tLR: 0.00008664\n",
            "Train Epoch: 12 [109312/123872 (88%)]\tLoss: 0.325801\tLR: 0.00008659\n",
            "Train Epoch: 12 [109568/123872 (88%)]\tLoss: 0.373868\tLR: 0.00008654\n",
            "Train Epoch: 12 [109824/123872 (89%)]\tLoss: 0.314135\tLR: 0.00008649\n",
            "Train Epoch: 12 [110080/123872 (89%)]\tLoss: 0.355279\tLR: 0.00008644\n",
            "Train Epoch: 12 [110080/123872 (89%)]\tLoss: 0.355279\n",
            "Train Epoch: 12 [110336/123872 (89%)]\tLoss: 0.319947\tLR: 0.00008639\n",
            "Train Epoch: 12 [110592/123872 (89%)]\tLoss: 0.334373\tLR: 0.00008634\n",
            "Train Epoch: 12 [110848/123872 (89%)]\tLoss: 0.368263\tLR: 0.00008629\n",
            "Train Epoch: 12 [111104/123872 (90%)]\tLoss: 0.381518\tLR: 0.00008624\n",
            "Train Epoch: 12 [111360/123872 (90%)]\tLoss: 0.384918\tLR: 0.00008619\n",
            "Train Epoch: 12 [111616/123872 (90%)]\tLoss: 0.331150\tLR: 0.00008613\n",
            "Train Epoch: 12 [111872/123872 (90%)]\tLoss: 0.317683\tLR: 0.00008608\n",
            "Train Epoch: 12 [112128/123872 (90%)]\tLoss: 0.328503\tLR: 0.00008603\n",
            "Train Epoch: 12 [112384/123872 (91%)]\tLoss: 0.354343\tLR: 0.00008598\n",
            "Train Epoch: 12 [112640/123872 (91%)]\tLoss: 0.295322\tLR: 0.00008593\n",
            "Train Epoch: 12 [112640/123872 (91%)]\tLoss: 0.295322\n",
            "Train Epoch: 12 [112896/123872 (91%)]\tLoss: 0.293656\tLR: 0.00008588\n",
            "Train Epoch: 12 [113152/123872 (91%)]\tLoss: 0.344370\tLR: 0.00008583\n",
            "Train Epoch: 12 [113408/123872 (92%)]\tLoss: 0.330271\tLR: 0.00008578\n",
            "Train Epoch: 12 [113664/123872 (92%)]\tLoss: 0.420350\tLR: 0.00008573\n",
            "Train Epoch: 12 [113920/123872 (92%)]\tLoss: 0.353222\tLR: 0.00008568\n",
            "Train Epoch: 12 [114176/123872 (92%)]\tLoss: 0.361455\tLR: 0.00008563\n",
            "Train Epoch: 12 [114432/123872 (92%)]\tLoss: 0.336604\tLR: 0.00008558\n",
            "Train Epoch: 12 [114688/123872 (93%)]\tLoss: 0.357120\tLR: 0.00008553\n",
            "Train Epoch: 12 [114944/123872 (93%)]\tLoss: 0.316096\tLR: 0.00008548\n",
            "Train Epoch: 12 [115200/123872 (93%)]\tLoss: 0.383777\tLR: 0.00008543\n",
            "Train Epoch: 12 [115200/123872 (93%)]\tLoss: 0.383777\n",
            "Train Epoch: 12 [115456/123872 (93%)]\tLoss: 0.347190\tLR: 0.00008538\n",
            "Train Epoch: 12 [115712/123872 (93%)]\tLoss: 0.403179\tLR: 0.00008533\n",
            "Train Epoch: 12 [115968/123872 (94%)]\tLoss: 0.310339\tLR: 0.00008528\n",
            "Train Epoch: 12 [116224/123872 (94%)]\tLoss: 0.407647\tLR: 0.00008523\n",
            "Train Epoch: 12 [116480/123872 (94%)]\tLoss: 0.333180\tLR: 0.00008518\n",
            "Train Epoch: 12 [116736/123872 (94%)]\tLoss: 0.319605\tLR: 0.00008513\n",
            "Train Epoch: 12 [116992/123872 (94%)]\tLoss: 0.331266\tLR: 0.00008508\n",
            "Train Epoch: 12 [117248/123872 (95%)]\tLoss: 0.325069\tLR: 0.00008503\n",
            "Train Epoch: 12 [117504/123872 (95%)]\tLoss: 0.388020\tLR: 0.00008498\n",
            "Train Epoch: 12 [117760/123872 (95%)]\tLoss: 0.360859\tLR: 0.00008493\n",
            "Train Epoch: 12 [117760/123872 (95%)]\tLoss: 0.360859\n",
            "Train Epoch: 12 [118016/123872 (95%)]\tLoss: 0.304557\tLR: 0.00008488\n",
            "Train Epoch: 12 [118272/123872 (95%)]\tLoss: 0.365843\tLR: 0.00008483\n",
            "Train Epoch: 12 [118528/123872 (96%)]\tLoss: 0.341198\tLR: 0.00008478\n",
            "Train Epoch: 12 [118784/123872 (96%)]\tLoss: 0.287033\tLR: 0.00008473\n",
            "Train Epoch: 12 [119040/123872 (96%)]\tLoss: 0.338874\tLR: 0.00008469\n",
            "Train Epoch: 12 [119296/123872 (96%)]\tLoss: 0.288281\tLR: 0.00008464\n",
            "Train Epoch: 12 [119552/123872 (96%)]\tLoss: 0.366598\tLR: 0.00008459\n",
            "Train Epoch: 12 [119808/123872 (97%)]\tLoss: 0.365349\tLR: 0.00008454\n",
            "Train Epoch: 12 [120064/123872 (97%)]\tLoss: 0.404732\tLR: 0.00008449\n",
            "Train Epoch: 12 [120320/123872 (97%)]\tLoss: 0.376283\tLR: 0.00008444\n",
            "Train Epoch: 12 [120320/123872 (97%)]\tLoss: 0.376283\n",
            "Train Epoch: 12 [120576/123872 (97%)]\tLoss: 0.341022\tLR: 0.00008439\n",
            "Train Epoch: 12 [120832/123872 (98%)]\tLoss: 0.345488\tLR: 0.00008434\n",
            "Train Epoch: 12 [121088/123872 (98%)]\tLoss: 0.318023\tLR: 0.00008429\n",
            "Train Epoch: 12 [121344/123872 (98%)]\tLoss: 0.329793\tLR: 0.00008424\n",
            "Train Epoch: 12 [121600/123872 (98%)]\tLoss: 0.334914\tLR: 0.00008419\n",
            "Train Epoch: 12 [121856/123872 (98%)]\tLoss: 0.294285\tLR: 0.00008415\n",
            "Train Epoch: 12 [122112/123872 (99%)]\tLoss: 0.342952\tLR: 0.00008410\n",
            "Train Epoch: 12 [122368/123872 (99%)]\tLoss: 0.331224\tLR: 0.00008405\n",
            "Train Epoch: 12 [122624/123872 (99%)]\tLoss: 0.324826\tLR: 0.00008400\n",
            "Train Epoch: 12 [122880/123872 (99%)]\tLoss: 0.347394\tLR: 0.00008395\n",
            "Train Epoch: 12 [122880/123872 (99%)]\tLoss: 0.347394\n",
            "Train Epoch: 12 [123136/123872 (99%)]\tLoss: 0.413837\tLR: 0.00008390\n",
            "Train Epoch: 12 [123392/123872 (100%)]\tLoss: 0.357231\tLR: 0.00008385\n",
            "Train Epoch: 12 [108192/123872 (100%)]\tLoss: 0.297656\tLR: 0.00008381\n",
            "\n",
            "Test set: Average loss: 0.0014, Accuracy: 25982/30970 (83.89%)\n",
            "\n",
            "Train Epoch: 13 [0/123872 (0%)]\tLoss: 0.354988\tLR: 0.00008376\n",
            "Train Epoch: 13 [0/123872 (0%)]\tLoss: 0.354988\n",
            "Train Epoch: 13 [256/123872 (0%)]\tLoss: 0.355305\tLR: 0.00008371\n",
            "Train Epoch: 13 [512/123872 (0%)]\tLoss: 0.336109\tLR: 0.00008366\n",
            "Train Epoch: 13 [768/123872 (1%)]\tLoss: 0.315416\tLR: 0.00008361\n",
            "Train Epoch: 13 [1024/123872 (1%)]\tLoss: 0.329329\tLR: 0.00008356\n",
            "Train Epoch: 13 [1280/123872 (1%)]\tLoss: 0.314886\tLR: 0.00008352\n",
            "Train Epoch: 13 [1536/123872 (1%)]\tLoss: 0.299876\tLR: 0.00008347\n",
            "Train Epoch: 13 [1792/123872 (1%)]\tLoss: 0.355875\tLR: 0.00008342\n",
            "Train Epoch: 13 [2048/123872 (2%)]\tLoss: 0.350173\tLR: 0.00008337\n",
            "Train Epoch: 13 [2304/123872 (2%)]\tLoss: 0.387542\tLR: 0.00008332\n",
            "Train Epoch: 13 [2560/123872 (2%)]\tLoss: 0.307340\tLR: 0.00008328\n",
            "Train Epoch: 13 [2560/123872 (2%)]\tLoss: 0.307340\n",
            "Train Epoch: 13 [2816/123872 (2%)]\tLoss: 0.322477\tLR: 0.00008323\n",
            "Train Epoch: 13 [3072/123872 (2%)]\tLoss: 0.350005\tLR: 0.00008318\n",
            "Train Epoch: 13 [3328/123872 (3%)]\tLoss: 0.350034\tLR: 0.00008313\n",
            "Train Epoch: 13 [3584/123872 (3%)]\tLoss: 0.363431\tLR: 0.00008308\n",
            "Train Epoch: 13 [3840/123872 (3%)]\tLoss: 0.388622\tLR: 0.00008304\n",
            "Train Epoch: 13 [4096/123872 (3%)]\tLoss: 0.309907\tLR: 0.00008299\n",
            "Train Epoch: 13 [4352/123872 (4%)]\tLoss: 0.306675\tLR: 0.00008294\n",
            "Train Epoch: 13 [4608/123872 (4%)]\tLoss: 0.349214\tLR: 0.00008289\n",
            "Train Epoch: 13 [4864/123872 (4%)]\tLoss: 0.299392\tLR: 0.00008285\n",
            "Train Epoch: 13 [5120/123872 (4%)]\tLoss: 0.334150\tLR: 0.00008280\n",
            "Train Epoch: 13 [5120/123872 (4%)]\tLoss: 0.334150\n",
            "Train Epoch: 13 [5376/123872 (4%)]\tLoss: 0.311123\tLR: 0.00008275\n",
            "Train Epoch: 13 [5632/123872 (5%)]\tLoss: 0.385754\tLR: 0.00008270\n",
            "Train Epoch: 13 [5888/123872 (5%)]\tLoss: 0.318514\tLR: 0.00008266\n",
            "Train Epoch: 13 [6144/123872 (5%)]\tLoss: 0.292568\tLR: 0.00008261\n",
            "Train Epoch: 13 [6400/123872 (5%)]\tLoss: 0.359487\tLR: 0.00008256\n",
            "Train Epoch: 13 [6656/123872 (5%)]\tLoss: 0.328657\tLR: 0.00008251\n",
            "Train Epoch: 13 [6912/123872 (6%)]\tLoss: 0.287469\tLR: 0.00008247\n",
            "Train Epoch: 13 [7168/123872 (6%)]\tLoss: 0.290433\tLR: 0.00008242\n",
            "Train Epoch: 13 [7424/123872 (6%)]\tLoss: 0.348507\tLR: 0.00008237\n",
            "Train Epoch: 13 [7680/123872 (6%)]\tLoss: 0.309929\tLR: 0.00008233\n",
            "Train Epoch: 13 [7680/123872 (6%)]\tLoss: 0.309929\n",
            "Train Epoch: 13 [7936/123872 (6%)]\tLoss: 0.318284\tLR: 0.00008228\n",
            "Train Epoch: 13 [8192/123872 (7%)]\tLoss: 0.369773\tLR: 0.00008223\n",
            "Train Epoch: 13 [8448/123872 (7%)]\tLoss: 0.385993\tLR: 0.00008219\n",
            "Train Epoch: 13 [8704/123872 (7%)]\tLoss: 0.339124\tLR: 0.00008214\n",
            "Train Epoch: 13 [8960/123872 (7%)]\tLoss: 0.407636\tLR: 0.00008209\n",
            "Train Epoch: 13 [9216/123872 (7%)]\tLoss: 0.342597\tLR: 0.00008205\n",
            "Train Epoch: 13 [9472/123872 (8%)]\tLoss: 0.338038\tLR: 0.00008200\n",
            "Train Epoch: 13 [9728/123872 (8%)]\tLoss: 0.331346\tLR: 0.00008195\n",
            "Train Epoch: 13 [9984/123872 (8%)]\tLoss: 0.332387\tLR: 0.00008191\n",
            "Train Epoch: 13 [10240/123872 (8%)]\tLoss: 0.353632\tLR: 0.00008186\n",
            "Train Epoch: 13 [10240/123872 (8%)]\tLoss: 0.353632\n",
            "Train Epoch: 13 [10496/123872 (8%)]\tLoss: 0.353746\tLR: 0.00008181\n",
            "Train Epoch: 13 [10752/123872 (9%)]\tLoss: 0.292307\tLR: 0.00008177\n",
            "Train Epoch: 13 [11008/123872 (9%)]\tLoss: 0.366023\tLR: 0.00008172\n",
            "Train Epoch: 13 [11264/123872 (9%)]\tLoss: 0.392384\tLR: 0.00008167\n",
            "Train Epoch: 13 [11520/123872 (9%)]\tLoss: 0.365258\tLR: 0.00008163\n",
            "Train Epoch: 13 [11776/123872 (10%)]\tLoss: 0.379662\tLR: 0.00008158\n",
            "Train Epoch: 13 [12032/123872 (10%)]\tLoss: 0.321861\tLR: 0.00008153\n",
            "Train Epoch: 13 [12288/123872 (10%)]\tLoss: 0.278106\tLR: 0.00008149\n",
            "Train Epoch: 13 [12544/123872 (10%)]\tLoss: 0.295690\tLR: 0.00008144\n",
            "Train Epoch: 13 [12800/123872 (10%)]\tLoss: 0.337270\tLR: 0.00008140\n",
            "Train Epoch: 13 [12800/123872 (10%)]\tLoss: 0.337270\n",
            "Train Epoch: 13 [13056/123872 (11%)]\tLoss: 0.328689\tLR: 0.00008135\n",
            "Train Epoch: 13 [13312/123872 (11%)]\tLoss: 0.304193\tLR: 0.00008130\n",
            "Train Epoch: 13 [13568/123872 (11%)]\tLoss: 0.364140\tLR: 0.00008126\n",
            "Train Epoch: 13 [13824/123872 (11%)]\tLoss: 0.294136\tLR: 0.00008121\n",
            "Train Epoch: 13 [14080/123872 (11%)]\tLoss: 0.365557\tLR: 0.00008117\n",
            "Train Epoch: 13 [14336/123872 (12%)]\tLoss: 0.370941\tLR: 0.00008112\n",
            "Train Epoch: 13 [14592/123872 (12%)]\tLoss: 0.396587\tLR: 0.00008108\n",
            "Train Epoch: 13 [14848/123872 (12%)]\tLoss: 0.355388\tLR: 0.00008103\n",
            "Train Epoch: 13 [15104/123872 (12%)]\tLoss: 0.292853\tLR: 0.00008098\n",
            "Train Epoch: 13 [15360/123872 (12%)]\tLoss: 0.321053\tLR: 0.00008094\n",
            "Train Epoch: 13 [15360/123872 (12%)]\tLoss: 0.321053\n",
            "Train Epoch: 13 [15616/123872 (13%)]\tLoss: 0.344205\tLR: 0.00008089\n",
            "Train Epoch: 13 [15872/123872 (13%)]\tLoss: 0.378929\tLR: 0.00008085\n",
            "Train Epoch: 13 [16128/123872 (13%)]\tLoss: 0.369422\tLR: 0.00008080\n",
            "Train Epoch: 13 [16384/123872 (13%)]\tLoss: 0.302122\tLR: 0.00008076\n",
            "Train Epoch: 13 [16640/123872 (13%)]\tLoss: 0.273553\tLR: 0.00008071\n",
            "Train Epoch: 13 [16896/123872 (14%)]\tLoss: 0.340104\tLR: 0.00008067\n",
            "Train Epoch: 13 [17152/123872 (14%)]\tLoss: 0.355851\tLR: 0.00008062\n",
            "Train Epoch: 13 [17408/123872 (14%)]\tLoss: 0.329250\tLR: 0.00008058\n",
            "Train Epoch: 13 [17664/123872 (14%)]\tLoss: 0.329019\tLR: 0.00008053\n",
            "Train Epoch: 13 [17920/123872 (14%)]\tLoss: 0.369704\tLR: 0.00008048\n",
            "Train Epoch: 13 [17920/123872 (14%)]\tLoss: 0.369704\n",
            "Train Epoch: 13 [18176/123872 (15%)]\tLoss: 0.312963\tLR: 0.00008044\n",
            "Train Epoch: 13 [18432/123872 (15%)]\tLoss: 0.344212\tLR: 0.00008039\n",
            "Train Epoch: 13 [18688/123872 (15%)]\tLoss: 0.318954\tLR: 0.00008035\n",
            "Train Epoch: 13 [18944/123872 (15%)]\tLoss: 0.284569\tLR: 0.00008030\n",
            "Train Epoch: 13 [19200/123872 (15%)]\tLoss: 0.365640\tLR: 0.00008026\n",
            "Train Epoch: 13 [19456/123872 (16%)]\tLoss: 0.337831\tLR: 0.00008022\n",
            "Train Epoch: 13 [19712/123872 (16%)]\tLoss: 0.322483\tLR: 0.00008017\n",
            "Train Epoch: 13 [19968/123872 (16%)]\tLoss: 0.305063\tLR: 0.00008013\n",
            "Train Epoch: 13 [20224/123872 (16%)]\tLoss: 0.329790\tLR: 0.00008008\n",
            "Train Epoch: 13 [20480/123872 (17%)]\tLoss: 0.325896\tLR: 0.00008004\n",
            "Train Epoch: 13 [20480/123872 (17%)]\tLoss: 0.325896\n",
            "Train Epoch: 13 [20736/123872 (17%)]\tLoss: 0.372647\tLR: 0.00007999\n",
            "Train Epoch: 13 [20992/123872 (17%)]\tLoss: 0.348069\tLR: 0.00007995\n",
            "Train Epoch: 13 [21248/123872 (17%)]\tLoss: 0.335437\tLR: 0.00007990\n",
            "Train Epoch: 13 [21504/123872 (17%)]\tLoss: 0.340078\tLR: 0.00007986\n",
            "Train Epoch: 13 [21760/123872 (18%)]\tLoss: 0.321767\tLR: 0.00007981\n",
            "Train Epoch: 13 [22016/123872 (18%)]\tLoss: 0.401318\tLR: 0.00007977\n",
            "Train Epoch: 13 [22272/123872 (18%)]\tLoss: 0.390238\tLR: 0.00007973\n",
            "Train Epoch: 13 [22528/123872 (18%)]\tLoss: 0.297883\tLR: 0.00007968\n",
            "Train Epoch: 13 [22784/123872 (18%)]\tLoss: 0.337747\tLR: 0.00007964\n",
            "Train Epoch: 13 [23040/123872 (19%)]\tLoss: 0.386105\tLR: 0.00007959\n",
            "Train Epoch: 13 [23040/123872 (19%)]\tLoss: 0.386105\n",
            "Train Epoch: 13 [23296/123872 (19%)]\tLoss: 0.333872\tLR: 0.00007955\n",
            "Train Epoch: 13 [23552/123872 (19%)]\tLoss: 0.364746\tLR: 0.00007950\n",
            "Train Epoch: 13 [23808/123872 (19%)]\tLoss: 0.315253\tLR: 0.00007946\n",
            "Train Epoch: 13 [24064/123872 (19%)]\tLoss: 0.302856\tLR: 0.00007942\n",
            "Train Epoch: 13 [24320/123872 (20%)]\tLoss: 0.314510\tLR: 0.00007937\n",
            "Train Epoch: 13 [24576/123872 (20%)]\tLoss: 0.336917\tLR: 0.00007933\n",
            "Train Epoch: 13 [24832/123872 (20%)]\tLoss: 0.344751\tLR: 0.00007928\n",
            "Train Epoch: 13 [25088/123872 (20%)]\tLoss: 0.323185\tLR: 0.00007924\n",
            "Train Epoch: 13 [25344/123872 (20%)]\tLoss: 0.358475\tLR: 0.00007920\n",
            "Train Epoch: 13 [25600/123872 (21%)]\tLoss: 0.348884\tLR: 0.00007915\n",
            "Train Epoch: 13 [25600/123872 (21%)]\tLoss: 0.348884\n",
            "Train Epoch: 13 [25856/123872 (21%)]\tLoss: 0.295360\tLR: 0.00007911\n",
            "Train Epoch: 13 [26112/123872 (21%)]\tLoss: 0.329413\tLR: 0.00007907\n",
            "Train Epoch: 13 [26368/123872 (21%)]\tLoss: 0.401052\tLR: 0.00007902\n",
            "Train Epoch: 13 [26624/123872 (21%)]\tLoss: 0.272413\tLR: 0.00007898\n",
            "Train Epoch: 13 [26880/123872 (22%)]\tLoss: 0.402269\tLR: 0.00007894\n",
            "Train Epoch: 13 [27136/123872 (22%)]\tLoss: 0.311564\tLR: 0.00007889\n",
            "Train Epoch: 13 [27392/123872 (22%)]\tLoss: 0.332640\tLR: 0.00007885\n",
            "Train Epoch: 13 [27648/123872 (22%)]\tLoss: 0.323346\tLR: 0.00007881\n",
            "Train Epoch: 13 [27904/123872 (23%)]\tLoss: 0.325287\tLR: 0.00007876\n",
            "Train Epoch: 13 [28160/123872 (23%)]\tLoss: 0.299443\tLR: 0.00007872\n",
            "Train Epoch: 13 [28160/123872 (23%)]\tLoss: 0.299443\n",
            "Train Epoch: 13 [28416/123872 (23%)]\tLoss: 0.402242\tLR: 0.00007868\n",
            "Train Epoch: 13 [28672/123872 (23%)]\tLoss: 0.312876\tLR: 0.00007863\n",
            "Train Epoch: 13 [28928/123872 (23%)]\tLoss: 0.303000\tLR: 0.00007859\n",
            "Train Epoch: 13 [29184/123872 (24%)]\tLoss: 0.292936\tLR: 0.00007855\n",
            "Train Epoch: 13 [29440/123872 (24%)]\tLoss: 0.363603\tLR: 0.00007850\n",
            "Train Epoch: 13 [29696/123872 (24%)]\tLoss: 0.310983\tLR: 0.00007846\n",
            "Train Epoch: 13 [29952/123872 (24%)]\tLoss: 0.287941\tLR: 0.00007842\n",
            "Train Epoch: 13 [30208/123872 (24%)]\tLoss: 0.282950\tLR: 0.00007838\n",
            "Train Epoch: 13 [30464/123872 (25%)]\tLoss: 0.338954\tLR: 0.00007833\n",
            "Train Epoch: 13 [30720/123872 (25%)]\tLoss: 0.289086\tLR: 0.00007829\n",
            "Train Epoch: 13 [30720/123872 (25%)]\tLoss: 0.289086\n",
            "Train Epoch: 13 [30976/123872 (25%)]\tLoss: 0.341516\tLR: 0.00007825\n",
            "Train Epoch: 13 [31232/123872 (25%)]\tLoss: 0.275023\tLR: 0.00007820\n",
            "Train Epoch: 13 [31488/123872 (25%)]\tLoss: 0.378083\tLR: 0.00007816\n",
            "Train Epoch: 13 [31744/123872 (26%)]\tLoss: 0.273383\tLR: 0.00007812\n",
            "Train Epoch: 13 [32000/123872 (26%)]\tLoss: 0.340870\tLR: 0.00007808\n",
            "Train Epoch: 13 [32256/123872 (26%)]\tLoss: 0.268143\tLR: 0.00007803\n",
            "Train Epoch: 13 [32512/123872 (26%)]\tLoss: 0.320484\tLR: 0.00007799\n",
            "Train Epoch: 13 [32768/123872 (26%)]\tLoss: 0.315588\tLR: 0.00007795\n",
            "Train Epoch: 13 [33024/123872 (27%)]\tLoss: 0.315352\tLR: 0.00007791\n",
            "Train Epoch: 13 [33280/123872 (27%)]\tLoss: 0.356725\tLR: 0.00007787\n",
            "Train Epoch: 13 [33280/123872 (27%)]\tLoss: 0.356725\n",
            "Train Epoch: 13 [33536/123872 (27%)]\tLoss: 0.385230\tLR: 0.00007782\n",
            "Train Epoch: 13 [33792/123872 (27%)]\tLoss: 0.382232\tLR: 0.00007778\n",
            "Train Epoch: 13 [34048/123872 (27%)]\tLoss: 0.363887\tLR: 0.00007774\n",
            "Train Epoch: 13 [34304/123872 (28%)]\tLoss: 0.310433\tLR: 0.00007770\n",
            "Train Epoch: 13 [34560/123872 (28%)]\tLoss: 0.337211\tLR: 0.00007766\n",
            "Train Epoch: 13 [34816/123872 (28%)]\tLoss: 0.383845\tLR: 0.00007761\n",
            "Train Epoch: 13 [35072/123872 (28%)]\tLoss: 0.323530\tLR: 0.00007757\n",
            "Train Epoch: 13 [35328/123872 (29%)]\tLoss: 0.333934\tLR: 0.00007753\n",
            "Train Epoch: 13 [35584/123872 (29%)]\tLoss: 0.369403\tLR: 0.00007749\n",
            "Train Epoch: 13 [35840/123872 (29%)]\tLoss: 0.352904\tLR: 0.00007745\n",
            "Train Epoch: 13 [35840/123872 (29%)]\tLoss: 0.352904\n",
            "Train Epoch: 13 [36096/123872 (29%)]\tLoss: 0.329291\tLR: 0.00007740\n",
            "Train Epoch: 13 [36352/123872 (29%)]\tLoss: 0.313024\tLR: 0.00007736\n",
            "Train Epoch: 13 [36608/123872 (30%)]\tLoss: 0.333951\tLR: 0.00007732\n",
            "Train Epoch: 13 [36864/123872 (30%)]\tLoss: 0.309082\tLR: 0.00007728\n",
            "Train Epoch: 13 [37120/123872 (30%)]\tLoss: 0.342338\tLR: 0.00007724\n",
            "Train Epoch: 13 [37376/123872 (30%)]\tLoss: 0.385561\tLR: 0.00007720\n",
            "Train Epoch: 13 [37632/123872 (30%)]\tLoss: 0.309053\tLR: 0.00007716\n",
            "Train Epoch: 13 [37888/123872 (31%)]\tLoss: 0.281457\tLR: 0.00007711\n",
            "Train Epoch: 13 [38144/123872 (31%)]\tLoss: 0.336594\tLR: 0.00007707\n",
            "Train Epoch: 13 [38400/123872 (31%)]\tLoss: 0.326705\tLR: 0.00007703\n",
            "Train Epoch: 13 [38400/123872 (31%)]\tLoss: 0.326705\n",
            "Train Epoch: 13 [38656/123872 (31%)]\tLoss: 0.312892\tLR: 0.00007699\n",
            "Train Epoch: 13 [38912/123872 (31%)]\tLoss: 0.347233\tLR: 0.00007695\n",
            "Train Epoch: 13 [39168/123872 (32%)]\tLoss: 0.335285\tLR: 0.00007691\n",
            "Train Epoch: 13 [39424/123872 (32%)]\tLoss: 0.317903\tLR: 0.00007687\n",
            "Train Epoch: 13 [39680/123872 (32%)]\tLoss: 0.364947\tLR: 0.00007683\n",
            "Train Epoch: 13 [39936/123872 (32%)]\tLoss: 0.349683\tLR: 0.00007678\n",
            "Train Epoch: 13 [40192/123872 (32%)]\tLoss: 0.261258\tLR: 0.00007674\n",
            "Train Epoch: 13 [40448/123872 (33%)]\tLoss: 0.254692\tLR: 0.00007670\n",
            "Train Epoch: 13 [40704/123872 (33%)]\tLoss: 0.294220\tLR: 0.00007666\n",
            "Train Epoch: 13 [40960/123872 (33%)]\tLoss: 0.255897\tLR: 0.00007662\n",
            "Train Epoch: 13 [40960/123872 (33%)]\tLoss: 0.255897\n",
            "Train Epoch: 13 [41216/123872 (33%)]\tLoss: 0.347290\tLR: 0.00007658\n",
            "Train Epoch: 13 [41472/123872 (33%)]\tLoss: 0.333475\tLR: 0.00007654\n",
            "Train Epoch: 13 [41728/123872 (34%)]\tLoss: 0.305872\tLR: 0.00007650\n",
            "Train Epoch: 13 [41984/123872 (34%)]\tLoss: 0.323653\tLR: 0.00007646\n",
            "Train Epoch: 13 [42240/123872 (34%)]\tLoss: 0.315904\tLR: 0.00007642\n",
            "Train Epoch: 13 [42496/123872 (34%)]\tLoss: 0.394121\tLR: 0.00007638\n",
            "Train Epoch: 13 [42752/123872 (35%)]\tLoss: 0.277661\tLR: 0.00007634\n",
            "Train Epoch: 13 [43008/123872 (35%)]\tLoss: 0.377000\tLR: 0.00007630\n",
            "Train Epoch: 13 [43264/123872 (35%)]\tLoss: 0.258235\tLR: 0.00007626\n",
            "Train Epoch: 13 [43520/123872 (35%)]\tLoss: 0.403073\tLR: 0.00007622\n",
            "Train Epoch: 13 [43520/123872 (35%)]\tLoss: 0.403073\n",
            "Train Epoch: 13 [43776/123872 (35%)]\tLoss: 0.335384\tLR: 0.00007618\n",
            "Train Epoch: 13 [44032/123872 (36%)]\tLoss: 0.323267\tLR: 0.00007614\n",
            "Train Epoch: 13 [44288/123872 (36%)]\tLoss: 0.313973\tLR: 0.00007610\n",
            "Train Epoch: 13 [44544/123872 (36%)]\tLoss: 0.342454\tLR: 0.00007606\n",
            "Train Epoch: 13 [44800/123872 (36%)]\tLoss: 0.345454\tLR: 0.00007601\n",
            "Train Epoch: 13 [45056/123872 (36%)]\tLoss: 0.333258\tLR: 0.00007597\n",
            "Train Epoch: 13 [45312/123872 (37%)]\tLoss: 0.325176\tLR: 0.00007594\n",
            "Train Epoch: 13 [45568/123872 (37%)]\tLoss: 0.379671\tLR: 0.00007590\n",
            "Train Epoch: 13 [45824/123872 (37%)]\tLoss: 0.318245\tLR: 0.00007586\n",
            "Train Epoch: 13 [46080/123872 (37%)]\tLoss: 0.341672\tLR: 0.00007582\n",
            "Train Epoch: 13 [46080/123872 (37%)]\tLoss: 0.341672\n",
            "Train Epoch: 13 [46336/123872 (37%)]\tLoss: 0.310819\tLR: 0.00007578\n",
            "Train Epoch: 13 [46592/123872 (38%)]\tLoss: 0.332821\tLR: 0.00007574\n",
            "Train Epoch: 13 [46848/123872 (38%)]\tLoss: 0.411248\tLR: 0.00007570\n",
            "Train Epoch: 13 [47104/123872 (38%)]\tLoss: 0.385296\tLR: 0.00007566\n",
            "Train Epoch: 13 [47360/123872 (38%)]\tLoss: 0.327106\tLR: 0.00007562\n",
            "Train Epoch: 13 [47616/123872 (38%)]\tLoss: 0.350888\tLR: 0.00007558\n",
            "Train Epoch: 13 [47872/123872 (39%)]\tLoss: 0.322706\tLR: 0.00007554\n",
            "Train Epoch: 13 [48128/123872 (39%)]\tLoss: 0.297321\tLR: 0.00007550\n",
            "Train Epoch: 13 [48384/123872 (39%)]\tLoss: 0.355140\tLR: 0.00007546\n",
            "Train Epoch: 13 [48640/123872 (39%)]\tLoss: 0.300054\tLR: 0.00007542\n",
            "Train Epoch: 13 [48640/123872 (39%)]\tLoss: 0.300054\n",
            "Train Epoch: 13 [48896/123872 (39%)]\tLoss: 0.330671\tLR: 0.00007538\n",
            "Train Epoch: 13 [49152/123872 (40%)]\tLoss: 0.317736\tLR: 0.00007534\n",
            "Train Epoch: 13 [49408/123872 (40%)]\tLoss: 0.352382\tLR: 0.00007530\n",
            "Train Epoch: 13 [49664/123872 (40%)]\tLoss: 0.332265\tLR: 0.00007526\n",
            "Train Epoch: 13 [49920/123872 (40%)]\tLoss: 0.348852\tLR: 0.00007522\n",
            "Train Epoch: 13 [50176/123872 (40%)]\tLoss: 0.272861\tLR: 0.00007518\n",
            "Train Epoch: 13 [50432/123872 (41%)]\tLoss: 0.346831\tLR: 0.00007515\n",
            "Train Epoch: 13 [50688/123872 (41%)]\tLoss: 0.354146\tLR: 0.00007511\n",
            "Train Epoch: 13 [50944/123872 (41%)]\tLoss: 0.311767\tLR: 0.00007507\n",
            "Train Epoch: 13 [51200/123872 (41%)]\tLoss: 0.315917\tLR: 0.00007503\n",
            "Train Epoch: 13 [51200/123872 (41%)]\tLoss: 0.315917\n",
            "Train Epoch: 13 [51456/123872 (42%)]\tLoss: 0.261343\tLR: 0.00007499\n",
            "Train Epoch: 13 [51712/123872 (42%)]\tLoss: 0.373615\tLR: 0.00007495\n",
            "Train Epoch: 13 [51968/123872 (42%)]\tLoss: 0.321512\tLR: 0.00007491\n",
            "Train Epoch: 13 [52224/123872 (42%)]\tLoss: 0.349506\tLR: 0.00007487\n",
            "Train Epoch: 13 [52480/123872 (42%)]\tLoss: 0.375706\tLR: 0.00007484\n",
            "Train Epoch: 13 [52736/123872 (43%)]\tLoss: 0.366438\tLR: 0.00007480\n",
            "Train Epoch: 13 [52992/123872 (43%)]\tLoss: 0.389115\tLR: 0.00007476\n",
            "Train Epoch: 13 [53248/123872 (43%)]\tLoss: 0.335387\tLR: 0.00007472\n",
            "Train Epoch: 13 [53504/123872 (43%)]\tLoss: 0.278057\tLR: 0.00007468\n",
            "Train Epoch: 13 [53760/123872 (43%)]\tLoss: 0.346685\tLR: 0.00007464\n",
            "Train Epoch: 13 [53760/123872 (43%)]\tLoss: 0.346685\n",
            "Train Epoch: 13 [54016/123872 (44%)]\tLoss: 0.349519\tLR: 0.00007461\n",
            "Train Epoch: 13 [54272/123872 (44%)]\tLoss: 0.346619\tLR: 0.00007457\n",
            "Train Epoch: 13 [54528/123872 (44%)]\tLoss: 0.398533\tLR: 0.00007453\n",
            "Train Epoch: 13 [54784/123872 (44%)]\tLoss: 0.259168\tLR: 0.00007449\n",
            "Train Epoch: 13 [55040/123872 (44%)]\tLoss: 0.352223\tLR: 0.00007445\n",
            "Train Epoch: 13 [55296/123872 (45%)]\tLoss: 0.356814\tLR: 0.00007441\n",
            "Train Epoch: 13 [55552/123872 (45%)]\tLoss: 0.277114\tLR: 0.00007438\n",
            "Train Epoch: 13 [55808/123872 (45%)]\tLoss: 0.335476\tLR: 0.00007434\n",
            "Train Epoch: 13 [56064/123872 (45%)]\tLoss: 0.371173\tLR: 0.00007430\n",
            "Train Epoch: 13 [56320/123872 (45%)]\tLoss: 0.304812\tLR: 0.00007426\n",
            "Train Epoch: 13 [56320/123872 (45%)]\tLoss: 0.304812\n",
            "Train Epoch: 13 [56576/123872 (46%)]\tLoss: 0.363543\tLR: 0.00007422\n",
            "Train Epoch: 13 [56832/123872 (46%)]\tLoss: 0.347970\tLR: 0.00007419\n",
            "Train Epoch: 13 [57088/123872 (46%)]\tLoss: 0.329839\tLR: 0.00007415\n",
            "Train Epoch: 13 [57344/123872 (46%)]\tLoss: 0.329605\tLR: 0.00007411\n",
            "Train Epoch: 13 [57600/123872 (46%)]\tLoss: 0.299795\tLR: 0.00007407\n",
            "Train Epoch: 13 [57856/123872 (47%)]\tLoss: 0.325405\tLR: 0.00007404\n",
            "Train Epoch: 13 [58112/123872 (47%)]\tLoss: 0.279194\tLR: 0.00007400\n",
            "Train Epoch: 13 [58368/123872 (47%)]\tLoss: 0.318797\tLR: 0.00007396\n",
            "Train Epoch: 13 [58624/123872 (47%)]\tLoss: 0.314912\tLR: 0.00007392\n",
            "Train Epoch: 13 [58880/123872 (48%)]\tLoss: 0.302931\tLR: 0.00007389\n",
            "Train Epoch: 13 [58880/123872 (48%)]\tLoss: 0.302931\n",
            "Train Epoch: 13 [59136/123872 (48%)]\tLoss: 0.396326\tLR: 0.00007385\n",
            "Train Epoch: 13 [59392/123872 (48%)]\tLoss: 0.321997\tLR: 0.00007381\n",
            "Train Epoch: 13 [59648/123872 (48%)]\tLoss: 0.249866\tLR: 0.00007377\n",
            "Train Epoch: 13 [59904/123872 (48%)]\tLoss: 0.315953\tLR: 0.00007374\n",
            "Train Epoch: 13 [60160/123872 (49%)]\tLoss: 0.376183\tLR: 0.00007370\n",
            "Train Epoch: 13 [60416/123872 (49%)]\tLoss: 0.289127\tLR: 0.00007366\n",
            "Train Epoch: 13 [60672/123872 (49%)]\tLoss: 0.378511\tLR: 0.00007363\n",
            "Train Epoch: 13 [60928/123872 (49%)]\tLoss: 0.335579\tLR: 0.00007359\n",
            "Train Epoch: 13 [61184/123872 (49%)]\tLoss: 0.313311\tLR: 0.00007355\n",
            "Train Epoch: 13 [61440/123872 (50%)]\tLoss: 0.360428\tLR: 0.00007352\n",
            "Train Epoch: 13 [61440/123872 (50%)]\tLoss: 0.360428\n",
            "Train Epoch: 13 [61696/123872 (50%)]\tLoss: 0.331082\tLR: 0.00007348\n",
            "Train Epoch: 13 [61952/123872 (50%)]\tLoss: 0.316275\tLR: 0.00007344\n",
            "Train Epoch: 13 [62208/123872 (50%)]\tLoss: 0.371893\tLR: 0.00007341\n",
            "Train Epoch: 13 [62464/123872 (50%)]\tLoss: 0.337827\tLR: 0.00007337\n",
            "Train Epoch: 13 [62720/123872 (51%)]\tLoss: 0.331420\tLR: 0.00007333\n",
            "Train Epoch: 13 [62976/123872 (51%)]\tLoss: 0.344106\tLR: 0.00007330\n",
            "Train Epoch: 13 [63232/123872 (51%)]\tLoss: 0.319334\tLR: 0.00007326\n",
            "Train Epoch: 13 [63488/123872 (51%)]\tLoss: 0.371509\tLR: 0.00007322\n",
            "Train Epoch: 13 [63744/123872 (51%)]\tLoss: 0.337249\tLR: 0.00007319\n",
            "Train Epoch: 13 [64000/123872 (52%)]\tLoss: 0.375941\tLR: 0.00007315\n",
            "Train Epoch: 13 [64000/123872 (52%)]\tLoss: 0.375941\n",
            "Train Epoch: 13 [64256/123872 (52%)]\tLoss: 0.336382\tLR: 0.00007311\n",
            "Train Epoch: 13 [64512/123872 (52%)]\tLoss: 0.333390\tLR: 0.00007308\n",
            "Train Epoch: 13 [64768/123872 (52%)]\tLoss: 0.315604\tLR: 0.00007304\n",
            "Train Epoch: 13 [65024/123872 (52%)]\tLoss: 0.361285\tLR: 0.00007300\n",
            "Train Epoch: 13 [65280/123872 (53%)]\tLoss: 0.322262\tLR: 0.00007297\n",
            "Train Epoch: 13 [65536/123872 (53%)]\tLoss: 0.340438\tLR: 0.00007293\n",
            "Train Epoch: 13 [65792/123872 (53%)]\tLoss: 0.369683\tLR: 0.00007290\n",
            "Train Epoch: 13 [66048/123872 (53%)]\tLoss: 0.377961\tLR: 0.00007286\n",
            "Train Epoch: 13 [66304/123872 (54%)]\tLoss: 0.387367\tLR: 0.00007282\n",
            "Train Epoch: 13 [66560/123872 (54%)]\tLoss: 0.266452\tLR: 0.00007279\n",
            "Train Epoch: 13 [66560/123872 (54%)]\tLoss: 0.266452\n",
            "Train Epoch: 13 [66816/123872 (54%)]\tLoss: 0.341181\tLR: 0.00007275\n",
            "Train Epoch: 13 [67072/123872 (54%)]\tLoss: 0.344196\tLR: 0.00007272\n",
            "Train Epoch: 13 [67328/123872 (54%)]\tLoss: 0.311126\tLR: 0.00007268\n",
            "Train Epoch: 13 [67584/123872 (55%)]\tLoss: 0.318559\tLR: 0.00007264\n",
            "Train Epoch: 13 [67840/123872 (55%)]\tLoss: 0.331717\tLR: 0.00007261\n",
            "Train Epoch: 13 [68096/123872 (55%)]\tLoss: 0.325304\tLR: 0.00007257\n",
            "Train Epoch: 13 [68352/123872 (55%)]\tLoss: 0.369197\tLR: 0.00007254\n",
            "Train Epoch: 13 [68608/123872 (55%)]\tLoss: 0.366905\tLR: 0.00007250\n",
            "Train Epoch: 13 [68864/123872 (56%)]\tLoss: 0.358598\tLR: 0.00007247\n",
            "Train Epoch: 13 [69120/123872 (56%)]\tLoss: 0.310203\tLR: 0.00007243\n",
            "Train Epoch: 13 [69120/123872 (56%)]\tLoss: 0.310203\n",
            "Train Epoch: 13 [69376/123872 (56%)]\tLoss: 0.369682\tLR: 0.00007240\n",
            "Train Epoch: 13 [69632/123872 (56%)]\tLoss: 0.366013\tLR: 0.00007236\n",
            "Train Epoch: 13 [69888/123872 (56%)]\tLoss: 0.286632\tLR: 0.00007233\n",
            "Train Epoch: 13 [70144/123872 (57%)]\tLoss: 0.373080\tLR: 0.00007229\n",
            "Train Epoch: 13 [70400/123872 (57%)]\tLoss: 0.354757\tLR: 0.00007226\n",
            "Train Epoch: 13 [70656/123872 (57%)]\tLoss: 0.385384\tLR: 0.00007222\n",
            "Train Epoch: 13 [70912/123872 (57%)]\tLoss: 0.310652\tLR: 0.00007218\n",
            "Train Epoch: 13 [71168/123872 (57%)]\tLoss: 0.319494\tLR: 0.00007215\n",
            "Train Epoch: 13 [71424/123872 (58%)]\tLoss: 0.324544\tLR: 0.00007211\n",
            "Train Epoch: 13 [71680/123872 (58%)]\tLoss: 0.339918\tLR: 0.00007208\n",
            "Train Epoch: 13 [71680/123872 (58%)]\tLoss: 0.339918\n",
            "Train Epoch: 13 [71936/123872 (58%)]\tLoss: 0.345527\tLR: 0.00007205\n",
            "Train Epoch: 13 [72192/123872 (58%)]\tLoss: 0.407825\tLR: 0.00007201\n",
            "Train Epoch: 13 [72448/123872 (58%)]\tLoss: 0.307698\tLR: 0.00007198\n",
            "Train Epoch: 13 [72704/123872 (59%)]\tLoss: 0.326597\tLR: 0.00007194\n",
            "Train Epoch: 13 [72960/123872 (59%)]\tLoss: 0.311095\tLR: 0.00007191\n",
            "Train Epoch: 13 [73216/123872 (59%)]\tLoss: 0.290085\tLR: 0.00007187\n",
            "Train Epoch: 13 [73472/123872 (59%)]\tLoss: 0.322466\tLR: 0.00007184\n",
            "Train Epoch: 13 [73728/123872 (60%)]\tLoss: 0.344509\tLR: 0.00007180\n",
            "Train Epoch: 13 [73984/123872 (60%)]\tLoss: 0.364549\tLR: 0.00007177\n",
            "Train Epoch: 13 [74240/123872 (60%)]\tLoss: 0.329012\tLR: 0.00007173\n",
            "Train Epoch: 13 [74240/123872 (60%)]\tLoss: 0.329012\n",
            "Train Epoch: 13 [74496/123872 (60%)]\tLoss: 0.310403\tLR: 0.00007170\n",
            "Train Epoch: 13 [74752/123872 (60%)]\tLoss: 0.329266\tLR: 0.00007166\n",
            "Train Epoch: 13 [75008/123872 (61%)]\tLoss: 0.354969\tLR: 0.00007163\n",
            "Train Epoch: 13 [75264/123872 (61%)]\tLoss: 0.304169\tLR: 0.00007160\n",
            "Train Epoch: 13 [75520/123872 (61%)]\tLoss: 0.359080\tLR: 0.00007156\n",
            "Train Epoch: 13 [75776/123872 (61%)]\tLoss: 0.266640\tLR: 0.00007153\n",
            "Train Epoch: 13 [76032/123872 (61%)]\tLoss: 0.277704\tLR: 0.00007149\n",
            "Train Epoch: 13 [76288/123872 (62%)]\tLoss: 0.314082\tLR: 0.00007146\n",
            "Train Epoch: 13 [76544/123872 (62%)]\tLoss: 0.358244\tLR: 0.00007143\n",
            "Train Epoch: 13 [76800/123872 (62%)]\tLoss: 0.341388\tLR: 0.00007139\n",
            "Train Epoch: 13 [76800/123872 (62%)]\tLoss: 0.341388\n",
            "Train Epoch: 13 [77056/123872 (62%)]\tLoss: 0.324970\tLR: 0.00007136\n",
            "Train Epoch: 13 [77312/123872 (62%)]\tLoss: 0.258126\tLR: 0.00007132\n",
            "Train Epoch: 13 [77568/123872 (63%)]\tLoss: 0.292212\tLR: 0.00007129\n",
            "Train Epoch: 13 [77824/123872 (63%)]\tLoss: 0.355215\tLR: 0.00007126\n",
            "Train Epoch: 13 [78080/123872 (63%)]\tLoss: 0.303453\tLR: 0.00007122\n",
            "Train Epoch: 13 [78336/123872 (63%)]\tLoss: 0.364410\tLR: 0.00007119\n",
            "Train Epoch: 13 [78592/123872 (63%)]\tLoss: 0.271572\tLR: 0.00007116\n",
            "Train Epoch: 13 [78848/123872 (64%)]\tLoss: 0.372368\tLR: 0.00007112\n",
            "Train Epoch: 13 [79104/123872 (64%)]\tLoss: 0.364028\tLR: 0.00007109\n",
            "Train Epoch: 13 [79360/123872 (64%)]\tLoss: 0.359332\tLR: 0.00007106\n",
            "Train Epoch: 13 [79360/123872 (64%)]\tLoss: 0.359332\n",
            "Train Epoch: 13 [79616/123872 (64%)]\tLoss: 0.352599\tLR: 0.00007102\n",
            "Train Epoch: 13 [79872/123872 (64%)]\tLoss: 0.405266\tLR: 0.00007099\n",
            "Train Epoch: 13 [80128/123872 (65%)]\tLoss: 0.403431\tLR: 0.00007096\n",
            "Train Epoch: 13 [80384/123872 (65%)]\tLoss: 0.331240\tLR: 0.00007092\n",
            "Train Epoch: 13 [80640/123872 (65%)]\tLoss: 0.293014\tLR: 0.00007089\n",
            "Train Epoch: 13 [80896/123872 (65%)]\tLoss: 0.360593\tLR: 0.00007086\n",
            "Train Epoch: 13 [81152/123872 (65%)]\tLoss: 0.350094\tLR: 0.00007082\n",
            "Train Epoch: 13 [81408/123872 (66%)]\tLoss: 0.326106\tLR: 0.00007079\n",
            "Train Epoch: 13 [81664/123872 (66%)]\tLoss: 0.356561\tLR: 0.00007076\n",
            "Train Epoch: 13 [81920/123872 (66%)]\tLoss: 0.349389\tLR: 0.00007072\n",
            "Train Epoch: 13 [81920/123872 (66%)]\tLoss: 0.349389\n",
            "Train Epoch: 13 [82176/123872 (66%)]\tLoss: 0.345000\tLR: 0.00007069\n",
            "Train Epoch: 13 [82432/123872 (67%)]\tLoss: 0.353527\tLR: 0.00007066\n",
            "Train Epoch: 13 [82688/123872 (67%)]\tLoss: 0.296010\tLR: 0.00007062\n",
            "Train Epoch: 13 [82944/123872 (67%)]\tLoss: 0.322742\tLR: 0.00007059\n",
            "Train Epoch: 13 [83200/123872 (67%)]\tLoss: 0.337276\tLR: 0.00007056\n",
            "Train Epoch: 13 [83456/123872 (67%)]\tLoss: 0.294826\tLR: 0.00007053\n",
            "Train Epoch: 13 [83712/123872 (68%)]\tLoss: 0.340227\tLR: 0.00007049\n",
            "Train Epoch: 13 [83968/123872 (68%)]\tLoss: 0.278693\tLR: 0.00007046\n",
            "Train Epoch: 13 [84224/123872 (68%)]\tLoss: 0.367372\tLR: 0.00007043\n",
            "Train Epoch: 13 [84480/123872 (68%)]\tLoss: 0.318341\tLR: 0.00007040\n",
            "Train Epoch: 13 [84480/123872 (68%)]\tLoss: 0.318341\n",
            "Train Epoch: 13 [84736/123872 (68%)]\tLoss: 0.277752\tLR: 0.00007036\n",
            "Train Epoch: 13 [84992/123872 (69%)]\tLoss: 0.374774\tLR: 0.00007033\n",
            "Train Epoch: 13 [85248/123872 (69%)]\tLoss: 0.316214\tLR: 0.00007030\n",
            "Train Epoch: 13 [85504/123872 (69%)]\tLoss: 0.300949\tLR: 0.00007027\n",
            "Train Epoch: 13 [85760/123872 (69%)]\tLoss: 0.338761\tLR: 0.00007024\n",
            "Train Epoch: 13 [86016/123872 (69%)]\tLoss: 0.332318\tLR: 0.00007020\n",
            "Train Epoch: 13 [86272/123872 (70%)]\tLoss: 0.390534\tLR: 0.00007017\n",
            "Train Epoch: 13 [86528/123872 (70%)]\tLoss: 0.283434\tLR: 0.00007014\n",
            "Train Epoch: 13 [86784/123872 (70%)]\tLoss: 0.371325\tLR: 0.00007011\n",
            "Train Epoch: 13 [87040/123872 (70%)]\tLoss: 0.299391\tLR: 0.00007007\n",
            "Train Epoch: 13 [87040/123872 (70%)]\tLoss: 0.299391\n",
            "Train Epoch: 13 [87296/123872 (70%)]\tLoss: 0.346100\tLR: 0.00007004\n",
            "Train Epoch: 13 [87552/123872 (71%)]\tLoss: 0.349880\tLR: 0.00007001\n",
            "Train Epoch: 13 [87808/123872 (71%)]\tLoss: 0.271431\tLR: 0.00006998\n",
            "Train Epoch: 13 [88064/123872 (71%)]\tLoss: 0.324978\tLR: 0.00006995\n",
            "Train Epoch: 13 [88320/123872 (71%)]\tLoss: 0.320838\tLR: 0.00006992\n",
            "Train Epoch: 13 [88576/123872 (71%)]\tLoss: 0.330675\tLR: 0.00006988\n",
            "Train Epoch: 13 [88832/123872 (72%)]\tLoss: 0.402458\tLR: 0.00006985\n",
            "Train Epoch: 13 [89088/123872 (72%)]\tLoss: 0.375865\tLR: 0.00006982\n",
            "Train Epoch: 13 [89344/123872 (72%)]\tLoss: 0.337950\tLR: 0.00006979\n",
            "Train Epoch: 13 [89600/123872 (72%)]\tLoss: 0.327767\tLR: 0.00006976\n",
            "Train Epoch: 13 [89600/123872 (72%)]\tLoss: 0.327767\n",
            "Train Epoch: 13 [89856/123872 (73%)]\tLoss: 0.280722\tLR: 0.00006973\n",
            "Train Epoch: 13 [90112/123872 (73%)]\tLoss: 0.334389\tLR: 0.00006970\n",
            "Train Epoch: 13 [90368/123872 (73%)]\tLoss: 0.317469\tLR: 0.00006966\n",
            "Train Epoch: 13 [90624/123872 (73%)]\tLoss: 0.376056\tLR: 0.00006963\n",
            "Train Epoch: 13 [90880/123872 (73%)]\tLoss: 0.319680\tLR: 0.00006960\n",
            "Train Epoch: 13 [91136/123872 (74%)]\tLoss: 0.400258\tLR: 0.00006957\n",
            "Train Epoch: 13 [91392/123872 (74%)]\tLoss: 0.337501\tLR: 0.00006954\n",
            "Train Epoch: 13 [91648/123872 (74%)]\tLoss: 0.365643\tLR: 0.00006951\n",
            "Train Epoch: 13 [91904/123872 (74%)]\tLoss: 0.325763\tLR: 0.00006948\n",
            "Train Epoch: 13 [92160/123872 (74%)]\tLoss: 0.303489\tLR: 0.00006945\n",
            "Train Epoch: 13 [92160/123872 (74%)]\tLoss: 0.303489\n",
            "Train Epoch: 13 [92416/123872 (75%)]\tLoss: 0.322862\tLR: 0.00006942\n",
            "Train Epoch: 13 [92672/123872 (75%)]\tLoss: 0.331541\tLR: 0.00006938\n",
            "Train Epoch: 13 [92928/123872 (75%)]\tLoss: 0.355778\tLR: 0.00006935\n",
            "Train Epoch: 13 [93184/123872 (75%)]\tLoss: 0.311766\tLR: 0.00006932\n",
            "Train Epoch: 13 [93440/123872 (75%)]\tLoss: 0.365277\tLR: 0.00006929\n",
            "Train Epoch: 13 [93696/123872 (76%)]\tLoss: 0.325022\tLR: 0.00006926\n",
            "Train Epoch: 13 [93952/123872 (76%)]\tLoss: 0.346897\tLR: 0.00006923\n",
            "Train Epoch: 13 [94208/123872 (76%)]\tLoss: 0.388010\tLR: 0.00006920\n",
            "Train Epoch: 13 [94464/123872 (76%)]\tLoss: 0.315134\tLR: 0.00006917\n",
            "Train Epoch: 13 [94720/123872 (76%)]\tLoss: 0.340163\tLR: 0.00006914\n",
            "Train Epoch: 13 [94720/123872 (76%)]\tLoss: 0.340163\n",
            "Train Epoch: 13 [94976/123872 (77%)]\tLoss: 0.367871\tLR: 0.00006911\n",
            "Train Epoch: 13 [95232/123872 (77%)]\tLoss: 0.350700\tLR: 0.00006908\n",
            "Train Epoch: 13 [95488/123872 (77%)]\tLoss: 0.320050\tLR: 0.00006905\n",
            "Train Epoch: 13 [95744/123872 (77%)]\tLoss: 0.337151\tLR: 0.00006902\n",
            "Train Epoch: 13 [96000/123872 (77%)]\tLoss: 0.299208\tLR: 0.00006899\n",
            "Train Epoch: 13 [96256/123872 (78%)]\tLoss: 0.351613\tLR: 0.00006896\n",
            "Train Epoch: 13 [96512/123872 (78%)]\tLoss: 0.324810\tLR: 0.00006893\n",
            "Train Epoch: 13 [96768/123872 (78%)]\tLoss: 0.326107\tLR: 0.00006890\n",
            "Train Epoch: 13 [97024/123872 (78%)]\tLoss: 0.338272\tLR: 0.00006887\n",
            "Train Epoch: 13 [97280/123872 (79%)]\tLoss: 0.318494\tLR: 0.00006884\n",
            "Train Epoch: 13 [97280/123872 (79%)]\tLoss: 0.318494\n",
            "Train Epoch: 13 [97536/123872 (79%)]\tLoss: 0.350948\tLR: 0.00006881\n",
            "Train Epoch: 13 [97792/123872 (79%)]\tLoss: 0.295159\tLR: 0.00006878\n",
            "Train Epoch: 13 [98048/123872 (79%)]\tLoss: 0.336219\tLR: 0.00006875\n",
            "Train Epoch: 13 [98304/123872 (79%)]\tLoss: 0.308589\tLR: 0.00006872\n",
            "Train Epoch: 13 [98560/123872 (80%)]\tLoss: 0.404055\tLR: 0.00006869\n",
            "Train Epoch: 13 [98816/123872 (80%)]\tLoss: 0.318450\tLR: 0.00006866\n",
            "Train Epoch: 13 [99072/123872 (80%)]\tLoss: 0.307438\tLR: 0.00006863\n",
            "Train Epoch: 13 [99328/123872 (80%)]\tLoss: 0.357831\tLR: 0.00006860\n",
            "Train Epoch: 13 [99584/123872 (80%)]\tLoss: 0.359098\tLR: 0.00006857\n",
            "Train Epoch: 13 [99840/123872 (81%)]\tLoss: 0.394384\tLR: 0.00006854\n",
            "Train Epoch: 13 [99840/123872 (81%)]\tLoss: 0.394384\n",
            "Train Epoch: 13 [100096/123872 (81%)]\tLoss: 0.323364\tLR: 0.00006851\n",
            "Train Epoch: 13 [100352/123872 (81%)]\tLoss: 0.331356\tLR: 0.00006848\n",
            "Train Epoch: 13 [100608/123872 (81%)]\tLoss: 0.365769\tLR: 0.00006845\n",
            "Train Epoch: 13 [100864/123872 (81%)]\tLoss: 0.333154\tLR: 0.00006842\n",
            "Train Epoch: 13 [101120/123872 (82%)]\tLoss: 0.329454\tLR: 0.00006839\n",
            "Train Epoch: 13 [101376/123872 (82%)]\tLoss: 0.383002\tLR: 0.00006836\n",
            "Train Epoch: 13 [101632/123872 (82%)]\tLoss: 0.295966\tLR: 0.00006834\n",
            "Train Epoch: 13 [101888/123872 (82%)]\tLoss: 0.320425\tLR: 0.00006831\n",
            "Train Epoch: 13 [102144/123872 (82%)]\tLoss: 0.317955\tLR: 0.00006828\n",
            "Train Epoch: 13 [102400/123872 (83%)]\tLoss: 0.309893\tLR: 0.00006825\n",
            "Train Epoch: 13 [102400/123872 (83%)]\tLoss: 0.309893\n",
            "Train Epoch: 13 [102656/123872 (83%)]\tLoss: 0.321048\tLR: 0.00006822\n",
            "Train Epoch: 13 [102912/123872 (83%)]\tLoss: 0.347494\tLR: 0.00006819\n",
            "Train Epoch: 13 [103168/123872 (83%)]\tLoss: 0.352565\tLR: 0.00006816\n",
            "Train Epoch: 13 [103424/123872 (83%)]\tLoss: 0.300151\tLR: 0.00006813\n",
            "Train Epoch: 13 [103680/123872 (84%)]\tLoss: 0.301339\tLR: 0.00006810\n",
            "Train Epoch: 13 [103936/123872 (84%)]\tLoss: 0.355168\tLR: 0.00006808\n",
            "Train Epoch: 13 [104192/123872 (84%)]\tLoss: 0.331802\tLR: 0.00006805\n",
            "Train Epoch: 13 [104448/123872 (84%)]\tLoss: 0.380546\tLR: 0.00006802\n",
            "Train Epoch: 13 [104704/123872 (85%)]\tLoss: 0.332460\tLR: 0.00006799\n",
            "Train Epoch: 13 [104960/123872 (85%)]\tLoss: 0.378279\tLR: 0.00006796\n",
            "Train Epoch: 13 [104960/123872 (85%)]\tLoss: 0.378279\n",
            "Train Epoch: 13 [105216/123872 (85%)]\tLoss: 0.352453\tLR: 0.00006793\n",
            "Train Epoch: 13 [105472/123872 (85%)]\tLoss: 0.308677\tLR: 0.00006790\n",
            "Train Epoch: 13 [105728/123872 (85%)]\tLoss: 0.311299\tLR: 0.00006788\n",
            "Train Epoch: 13 [105984/123872 (86%)]\tLoss: 0.381462\tLR: 0.00006785\n",
            "Train Epoch: 13 [106240/123872 (86%)]\tLoss: 0.278405\tLR: 0.00006782\n",
            "Train Epoch: 13 [106496/123872 (86%)]\tLoss: 0.288936\tLR: 0.00006779\n",
            "Train Epoch: 13 [106752/123872 (86%)]\tLoss: 0.370633\tLR: 0.00006776\n",
            "Train Epoch: 13 [107008/123872 (86%)]\tLoss: 0.341888\tLR: 0.00006774\n",
            "Train Epoch: 13 [107264/123872 (87%)]\tLoss: 0.310681\tLR: 0.00006771\n",
            "Train Epoch: 13 [107520/123872 (87%)]\tLoss: 0.394939\tLR: 0.00006768\n",
            "Train Epoch: 13 [107520/123872 (87%)]\tLoss: 0.394939\n",
            "Train Epoch: 13 [107776/123872 (87%)]\tLoss: 0.331705\tLR: 0.00006765\n",
            "Train Epoch: 13 [108032/123872 (87%)]\tLoss: 0.346486\tLR: 0.00006762\n",
            "Train Epoch: 13 [108288/123872 (87%)]\tLoss: 0.290849\tLR: 0.00006760\n",
            "Train Epoch: 13 [108544/123872 (88%)]\tLoss: 0.291417\tLR: 0.00006757\n",
            "Train Epoch: 13 [108800/123872 (88%)]\tLoss: 0.311512\tLR: 0.00006754\n",
            "Train Epoch: 13 [109056/123872 (88%)]\tLoss: 0.290194\tLR: 0.00006751\n",
            "Train Epoch: 13 [109312/123872 (88%)]\tLoss: 0.340871\tLR: 0.00006748\n",
            "Train Epoch: 13 [109568/123872 (88%)]\tLoss: 0.361277\tLR: 0.00006746\n",
            "Train Epoch: 13 [109824/123872 (89%)]\tLoss: 0.349094\tLR: 0.00006743\n",
            "Train Epoch: 13 [110080/123872 (89%)]\tLoss: 0.306690\tLR: 0.00006740\n",
            "Train Epoch: 13 [110080/123872 (89%)]\tLoss: 0.306690\n",
            "Train Epoch: 13 [110336/123872 (89%)]\tLoss: 0.360916\tLR: 0.00006737\n",
            "Train Epoch: 13 [110592/123872 (89%)]\tLoss: 0.392253\tLR: 0.00006735\n",
            "Train Epoch: 13 [110848/123872 (89%)]\tLoss: 0.406130\tLR: 0.00006732\n",
            "Train Epoch: 13 [111104/123872 (90%)]\tLoss: 0.381944\tLR: 0.00006729\n",
            "Train Epoch: 13 [111360/123872 (90%)]\tLoss: 0.364218\tLR: 0.00006727\n",
            "Train Epoch: 13 [111616/123872 (90%)]\tLoss: 0.348888\tLR: 0.00006724\n",
            "Train Epoch: 13 [111872/123872 (90%)]\tLoss: 0.336134\tLR: 0.00006721\n",
            "Train Epoch: 13 [112128/123872 (90%)]\tLoss: 0.282104\tLR: 0.00006718\n",
            "Train Epoch: 13 [112384/123872 (91%)]\tLoss: 0.292010\tLR: 0.00006716\n",
            "Train Epoch: 13 [112640/123872 (91%)]\tLoss: 0.347019\tLR: 0.00006713\n",
            "Train Epoch: 13 [112640/123872 (91%)]\tLoss: 0.347019\n",
            "Train Epoch: 13 [112896/123872 (91%)]\tLoss: 0.305292\tLR: 0.00006710\n",
            "Train Epoch: 13 [113152/123872 (91%)]\tLoss: 0.329790\tLR: 0.00006708\n",
            "Train Epoch: 13 [113408/123872 (92%)]\tLoss: 0.310639\tLR: 0.00006705\n",
            "Train Epoch: 13 [113664/123872 (92%)]\tLoss: 0.360171\tLR: 0.00006702\n",
            "Train Epoch: 13 [113920/123872 (92%)]\tLoss: 0.374988\tLR: 0.00006700\n",
            "Train Epoch: 13 [114176/123872 (92%)]\tLoss: 0.320147\tLR: 0.00006697\n",
            "Train Epoch: 13 [114432/123872 (92%)]\tLoss: 0.330676\tLR: 0.00006694\n",
            "Train Epoch: 13 [114688/123872 (93%)]\tLoss: 0.299080\tLR: 0.00006692\n",
            "Train Epoch: 13 [114944/123872 (93%)]\tLoss: 0.293705\tLR: 0.00006689\n",
            "Train Epoch: 13 [115200/123872 (93%)]\tLoss: 0.382514\tLR: 0.00006686\n",
            "Train Epoch: 13 [115200/123872 (93%)]\tLoss: 0.382514\n",
            "Train Epoch: 13 [115456/123872 (93%)]\tLoss: 0.337507\tLR: 0.00006684\n",
            "Train Epoch: 13 [115712/123872 (93%)]\tLoss: 0.327443\tLR: 0.00006681\n",
            "Train Epoch: 13 [115968/123872 (94%)]\tLoss: 0.319118\tLR: 0.00006678\n",
            "Train Epoch: 13 [116224/123872 (94%)]\tLoss: 0.365681\tLR: 0.00006676\n",
            "Train Epoch: 13 [116480/123872 (94%)]\tLoss: 0.321726\tLR: 0.00006673\n",
            "Train Epoch: 13 [116736/123872 (94%)]\tLoss: 0.292705\tLR: 0.00006671\n",
            "Train Epoch: 13 [116992/123872 (94%)]\tLoss: 0.294469\tLR: 0.00006668\n",
            "Train Epoch: 13 [117248/123872 (95%)]\tLoss: 0.337891\tLR: 0.00006665\n",
            "Train Epoch: 13 [117504/123872 (95%)]\tLoss: 0.348372\tLR: 0.00006663\n",
            "Train Epoch: 13 [117760/123872 (95%)]\tLoss: 0.339973\tLR: 0.00006660\n",
            "Train Epoch: 13 [117760/123872 (95%)]\tLoss: 0.339973\n",
            "Train Epoch: 13 [118016/123872 (95%)]\tLoss: 0.337123\tLR: 0.00006658\n",
            "Train Epoch: 13 [118272/123872 (95%)]\tLoss: 0.331669\tLR: 0.00006655\n",
            "Train Epoch: 13 [118528/123872 (96%)]\tLoss: 0.350485\tLR: 0.00006652\n",
            "Train Epoch: 13 [118784/123872 (96%)]\tLoss: 0.292709\tLR: 0.00006650\n",
            "Train Epoch: 13 [119040/123872 (96%)]\tLoss: 0.393620\tLR: 0.00006647\n",
            "Train Epoch: 13 [119296/123872 (96%)]\tLoss: 0.413533\tLR: 0.00006645\n",
            "Train Epoch: 13 [119552/123872 (96%)]\tLoss: 0.329600\tLR: 0.00006642\n",
            "Train Epoch: 13 [119808/123872 (97%)]\tLoss: 0.398180\tLR: 0.00006640\n",
            "Train Epoch: 13 [120064/123872 (97%)]\tLoss: 0.372358\tLR: 0.00006637\n",
            "Train Epoch: 13 [120320/123872 (97%)]\tLoss: 0.291653\tLR: 0.00006634\n",
            "Train Epoch: 13 [120320/123872 (97%)]\tLoss: 0.291653\n",
            "Train Epoch: 13 [120576/123872 (97%)]\tLoss: 0.356714\tLR: 0.00006632\n",
            "Train Epoch: 13 [120832/123872 (98%)]\tLoss: 0.415462\tLR: 0.00006629\n",
            "Train Epoch: 13 [121088/123872 (98%)]\tLoss: 0.404032\tLR: 0.00006627\n",
            "Train Epoch: 13 [121344/123872 (98%)]\tLoss: 0.328371\tLR: 0.00006624\n",
            "Train Epoch: 13 [121600/123872 (98%)]\tLoss: 0.321300\tLR: 0.00006622\n",
            "Train Epoch: 13 [121856/123872 (98%)]\tLoss: 0.353722\tLR: 0.00006619\n",
            "Train Epoch: 13 [122112/123872 (99%)]\tLoss: 0.307632\tLR: 0.00006617\n",
            "Train Epoch: 13 [122368/123872 (99%)]\tLoss: 0.351209\tLR: 0.00006614\n",
            "Train Epoch: 13 [122624/123872 (99%)]\tLoss: 0.312936\tLR: 0.00006612\n",
            "Train Epoch: 13 [122880/123872 (99%)]\tLoss: 0.301360\tLR: 0.00006609\n",
            "Train Epoch: 13 [122880/123872 (99%)]\tLoss: 0.301360\n",
            "Train Epoch: 13 [123136/123872 (99%)]\tLoss: 0.273285\tLR: 0.00006607\n",
            "Train Epoch: 13 [123392/123872 (100%)]\tLoss: 0.354237\tLR: 0.00006604\n",
            "Train Epoch: 13 [108192/123872 (100%)]\tLoss: 0.381353\tLR: 0.00006602\n",
            "\n",
            "Test set: Average loss: 0.0014, Accuracy: 26111/30970 (84.31%)\n",
            "\n",
            "Train Epoch: 14 [0/123872 (0%)]\tLoss: 0.390817\tLR: 0.00006599\n",
            "Train Epoch: 14 [0/123872 (0%)]\tLoss: 0.390817\n",
            "Train Epoch: 14 [256/123872 (0%)]\tLoss: 0.363912\tLR: 0.00006597\n",
            "Train Epoch: 14 [512/123872 (0%)]\tLoss: 0.278852\tLR: 0.00006594\n",
            "Train Epoch: 14 [768/123872 (1%)]\tLoss: 0.311173\tLR: 0.00006592\n",
            "Train Epoch: 14 [1024/123872 (1%)]\tLoss: 0.321882\tLR: 0.00006590\n",
            "Train Epoch: 14 [1280/123872 (1%)]\tLoss: 0.297146\tLR: 0.00006587\n",
            "Train Epoch: 14 [1536/123872 (1%)]\tLoss: 0.364989\tLR: 0.00006585\n",
            "Train Epoch: 14 [1792/123872 (1%)]\tLoss: 0.353394\tLR: 0.00006582\n",
            "Train Epoch: 14 [2048/123872 (2%)]\tLoss: 0.362623\tLR: 0.00006580\n",
            "Train Epoch: 14 [2304/123872 (2%)]\tLoss: 0.376136\tLR: 0.00006577\n",
            "Train Epoch: 14 [2560/123872 (2%)]\tLoss: 0.344717\tLR: 0.00006575\n",
            "Train Epoch: 14 [2560/123872 (2%)]\tLoss: 0.344717\n",
            "Train Epoch: 14 [2816/123872 (2%)]\tLoss: 0.352123\tLR: 0.00006572\n",
            "Train Epoch: 14 [3072/123872 (2%)]\tLoss: 0.367511\tLR: 0.00006570\n",
            "Train Epoch: 14 [3328/123872 (3%)]\tLoss: 0.342313\tLR: 0.00006568\n",
            "Train Epoch: 14 [3584/123872 (3%)]\tLoss: 0.338359\tLR: 0.00006565\n",
            "Train Epoch: 14 [3840/123872 (3%)]\tLoss: 0.310807\tLR: 0.00006563\n",
            "Train Epoch: 14 [4096/123872 (3%)]\tLoss: 0.321728\tLR: 0.00006560\n",
            "Train Epoch: 14 [4352/123872 (4%)]\tLoss: 0.355806\tLR: 0.00006558\n",
            "Train Epoch: 14 [4608/123872 (4%)]\tLoss: 0.305630\tLR: 0.00006556\n",
            "Train Epoch: 14 [4864/123872 (4%)]\tLoss: 0.297055\tLR: 0.00006553\n",
            "Train Epoch: 14 [5120/123872 (4%)]\tLoss: 0.313025\tLR: 0.00006551\n",
            "Train Epoch: 14 [5120/123872 (4%)]\tLoss: 0.313025\n",
            "Train Epoch: 14 [5376/123872 (4%)]\tLoss: 0.356830\tLR: 0.00006549\n",
            "Train Epoch: 14 [5632/123872 (5%)]\tLoss: 0.337947\tLR: 0.00006546\n",
            "Train Epoch: 14 [5888/123872 (5%)]\tLoss: 0.345314\tLR: 0.00006544\n",
            "Train Epoch: 14 [6144/123872 (5%)]\tLoss: 0.327935\tLR: 0.00006541\n",
            "Train Epoch: 14 [6400/123872 (5%)]\tLoss: 0.335844\tLR: 0.00006539\n",
            "Train Epoch: 14 [6656/123872 (5%)]\tLoss: 0.319518\tLR: 0.00006537\n",
            "Train Epoch: 14 [6912/123872 (6%)]\tLoss: 0.272337\tLR: 0.00006534\n",
            "Train Epoch: 14 [7168/123872 (6%)]\tLoss: 0.253415\tLR: 0.00006532\n",
            "Train Epoch: 14 [7424/123872 (6%)]\tLoss: 0.315608\tLR: 0.00006530\n",
            "Train Epoch: 14 [7680/123872 (6%)]\tLoss: 0.327281\tLR: 0.00006527\n",
            "Train Epoch: 14 [7680/123872 (6%)]\tLoss: 0.327281\n",
            "Train Epoch: 14 [7936/123872 (6%)]\tLoss: 0.313800\tLR: 0.00006525\n",
            "Train Epoch: 14 [8192/123872 (7%)]\tLoss: 0.321437\tLR: 0.00006523\n",
            "Train Epoch: 14 [8448/123872 (7%)]\tLoss: 0.349862\tLR: 0.00006521\n",
            "Train Epoch: 14 [8704/123872 (7%)]\tLoss: 0.369613\tLR: 0.00006518\n",
            "Train Epoch: 14 [8960/123872 (7%)]\tLoss: 0.323515\tLR: 0.00006516\n",
            "Train Epoch: 14 [9216/123872 (7%)]\tLoss: 0.348963\tLR: 0.00006514\n",
            "Train Epoch: 14 [9472/123872 (8%)]\tLoss: 0.336485\tLR: 0.00006511\n",
            "Train Epoch: 14 [9728/123872 (8%)]\tLoss: 0.292125\tLR: 0.00006509\n",
            "Train Epoch: 14 [9984/123872 (8%)]\tLoss: 0.316701\tLR: 0.00006507\n",
            "Train Epoch: 14 [10240/123872 (8%)]\tLoss: 0.282696\tLR: 0.00006505\n",
            "Train Epoch: 14 [10240/123872 (8%)]\tLoss: 0.282696\n",
            "Train Epoch: 14 [10496/123872 (8%)]\tLoss: 0.273309\tLR: 0.00006502\n",
            "Train Epoch: 14 [10752/123872 (9%)]\tLoss: 0.317528\tLR: 0.00006500\n",
            "Train Epoch: 14 [11008/123872 (9%)]\tLoss: 0.303684\tLR: 0.00006498\n",
            "Train Epoch: 14 [11264/123872 (9%)]\tLoss: 0.357977\tLR: 0.00006495\n",
            "Train Epoch: 14 [11520/123872 (9%)]\tLoss: 0.231562\tLR: 0.00006493\n",
            "Train Epoch: 14 [11776/123872 (10%)]\tLoss: 0.328238\tLR: 0.00006491\n",
            "Train Epoch: 14 [12032/123872 (10%)]\tLoss: 0.366924\tLR: 0.00006489\n",
            "Train Epoch: 14 [12288/123872 (10%)]\tLoss: 0.322550\tLR: 0.00006487\n",
            "Train Epoch: 14 [12544/123872 (10%)]\tLoss: 0.307255\tLR: 0.00006484\n",
            "Train Epoch: 14 [12800/123872 (10%)]\tLoss: 0.303975\tLR: 0.00006482\n",
            "Train Epoch: 14 [12800/123872 (10%)]\tLoss: 0.303975\n",
            "Train Epoch: 14 [13056/123872 (11%)]\tLoss: 0.351474\tLR: 0.00006480\n",
            "Train Epoch: 14 [13312/123872 (11%)]\tLoss: 0.292246\tLR: 0.00006478\n",
            "Train Epoch: 14 [13568/123872 (11%)]\tLoss: 0.337241\tLR: 0.00006475\n",
            "Train Epoch: 14 [13824/123872 (11%)]\tLoss: 0.420321\tLR: 0.00006473\n",
            "Train Epoch: 14 [14080/123872 (11%)]\tLoss: 0.314743\tLR: 0.00006471\n",
            "Train Epoch: 14 [14336/123872 (12%)]\tLoss: 0.318333\tLR: 0.00006469\n",
            "Train Epoch: 14 [14592/123872 (12%)]\tLoss: 0.320162\tLR: 0.00006467\n",
            "Train Epoch: 14 [14848/123872 (12%)]\tLoss: 0.375254\tLR: 0.00006464\n",
            "Train Epoch: 14 [15104/123872 (12%)]\tLoss: 0.326793\tLR: 0.00006462\n",
            "Train Epoch: 14 [15360/123872 (12%)]\tLoss: 0.322716\tLR: 0.00006460\n",
            "Train Epoch: 14 [15360/123872 (12%)]\tLoss: 0.322716\n",
            "Train Epoch: 14 [15616/123872 (13%)]\tLoss: 0.324670\tLR: 0.00006458\n",
            "Train Epoch: 14 [15872/123872 (13%)]\tLoss: 0.325620\tLR: 0.00006456\n",
            "Train Epoch: 14 [16128/123872 (13%)]\tLoss: 0.331287\tLR: 0.00006454\n",
            "Train Epoch: 14 [16384/123872 (13%)]\tLoss: 0.330529\tLR: 0.00006451\n",
            "Train Epoch: 14 [16640/123872 (13%)]\tLoss: 0.345841\tLR: 0.00006449\n",
            "Train Epoch: 14 [16896/123872 (14%)]\tLoss: 0.319854\tLR: 0.00006447\n",
            "Train Epoch: 14 [17152/123872 (14%)]\tLoss: 0.363351\tLR: 0.00006445\n",
            "Train Epoch: 14 [17408/123872 (14%)]\tLoss: 0.318811\tLR: 0.00006443\n",
            "Train Epoch: 14 [17664/123872 (14%)]\tLoss: 0.323503\tLR: 0.00006441\n",
            "Train Epoch: 14 [17920/123872 (14%)]\tLoss: 0.308461\tLR: 0.00006439\n",
            "Train Epoch: 14 [17920/123872 (14%)]\tLoss: 0.308461\n",
            "Train Epoch: 14 [18176/123872 (15%)]\tLoss: 0.322321\tLR: 0.00006437\n",
            "Train Epoch: 14 [18432/123872 (15%)]\tLoss: 0.376799\tLR: 0.00006434\n",
            "Train Epoch: 14 [18688/123872 (15%)]\tLoss: 0.421106\tLR: 0.00006432\n",
            "Train Epoch: 14 [18944/123872 (15%)]\tLoss: 0.349828\tLR: 0.00006430\n",
            "Train Epoch: 14 [19200/123872 (15%)]\tLoss: 0.320673\tLR: 0.00006428\n",
            "Train Epoch: 14 [19456/123872 (16%)]\tLoss: 0.295233\tLR: 0.00006426\n",
            "Train Epoch: 14 [19712/123872 (16%)]\tLoss: 0.349055\tLR: 0.00006424\n",
            "Train Epoch: 14 [19968/123872 (16%)]\tLoss: 0.294683\tLR: 0.00006422\n",
            "Train Epoch: 14 [20224/123872 (16%)]\tLoss: 0.348336\tLR: 0.00006420\n",
            "Train Epoch: 14 [20480/123872 (17%)]\tLoss: 0.325744\tLR: 0.00006418\n",
            "Train Epoch: 14 [20480/123872 (17%)]\tLoss: 0.325744\n",
            "Train Epoch: 14 [20736/123872 (17%)]\tLoss: 0.330955\tLR: 0.00006416\n",
            "Train Epoch: 14 [20992/123872 (17%)]\tLoss: 0.298349\tLR: 0.00006414\n",
            "Train Epoch: 14 [21248/123872 (17%)]\tLoss: 0.284719\tLR: 0.00006412\n",
            "Train Epoch: 14 [21504/123872 (17%)]\tLoss: 0.336133\tLR: 0.00006410\n",
            "Train Epoch: 14 [21760/123872 (18%)]\tLoss: 0.403996\tLR: 0.00006407\n",
            "Train Epoch: 14 [22016/123872 (18%)]\tLoss: 0.388741\tLR: 0.00006405\n",
            "Train Epoch: 14 [22272/123872 (18%)]\tLoss: 0.305659\tLR: 0.00006403\n",
            "Train Epoch: 14 [22528/123872 (18%)]\tLoss: 0.368234\tLR: 0.00006401\n",
            "Train Epoch: 14 [22784/123872 (18%)]\tLoss: 0.326162\tLR: 0.00006399\n",
            "Train Epoch: 14 [23040/123872 (19%)]\tLoss: 0.283366\tLR: 0.00006397\n",
            "Train Epoch: 14 [23040/123872 (19%)]\tLoss: 0.283366\n",
            "Train Epoch: 14 [23296/123872 (19%)]\tLoss: 0.342864\tLR: 0.00006395\n",
            "Train Epoch: 14 [23552/123872 (19%)]\tLoss: 0.365930\tLR: 0.00006393\n",
            "Train Epoch: 14 [23808/123872 (19%)]\tLoss: 0.348855\tLR: 0.00006391\n",
            "Train Epoch: 14 [24064/123872 (19%)]\tLoss: 0.342565\tLR: 0.00006389\n",
            "Train Epoch: 14 [24320/123872 (20%)]\tLoss: 0.304487\tLR: 0.00006387\n",
            "Train Epoch: 14 [24576/123872 (20%)]\tLoss: 0.335053\tLR: 0.00006385\n",
            "Train Epoch: 14 [24832/123872 (20%)]\tLoss: 0.383525\tLR: 0.00006383\n",
            "Train Epoch: 14 [25088/123872 (20%)]\tLoss: 0.384486\tLR: 0.00006381\n",
            "Train Epoch: 14 [25344/123872 (20%)]\tLoss: 0.347143\tLR: 0.00006379\n",
            "Train Epoch: 14 [25600/123872 (21%)]\tLoss: 0.332770\tLR: 0.00006377\n",
            "Train Epoch: 14 [25600/123872 (21%)]\tLoss: 0.332770\n",
            "Train Epoch: 14 [25856/123872 (21%)]\tLoss: 0.359393\tLR: 0.00006375\n",
            "Train Epoch: 14 [26112/123872 (21%)]\tLoss: 0.280302\tLR: 0.00006373\n",
            "Train Epoch: 14 [26368/123872 (21%)]\tLoss: 0.352316\tLR: 0.00006372\n",
            "Train Epoch: 14 [26624/123872 (21%)]\tLoss: 0.324272\tLR: 0.00006370\n",
            "Train Epoch: 14 [26880/123872 (22%)]\tLoss: 0.319922\tLR: 0.00006368\n",
            "Train Epoch: 14 [27136/123872 (22%)]\tLoss: 0.321060\tLR: 0.00006366\n",
            "Train Epoch: 14 [27392/123872 (22%)]\tLoss: 0.342661\tLR: 0.00006364\n",
            "Train Epoch: 14 [27648/123872 (22%)]\tLoss: 0.267142\tLR: 0.00006362\n",
            "Train Epoch: 14 [27904/123872 (23%)]\tLoss: 0.322882\tLR: 0.00006360\n",
            "Train Epoch: 14 [28160/123872 (23%)]\tLoss: 0.296861\tLR: 0.00006358\n",
            "Train Epoch: 14 [28160/123872 (23%)]\tLoss: 0.296861\n",
            "Train Epoch: 14 [28416/123872 (23%)]\tLoss: 0.311843\tLR: 0.00006356\n",
            "Train Epoch: 14 [28672/123872 (23%)]\tLoss: 0.377584\tLR: 0.00006354\n",
            "Train Epoch: 14 [28928/123872 (23%)]\tLoss: 0.400445\tLR: 0.00006352\n",
            "Train Epoch: 14 [29184/123872 (24%)]\tLoss: 0.338540\tLR: 0.00006350\n",
            "Train Epoch: 14 [29440/123872 (24%)]\tLoss: 0.377411\tLR: 0.00006348\n",
            "Train Epoch: 14 [29696/123872 (24%)]\tLoss: 0.322252\tLR: 0.00006347\n",
            "Train Epoch: 14 [29952/123872 (24%)]\tLoss: 0.361324\tLR: 0.00006345\n",
            "Train Epoch: 14 [30208/123872 (24%)]\tLoss: 0.300377\tLR: 0.00006343\n",
            "Train Epoch: 14 [30464/123872 (25%)]\tLoss: 0.309423\tLR: 0.00006341\n",
            "Train Epoch: 14 [30720/123872 (25%)]\tLoss: 0.326556\tLR: 0.00006339\n",
            "Train Epoch: 14 [30720/123872 (25%)]\tLoss: 0.326556\n",
            "Train Epoch: 14 [30976/123872 (25%)]\tLoss: 0.344288\tLR: 0.00006337\n",
            "Train Epoch: 14 [31232/123872 (25%)]\tLoss: 0.366840\tLR: 0.00006335\n",
            "Train Epoch: 14 [31488/123872 (25%)]\tLoss: 0.345156\tLR: 0.00006334\n",
            "Train Epoch: 14 [31744/123872 (26%)]\tLoss: 0.407295\tLR: 0.00006332\n",
            "Train Epoch: 14 [32000/123872 (26%)]\tLoss: 0.314001\tLR: 0.00006330\n",
            "Train Epoch: 14 [32256/123872 (26%)]\tLoss: 0.331458\tLR: 0.00006328\n",
            "Train Epoch: 14 [32512/123872 (26%)]\tLoss: 0.349020\tLR: 0.00006326\n",
            "Train Epoch: 14 [32768/123872 (26%)]\tLoss: 0.287309\tLR: 0.00006324\n",
            "Train Epoch: 14 [33024/123872 (27%)]\tLoss: 0.324267\tLR: 0.00006323\n",
            "Train Epoch: 14 [33280/123872 (27%)]\tLoss: 0.312832\tLR: 0.00006321\n",
            "Train Epoch: 14 [33280/123872 (27%)]\tLoss: 0.312832\n",
            "Train Epoch: 14 [33536/123872 (27%)]\tLoss: 0.282892\tLR: 0.00006319\n",
            "Train Epoch: 14 [33792/123872 (27%)]\tLoss: 0.426659\tLR: 0.00006317\n",
            "Train Epoch: 14 [34048/123872 (27%)]\tLoss: 0.290259\tLR: 0.00006315\n",
            "Train Epoch: 14 [34304/123872 (28%)]\tLoss: 0.343177\tLR: 0.00006313\n",
            "Train Epoch: 14 [34560/123872 (28%)]\tLoss: 0.288373\tLR: 0.00006312\n",
            "Train Epoch: 14 [34816/123872 (28%)]\tLoss: 0.339085\tLR: 0.00006310\n",
            "Train Epoch: 14 [35072/123872 (28%)]\tLoss: 0.351593\tLR: 0.00006308\n",
            "Train Epoch: 14 [35328/123872 (29%)]\tLoss: 0.353883\tLR: 0.00006306\n",
            "Train Epoch: 14 [35584/123872 (29%)]\tLoss: 0.367815\tLR: 0.00006305\n",
            "Train Epoch: 14 [35840/123872 (29%)]\tLoss: 0.276785\tLR: 0.00006303\n",
            "Train Epoch: 14 [35840/123872 (29%)]\tLoss: 0.276785\n",
            "Train Epoch: 14 [36096/123872 (29%)]\tLoss: 0.332182\tLR: 0.00006301\n",
            "Train Epoch: 14 [36352/123872 (29%)]\tLoss: 0.334887\tLR: 0.00006299\n",
            "Train Epoch: 14 [36608/123872 (30%)]\tLoss: 0.291358\tLR: 0.00006298\n",
            "Train Epoch: 14 [36864/123872 (30%)]\tLoss: 0.339803\tLR: 0.00006296\n",
            "Train Epoch: 14 [37120/123872 (30%)]\tLoss: 0.276663\tLR: 0.00006294\n",
            "Train Epoch: 14 [37376/123872 (30%)]\tLoss: 0.282027\tLR: 0.00006292\n",
            "Train Epoch: 14 [37632/123872 (30%)]\tLoss: 0.352891\tLR: 0.00006291\n",
            "Train Epoch: 14 [37888/123872 (31%)]\tLoss: 0.320135\tLR: 0.00006289\n",
            "Train Epoch: 14 [38144/123872 (31%)]\tLoss: 0.399261\tLR: 0.00006287\n",
            "Train Epoch: 14 [38400/123872 (31%)]\tLoss: 0.275049\tLR: 0.00006285\n",
            "Train Epoch: 14 [38400/123872 (31%)]\tLoss: 0.275049\n",
            "Train Epoch: 14 [38656/123872 (31%)]\tLoss: 0.339701\tLR: 0.00006284\n",
            "Train Epoch: 14 [38912/123872 (31%)]\tLoss: 0.271118\tLR: 0.00006282\n",
            "Train Epoch: 14 [39168/123872 (32%)]\tLoss: 0.366722\tLR: 0.00006280\n",
            "Train Epoch: 14 [39424/123872 (32%)]\tLoss: 0.342720\tLR: 0.00006279\n",
            "Train Epoch: 14 [39680/123872 (32%)]\tLoss: 0.318198\tLR: 0.00006277\n",
            "Train Epoch: 14 [39936/123872 (32%)]\tLoss: 0.288733\tLR: 0.00006275\n",
            "Train Epoch: 14 [40192/123872 (32%)]\tLoss: 0.282070\tLR: 0.00006274\n",
            "Train Epoch: 14 [40448/123872 (33%)]\tLoss: 0.310352\tLR: 0.00006272\n",
            "Train Epoch: 14 [40704/123872 (33%)]\tLoss: 0.334110\tLR: 0.00006270\n",
            "Train Epoch: 14 [40960/123872 (33%)]\tLoss: 0.345615\tLR: 0.00006269\n",
            "Train Epoch: 14 [40960/123872 (33%)]\tLoss: 0.345615\n",
            "Train Epoch: 14 [41216/123872 (33%)]\tLoss: 0.292785\tLR: 0.00006267\n",
            "Train Epoch: 14 [41472/123872 (33%)]\tLoss: 0.363277\tLR: 0.00006265\n",
            "Train Epoch: 14 [41728/123872 (34%)]\tLoss: 0.288804\tLR: 0.00006264\n",
            "Train Epoch: 14 [41984/123872 (34%)]\tLoss: 0.319773\tLR: 0.00006262\n",
            "Train Epoch: 14 [42240/123872 (34%)]\tLoss: 0.325973\tLR: 0.00006260\n",
            "Train Epoch: 14 [42496/123872 (34%)]\tLoss: 0.354365\tLR: 0.00006259\n",
            "Train Epoch: 14 [42752/123872 (35%)]\tLoss: 0.329467\tLR: 0.00006257\n",
            "Train Epoch: 14 [43008/123872 (35%)]\tLoss: 0.372516\tLR: 0.00006255\n",
            "Train Epoch: 14 [43264/123872 (35%)]\tLoss: 0.297618\tLR: 0.00006254\n",
            "Train Epoch: 14 [43520/123872 (35%)]\tLoss: 0.357622\tLR: 0.00006252\n",
            "Train Epoch: 14 [43520/123872 (35%)]\tLoss: 0.357622\n",
            "Train Epoch: 14 [43776/123872 (35%)]\tLoss: 0.337369\tLR: 0.00006251\n",
            "Train Epoch: 14 [44032/123872 (36%)]\tLoss: 0.366519\tLR: 0.00006249\n",
            "Train Epoch: 14 [44288/123872 (36%)]\tLoss: 0.304240\tLR: 0.00006247\n",
            "Train Epoch: 14 [44544/123872 (36%)]\tLoss: 0.329309\tLR: 0.00006246\n",
            "Train Epoch: 14 [44800/123872 (36%)]\tLoss: 0.272753\tLR: 0.00006244\n",
            "Train Epoch: 14 [45056/123872 (36%)]\tLoss: 0.338827\tLR: 0.00006243\n",
            "Train Epoch: 14 [45312/123872 (37%)]\tLoss: 0.229849\tLR: 0.00006241\n",
            "Train Epoch: 14 [45568/123872 (37%)]\tLoss: 0.343607\tLR: 0.00006240\n",
            "Train Epoch: 14 [45824/123872 (37%)]\tLoss: 0.308231\tLR: 0.00006238\n",
            "Train Epoch: 14 [46080/123872 (37%)]\tLoss: 0.364194\tLR: 0.00006236\n",
            "Train Epoch: 14 [46080/123872 (37%)]\tLoss: 0.364194\n",
            "Train Epoch: 14 [46336/123872 (37%)]\tLoss: 0.275399\tLR: 0.00006235\n",
            "Train Epoch: 14 [46592/123872 (38%)]\tLoss: 0.309088\tLR: 0.00006233\n",
            "Train Epoch: 14 [46848/123872 (38%)]\tLoss: 0.231952\tLR: 0.00006232\n",
            "Train Epoch: 14 [47104/123872 (38%)]\tLoss: 0.313177\tLR: 0.00006230\n",
            "Train Epoch: 14 [47360/123872 (38%)]\tLoss: 0.334877\tLR: 0.00006229\n",
            "Train Epoch: 14 [47616/123872 (38%)]\tLoss: 0.396247\tLR: 0.00006227\n",
            "Train Epoch: 14 [47872/123872 (39%)]\tLoss: 0.331869\tLR: 0.00006226\n",
            "Train Epoch: 14 [48128/123872 (39%)]\tLoss: 0.289068\tLR: 0.00006224\n",
            "Train Epoch: 14 [48384/123872 (39%)]\tLoss: 0.383559\tLR: 0.00006223\n",
            "Train Epoch: 14 [48640/123872 (39%)]\tLoss: 0.269125\tLR: 0.00006221\n",
            "Train Epoch: 14 [48640/123872 (39%)]\tLoss: 0.269125\n",
            "Train Epoch: 14 [48896/123872 (39%)]\tLoss: 0.330142\tLR: 0.00006220\n",
            "Train Epoch: 14 [49152/123872 (40%)]\tLoss: 0.387662\tLR: 0.00006218\n",
            "Train Epoch: 14 [49408/123872 (40%)]\tLoss: 0.325857\tLR: 0.00006217\n",
            "Train Epoch: 14 [49664/123872 (40%)]\tLoss: 0.309787\tLR: 0.00006215\n",
            "Train Epoch: 14 [49920/123872 (40%)]\tLoss: 0.333416\tLR: 0.00006214\n",
            "Train Epoch: 14 [50176/123872 (40%)]\tLoss: 0.359093\tLR: 0.00006212\n",
            "Train Epoch: 14 [50432/123872 (41%)]\tLoss: 0.345479\tLR: 0.00006211\n",
            "Train Epoch: 14 [50688/123872 (41%)]\tLoss: 0.311055\tLR: 0.00006209\n",
            "Train Epoch: 14 [50944/123872 (41%)]\tLoss: 0.328639\tLR: 0.00006208\n",
            "Train Epoch: 14 [51200/123872 (41%)]\tLoss: 0.357619\tLR: 0.00006206\n",
            "Train Epoch: 14 [51200/123872 (41%)]\tLoss: 0.357619\n",
            "Train Epoch: 14 [51456/123872 (42%)]\tLoss: 0.331127\tLR: 0.00006205\n",
            "Train Epoch: 14 [51712/123872 (42%)]\tLoss: 0.320000\tLR: 0.00006203\n",
            "Train Epoch: 14 [51968/123872 (42%)]\tLoss: 0.348974\tLR: 0.00006202\n",
            "Train Epoch: 14 [52224/123872 (42%)]\tLoss: 0.313500\tLR: 0.00006200\n",
            "Train Epoch: 14 [52480/123872 (42%)]\tLoss: 0.322115\tLR: 0.00006199\n",
            "Train Epoch: 14 [52736/123872 (43%)]\tLoss: 0.388164\tLR: 0.00006198\n",
            "Train Epoch: 14 [52992/123872 (43%)]\tLoss: 0.302354\tLR: 0.00006196\n",
            "Train Epoch: 14 [53248/123872 (43%)]\tLoss: 0.247809\tLR: 0.00006195\n",
            "Train Epoch: 14 [53504/123872 (43%)]\tLoss: 0.347627\tLR: 0.00006193\n",
            "Train Epoch: 14 [53760/123872 (43%)]\tLoss: 0.330608\tLR: 0.00006192\n",
            "Train Epoch: 14 [53760/123872 (43%)]\tLoss: 0.330608\n",
            "Train Epoch: 14 [54016/123872 (44%)]\tLoss: 0.294636\tLR: 0.00006191\n",
            "Train Epoch: 14 [54272/123872 (44%)]\tLoss: 0.317898\tLR: 0.00006189\n",
            "Train Epoch: 14 [54528/123872 (44%)]\tLoss: 0.341862\tLR: 0.00006188\n",
            "Train Epoch: 14 [54784/123872 (44%)]\tLoss: 0.315467\tLR: 0.00006186\n",
            "Train Epoch: 14 [55040/123872 (44%)]\tLoss: 0.253645\tLR: 0.00006185\n",
            "Train Epoch: 14 [55296/123872 (45%)]\tLoss: 0.298741\tLR: 0.00006184\n",
            "Train Epoch: 14 [55552/123872 (45%)]\tLoss: 0.344158\tLR: 0.00006182\n",
            "Train Epoch: 14 [55808/123872 (45%)]\tLoss: 0.318953\tLR: 0.00006181\n",
            "Train Epoch: 14 [56064/123872 (45%)]\tLoss: 0.331179\tLR: 0.00006180\n",
            "Train Epoch: 14 [56320/123872 (45%)]\tLoss: 0.331765\tLR: 0.00006178\n",
            "Train Epoch: 14 [56320/123872 (45%)]\tLoss: 0.331765\n",
            "Train Epoch: 14 [56576/123872 (46%)]\tLoss: 0.338996\tLR: 0.00006177\n",
            "Train Epoch: 14 [56832/123872 (46%)]\tLoss: 0.373890\tLR: 0.00006175\n",
            "Train Epoch: 14 [57088/123872 (46%)]\tLoss: 0.346807\tLR: 0.00006174\n",
            "Train Epoch: 14 [57344/123872 (46%)]\tLoss: 0.334357\tLR: 0.00006173\n",
            "Train Epoch: 14 [57600/123872 (46%)]\tLoss: 0.278723\tLR: 0.00006171\n",
            "Train Epoch: 14 [57856/123872 (47%)]\tLoss: 0.293587\tLR: 0.00006170\n",
            "Train Epoch: 14 [58112/123872 (47%)]\tLoss: 0.317447\tLR: 0.00006169\n",
            "Train Epoch: 14 [58368/123872 (47%)]\tLoss: 0.310246\tLR: 0.00006168\n",
            "Train Epoch: 14 [58624/123872 (47%)]\tLoss: 0.320700\tLR: 0.00006166\n",
            "Train Epoch: 14 [58880/123872 (48%)]\tLoss: 0.327925\tLR: 0.00006165\n",
            "Train Epoch: 14 [58880/123872 (48%)]\tLoss: 0.327925\n",
            "Train Epoch: 14 [59136/123872 (48%)]\tLoss: 0.279266\tLR: 0.00006164\n",
            "Train Epoch: 14 [59392/123872 (48%)]\tLoss: 0.323067\tLR: 0.00006162\n",
            "Train Epoch: 14 [59648/123872 (48%)]\tLoss: 0.326578\tLR: 0.00006161\n",
            "Train Epoch: 14 [59904/123872 (48%)]\tLoss: 0.369830\tLR: 0.00006160\n",
            "Train Epoch: 14 [60160/123872 (49%)]\tLoss: 0.336067\tLR: 0.00006158\n",
            "Train Epoch: 14 [60416/123872 (49%)]\tLoss: 0.269489\tLR: 0.00006157\n",
            "Train Epoch: 14 [60672/123872 (49%)]\tLoss: 0.294587\tLR: 0.00006156\n",
            "Train Epoch: 14 [60928/123872 (49%)]\tLoss: 0.297763\tLR: 0.00006155\n",
            "Train Epoch: 14 [61184/123872 (49%)]\tLoss: 0.341597\tLR: 0.00006153\n",
            "Train Epoch: 14 [61440/123872 (50%)]\tLoss: 0.382053\tLR: 0.00006152\n",
            "Train Epoch: 14 [61440/123872 (50%)]\tLoss: 0.382053\n",
            "Train Epoch: 14 [61696/123872 (50%)]\tLoss: 0.310259\tLR: 0.00006151\n",
            "Train Epoch: 14 [61952/123872 (50%)]\tLoss: 0.314121\tLR: 0.00006150\n",
            "Train Epoch: 14 [62208/123872 (50%)]\tLoss: 0.364521\tLR: 0.00006148\n",
            "Train Epoch: 14 [62464/123872 (50%)]\tLoss: 0.280375\tLR: 0.00006147\n",
            "Train Epoch: 14 [62720/123872 (51%)]\tLoss: 0.375585\tLR: 0.00006146\n",
            "Train Epoch: 14 [62976/123872 (51%)]\tLoss: 0.267659\tLR: 0.00006145\n",
            "Train Epoch: 14 [63232/123872 (51%)]\tLoss: 0.321010\tLR: 0.00006144\n",
            "Train Epoch: 14 [63488/123872 (51%)]\tLoss: 0.318522\tLR: 0.00006142\n",
            "Train Epoch: 14 [63744/123872 (51%)]\tLoss: 0.331151\tLR: 0.00006141\n",
            "Train Epoch: 14 [64000/123872 (52%)]\tLoss: 0.300887\tLR: 0.00006140\n",
            "Train Epoch: 14 [64000/123872 (52%)]\tLoss: 0.300887\n",
            "Train Epoch: 14 [64256/123872 (52%)]\tLoss: 0.376729\tLR: 0.00006139\n",
            "Train Epoch: 14 [64512/123872 (52%)]\tLoss: 0.307325\tLR: 0.00006137\n",
            "Train Epoch: 14 [64768/123872 (52%)]\tLoss: 0.266377\tLR: 0.00006136\n",
            "Train Epoch: 14 [65024/123872 (52%)]\tLoss: 0.332363\tLR: 0.00006135\n",
            "Train Epoch: 14 [65280/123872 (53%)]\tLoss: 0.310757\tLR: 0.00006134\n",
            "Train Epoch: 14 [65536/123872 (53%)]\tLoss: 0.348608\tLR: 0.00006133\n",
            "Train Epoch: 14 [65792/123872 (53%)]\tLoss: 0.326322\tLR: 0.00006132\n",
            "Train Epoch: 14 [66048/123872 (53%)]\tLoss: 0.355626\tLR: 0.00006130\n",
            "Train Epoch: 14 [66304/123872 (54%)]\tLoss: 0.260112\tLR: 0.00006129\n",
            "Train Epoch: 14 [66560/123872 (54%)]\tLoss: 0.317811\tLR: 0.00006128\n",
            "Train Epoch: 14 [66560/123872 (54%)]\tLoss: 0.317811\n",
            "Train Epoch: 14 [66816/123872 (54%)]\tLoss: 0.327742\tLR: 0.00006127\n",
            "Train Epoch: 14 [67072/123872 (54%)]\tLoss: 0.281754\tLR: 0.00006126\n",
            "Train Epoch: 14 [67328/123872 (54%)]\tLoss: 0.319392\tLR: 0.00006125\n",
            "Train Epoch: 14 [67584/123872 (55%)]\tLoss: 0.314244\tLR: 0.00006124\n",
            "Train Epoch: 14 [67840/123872 (55%)]\tLoss: 0.271869\tLR: 0.00006122\n",
            "Train Epoch: 14 [68096/123872 (55%)]\tLoss: 0.351862\tLR: 0.00006121\n",
            "Train Epoch: 14 [68352/123872 (55%)]\tLoss: 0.271623\tLR: 0.00006120\n",
            "Train Epoch: 14 [68608/123872 (55%)]\tLoss: 0.347292\tLR: 0.00006119\n",
            "Train Epoch: 14 [68864/123872 (56%)]\tLoss: 0.356152\tLR: 0.00006118\n",
            "Train Epoch: 14 [69120/123872 (56%)]\tLoss: 0.337119\tLR: 0.00006117\n",
            "Train Epoch: 14 [69120/123872 (56%)]\tLoss: 0.337119\n",
            "Train Epoch: 14 [69376/123872 (56%)]\tLoss: 0.370366\tLR: 0.00006116\n",
            "Train Epoch: 14 [69632/123872 (56%)]\tLoss: 0.341305\tLR: 0.00006115\n",
            "Train Epoch: 14 [69888/123872 (56%)]\tLoss: 0.299238\tLR: 0.00006114\n",
            "Train Epoch: 14 [70144/123872 (57%)]\tLoss: 0.293174\tLR: 0.00006113\n",
            "Train Epoch: 14 [70400/123872 (57%)]\tLoss: 0.309795\tLR: 0.00006111\n",
            "Train Epoch: 14 [70656/123872 (57%)]\tLoss: 0.343794\tLR: 0.00006110\n",
            "Train Epoch: 14 [70912/123872 (57%)]\tLoss: 0.377485\tLR: 0.00006109\n",
            "Train Epoch: 14 [71168/123872 (57%)]\tLoss: 0.259080\tLR: 0.00006108\n",
            "Train Epoch: 14 [71424/123872 (58%)]\tLoss: 0.336902\tLR: 0.00006107\n",
            "Train Epoch: 14 [71680/123872 (58%)]\tLoss: 0.299876\tLR: 0.00006106\n",
            "Train Epoch: 14 [71680/123872 (58%)]\tLoss: 0.299876\n",
            "Train Epoch: 14 [71936/123872 (58%)]\tLoss: 0.315669\tLR: 0.00006105\n",
            "Train Epoch: 14 [72192/123872 (58%)]\tLoss: 0.312795\tLR: 0.00006104\n",
            "Train Epoch: 14 [72448/123872 (58%)]\tLoss: 0.363545\tLR: 0.00006103\n",
            "Train Epoch: 14 [72704/123872 (59%)]\tLoss: 0.294355\tLR: 0.00006102\n",
            "Train Epoch: 14 [72960/123872 (59%)]\tLoss: 0.365349\tLR: 0.00006101\n",
            "Train Epoch: 14 [73216/123872 (59%)]\tLoss: 0.321515\tLR: 0.00006100\n",
            "Train Epoch: 14 [73472/123872 (59%)]\tLoss: 0.275268\tLR: 0.00006099\n",
            "Train Epoch: 14 [73728/123872 (60%)]\tLoss: 0.293106\tLR: 0.00006098\n",
            "Train Epoch: 14 [73984/123872 (60%)]\tLoss: 0.443702\tLR: 0.00006097\n",
            "Train Epoch: 14 [74240/123872 (60%)]\tLoss: 0.337041\tLR: 0.00006096\n",
            "Train Epoch: 14 [74240/123872 (60%)]\tLoss: 0.337041\n",
            "Train Epoch: 14 [74496/123872 (60%)]\tLoss: 0.404239\tLR: 0.00006095\n",
            "Train Epoch: 14 [74752/123872 (60%)]\tLoss: 0.321196\tLR: 0.00006094\n",
            "Train Epoch: 14 [75008/123872 (61%)]\tLoss: 0.290553\tLR: 0.00006093\n",
            "Train Epoch: 14 [75264/123872 (61%)]\tLoss: 0.306479\tLR: 0.00006092\n",
            "Train Epoch: 14 [75520/123872 (61%)]\tLoss: 0.307013\tLR: 0.00006091\n",
            "Train Epoch: 14 [75776/123872 (61%)]\tLoss: 0.331087\tLR: 0.00006090\n",
            "Train Epoch: 14 [76032/123872 (61%)]\tLoss: 0.367681\tLR: 0.00006089\n",
            "Train Epoch: 14 [76288/123872 (62%)]\tLoss: 0.309428\tLR: 0.00006088\n",
            "Train Epoch: 14 [76544/123872 (62%)]\tLoss: 0.332781\tLR: 0.00006087\n",
            "Train Epoch: 14 [76800/123872 (62%)]\tLoss: 0.314574\tLR: 0.00006086\n",
            "Train Epoch: 14 [76800/123872 (62%)]\tLoss: 0.314574\n",
            "Train Epoch: 14 [77056/123872 (62%)]\tLoss: 0.289393\tLR: 0.00006085\n",
            "Train Epoch: 14 [77312/123872 (62%)]\tLoss: 0.255501\tLR: 0.00006084\n",
            "Train Epoch: 14 [77568/123872 (63%)]\tLoss: 0.348743\tLR: 0.00006084\n",
            "Train Epoch: 14 [77824/123872 (63%)]\tLoss: 0.348165\tLR: 0.00006083\n",
            "Train Epoch: 14 [78080/123872 (63%)]\tLoss: 0.441278\tLR: 0.00006082\n",
            "Train Epoch: 14 [78336/123872 (63%)]\tLoss: 0.356085\tLR: 0.00006081\n",
            "Train Epoch: 14 [78592/123872 (63%)]\tLoss: 0.270907\tLR: 0.00006080\n",
            "Train Epoch: 14 [78848/123872 (64%)]\tLoss: 0.309301\tLR: 0.00006079\n",
            "Train Epoch: 14 [79104/123872 (64%)]\tLoss: 0.335647\tLR: 0.00006078\n",
            "Train Epoch: 14 [79360/123872 (64%)]\tLoss: 0.311334\tLR: 0.00006077\n",
            "Train Epoch: 14 [79360/123872 (64%)]\tLoss: 0.311334\n",
            "Train Epoch: 14 [79616/123872 (64%)]\tLoss: 0.333663\tLR: 0.00006076\n",
            "Train Epoch: 14 [79872/123872 (64%)]\tLoss: 0.321327\tLR: 0.00006075\n",
            "Train Epoch: 14 [80128/123872 (65%)]\tLoss: 0.289568\tLR: 0.00006074\n",
            "Train Epoch: 14 [80384/123872 (65%)]\tLoss: 0.305924\tLR: 0.00006074\n",
            "Train Epoch: 14 [80640/123872 (65%)]\tLoss: 0.367394\tLR: 0.00006073\n",
            "Train Epoch: 14 [80896/123872 (65%)]\tLoss: 0.338519\tLR: 0.00006072\n",
            "Train Epoch: 14 [81152/123872 (65%)]\tLoss: 0.408880\tLR: 0.00006071\n",
            "Train Epoch: 14 [81408/123872 (66%)]\tLoss: 0.334944\tLR: 0.00006070\n",
            "Train Epoch: 14 [81664/123872 (66%)]\tLoss: 0.314246\tLR: 0.00006069\n",
            "Train Epoch: 14 [81920/123872 (66%)]\tLoss: 0.286569\tLR: 0.00006068\n",
            "Train Epoch: 14 [81920/123872 (66%)]\tLoss: 0.286569\n",
            "Train Epoch: 14 [82176/123872 (66%)]\tLoss: 0.325635\tLR: 0.00006068\n",
            "Train Epoch: 14 [82432/123872 (67%)]\tLoss: 0.313199\tLR: 0.00006067\n",
            "Train Epoch: 14 [82688/123872 (67%)]\tLoss: 0.270467\tLR: 0.00006066\n",
            "Train Epoch: 14 [82944/123872 (67%)]\tLoss: 0.342452\tLR: 0.00006065\n",
            "Train Epoch: 14 [83200/123872 (67%)]\tLoss: 0.342233\tLR: 0.00006064\n",
            "Train Epoch: 14 [83456/123872 (67%)]\tLoss: 0.317285\tLR: 0.00006064\n",
            "Train Epoch: 14 [83712/123872 (68%)]\tLoss: 0.351135\tLR: 0.00006063\n",
            "Train Epoch: 14 [83968/123872 (68%)]\tLoss: 0.306875\tLR: 0.00006062\n",
            "Train Epoch: 14 [84224/123872 (68%)]\tLoss: 0.342289\tLR: 0.00006061\n",
            "Train Epoch: 14 [84480/123872 (68%)]\tLoss: 0.351718\tLR: 0.00006060\n",
            "Train Epoch: 14 [84480/123872 (68%)]\tLoss: 0.351718\n",
            "Train Epoch: 14 [84736/123872 (68%)]\tLoss: 0.310934\tLR: 0.00006060\n",
            "Train Epoch: 14 [84992/123872 (69%)]\tLoss: 0.326335\tLR: 0.00006059\n",
            "Train Epoch: 14 [85248/123872 (69%)]\tLoss: 0.364458\tLR: 0.00006058\n",
            "Train Epoch: 14 [85504/123872 (69%)]\tLoss: 0.298215\tLR: 0.00006057\n",
            "Train Epoch: 14 [85760/123872 (69%)]\tLoss: 0.319188\tLR: 0.00006056\n",
            "Train Epoch: 14 [86016/123872 (69%)]\tLoss: 0.319748\tLR: 0.00006056\n",
            "Train Epoch: 14 [86272/123872 (70%)]\tLoss: 0.343696\tLR: 0.00006055\n",
            "Train Epoch: 14 [86528/123872 (70%)]\tLoss: 0.319659\tLR: 0.00006054\n",
            "Train Epoch: 14 [86784/123872 (70%)]\tLoss: 0.313788\tLR: 0.00006053\n",
            "Train Epoch: 14 [87040/123872 (70%)]\tLoss: 0.322042\tLR: 0.00006053\n",
            "Train Epoch: 14 [87040/123872 (70%)]\tLoss: 0.322042\n",
            "Train Epoch: 14 [87296/123872 (70%)]\tLoss: 0.312413\tLR: 0.00006052\n",
            "Train Epoch: 14 [87552/123872 (71%)]\tLoss: 0.335251\tLR: 0.00006051\n",
            "Train Epoch: 14 [87808/123872 (71%)]\tLoss: 0.330867\tLR: 0.00006051\n",
            "Train Epoch: 14 [88064/123872 (71%)]\tLoss: 0.393443\tLR: 0.00006050\n",
            "Train Epoch: 14 [88320/123872 (71%)]\tLoss: 0.330175\tLR: 0.00006049\n",
            "Train Epoch: 14 [88576/123872 (71%)]\tLoss: 0.337417\tLR: 0.00006048\n",
            "Train Epoch: 14 [88832/123872 (72%)]\tLoss: 0.301848\tLR: 0.00006048\n",
            "Train Epoch: 14 [89088/123872 (72%)]\tLoss: 0.357641\tLR: 0.00006047\n",
            "Train Epoch: 14 [89344/123872 (72%)]\tLoss: 0.318495\tLR: 0.00006046\n",
            "Train Epoch: 14 [89600/123872 (72%)]\tLoss: 0.358089\tLR: 0.00006046\n",
            "Train Epoch: 14 [89600/123872 (72%)]\tLoss: 0.358089\n",
            "Train Epoch: 14 [89856/123872 (73%)]\tLoss: 0.316588\tLR: 0.00006045\n",
            "Train Epoch: 14 [90112/123872 (73%)]\tLoss: 0.357389\tLR: 0.00006044\n",
            "Train Epoch: 14 [90368/123872 (73%)]\tLoss: 0.266458\tLR: 0.00006044\n",
            "Train Epoch: 14 [90624/123872 (73%)]\tLoss: 0.293136\tLR: 0.00006043\n",
            "Train Epoch: 14 [90880/123872 (73%)]\tLoss: 0.301031\tLR: 0.00006042\n",
            "Train Epoch: 14 [91136/123872 (74%)]\tLoss: 0.263291\tLR: 0.00006042\n",
            "Train Epoch: 14 [91392/123872 (74%)]\tLoss: 0.338624\tLR: 0.00006041\n",
            "Train Epoch: 14 [91648/123872 (74%)]\tLoss: 0.325597\tLR: 0.00006040\n",
            "Train Epoch: 14 [91904/123872 (74%)]\tLoss: 0.287271\tLR: 0.00006040\n",
            "Train Epoch: 14 [92160/123872 (74%)]\tLoss: 0.322821\tLR: 0.00006039\n",
            "Train Epoch: 14 [92160/123872 (74%)]\tLoss: 0.322821\n",
            "Train Epoch: 14 [92416/123872 (75%)]\tLoss: 0.319223\tLR: 0.00006038\n",
            "Train Epoch: 14 [92672/123872 (75%)]\tLoss: 0.340065\tLR: 0.00006038\n",
            "Train Epoch: 14 [92928/123872 (75%)]\tLoss: 0.361902\tLR: 0.00006037\n",
            "Train Epoch: 14 [93184/123872 (75%)]\tLoss: 0.325971\tLR: 0.00006037\n",
            "Train Epoch: 14 [93440/123872 (75%)]\tLoss: 0.323076\tLR: 0.00006036\n",
            "Train Epoch: 14 [93696/123872 (76%)]\tLoss: 0.337681\tLR: 0.00006035\n",
            "Train Epoch: 14 [93952/123872 (76%)]\tLoss: 0.321569\tLR: 0.00006035\n",
            "Train Epoch: 14 [94208/123872 (76%)]\tLoss: 0.328419\tLR: 0.00006034\n",
            "Train Epoch: 14 [94464/123872 (76%)]\tLoss: 0.296466\tLR: 0.00006034\n",
            "Train Epoch: 14 [94720/123872 (76%)]\tLoss: 0.322418\tLR: 0.00006033\n",
            "Train Epoch: 14 [94720/123872 (76%)]\tLoss: 0.322418\n",
            "Train Epoch: 14 [94976/123872 (77%)]\tLoss: 0.344818\tLR: 0.00006032\n",
            "Train Epoch: 14 [95232/123872 (77%)]\tLoss: 0.350734\tLR: 0.00006032\n",
            "Train Epoch: 14 [95488/123872 (77%)]\tLoss: 0.306614\tLR: 0.00006031\n",
            "Train Epoch: 14 [95744/123872 (77%)]\tLoss: 0.304047\tLR: 0.00006031\n",
            "Train Epoch: 14 [96000/123872 (77%)]\tLoss: 0.378626\tLR: 0.00006030\n",
            "Train Epoch: 14 [96256/123872 (78%)]\tLoss: 0.311215\tLR: 0.00006030\n",
            "Train Epoch: 14 [96512/123872 (78%)]\tLoss: 0.305581\tLR: 0.00006029\n",
            "Train Epoch: 14 [96768/123872 (78%)]\tLoss: 0.319002\tLR: 0.00006028\n",
            "Train Epoch: 14 [97024/123872 (78%)]\tLoss: 0.277606\tLR: 0.00006028\n",
            "Train Epoch: 14 [97280/123872 (79%)]\tLoss: 0.363551\tLR: 0.00006027\n",
            "Train Epoch: 14 [97280/123872 (79%)]\tLoss: 0.363551\n",
            "Train Epoch: 14 [97536/123872 (79%)]\tLoss: 0.294366\tLR: 0.00006027\n",
            "Train Epoch: 14 [97792/123872 (79%)]\tLoss: 0.336834\tLR: 0.00006026\n",
            "Train Epoch: 14 [98048/123872 (79%)]\tLoss: 0.351032\tLR: 0.00006026\n",
            "Train Epoch: 14 [98304/123872 (79%)]\tLoss: 0.335327\tLR: 0.00006025\n",
            "Train Epoch: 14 [98560/123872 (80%)]\tLoss: 0.309414\tLR: 0.00006025\n",
            "Train Epoch: 14 [98816/123872 (80%)]\tLoss: 0.350037\tLR: 0.00006024\n",
            "Train Epoch: 14 [99072/123872 (80%)]\tLoss: 0.352721\tLR: 0.00006024\n",
            "Train Epoch: 14 [99328/123872 (80%)]\tLoss: 0.350302\tLR: 0.00006023\n",
            "Train Epoch: 14 [99584/123872 (80%)]\tLoss: 0.368575\tLR: 0.00006023\n",
            "Train Epoch: 14 [99840/123872 (81%)]\tLoss: 0.292591\tLR: 0.00006022\n",
            "Train Epoch: 14 [99840/123872 (81%)]\tLoss: 0.292591\n",
            "Train Epoch: 14 [100096/123872 (81%)]\tLoss: 0.339505\tLR: 0.00006022\n",
            "Train Epoch: 14 [100352/123872 (81%)]\tLoss: 0.276780\tLR: 0.00006021\n",
            "Train Epoch: 14 [100608/123872 (81%)]\tLoss: 0.293825\tLR: 0.00006021\n",
            "Train Epoch: 14 [100864/123872 (81%)]\tLoss: 0.355973\tLR: 0.00006020\n",
            "Train Epoch: 14 [101120/123872 (82%)]\tLoss: 0.298363\tLR: 0.00006020\n",
            "Train Epoch: 14 [101376/123872 (82%)]\tLoss: 0.318508\tLR: 0.00006020\n",
            "Train Epoch: 14 [101632/123872 (82%)]\tLoss: 0.342551\tLR: 0.00006019\n",
            "Train Epoch: 14 [101888/123872 (82%)]\tLoss: 0.269151\tLR: 0.00006019\n",
            "Train Epoch: 14 [102144/123872 (82%)]\tLoss: 0.269997\tLR: 0.00006018\n",
            "Train Epoch: 14 [102400/123872 (83%)]\tLoss: 0.274711\tLR: 0.00006018\n",
            "Train Epoch: 14 [102400/123872 (83%)]\tLoss: 0.274711\n",
            "Train Epoch: 14 [102656/123872 (83%)]\tLoss: 0.365169\tLR: 0.00006017\n",
            "Train Epoch: 14 [102912/123872 (83%)]\tLoss: 0.306709\tLR: 0.00006017\n",
            "Train Epoch: 14 [103168/123872 (83%)]\tLoss: 0.351213\tLR: 0.00006017\n",
            "Train Epoch: 14 [103424/123872 (83%)]\tLoss: 0.391675\tLR: 0.00006016\n",
            "Train Epoch: 14 [103680/123872 (84%)]\tLoss: 0.357160\tLR: 0.00006016\n",
            "Train Epoch: 14 [103936/123872 (84%)]\tLoss: 0.286269\tLR: 0.00006015\n",
            "Train Epoch: 14 [104192/123872 (84%)]\tLoss: 0.320759\tLR: 0.00006015\n",
            "Train Epoch: 14 [104448/123872 (84%)]\tLoss: 0.362451\tLR: 0.00006015\n",
            "Train Epoch: 14 [104704/123872 (85%)]\tLoss: 0.358590\tLR: 0.00006014\n",
            "Train Epoch: 14 [104960/123872 (85%)]\tLoss: 0.329701\tLR: 0.00006014\n",
            "Train Epoch: 14 [104960/123872 (85%)]\tLoss: 0.329701\n",
            "Train Epoch: 14 [105216/123872 (85%)]\tLoss: 0.436567\tLR: 0.00006013\n",
            "Train Epoch: 14 [105472/123872 (85%)]\tLoss: 0.336079\tLR: 0.00006013\n",
            "Train Epoch: 14 [105728/123872 (85%)]\tLoss: 0.304125\tLR: 0.00006013\n",
            "Train Epoch: 14 [105984/123872 (86%)]\tLoss: 0.288665\tLR: 0.00006012\n",
            "Train Epoch: 14 [106240/123872 (86%)]\tLoss: 0.258189\tLR: 0.00006012\n",
            "Train Epoch: 14 [106496/123872 (86%)]\tLoss: 0.344625\tLR: 0.00006012\n",
            "Train Epoch: 14 [106752/123872 (86%)]\tLoss: 0.358973\tLR: 0.00006011\n",
            "Train Epoch: 14 [107008/123872 (86%)]\tLoss: 0.316321\tLR: 0.00006011\n",
            "Train Epoch: 14 [107264/123872 (87%)]\tLoss: 0.309795\tLR: 0.00006011\n",
            "Train Epoch: 14 [107520/123872 (87%)]\tLoss: 0.309635\tLR: 0.00006010\n",
            "Train Epoch: 14 [107520/123872 (87%)]\tLoss: 0.309635\n",
            "Train Epoch: 14 [107776/123872 (87%)]\tLoss: 0.327409\tLR: 0.00006010\n",
            "Train Epoch: 14 [108032/123872 (87%)]\tLoss: 0.314506\tLR: 0.00006010\n",
            "Train Epoch: 14 [108288/123872 (87%)]\tLoss: 0.288887\tLR: 0.00006009\n",
            "Train Epoch: 14 [108544/123872 (88%)]\tLoss: 0.299570\tLR: 0.00006009\n",
            "Train Epoch: 14 [108800/123872 (88%)]\tLoss: 0.295994\tLR: 0.00006009\n",
            "Train Epoch: 14 [109056/123872 (88%)]\tLoss: 0.285987\tLR: 0.00006008\n",
            "Train Epoch: 14 [109312/123872 (88%)]\tLoss: 0.330260\tLR: 0.00006008\n",
            "Train Epoch: 14 [109568/123872 (88%)]\tLoss: 0.351457\tLR: 0.00006008\n",
            "Train Epoch: 14 [109824/123872 (89%)]\tLoss: 0.263507\tLR: 0.00006008\n",
            "Train Epoch: 14 [110080/123872 (89%)]\tLoss: 0.296785\tLR: 0.00006007\n",
            "Train Epoch: 14 [110080/123872 (89%)]\tLoss: 0.296785\n",
            "Train Epoch: 14 [110336/123872 (89%)]\tLoss: 0.347952\tLR: 0.00006007\n",
            "Train Epoch: 14 [110592/123872 (89%)]\tLoss: 0.355420\tLR: 0.00006007\n",
            "Train Epoch: 14 [110848/123872 (89%)]\tLoss: 0.367715\tLR: 0.00006006\n",
            "Train Epoch: 14 [111104/123872 (90%)]\tLoss: 0.329853\tLR: 0.00006006\n",
            "Train Epoch: 14 [111360/123872 (90%)]\tLoss: 0.354093\tLR: 0.00006006\n",
            "Train Epoch: 14 [111616/123872 (90%)]\tLoss: 0.370887\tLR: 0.00006006\n",
            "Train Epoch: 14 [111872/123872 (90%)]\tLoss: 0.363980\tLR: 0.00006005\n",
            "Train Epoch: 14 [112128/123872 (90%)]\tLoss: 0.331088\tLR: 0.00006005\n",
            "Train Epoch: 14 [112384/123872 (91%)]\tLoss: 0.391785\tLR: 0.00006005\n",
            "Train Epoch: 14 [112640/123872 (91%)]\tLoss: 0.385580\tLR: 0.00006005\n",
            "Train Epoch: 14 [112640/123872 (91%)]\tLoss: 0.385580\n",
            "Train Epoch: 14 [112896/123872 (91%)]\tLoss: 0.363341\tLR: 0.00006005\n",
            "Train Epoch: 14 [113152/123872 (91%)]\tLoss: 0.333568\tLR: 0.00006004\n",
            "Train Epoch: 14 [113408/123872 (92%)]\tLoss: 0.322673\tLR: 0.00006004\n",
            "Train Epoch: 14 [113664/123872 (92%)]\tLoss: 0.408355\tLR: 0.00006004\n",
            "Train Epoch: 14 [113920/123872 (92%)]\tLoss: 0.320773\tLR: 0.00006004\n",
            "Train Epoch: 14 [114176/123872 (92%)]\tLoss: 0.340238\tLR: 0.00006004\n",
            "Train Epoch: 14 [114432/123872 (92%)]\tLoss: 0.322484\tLR: 0.00006003\n",
            "Train Epoch: 14 [114688/123872 (93%)]\tLoss: 0.332399\tLR: 0.00006003\n",
            "Train Epoch: 14 [114944/123872 (93%)]\tLoss: 0.285887\tLR: 0.00006003\n",
            "Train Epoch: 14 [115200/123872 (93%)]\tLoss: 0.326474\tLR: 0.00006003\n",
            "Train Epoch: 14 [115200/123872 (93%)]\tLoss: 0.326474\n",
            "Train Epoch: 14 [115456/123872 (93%)]\tLoss: 0.307215\tLR: 0.00006003\n",
            "Train Epoch: 14 [115712/123872 (93%)]\tLoss: 0.334229\tLR: 0.00006002\n",
            "Train Epoch: 14 [115968/123872 (94%)]\tLoss: 0.274024\tLR: 0.00006002\n",
            "Train Epoch: 14 [116224/123872 (94%)]\tLoss: 0.324990\tLR: 0.00006002\n",
            "Train Epoch: 14 [116480/123872 (94%)]\tLoss: 0.321051\tLR: 0.00006002\n",
            "Train Epoch: 14 [116736/123872 (94%)]\tLoss: 0.247172\tLR: 0.00006002\n",
            "Train Epoch: 14 [116992/123872 (94%)]\tLoss: 0.346511\tLR: 0.00006002\n",
            "Train Epoch: 14 [117248/123872 (95%)]\tLoss: 0.419540\tLR: 0.00006002\n",
            "Train Epoch: 14 [117504/123872 (95%)]\tLoss: 0.295408\tLR: 0.00006001\n",
            "Train Epoch: 14 [117760/123872 (95%)]\tLoss: 0.369410\tLR: 0.00006001\n",
            "Train Epoch: 14 [117760/123872 (95%)]\tLoss: 0.369410\n",
            "Train Epoch: 14 [118016/123872 (95%)]\tLoss: 0.345103\tLR: 0.00006001\n",
            "Train Epoch: 14 [118272/123872 (95%)]\tLoss: 0.267146\tLR: 0.00006001\n",
            "Train Epoch: 14 [118528/123872 (96%)]\tLoss: 0.341206\tLR: 0.00006001\n",
            "Train Epoch: 14 [118784/123872 (96%)]\tLoss: 0.283653\tLR: 0.00006001\n",
            "Train Epoch: 14 [119040/123872 (96%)]\tLoss: 0.314913\tLR: 0.00006001\n",
            "Train Epoch: 14 [119296/123872 (96%)]\tLoss: 0.376328\tLR: 0.00006001\n",
            "Train Epoch: 14 [119552/123872 (96%)]\tLoss: 0.387544\tLR: 0.00006001\n",
            "Train Epoch: 14 [119808/123872 (97%)]\tLoss: 0.381222\tLR: 0.00006001\n",
            "Train Epoch: 14 [120064/123872 (97%)]\tLoss: 0.318383\tLR: 0.00006001\n",
            "Train Epoch: 14 [120320/123872 (97%)]\tLoss: 0.389338\tLR: 0.00006000\n",
            "Train Epoch: 14 [120320/123872 (97%)]\tLoss: 0.389338\n",
            "Train Epoch: 14 [120576/123872 (97%)]\tLoss: 0.393414\tLR: 0.00006000\n",
            "Train Epoch: 14 [120832/123872 (98%)]\tLoss: 0.304113\tLR: 0.00006000\n",
            "Train Epoch: 14 [121088/123872 (98%)]\tLoss: 0.386231\tLR: 0.00006000\n",
            "Train Epoch: 14 [121344/123872 (98%)]\tLoss: 0.375647\tLR: 0.00006000\n",
            "Train Epoch: 14 [121600/123872 (98%)]\tLoss: 0.362556\tLR: 0.00006000\n",
            "Train Epoch: 14 [121856/123872 (98%)]\tLoss: 0.338218\tLR: 0.00006000\n",
            "Train Epoch: 14 [122112/123872 (99%)]\tLoss: 0.354159\tLR: 0.00006000\n",
            "Train Epoch: 14 [122368/123872 (99%)]\tLoss: 0.320673\tLR: 0.00006000\n",
            "Train Epoch: 14 [122624/123872 (99%)]\tLoss: 0.359551\tLR: 0.00006000\n",
            "Train Epoch: 14 [122880/123872 (99%)]\tLoss: 0.314817\tLR: 0.00006000\n",
            "Train Epoch: 14 [122880/123872 (99%)]\tLoss: 0.314817\n",
            "Train Epoch: 14 [123136/123872 (99%)]\tLoss: 0.344923\tLR: 0.00006000\n",
            "Train Epoch: 14 [123392/123872 (100%)]\tLoss: 0.280712\tLR: 0.00006000\n",
            "Train Epoch: 14 [108192/123872 (100%)]\tLoss: 0.324517\tLR: 0.00006000\n",
            "\n",
            "Test set: Average loss: 0.0014, Accuracy: 26122/30970 (84.35%)\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to trained_model_15_human_enhancers_ensembl_42_1M.pt\n",
            "Accuracies saved to accuracies_15_human_enhancers_ensembl_42_1M.json\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train Epoch: 5 [82944/123872 (67%)]\tLoss: 0.431155\tLR: 0.00043577\n",
            "Train Epoch: 5 [83200/123872 (67%)]\tLoss: 0.381182\tLR: 0.00043566\n",
            "Train Epoch: 5 [83456/123872 (67%)]\tLoss: 0.386965\tLR: 0.00043556\n",
            "Train Epoch: 5 [83712/123872 (68%)]\tLoss: 0.432067\tLR: 0.00043545\n",
            "Train Epoch: 5 [83968/123872 (68%)]\tLoss: 0.451935\tLR: 0.00043534\n",
            "Train Epoch: 5 [84224/123872 (68%)]\tLoss: 0.435333\tLR: 0.00043523\n",
            "Train Epoch: 5 [84480/123872 (68%)]\tLoss: 0.377047\tLR: 0.00043512\n",
            "Train Epoch: 5 [84480/123872 (68%)]\tLoss: 0.377047\n",
            "Train Epoch: 5 [84736/123872 (68%)]\tLoss: 0.496421\tLR: 0.00043501\n",
            "Train Epoch: 5 [84992/123872 (69%)]\tLoss: 0.357444\tLR: 0.00043490\n",
            "Train Epoch: 5 [85248/123872 (69%)]\tLoss: 0.468186\tLR: 0.00043479\n",
            "Train Epoch: 5 [85504/123872 (69%)]\tLoss: 0.414880\tLR: 0.00043469\n",
            "Train Epoch: 5 [85760/123872 (69%)]\tLoss: 0.459414\tLR: 0.00043458\n",
            "Train Epoch: 5 [86016/123872 (69%)]\tLoss: 0.381890\tLR: 0.00043447\n",
            "Train Epoch: 5 [86272/123872 (70%)]\tLoss: 0.450474\tLR: 0.00043436\n",
            "Train Epoch: 5 [86528/123872 (70%)]\tLoss: 0.431379\tLR: 0.00043425\n",
            "Train Epoch: 5 [86784/123872 (70%)]\tLoss: 0.469967\tLR: 0.00043414\n",
            "Train Epoch: 5 [87040/123872 (70%)]\tLoss: 0.454195\tLR: 0.00043403\n",
            "Train Epoch: 5 [87040/123872 (70%)]\tLoss: 0.454195\n",
            "Train Epoch: 5 [87296/123872 (70%)]\tLoss: 0.399891\tLR: 0.00043392\n",
            "Train Epoch: 5 [87552/123872 (71%)]\tLoss: 0.416090\tLR: 0.00043381\n",
            "Train Epoch: 5 [87808/123872 (71%)]\tLoss: 0.400871\tLR: 0.00043371\n",
            "Train Epoch: 5 [88064/123872 (71%)]\tLoss: 0.407622\tLR: 0.00043360\n",
            "Train Epoch: 5 [88320/123872 (71%)]\tLoss: 0.430097\tLR: 0.00043349\n",
            "Train Epoch: 5 [88576/123872 (71%)]\tLoss: 0.424713\tLR: 0.00043338\n",
            "Train Epoch: 5 [88832/123872 (72%)]\tLoss: 0.416980\tLR: 0.00043327\n",
            "Train Epoch: 5 [89088/123872 (72%)]\tLoss: 0.473135\tLR: 0.00043316\n",
            "Train Epoch: 5 [89344/123872 (72%)]\tLoss: 0.438526\tLR: 0.00043305\n",
            "Train Epoch: 5 [89600/123872 (72%)]\tLoss: 0.484848\tLR: 0.00043294\n",
            "Train Epoch: 5 [89600/123872 (72%)]\tLoss: 0.484848\n",
            "Train Epoch: 5 [89856/123872 (73%)]\tLoss: 0.406262\tLR: 0.00043283\n",
            "Train Epoch: 5 [90112/123872 (73%)]\tLoss: 0.436793\tLR: 0.00043272\n",
            "Train Epoch: 5 [90368/123872 (73%)]\tLoss: 0.385662\tLR: 0.00043262\n",
            "Train Epoch: 5 [90624/123872 (73%)]\tLoss: 0.484427\tLR: 0.00043251\n",
            "Train Epoch: 5 [90880/123872 (73%)]\tLoss: 0.415284\tLR: 0.00043240\n",
            "Train Epoch: 5 [91136/123872 (74%)]\tLoss: 0.417999\tLR: 0.00043229\n",
            "Train Epoch: 5 [91392/123872 (74%)]\tLoss: 0.474367\tLR: 0.00043218\n",
            "Train Epoch: 5 [91648/123872 (74%)]\tLoss: 0.413137\tLR: 0.00043207\n",
            "Train Epoch: 5 [91904/123872 (74%)]\tLoss: 0.473475\tLR: 0.00043196\n",
            "Train Epoch: 5 [92160/123872 (74%)]\tLoss: 0.423831\tLR: 0.00043185\n",
            "Train Epoch: 5 [92160/123872 (74%)]\tLoss: 0.423831\n",
            "Train Epoch: 5 [92416/123872 (75%)]\tLoss: 0.452411\tLR: 0.00043174\n",
            "Train Epoch: 5 [92672/123872 (75%)]\tLoss: 0.423835\tLR: 0.00043163\n",
            "Train Epoch: 5 [92928/123872 (75%)]\tLoss: 0.465430\tLR: 0.00043152\n",
            "Train Epoch: 5 [93184/123872 (75%)]\tLoss: 0.446851\tLR: 0.00043141\n",
            "Train Epoch: 5 [93440/123872 (75%)]\tLoss: 0.480010\tLR: 0.00043130\n",
            "Train Epoch: 5 [93696/123872 (76%)]\tLoss: 0.402270\tLR: 0.00043119\n",
            "Train Epoch: 5 [93952/123872 (76%)]\tLoss: 0.405210\tLR: 0.00043109\n",
            "Train Epoch: 5 [94208/123872 (76%)]\tLoss: 0.412941\tLR: 0.00043098\n",
            "Train Epoch: 5 [94464/123872 (76%)]\tLoss: 0.396992\tLR: 0.00043087\n",
            "Train Epoch: 5 [94720/123872 (76%)]\tLoss: 0.393579\tLR: 0.00043076\n",
            "Train Epoch: 5 [94720/123872 (76%)]\tLoss: 0.393579\n",
            "Train Epoch: 5 [94976/123872 (77%)]\tLoss: 0.399048\tLR: 0.00043065\n",
            "Train Epoch: 5 [95232/123872 (77%)]\tLoss: 0.498638\tLR: 0.00043054\n",
            "Train Epoch: 5 [95488/123872 (77%)]\tLoss: 0.378425\tLR: 0.00043043\n",
            "Train Epoch: 5 [95744/123872 (77%)]\tLoss: 0.424143\tLR: 0.00043032\n",
            "Train Epoch: 5 [96000/123872 (77%)]\tLoss: 0.397129\tLR: 0.00043021\n",
            "Train Epoch: 5 [96256/123872 (78%)]\tLoss: 0.443518\tLR: 0.00043010\n",
            "Train Epoch: 5 [96512/123872 (78%)]\tLoss: 0.483996\tLR: 0.00042999\n",
            "Train Epoch: 5 [96768/123872 (78%)]\tLoss: 0.381735\tLR: 0.00042988\n",
            "Train Epoch: 5 [97024/123872 (78%)]\tLoss: 0.469277\tLR: 0.00042977\n",
            "Train Epoch: 5 [97280/123872 (79%)]\tLoss: 0.476819\tLR: 0.00042966\n",
            "Train Epoch: 5 [97280/123872 (79%)]\tLoss: 0.476819\n",
            "Train Epoch: 5 [97536/123872 (79%)]\tLoss: 0.427097\tLR: 0.00042955\n",
            "Train Epoch: 5 [97792/123872 (79%)]\tLoss: 0.362434\tLR: 0.00042944\n",
            "Train Epoch: 5 [98048/123872 (79%)]\tLoss: 0.437209\tLR: 0.00042933\n",
            "Train Epoch: 5 [98304/123872 (79%)]\tLoss: 0.438531\tLR: 0.00042922\n",
            "Train Epoch: 5 [98560/123872 (80%)]\tLoss: 0.414686\tLR: 0.00042911\n",
            "Train Epoch: 5 [98816/123872 (80%)]\tLoss: 0.358761\tLR: 0.00042900\n",
            "Train Epoch: 5 [99072/123872 (80%)]\tLoss: 0.445381\tLR: 0.00042889\n",
            "Train Epoch: 5 [99328/123872 (80%)]\tLoss: 0.393858\tLR: 0.00042878\n",
            "Train Epoch: 5 [99584/123872 (80%)]\tLoss: 0.454617\tLR: 0.00042867\n",
            "Train Epoch: 5 [99840/123872 (81%)]\tLoss: 0.461385\tLR: 0.00042856\n",
            "Train Epoch: 5 [99840/123872 (81%)]\tLoss: 0.461385\n",
            "Train Epoch: 5 [100096/123872 (81%)]\tLoss: 0.384480\tLR: 0.00042845\n",
            "Train Epoch: 5 [100352/123872 (81%)]\tLoss: 0.499272\tLR: 0.00042834\n",
            "Train Epoch: 5 [100608/123872 (81%)]\tLoss: 0.420389\tLR: 0.00042823\n",
            "Train Epoch: 5 [100864/123872 (81%)]\tLoss: 0.400096\tLR: 0.00042812\n",
            "Train Epoch: 5 [101120/123872 (82%)]\tLoss: 0.429980\tLR: 0.00042801\n",
            "Train Epoch: 5 [101376/123872 (82%)]\tLoss: 0.435014\tLR: 0.00042790\n",
            "Train Epoch: 5 [101632/123872 (82%)]\tLoss: 0.424614\tLR: 0.00042779\n",
            "Train Epoch: 5 [101888/123872 (82%)]\tLoss: 0.453506\tLR: 0.00042768\n",
            "Train Epoch: 5 [102144/123872 (82%)]\tLoss: 0.438701\tLR: 0.00042757\n",
            "Train Epoch: 5 [102400/123872 (83%)]\tLoss: 0.417769\tLR: 0.00042746\n",
            "Train Epoch: 5 [102400/123872 (83%)]\tLoss: 0.417769\n",
            "Train Epoch: 5 [102656/123872 (83%)]\tLoss: 0.407617\tLR: 0.00042735\n",
            "Train Epoch: 5 [102912/123872 (83%)]\tLoss: 0.484754\tLR: 0.00042724\n",
            "Train Epoch: 5 [103168/123872 (83%)]\tLoss: 0.394310\tLR: 0.00042713\n",
            "Train Epoch: 5 [103424/123872 (83%)]\tLoss: 0.369793\tLR: 0.00042702\n",
            "Train Epoch: 5 [103680/123872 (84%)]\tLoss: 0.431387\tLR: 0.00042691\n",
            "Train Epoch: 5 [103936/123872 (84%)]\tLoss: 0.359474\tLR: 0.00042680\n",
            "Train Epoch: 5 [104192/123872 (84%)]\tLoss: 0.405341\tLR: 0.00042669\n",
            "Train Epoch: 5 [104448/123872 (84%)]\tLoss: 0.411362\tLR: 0.00042658\n",
            "Train Epoch: 5 [104704/123872 (85%)]\tLoss: 0.491167\tLR: 0.00042647\n",
            "Train Epoch: 5 [104960/123872 (85%)]\tLoss: 0.426993\tLR: 0.00042636\n",
            "Train Epoch: 5 [104960/123872 (85%)]\tLoss: 0.426993\n",
            "Train Epoch: 5 [105216/123872 (85%)]\tLoss: 0.408904\tLR: 0.00042625\n",
            "Train Epoch: 5 [105472/123872 (85%)]\tLoss: 0.368144\tLR: 0.00042614\n",
            "Train Epoch: 5 [105728/123872 (85%)]\tLoss: 0.401626\tLR: 0.00042603\n",
            "Train Epoch: 5 [105984/123872 (86%)]\tLoss: 0.410752\tLR: 0.00042592\n",
            "Train Epoch: 5 [106240/123872 (86%)]\tLoss: 0.467324\tLR: 0.00042581\n",
            "Train Epoch: 5 [106496/123872 (86%)]\tLoss: 0.470405\tLR: 0.00042570\n",
            "Train Epoch: 5 [106752/123872 (86%)]\tLoss: 0.416681\tLR: 0.00042559\n",
            "Train Epoch: 5 [107008/123872 (86%)]\tLoss: 0.433159\tLR: 0.00042548\n",
            "Train Epoch: 5 [107264/123872 (87%)]\tLoss: 0.458597\tLR: 0.00042537\n",
            "Train Epoch: 5 [107520/123872 (87%)]\tLoss: 0.431645\tLR: 0.00042526\n",
            "Train Epoch: 5 [107520/123872 (87%)]\tLoss: 0.431645\n",
            "Train Epoch: 5 [107776/123872 (87%)]\tLoss: 0.437661\tLR: 0.00042515\n",
            "Train Epoch: 5 [108032/123872 (87%)]\tLoss: 0.457676\tLR: 0.00042504\n",
            "Train Epoch: 5 [108288/123872 (87%)]\tLoss: 0.494216\tLR: 0.00042493\n",
            "Train Epoch: 5 [108544/123872 (88%)]\tLoss: 0.334014\tLR: 0.00042482\n",
            "Train Epoch: 5 [108800/123872 (88%)]\tLoss: 0.410659\tLR: 0.00042471\n",
            "Train Epoch: 5 [109056/123872 (88%)]\tLoss: 0.444430\tLR: 0.00042460\n",
            "Train Epoch: 5 [109312/123872 (88%)]\tLoss: 0.420304\tLR: 0.00042449\n",
            "Train Epoch: 5 [109568/123872 (88%)]\tLoss: 0.376279\tLR: 0.00042438\n",
            "Train Epoch: 5 [109824/123872 (89%)]\tLoss: 0.425271\tLR: 0.00042426\n",
            "Train Epoch: 5 [110080/123872 (89%)]\tLoss: 0.460488\tLR: 0.00042415\n",
            "Train Epoch: 5 [110080/123872 (89%)]\tLoss: 0.460488\n",
            "Train Epoch: 5 [110336/123872 (89%)]\tLoss: 0.431651\tLR: 0.00042404\n",
            "Train Epoch: 5 [110592/123872 (89%)]\tLoss: 0.398761\tLR: 0.00042393\n",
            "Train Epoch: 5 [110848/123872 (89%)]\tLoss: 0.454656\tLR: 0.00042382\n",
            "Train Epoch: 5 [111104/123872 (90%)]\tLoss: 0.446794\tLR: 0.00042371\n",
            "Train Epoch: 5 [111360/123872 (90%)]\tLoss: 0.407984\tLR: 0.00042360\n",
            "Train Epoch: 5 [111616/123872 (90%)]\tLoss: 0.410734\tLR: 0.00042349\n",
            "Train Epoch: 5 [111872/123872 (90%)]\tLoss: 0.409220\tLR: 0.00042338\n",
            "Train Epoch: 5 [112128/123872 (90%)]\tLoss: 0.449793\tLR: 0.00042327\n",
            "Train Epoch: 5 [112384/123872 (91%)]\tLoss: 0.421682\tLR: 0.00042316\n",
            "Train Epoch: 5 [112640/123872 (91%)]\tLoss: 0.419632\tLR: 0.00042305\n",
            "Train Epoch: 5 [112640/123872 (91%)]\tLoss: 0.419632\n",
            "Train Epoch: 5 [112896/123872 (91%)]\tLoss: 0.422977\tLR: 0.00042294\n",
            "Train Epoch: 5 [113152/123872 (91%)]\tLoss: 0.419275\tLR: 0.00042283\n",
            "Train Epoch: 5 [113408/123872 (92%)]\tLoss: 0.420298\tLR: 0.00042271\n",
            "Train Epoch: 5 [113664/123872 (92%)]\tLoss: 0.409911\tLR: 0.00042260\n",
            "Train Epoch: 5 [113920/123872 (92%)]\tLoss: 0.447133\tLR: 0.00042249\n",
            "Train Epoch: 5 [114176/123872 (92%)]\tLoss: 0.389176\tLR: 0.00042238\n",
            "Train Epoch: 5 [114432/123872 (92%)]\tLoss: 0.379855\tLR: 0.00042227\n",
            "Train Epoch: 5 [114688/123872 (93%)]\tLoss: 0.401131\tLR: 0.00042216\n",
            "Train Epoch: 5 [114944/123872 (93%)]\tLoss: 0.488684\tLR: 0.00042205\n",
            "Train Epoch: 5 [115200/123872 (93%)]\tLoss: 0.431976\tLR: 0.00042194\n",
            "Train Epoch: 5 [115200/123872 (93%)]\tLoss: 0.431976\n",
            "Train Epoch: 5 [115456/123872 (93%)]\tLoss: 0.391394\tLR: 0.00042183\n",
            "Train Epoch: 5 [115712/123872 (93%)]\tLoss: 0.408977\tLR: 0.00042172\n",
            "Train Epoch: 5 [115968/123872 (94%)]\tLoss: 0.426812\tLR: 0.00042161\n",
            "Train Epoch: 5 [116224/123872 (94%)]\tLoss: 0.401530\tLR: 0.00042149\n",
            "Train Epoch: 5 [116480/123872 (94%)]\tLoss: 0.421797\tLR: 0.00042138\n",
            "Train Epoch: 5 [116736/123872 (94%)]\tLoss: 0.473599\tLR: 0.00042127\n",
            "Train Epoch: 5 [116992/123872 (94%)]\tLoss: 0.445970\tLR: 0.00042116\n",
            "Train Epoch: 5 [117248/123872 (95%)]\tLoss: 0.397028\tLR: 0.00042105\n",
            "Train Epoch: 5 [117504/123872 (95%)]\tLoss: 0.390376\tLR: 0.00042094\n",
            "Train Epoch: 5 [117760/123872 (95%)]\tLoss: 0.462775\tLR: 0.00042083\n",
            "Train Epoch: 5 [117760/123872 (95%)]\tLoss: 0.462775\n",
            "Train Epoch: 5 [118016/123872 (95%)]\tLoss: 0.481673\tLR: 0.00042072\n",
            "Train Epoch: 5 [118272/123872 (95%)]\tLoss: 0.459917\tLR: 0.00042061\n",
            "Train Epoch: 5 [118528/123872 (96%)]\tLoss: 0.398254\tLR: 0.00042049\n",
            "Train Epoch: 5 [118784/123872 (96%)]\tLoss: 0.379508\tLR: 0.00042038\n",
            "Train Epoch: 5 [119040/123872 (96%)]\tLoss: 0.428689\tLR: 0.00042027\n",
            "Train Epoch: 5 [119296/123872 (96%)]\tLoss: 0.442242\tLR: 0.00042016\n",
            "Train Epoch: 5 [119552/123872 (96%)]\tLoss: 0.423213\tLR: 0.00042005\n",
            "Train Epoch: 5 [119808/123872 (97%)]\tLoss: 0.413191\tLR: 0.00041994\n",
            "Train Epoch: 5 [120064/123872 (97%)]\tLoss: 0.459132\tLR: 0.00041983\n",
            "Train Epoch: 5 [120320/123872 (97%)]\tLoss: 0.394192\tLR: 0.00041972\n",
            "Train Epoch: 5 [120320/123872 (97%)]\tLoss: 0.394192\n",
            "Train Epoch: 5 [120576/123872 (97%)]\tLoss: 0.428009\tLR: 0.00041960\n",
            "Train Epoch: 5 [120832/123872 (98%)]\tLoss: 0.479104\tLR: 0.00041949\n",
            "Train Epoch: 5 [121088/123872 (98%)]\tLoss: 0.442230\tLR: 0.00041938\n",
            "Train Epoch: 5 [121344/123872 (98%)]\tLoss: 0.370516\tLR: 0.00041927\n",
            "Train Epoch: 5 [121600/123872 (98%)]\tLoss: 0.375997\tLR: 0.00041916\n",
            "Train Epoch: 5 [121856/123872 (98%)]\tLoss: 0.493962\tLR: 0.00041905\n",
            "Train Epoch: 5 [122112/123872 (99%)]\tLoss: 0.449165\tLR: 0.00041894\n",
            "Train Epoch: 5 [122368/123872 (99%)]\tLoss: 0.369168\tLR: 0.00041883\n",
            "Train Epoch: 5 [122624/123872 (99%)]\tLoss: 0.404872\tLR: 0.00041871\n",
            "Train Epoch: 5 [122880/123872 (99%)]\tLoss: 0.396190\tLR: 0.00041860\n",
            "Train Epoch: 5 [122880/123872 (99%)]\tLoss: 0.396190\n",
            "Train Epoch: 5 [123136/123872 (99%)]\tLoss: 0.436187\tLR: 0.00041849\n",
            "Train Epoch: 5 [123392/123872 (100%)]\tLoss: 0.499299\tLR: 0.00041838\n",
            "Train Epoch: 5 [108192/123872 (100%)]\tLoss: 0.446214\tLR: 0.00041827\n",
            "\n",
            "Test set: Average loss: 0.0017, Accuracy: 24722/30970 (79.83%)\n",
            "\n",
            "Train Epoch: 6 [0/123872 (0%)]\tLoss: 0.402663\tLR: 0.00041816\n",
            "Train Epoch: 6 [0/123872 (0%)]\tLoss: 0.402663\n",
            "Train Epoch: 6 [256/123872 (0%)]\tLoss: 0.428518\tLR: 0.00041804\n",
            "Train Epoch: 6 [512/123872 (0%)]\tLoss: 0.469539\tLR: 0.00041793\n",
            "Train Epoch: 6 [768/123872 (1%)]\tLoss: 0.459936\tLR: 0.00041782\n",
            "Train Epoch: 6 [1024/123872 (1%)]\tLoss: 0.390581\tLR: 0.00041771\n",
            "Train Epoch: 6 [1280/123872 (1%)]\tLoss: 0.344123\tLR: 0.00041760\n",
            "Train Epoch: 6 [1536/123872 (1%)]\tLoss: 0.423091\tLR: 0.00041749\n",
            "Train Epoch: 6 [1792/123872 (1%)]\tLoss: 0.510523\tLR: 0.00041738\n",
            "Train Epoch: 6 [2048/123872 (2%)]\tLoss: 0.390875\tLR: 0.00041726\n",
            "Train Epoch: 6 [2304/123872 (2%)]\tLoss: 0.443740\tLR: 0.00041715\n",
            "Train Epoch: 6 [2560/123872 (2%)]\tLoss: 0.462518\tLR: 0.00041704\n",
            "Train Epoch: 6 [2560/123872 (2%)]\tLoss: 0.462518\n",
            "Train Epoch: 6 [2816/123872 (2%)]\tLoss: 0.410637\tLR: 0.00041693\n",
            "Train Epoch: 6 [3072/123872 (2%)]\tLoss: 0.408421\tLR: 0.00041682\n",
            "Train Epoch: 6 [3328/123872 (3%)]\tLoss: 0.452429\tLR: 0.00041670\n",
            "Train Epoch: 6 [3584/123872 (3%)]\tLoss: 0.371458\tLR: 0.00041659\n",
            "Train Epoch: 6 [3840/123872 (3%)]\tLoss: 0.419313\tLR: 0.00041648\n",
            "Train Epoch: 6 [4096/123872 (3%)]\tLoss: 0.474060\tLR: 0.00041637\n",
            "Train Epoch: 6 [4352/123872 (4%)]\tLoss: 0.465614\tLR: 0.00041626\n",
            "Train Epoch: 6 [4608/123872 (4%)]\tLoss: 0.377922\tLR: 0.00041615\n",
            "Train Epoch: 6 [4864/123872 (4%)]\tLoss: 0.423542\tLR: 0.00041603\n",
            "Train Epoch: 6 [5120/123872 (4%)]\tLoss: 0.465328\tLR: 0.00041592\n",
            "Train Epoch: 6 [5120/123872 (4%)]\tLoss: 0.465328\n",
            "Train Epoch: 6 [5376/123872 (4%)]\tLoss: 0.425751\tLR: 0.00041581\n",
            "Train Epoch: 6 [5632/123872 (5%)]\tLoss: 0.400151\tLR: 0.00041570\n",
            "Train Epoch: 6 [5888/123872 (5%)]\tLoss: 0.360428\tLR: 0.00041559\n",
            "Train Epoch: 6 [6144/123872 (5%)]\tLoss: 0.392585\tLR: 0.00041547\n",
            "Train Epoch: 6 [6400/123872 (5%)]\tLoss: 0.436248\tLR: 0.00041536\n",
            "Train Epoch: 6 [6656/123872 (5%)]\tLoss: 0.422733\tLR: 0.00041525\n",
            "Train Epoch: 6 [6912/123872 (6%)]\tLoss: 0.464038\tLR: 0.00041514\n",
            "Train Epoch: 6 [7168/123872 (6%)]\tLoss: 0.417326\tLR: 0.00041503\n",
            "Train Epoch: 6 [7424/123872 (6%)]\tLoss: 0.409100\tLR: 0.00041491\n",
            "Train Epoch: 6 [7680/123872 (6%)]\tLoss: 0.398404\tLR: 0.00041480\n",
            "Train Epoch: 6 [7680/123872 (6%)]\tLoss: 0.398404\n",
            "Train Epoch: 6 [7936/123872 (6%)]\tLoss: 0.425041\tLR: 0.00041469\n",
            "Train Epoch: 6 [8192/123872 (7%)]\tLoss: 0.420743\tLR: 0.00041458\n",
            "Train Epoch: 6 [8448/123872 (7%)]\tLoss: 0.416576\tLR: 0.00041447\n",
            "Train Epoch: 6 [8704/123872 (7%)]\tLoss: 0.367735\tLR: 0.00041435\n",
            "Train Epoch: 6 [8960/123872 (7%)]\tLoss: 0.447550\tLR: 0.00041424\n",
            "Train Epoch: 6 [9216/123872 (7%)]\tLoss: 0.385007\tLR: 0.00041413\n",
            "Train Epoch: 6 [9472/123872 (8%)]\tLoss: 0.402352\tLR: 0.00041402\n",
            "Train Epoch: 6 [9728/123872 (8%)]\tLoss: 0.447001\tLR: 0.00041391\n",
            "Train Epoch: 6 [9984/123872 (8%)]\tLoss: 0.400015\tLR: 0.00041379\n",
            "Train Epoch: 6 [10240/123872 (8%)]\tLoss: 0.382102\tLR: 0.00041368\n",
            "Train Epoch: 6 [10240/123872 (8%)]\tLoss: 0.382102\n",
            "Train Epoch: 6 [10496/123872 (8%)]\tLoss: 0.535140\tLR: 0.00041357\n",
            "Train Epoch: 6 [10752/123872 (9%)]\tLoss: 0.399746\tLR: 0.00041346\n",
            "Train Epoch: 6 [11008/123872 (9%)]\tLoss: 0.422626\tLR: 0.00041334\n",
            "Train Epoch: 6 [11264/123872 (9%)]\tLoss: 0.472969\tLR: 0.00041323\n",
            "Train Epoch: 6 [11520/123872 (9%)]\tLoss: 0.456933\tLR: 0.00041312\n",
            "Train Epoch: 6 [11776/123872 (10%)]\tLoss: 0.407079\tLR: 0.00041301\n",
            "Train Epoch: 6 [12032/123872 (10%)]\tLoss: 0.436674\tLR: 0.00041290\n",
            "Train Epoch: 6 [12288/123872 (10%)]\tLoss: 0.388146\tLR: 0.00041278\n",
            "Train Epoch: 6 [12544/123872 (10%)]\tLoss: 0.396131\tLR: 0.00041267\n",
            "Train Epoch: 6 [12800/123872 (10%)]\tLoss: 0.404505\tLR: 0.00041256\n",
            "Train Epoch: 6 [12800/123872 (10%)]\tLoss: 0.404505\n",
            "Train Epoch: 6 [13056/123872 (11%)]\tLoss: 0.441886\tLR: 0.00041245\n",
            "Train Epoch: 6 [13312/123872 (11%)]\tLoss: 0.415059\tLR: 0.00041233\n",
            "Train Epoch: 6 [13568/123872 (11%)]\tLoss: 0.415783\tLR: 0.00041222\n",
            "Train Epoch: 6 [13824/123872 (11%)]\tLoss: 0.378590\tLR: 0.00041211\n",
            "Train Epoch: 6 [14080/123872 (11%)]\tLoss: 0.406660\tLR: 0.00041200\n",
            "Train Epoch: 6 [14336/123872 (12%)]\tLoss: 0.382899\tLR: 0.00041188\n",
            "Train Epoch: 6 [14592/123872 (12%)]\tLoss: 0.411282\tLR: 0.00041177\n",
            "Train Epoch: 6 [14848/123872 (12%)]\tLoss: 0.457695\tLR: 0.00041166\n",
            "Train Epoch: 6 [15104/123872 (12%)]\tLoss: 0.394723\tLR: 0.00041155\n",
            "Train Epoch: 6 [15360/123872 (12%)]\tLoss: 0.325996\tLR: 0.00041143\n",
            "Train Epoch: 6 [15360/123872 (12%)]\tLoss: 0.325996\n",
            "Train Epoch: 6 [15616/123872 (13%)]\tLoss: 0.436692\tLR: 0.00041132\n",
            "Train Epoch: 6 [15872/123872 (13%)]\tLoss: 0.345423\tLR: 0.00041121\n",
            "Train Epoch: 6 [16128/123872 (13%)]\tLoss: 0.418781\tLR: 0.00041110\n",
            "Train Epoch: 6 [16384/123872 (13%)]\tLoss: 0.372102\tLR: 0.00041098\n",
            "Train Epoch: 6 [16640/123872 (13%)]\tLoss: 0.373380\tLR: 0.00041087\n",
            "Train Epoch: 6 [16896/123872 (14%)]\tLoss: 0.410398\tLR: 0.00041076\n",
            "Train Epoch: 6 [17152/123872 (14%)]\tLoss: 0.472381\tLR: 0.00041065\n",
            "Train Epoch: 6 [17408/123872 (14%)]\tLoss: 0.352597\tLR: 0.00041053\n",
            "Train Epoch: 6 [17664/123872 (14%)]\tLoss: 0.407362\tLR: 0.00041042\n",
            "Train Epoch: 6 [17920/123872 (14%)]\tLoss: 0.406968\tLR: 0.00041031\n",
            "Train Epoch: 6 [17920/123872 (14%)]\tLoss: 0.406968\n",
            "Train Epoch: 6 [18176/123872 (15%)]\tLoss: 0.430449\tLR: 0.00041020\n",
            "Train Epoch: 6 [18432/123872 (15%)]\tLoss: 0.472313\tLR: 0.00041008\n",
            "Train Epoch: 6 [18688/123872 (15%)]\tLoss: 0.393935\tLR: 0.00040997\n",
            "Train Epoch: 6 [18944/123872 (15%)]\tLoss: 0.460351\tLR: 0.00040986\n",
            "Train Epoch: 6 [19200/123872 (15%)]\tLoss: 0.416431\tLR: 0.00040974\n",
            "Train Epoch: 6 [19456/123872 (16%)]\tLoss: 0.430691\tLR: 0.00040963\n",
            "Train Epoch: 6 [19712/123872 (16%)]\tLoss: 0.428452\tLR: 0.00040952\n",
            "Train Epoch: 6 [19968/123872 (16%)]\tLoss: 0.393238\tLR: 0.00040941\n",
            "Train Epoch: 6 [20224/123872 (16%)]\tLoss: 0.379922\tLR: 0.00040929\n",
            "Train Epoch: 6 [20480/123872 (17%)]\tLoss: 0.476105\tLR: 0.00040918\n",
            "Train Epoch: 6 [20480/123872 (17%)]\tLoss: 0.476105\n",
            "Train Epoch: 6 [20736/123872 (17%)]\tLoss: 0.373933\tLR: 0.00040907\n",
            "Train Epoch: 6 [20992/123872 (17%)]\tLoss: 0.431254\tLR: 0.00040896\n",
            "Train Epoch: 6 [21248/123872 (17%)]\tLoss: 0.375257\tLR: 0.00040884\n",
            "Train Epoch: 6 [21504/123872 (17%)]\tLoss: 0.405547\tLR: 0.00040873\n",
            "Train Epoch: 6 [21760/123872 (18%)]\tLoss: 0.450859\tLR: 0.00040862\n",
            "Train Epoch: 6 [22016/123872 (18%)]\tLoss: 0.341022\tLR: 0.00040850\n",
            "Train Epoch: 6 [22272/123872 (18%)]\tLoss: 0.432855\tLR: 0.00040839\n",
            "Train Epoch: 6 [22528/123872 (18%)]\tLoss: 0.373231\tLR: 0.00040828\n",
            "Train Epoch: 6 [22784/123872 (18%)]\tLoss: 0.416638\tLR: 0.00040817\n",
            "Train Epoch: 6 [23040/123872 (19%)]\tLoss: 0.414995\tLR: 0.00040805\n",
            "Train Epoch: 6 [23040/123872 (19%)]\tLoss: 0.414995\n",
            "Train Epoch: 6 [23296/123872 (19%)]\tLoss: 0.404512\tLR: 0.00040794\n",
            "Train Epoch: 6 [23552/123872 (19%)]\tLoss: 0.427495\tLR: 0.00040783\n",
            "Train Epoch: 6 [23808/123872 (19%)]\tLoss: 0.424071\tLR: 0.00040771\n",
            "Train Epoch: 6 [24064/123872 (19%)]\tLoss: 0.410338\tLR: 0.00040760\n",
            "Train Epoch: 6 [24320/123872 (20%)]\tLoss: 0.392825\tLR: 0.00040749\n",
            "Train Epoch: 6 [24576/123872 (20%)]\tLoss: 0.386255\tLR: 0.00040737\n",
            "Train Epoch: 6 [24832/123872 (20%)]\tLoss: 0.364098\tLR: 0.00040726\n",
            "Train Epoch: 6 [25088/123872 (20%)]\tLoss: 0.440144\tLR: 0.00040715\n",
            "Train Epoch: 6 [25344/123872 (20%)]\tLoss: 0.398724\tLR: 0.00040703\n",
            "Train Epoch: 6 [25600/123872 (21%)]\tLoss: 0.376947\tLR: 0.00040692\n",
            "Train Epoch: 6 [25600/123872 (21%)]\tLoss: 0.376947\n",
            "Train Epoch: 6 [25856/123872 (21%)]\tLoss: 0.435419\tLR: 0.00040681\n",
            "Train Epoch: 6 [26112/123872 (21%)]\tLoss: 0.434530\tLR: 0.00040670\n",
            "Train Epoch: 6 [26368/123872 (21%)]\tLoss: 0.442711\tLR: 0.00040658\n",
            "Train Epoch: 6 [26624/123872 (21%)]\tLoss: 0.469863\tLR: 0.00040647\n",
            "Train Epoch: 6 [26880/123872 (22%)]\tLoss: 0.379988\tLR: 0.00040636\n",
            "Train Epoch: 6 [27136/123872 (22%)]\tLoss: 0.410777\tLR: 0.00040624\n",
            "Train Epoch: 6 [27392/123872 (22%)]\tLoss: 0.428951\tLR: 0.00040613\n",
            "Train Epoch: 6 [27648/123872 (22%)]\tLoss: 0.462112\tLR: 0.00040602\n",
            "Train Epoch: 6 [27904/123872 (23%)]\tLoss: 0.428273\tLR: 0.00040590\n",
            "Train Epoch: 6 [28160/123872 (23%)]\tLoss: 0.408648\tLR: 0.00040579\n",
            "Train Epoch: 6 [28160/123872 (23%)]\tLoss: 0.408648\n",
            "Train Epoch: 6 [28416/123872 (23%)]\tLoss: 0.477889\tLR: 0.00040568\n",
            "Train Epoch: 6 [28672/123872 (23%)]\tLoss: 0.457064\tLR: 0.00040556\n",
            "Train Epoch: 6 [28928/123872 (23%)]\tLoss: 0.425895\tLR: 0.00040545\n",
            "Train Epoch: 6 [29184/123872 (24%)]\tLoss: 0.445710\tLR: 0.00040534\n",
            "Train Epoch: 6 [29440/123872 (24%)]\tLoss: 0.429702\tLR: 0.00040522\n",
            "Train Epoch: 6 [29696/123872 (24%)]\tLoss: 0.371578\tLR: 0.00040511\n",
            "Train Epoch: 6 [29952/123872 (24%)]\tLoss: 0.396601\tLR: 0.00040500\n",
            "Train Epoch: 6 [30208/123872 (24%)]\tLoss: 0.447773\tLR: 0.00040488\n",
            "Train Epoch: 6 [30464/123872 (25%)]\tLoss: 0.443258\tLR: 0.00040477\n",
            "Train Epoch: 6 [30720/123872 (25%)]\tLoss: 0.430474\tLR: 0.00040466\n",
            "Train Epoch: 6 [30720/123872 (25%)]\tLoss: 0.430474\n",
            "Train Epoch: 6 [30976/123872 (25%)]\tLoss: 0.391170\tLR: 0.00040454\n",
            "Train Epoch: 6 [31232/123872 (25%)]\tLoss: 0.402858\tLR: 0.00040443\n",
            "Train Epoch: 6 [31488/123872 (25%)]\tLoss: 0.372144\tLR: 0.00040432\n",
            "Train Epoch: 6 [31744/123872 (26%)]\tLoss: 0.394262\tLR: 0.00040420\n",
            "Train Epoch: 6 [32000/123872 (26%)]\tLoss: 0.451159\tLR: 0.00040409\n",
            "Train Epoch: 6 [32256/123872 (26%)]\tLoss: 0.437756\tLR: 0.00040398\n",
            "Train Epoch: 6 [32512/123872 (26%)]\tLoss: 0.396795\tLR: 0.00040386\n",
            "Train Epoch: 6 [32768/123872 (26%)]\tLoss: 0.425531\tLR: 0.00040375\n",
            "Train Epoch: 6 [33024/123872 (27%)]\tLoss: 0.434235\tLR: 0.00040364\n",
            "Train Epoch: 6 [33280/123872 (27%)]\tLoss: 0.393728\tLR: 0.00040352\n",
            "Train Epoch: 6 [33280/123872 (27%)]\tLoss: 0.393728\n",
            "Train Epoch: 6 [33536/123872 (27%)]\tLoss: 0.404734\tLR: 0.00040341\n",
            "Train Epoch: 6 [33792/123872 (27%)]\tLoss: 0.447557\tLR: 0.00040329\n",
            "Train Epoch: 6 [34048/123872 (27%)]\tLoss: 0.390797\tLR: 0.00040318\n",
            "Train Epoch: 6 [34304/123872 (28%)]\tLoss: 0.409057\tLR: 0.00040307\n",
            "Train Epoch: 6 [34560/123872 (28%)]\tLoss: 0.402023\tLR: 0.00040295\n",
            "Train Epoch: 6 [34816/123872 (28%)]\tLoss: 0.397542\tLR: 0.00040284\n",
            "Train Epoch: 6 [35072/123872 (28%)]\tLoss: 0.427817\tLR: 0.00040273\n",
            "Train Epoch: 6 [35328/123872 (29%)]\tLoss: 0.442228\tLR: 0.00040261\n",
            "Train Epoch: 6 [35584/123872 (29%)]\tLoss: 0.387567\tLR: 0.00040250\n",
            "Train Epoch: 6 [35840/123872 (29%)]\tLoss: 0.368978\tLR: 0.00040239\n",
            "Train Epoch: 6 [35840/123872 (29%)]\tLoss: 0.368978\n",
            "Train Epoch: 6 [36096/123872 (29%)]\tLoss: 0.403154\tLR: 0.00040227\n",
            "Train Epoch: 6 [36352/123872 (29%)]\tLoss: 0.395595\tLR: 0.00040216\n",
            "Train Epoch: 6 [36608/123872 (30%)]\tLoss: 0.439351\tLR: 0.00040204\n",
            "Train Epoch: 6 [36864/123872 (30%)]\tLoss: 0.425673\tLR: 0.00040193\n",
            "Train Epoch: 6 [37120/123872 (30%)]\tLoss: 0.426468\tLR: 0.00040182\n",
            "Train Epoch: 6 [37376/123872 (30%)]\tLoss: 0.444845\tLR: 0.00040170\n",
            "Train Epoch: 6 [37632/123872 (30%)]\tLoss: 0.404053\tLR: 0.00040159\n",
            "Train Epoch: 6 [37888/123872 (31%)]\tLoss: 0.391997\tLR: 0.00040148\n",
            "Train Epoch: 6 [38144/123872 (31%)]\tLoss: 0.384815\tLR: 0.00040136\n",
            "Train Epoch: 6 [38400/123872 (31%)]\tLoss: 0.451262\tLR: 0.00040125\n",
            "Train Epoch: 6 [38400/123872 (31%)]\tLoss: 0.451262\n",
            "Train Epoch: 6 [38656/123872 (31%)]\tLoss: 0.408204\tLR: 0.00040113\n",
            "Train Epoch: 6 [38912/123872 (31%)]\tLoss: 0.419424\tLR: 0.00040102\n",
            "Train Epoch: 6 [39168/123872 (32%)]\tLoss: 0.448785\tLR: 0.00040091\n",
            "Train Epoch: 6 [39424/123872 (32%)]\tLoss: 0.389402\tLR: 0.00040079\n",
            "Train Epoch: 6 [39680/123872 (32%)]\tLoss: 0.389111\tLR: 0.00040068\n",
            "Train Epoch: 6 [39936/123872 (32%)]\tLoss: 0.439356\tLR: 0.00040056\n",
            "Train Epoch: 6 [40192/123872 (32%)]\tLoss: 0.442380\tLR: 0.00040045\n",
            "Train Epoch: 6 [40448/123872 (33%)]\tLoss: 0.402481\tLR: 0.00040034\n",
            "Train Epoch: 6 [40704/123872 (33%)]\tLoss: 0.424258\tLR: 0.00040022\n",
            "Train Epoch: 6 [40960/123872 (33%)]\tLoss: 0.427905\tLR: 0.00040011\n",
            "Train Epoch: 6 [40960/123872 (33%)]\tLoss: 0.427905\n",
            "Train Epoch: 6 [41216/123872 (33%)]\tLoss: 0.351196\tLR: 0.00040000\n",
            "Train Epoch: 6 [41472/123872 (33%)]\tLoss: 0.433187\tLR: 0.00039988\n",
            "Train Epoch: 6 [41728/123872 (34%)]\tLoss: 0.389013\tLR: 0.00039977\n",
            "Train Epoch: 6 [41984/123872 (34%)]\tLoss: 0.403479\tLR: 0.00039965\n",
            "Train Epoch: 6 [42240/123872 (34%)]\tLoss: 0.423149\tLR: 0.00039954\n",
            "Train Epoch: 6 [42496/123872 (34%)]\tLoss: 0.429325\tLR: 0.00039943\n",
            "Train Epoch: 6 [42752/123872 (35%)]\tLoss: 0.367320\tLR: 0.00039931\n",
            "Train Epoch: 6 [43008/123872 (35%)]\tLoss: 0.484627\tLR: 0.00039920\n",
            "Train Epoch: 6 [43264/123872 (35%)]\tLoss: 0.459992\tLR: 0.00039908\n",
            "Train Epoch: 6 [43520/123872 (35%)]\tLoss: 0.447964\tLR: 0.00039897\n",
            "Train Epoch: 6 [43520/123872 (35%)]\tLoss: 0.447964\n",
            "Train Epoch: 6 [43776/123872 (35%)]\tLoss: 0.410108\tLR: 0.00039885\n",
            "Train Epoch: 6 [44032/123872 (36%)]\tLoss: 0.388853\tLR: 0.00039874\n",
            "Train Epoch: 6 [44288/123872 (36%)]\tLoss: 0.447427\tLR: 0.00039863\n",
            "Train Epoch: 6 [44544/123872 (36%)]\tLoss: 0.428930\tLR: 0.00039851\n",
            "Train Epoch: 6 [44800/123872 (36%)]\tLoss: 0.471383\tLR: 0.00039840\n",
            "Train Epoch: 6 [45056/123872 (36%)]\tLoss: 0.440444\tLR: 0.00039828\n",
            "Train Epoch: 6 [45312/123872 (37%)]\tLoss: 0.450016\tLR: 0.00039817\n",
            "Train Epoch: 6 [45568/123872 (37%)]\tLoss: 0.445359\tLR: 0.00039806\n",
            "Train Epoch: 6 [45824/123872 (37%)]\tLoss: 0.442439\tLR: 0.00039794\n",
            "Train Epoch: 6 [46080/123872 (37%)]\tLoss: 0.344818\tLR: 0.00039783\n",
            "Train Epoch: 6 [46080/123872 (37%)]\tLoss: 0.344818\n",
            "Train Epoch: 6 [46336/123872 (37%)]\tLoss: 0.415856\tLR: 0.00039771\n",
            "Train Epoch: 6 [46592/123872 (38%)]\tLoss: 0.395281\tLR: 0.00039760\n",
            "Train Epoch: 6 [46848/123872 (38%)]\tLoss: 0.403905\tLR: 0.00039748\n",
            "Train Epoch: 6 [47104/123872 (38%)]\tLoss: 0.456308\tLR: 0.00039737\n",
            "Train Epoch: 6 [47360/123872 (38%)]\tLoss: 0.419881\tLR: 0.00039726\n",
            "Train Epoch: 6 [47616/123872 (38%)]\tLoss: 0.411873\tLR: 0.00039714\n",
            "Train Epoch: 6 [47872/123872 (39%)]\tLoss: 0.467815\tLR: 0.00039703\n",
            "Train Epoch: 6 [48128/123872 (39%)]\tLoss: 0.405846\tLR: 0.00039691\n",
            "Train Epoch: 6 [48384/123872 (39%)]\tLoss: 0.398189\tLR: 0.00039680\n",
            "Train Epoch: 6 [48640/123872 (39%)]\tLoss: 0.415493\tLR: 0.00039668\n",
            "Train Epoch: 6 [48640/123872 (39%)]\tLoss: 0.415493\n",
            "Train Epoch: 6 [48896/123872 (39%)]\tLoss: 0.409203\tLR: 0.00039657\n",
            "Train Epoch: 6 [49152/123872 (40%)]\tLoss: 0.397938\tLR: 0.00039646\n",
            "Train Epoch: 6 [49408/123872 (40%)]\tLoss: 0.506610\tLR: 0.00039634\n",
            "Train Epoch: 6 [49664/123872 (40%)]\tLoss: 0.433095\tLR: 0.00039623\n",
            "Train Epoch: 6 [49920/123872 (40%)]\tLoss: 0.480231\tLR: 0.00039611\n",
            "Train Epoch: 6 [50176/123872 (40%)]\tLoss: 0.368008\tLR: 0.00039600\n",
            "Train Epoch: 6 [50432/123872 (41%)]\tLoss: 0.457291\tLR: 0.00039588\n",
            "Train Epoch: 6 [50688/123872 (41%)]\tLoss: 0.414684\tLR: 0.00039577\n",
            "Train Epoch: 6 [50944/123872 (41%)]\tLoss: 0.458481\tLR: 0.00039565\n",
            "Train Epoch: 6 [51200/123872 (41%)]\tLoss: 0.393059\tLR: 0.00039554\n",
            "Train Epoch: 6 [51200/123872 (41%)]\tLoss: 0.393059\n",
            "Train Epoch: 6 [51456/123872 (42%)]\tLoss: 0.407703\tLR: 0.00039543\n",
            "Train Epoch: 6 [51712/123872 (42%)]\tLoss: 0.384440\tLR: 0.00039531\n",
            "Train Epoch: 6 [51968/123872 (42%)]\tLoss: 0.361892\tLR: 0.00039520\n",
            "Train Epoch: 6 [52224/123872 (42%)]\tLoss: 0.436860\tLR: 0.00039508\n",
            "Train Epoch: 6 [52480/123872 (42%)]\tLoss: 0.405821\tLR: 0.00039497\n",
            "Train Epoch: 6 [52736/123872 (43%)]\tLoss: 0.407308\tLR: 0.00039485\n",
            "Train Epoch: 6 [52992/123872 (43%)]\tLoss: 0.402071\tLR: 0.00039474\n",
            "Train Epoch: 6 [53248/123872 (43%)]\tLoss: 0.464261\tLR: 0.00039462\n",
            "Train Epoch: 6 [53504/123872 (43%)]\tLoss: 0.410992\tLR: 0.00039451\n",
            "Train Epoch: 6 [53760/123872 (43%)]\tLoss: 0.461296\tLR: 0.00039439\n",
            "Train Epoch: 6 [53760/123872 (43%)]\tLoss: 0.461296\n",
            "Train Epoch: 6 [54016/123872 (44%)]\tLoss: 0.402382\tLR: 0.00039428\n",
            "Train Epoch: 6 [54272/123872 (44%)]\tLoss: 0.336071\tLR: 0.00039417\n",
            "Train Epoch: 6 [54528/123872 (44%)]\tLoss: 0.441547\tLR: 0.00039405\n",
            "Train Epoch: 6 [54784/123872 (44%)]\tLoss: 0.428496\tLR: 0.00039394\n",
            "Train Epoch: 6 [55040/123872 (44%)]\tLoss: 0.420864\tLR: 0.00039382\n",
            "Train Epoch: 6 [55296/123872 (45%)]\tLoss: 0.411702\tLR: 0.00039371\n",
            "Train Epoch: 6 [55552/123872 (45%)]\tLoss: 0.490702\tLR: 0.00039359\n",
            "Train Epoch: 6 [55808/123872 (45%)]\tLoss: 0.319955\tLR: 0.00039348\n",
            "Train Epoch: 6 [56064/123872 (45%)]\tLoss: 0.395990\tLR: 0.00039336\n",
            "Train Epoch: 6 [56320/123872 (45%)]\tLoss: 0.446007\tLR: 0.00039325\n",
            "Train Epoch: 6 [56320/123872 (45%)]\tLoss: 0.446007\n",
            "Train Epoch: 6 [56576/123872 (46%)]\tLoss: 0.452400\tLR: 0.00039313\n",
            "Train Epoch: 6 [56832/123872 (46%)]\tLoss: 0.462119\tLR: 0.00039302\n",
            "Train Epoch: 6 [57088/123872 (46%)]\tLoss: 0.444824\tLR: 0.00039290\n",
            "Train Epoch: 6 [57344/123872 (46%)]\tLoss: 0.423059\tLR: 0.00039279\n",
            "Train Epoch: 6 [57600/123872 (46%)]\tLoss: 0.424170\tLR: 0.00039267\n",
            "Train Epoch: 6 [57856/123872 (47%)]\tLoss: 0.395176\tLR: 0.00039256\n",
            "Train Epoch: 6 [58112/123872 (47%)]\tLoss: 0.459999\tLR: 0.00039244\n",
            "Train Epoch: 6 [58368/123872 (47%)]\tLoss: 0.356058\tLR: 0.00039233\n",
            "Train Epoch: 6 [58624/123872 (47%)]\tLoss: 0.379388\tLR: 0.00039222\n",
            "Train Epoch: 6 [58880/123872 (48%)]\tLoss: 0.399705\tLR: 0.00039210\n",
            "Train Epoch: 6 [58880/123872 (48%)]\tLoss: 0.399705\n",
            "Train Epoch: 6 [59136/123872 (48%)]\tLoss: 0.418189\tLR: 0.00039199\n",
            "Train Epoch: 6 [59392/123872 (48%)]\tLoss: 0.383837\tLR: 0.00039187\n",
            "Train Epoch: 6 [59648/123872 (48%)]\tLoss: 0.441635\tLR: 0.00039176\n",
            "Train Epoch: 6 [59904/123872 (48%)]\tLoss: 0.379945\tLR: 0.00039164\n",
            "Train Epoch: 6 [60160/123872 (49%)]\tLoss: 0.445694\tLR: 0.00039153\n",
            "Train Epoch: 6 [60416/123872 (49%)]\tLoss: 0.400031\tLR: 0.00039141\n",
            "Train Epoch: 6 [60672/123872 (49%)]\tLoss: 0.412154\tLR: 0.00039130\n",
            "Train Epoch: 6 [60928/123872 (49%)]\tLoss: 0.412281\tLR: 0.00039118\n",
            "Train Epoch: 6 [61184/123872 (49%)]\tLoss: 0.421313\tLR: 0.00039107\n",
            "Train Epoch: 6 [61440/123872 (50%)]\tLoss: 0.421853\tLR: 0.00039095\n",
            "Train Epoch: 6 [61440/123872 (50%)]\tLoss: 0.421853\n",
            "Train Epoch: 6 [61696/123872 (50%)]\tLoss: 0.397751\tLR: 0.00039084\n",
            "Train Epoch: 6 [61952/123872 (50%)]\tLoss: 0.489802\tLR: 0.00039072\n",
            "Train Epoch: 6 [62208/123872 (50%)]\tLoss: 0.434185\tLR: 0.00039061\n",
            "Train Epoch: 6 [62464/123872 (50%)]\tLoss: 0.374681\tLR: 0.00039049\n",
            "Train Epoch: 6 [62720/123872 (51%)]\tLoss: 0.425859\tLR: 0.00039038\n",
            "Train Epoch: 6 [62976/123872 (51%)]\tLoss: 0.402555\tLR: 0.00039026\n",
            "Train Epoch: 6 [63232/123872 (51%)]\tLoss: 0.346774\tLR: 0.00039015\n",
            "Train Epoch: 6 [63488/123872 (51%)]\tLoss: 0.424345\tLR: 0.00039003\n",
            "Train Epoch: 6 [63744/123872 (51%)]\tLoss: 0.408431\tLR: 0.00038992\n",
            "Train Epoch: 6 [64000/123872 (52%)]\tLoss: 0.396060\tLR: 0.00038980\n",
            "Train Epoch: 6 [64000/123872 (52%)]\tLoss: 0.396060\n",
            "Train Epoch: 6 [64256/123872 (52%)]\tLoss: 0.354142\tLR: 0.00038969\n",
            "Train Epoch: 6 [64512/123872 (52%)]\tLoss: 0.413933\tLR: 0.00038957\n",
            "Train Epoch: 6 [64768/123872 (52%)]\tLoss: 0.473068\tLR: 0.00038946\n",
            "Train Epoch: 6 [65024/123872 (52%)]\tLoss: 0.485927\tLR: 0.00038934\n",
            "Train Epoch: 6 [65280/123872 (53%)]\tLoss: 0.559687\tLR: 0.00038923\n",
            "Train Epoch: 6 [65536/123872 (53%)]\tLoss: 0.435589\tLR: 0.00038911\n",
            "Train Epoch: 6 [65792/123872 (53%)]\tLoss: 0.392058\tLR: 0.00038900\n",
            "Train Epoch: 6 [66048/123872 (53%)]\tLoss: 0.490347\tLR: 0.00038888\n",
            "Train Epoch: 6 [66304/123872 (54%)]\tLoss: 0.487207\tLR: 0.00038877\n",
            "Train Epoch: 6 [66560/123872 (54%)]\tLoss: 0.544813\tLR: 0.00038865\n",
            "Train Epoch: 6 [66560/123872 (54%)]\tLoss: 0.544813\n",
            "Train Epoch: 6 [66816/123872 (54%)]\tLoss: 0.440698\tLR: 0.00038853\n",
            "Train Epoch: 6 [67072/123872 (54%)]\tLoss: 0.477926\tLR: 0.00038842\n",
            "Train Epoch: 6 [67328/123872 (54%)]\tLoss: 0.446164\tLR: 0.00038830\n",
            "Train Epoch: 6 [67584/123872 (55%)]\tLoss: 0.467567\tLR: 0.00038819\n",
            "Train Epoch: 6 [67840/123872 (55%)]\tLoss: 0.407414\tLR: 0.00038807\n",
            "Train Epoch: 6 [68096/123872 (55%)]\tLoss: 0.422079\tLR: 0.00038796\n",
            "Train Epoch: 6 [68352/123872 (55%)]\tLoss: 0.428635\tLR: 0.00038784\n",
            "Train Epoch: 6 [68608/123872 (55%)]\tLoss: 0.414552\tLR: 0.00038773\n",
            "Train Epoch: 6 [68864/123872 (56%)]\tLoss: 0.406400\tLR: 0.00038761\n",
            "Train Epoch: 6 [69120/123872 (56%)]\tLoss: 0.427220\tLR: 0.00038750\n",
            "Train Epoch: 6 [69120/123872 (56%)]\tLoss: 0.427220\n",
            "Train Epoch: 6 [69376/123872 (56%)]\tLoss: 0.376054\tLR: 0.00038738\n",
            "Train Epoch: 6 [69632/123872 (56%)]\tLoss: 0.437056\tLR: 0.00038727\n",
            "Train Epoch: 6 [69888/123872 (56%)]\tLoss: 0.394967\tLR: 0.00038715\n",
            "Train Epoch: 6 [70144/123872 (57%)]\tLoss: 0.376859\tLR: 0.00038704\n",
            "Train Epoch: 6 [70400/123872 (57%)]\tLoss: 0.422171\tLR: 0.00038692\n",
            "Train Epoch: 6 [70656/123872 (57%)]\tLoss: 0.420916\tLR: 0.00038681\n",
            "Train Epoch: 6 [70912/123872 (57%)]\tLoss: 0.475013\tLR: 0.00038669\n",
            "Train Epoch: 6 [71168/123872 (57%)]\tLoss: 0.414918\tLR: 0.00038657\n",
            "Train Epoch: 6 [71424/123872 (58%)]\tLoss: 0.393587\tLR: 0.00038646\n",
            "Train Epoch: 6 [71680/123872 (58%)]\tLoss: 0.439950\tLR: 0.00038634\n",
            "Train Epoch: 6 [71680/123872 (58%)]\tLoss: 0.439950\n",
            "Train Epoch: 6 [71936/123872 (58%)]\tLoss: 0.370525\tLR: 0.00038623\n",
            "Train Epoch: 6 [72192/123872 (58%)]\tLoss: 0.419575\tLR: 0.00038611\n",
            "Train Epoch: 6 [72448/123872 (58%)]\tLoss: 0.402155\tLR: 0.00038600\n",
            "Train Epoch: 6 [72704/123872 (59%)]\tLoss: 0.390497\tLR: 0.00038588\n",
            "Train Epoch: 6 [72960/123872 (59%)]\tLoss: 0.456850\tLR: 0.00038577\n",
            "Train Epoch: 6 [73216/123872 (59%)]\tLoss: 0.458329\tLR: 0.00038565\n",
            "Train Epoch: 6 [73472/123872 (59%)]\tLoss: 0.456661\tLR: 0.00038554\n",
            "Train Epoch: 6 [73728/123872 (60%)]\tLoss: 0.420219\tLR: 0.00038542\n",
            "Train Epoch: 6 [73984/123872 (60%)]\tLoss: 0.378497\tLR: 0.00038530\n",
            "Train Epoch: 6 [74240/123872 (60%)]\tLoss: 0.405212\tLR: 0.00038519\n",
            "Train Epoch: 6 [74240/123872 (60%)]\tLoss: 0.405212\n",
            "Train Epoch: 6 [74496/123872 (60%)]\tLoss: 0.390323\tLR: 0.00038507\n",
            "Train Epoch: 6 [74752/123872 (60%)]\tLoss: 0.361591\tLR: 0.00038496\n",
            "Train Epoch: 6 [75008/123872 (61%)]\tLoss: 0.462447\tLR: 0.00038484\n",
            "Train Epoch: 6 [75264/123872 (61%)]\tLoss: 0.412254\tLR: 0.00038473\n",
            "Train Epoch: 6 [75520/123872 (61%)]\tLoss: 0.416228\tLR: 0.00038461\n",
            "Train Epoch: 6 [75776/123872 (61%)]\tLoss: 0.436958\tLR: 0.00038450\n",
            "Train Epoch: 6 [76032/123872 (61%)]\tLoss: 0.497653\tLR: 0.00038438\n",
            "Train Epoch: 6 [76288/123872 (62%)]\tLoss: 0.414365\tLR: 0.00038426\n",
            "Train Epoch: 6 [76544/123872 (62%)]\tLoss: 0.425231\tLR: 0.00038415\n",
            "Train Epoch: 6 [76800/123872 (62%)]\tLoss: 0.412072\tLR: 0.00038403\n",
            "Train Epoch: 6 [76800/123872 (62%)]\tLoss: 0.412072\n",
            "Train Epoch: 6 [77056/123872 (62%)]\tLoss: 0.425707\tLR: 0.00038392\n",
            "Train Epoch: 6 [77312/123872 (62%)]\tLoss: 0.386071\tLR: 0.00038380\n",
            "Train Epoch: 6 [77568/123872 (63%)]\tLoss: 0.452665\tLR: 0.00038369\n",
            "Train Epoch: 6 [77824/123872 (63%)]\tLoss: 0.401649\tLR: 0.00038357\n",
            "Train Epoch: 6 [78080/123872 (63%)]\tLoss: 0.427982\tLR: 0.00038346\n",
            "Train Epoch: 6 [78336/123872 (63%)]\tLoss: 0.472710\tLR: 0.00038334\n",
            "Train Epoch: 6 [78592/123872 (63%)]\tLoss: 0.330709\tLR: 0.00038322\n",
            "Train Epoch: 6 [78848/123872 (64%)]\tLoss: 0.411568\tLR: 0.00038311\n",
            "Train Epoch: 6 [79104/123872 (64%)]\tLoss: 0.396832\tLR: 0.00038299\n",
            "Train Epoch: 6 [79360/123872 (64%)]\tLoss: 0.369094\tLR: 0.00038288\n",
            "Train Epoch: 6 [79360/123872 (64%)]\tLoss: 0.369094\n",
            "Train Epoch: 6 [79616/123872 (64%)]\tLoss: 0.414303\tLR: 0.00038276\n",
            "Train Epoch: 6 [79872/123872 (64%)]\tLoss: 0.390504\tLR: 0.00038265\n",
            "Train Epoch: 6 [80128/123872 (65%)]\tLoss: 0.371276\tLR: 0.00038253\n",
            "Train Epoch: 6 [80384/123872 (65%)]\tLoss: 0.458984\tLR: 0.00038241\n",
            "Train Epoch: 6 [80640/123872 (65%)]\tLoss: 0.386038\tLR: 0.00038230\n",
            "Train Epoch: 6 [80896/123872 (65%)]\tLoss: 0.444109\tLR: 0.00038218\n",
            "Train Epoch: 6 [81152/123872 (65%)]\tLoss: 0.375248\tLR: 0.00038207\n",
            "Train Epoch: 6 [81408/123872 (66%)]\tLoss: 0.406436\tLR: 0.00038195\n",
            "Train Epoch: 6 [81664/123872 (66%)]\tLoss: 0.401749\tLR: 0.00038184\n",
            "Train Epoch: 6 [81920/123872 (66%)]\tLoss: 0.440284\tLR: 0.00038172\n",
            "Train Epoch: 6 [81920/123872 (66%)]\tLoss: 0.440284\n",
            "Train Epoch: 6 [82176/123872 (66%)]\tLoss: 0.436014\tLR: 0.00038160\n",
            "Train Epoch: 6 [82432/123872 (67%)]\tLoss: 0.402872\tLR: 0.00038149\n",
            "Train Epoch: 6 [82688/123872 (67%)]\tLoss: 0.403203\tLR: 0.00038137\n",
            "Train Epoch: 6 [82944/123872 (67%)]\tLoss: 0.409923\tLR: 0.00038126\n",
            "Train Epoch: 6 [83200/123872 (67%)]\tLoss: 0.359283\tLR: 0.00038114\n",
            "Train Epoch: 6 [83456/123872 (67%)]\tLoss: 0.414960\tLR: 0.00038102\n",
            "Train Epoch: 6 [83712/123872 (68%)]\tLoss: 0.407692\tLR: 0.00038091\n",
            "Train Epoch: 6 [83968/123872 (68%)]\tLoss: 0.381043\tLR: 0.00038079\n",
            "Train Epoch: 6 [84224/123872 (68%)]\tLoss: 0.406224\tLR: 0.00038068\n",
            "Train Epoch: 6 [84480/123872 (68%)]\tLoss: 0.434372\tLR: 0.00038056\n",
            "Train Epoch: 6 [84480/123872 (68%)]\tLoss: 0.434372\n",
            "Train Epoch: 6 [84736/123872 (68%)]\tLoss: 0.399313\tLR: 0.00038044\n",
            "Train Epoch: 6 [84992/123872 (69%)]\tLoss: 0.382357\tLR: 0.00038033\n",
            "Train Epoch: 6 [85248/123872 (69%)]\tLoss: 0.393776\tLR: 0.00038021\n",
            "Train Epoch: 6 [85504/123872 (69%)]\tLoss: 0.362075\tLR: 0.00038010\n",
            "Train Epoch: 6 [85760/123872 (69%)]\tLoss: 0.483618\tLR: 0.00037998\n",
            "Train Epoch: 6 [86016/123872 (69%)]\tLoss: 0.415831\tLR: 0.00037986\n",
            "Train Epoch: 6 [86272/123872 (70%)]\tLoss: 0.422700\tLR: 0.00037975\n",
            "Train Epoch: 6 [86528/123872 (70%)]\tLoss: 0.430101\tLR: 0.00037963\n",
            "Train Epoch: 6 [86784/123872 (70%)]\tLoss: 0.393576\tLR: 0.00037952\n",
            "Train Epoch: 6 [87040/123872 (70%)]\tLoss: 0.430542\tLR: 0.00037940\n",
            "Train Epoch: 6 [87040/123872 (70%)]\tLoss: 0.430542\n",
            "Train Epoch: 6 [87296/123872 (70%)]\tLoss: 0.436646\tLR: 0.00037928\n",
            "Train Epoch: 6 [87552/123872 (71%)]\tLoss: 0.413940\tLR: 0.00037917\n",
            "Train Epoch: 6 [87808/123872 (71%)]\tLoss: 0.400942\tLR: 0.00037905\n",
            "Train Epoch: 6 [88064/123872 (71%)]\tLoss: 0.397179\tLR: 0.00037894\n",
            "Train Epoch: 6 [88320/123872 (71%)]\tLoss: 0.392820\tLR: 0.00037882\n",
            "Train Epoch: 6 [88576/123872 (71%)]\tLoss: 0.488828\tLR: 0.00037870\n",
            "Train Epoch: 6 [88832/123872 (72%)]\tLoss: 0.419210\tLR: 0.00037859\n",
            "Train Epoch: 6 [89088/123872 (72%)]\tLoss: 0.448212\tLR: 0.00037847\n",
            "Train Epoch: 6 [89344/123872 (72%)]\tLoss: 0.384960\tLR: 0.00037836\n",
            "Train Epoch: 6 [89600/123872 (72%)]\tLoss: 0.413265\tLR: 0.00037824\n",
            "Train Epoch: 6 [89600/123872 (72%)]\tLoss: 0.413265\n",
            "Train Epoch: 6 [89856/123872 (73%)]\tLoss: 0.421986\tLR: 0.00037812\n",
            "Train Epoch: 6 [90112/123872 (73%)]\tLoss: 0.407365\tLR: 0.00037801\n",
            "Train Epoch: 6 [90368/123872 (73%)]\tLoss: 0.360298\tLR: 0.00037789\n",
            "Train Epoch: 6 [90624/123872 (73%)]\tLoss: 0.410048\tLR: 0.00037778\n",
            "Train Epoch: 6 [90880/123872 (73%)]\tLoss: 0.403541\tLR: 0.00037766\n",
            "Train Epoch: 6 [91136/123872 (74%)]\tLoss: 0.384108\tLR: 0.00037754\n",
            "Train Epoch: 6 [91392/123872 (74%)]\tLoss: 0.419387\tLR: 0.00037743\n",
            "Train Epoch: 6 [91648/123872 (74%)]\tLoss: 0.416450\tLR: 0.00037731\n",
            "Train Epoch: 6 [91904/123872 (74%)]\tLoss: 0.450737\tLR: 0.00037719\n",
            "Train Epoch: 6 [92160/123872 (74%)]\tLoss: 0.408839\tLR: 0.00037708\n",
            "Train Epoch: 6 [92160/123872 (74%)]\tLoss: 0.408839\n",
            "Train Epoch: 6 [92416/123872 (75%)]\tLoss: 0.398382\tLR: 0.00037696\n",
            "Train Epoch: 6 [92672/123872 (75%)]\tLoss: 0.500673\tLR: 0.00037685\n",
            "Train Epoch: 6 [92928/123872 (75%)]\tLoss: 0.387078\tLR: 0.00037673\n",
            "Train Epoch: 6 [93184/123872 (75%)]\tLoss: 0.356277\tLR: 0.00037661\n",
            "Train Epoch: 6 [93440/123872 (75%)]\tLoss: 0.387263\tLR: 0.00037650\n",
            "Train Epoch: 6 [93696/123872 (76%)]\tLoss: 0.396471\tLR: 0.00037638\n",
            "Train Epoch: 6 [93952/123872 (76%)]\tLoss: 0.445046\tLR: 0.00037627\n",
            "Train Epoch: 6 [94208/123872 (76%)]\tLoss: 0.510308\tLR: 0.00037615\n",
            "Train Epoch: 6 [94464/123872 (76%)]\tLoss: 0.395549\tLR: 0.00037603\n",
            "Train Epoch: 6 [94720/123872 (76%)]\tLoss: 0.330867\tLR: 0.00037592\n",
            "Train Epoch: 6 [94720/123872 (76%)]\tLoss: 0.330867\n",
            "Train Epoch: 6 [94976/123872 (77%)]\tLoss: 0.428491\tLR: 0.00037580\n",
            "Train Epoch: 6 [95232/123872 (77%)]\tLoss: 0.379937\tLR: 0.00037568\n",
            "Train Epoch: 6 [95488/123872 (77%)]\tLoss: 0.359439\tLR: 0.00037557\n",
            "Train Epoch: 6 [95744/123872 (77%)]\tLoss: 0.431872\tLR: 0.00037545\n",
            "Train Epoch: 6 [96000/123872 (77%)]\tLoss: 0.391197\tLR: 0.00037533\n",
            "Train Epoch: 6 [96256/123872 (78%)]\tLoss: 0.410929\tLR: 0.00037522\n",
            "Train Epoch: 6 [96512/123872 (78%)]\tLoss: 0.353540\tLR: 0.00037510\n",
            "Train Epoch: 6 [96768/123872 (78%)]\tLoss: 0.384759\tLR: 0.00037499\n",
            "Train Epoch: 6 [97024/123872 (78%)]\tLoss: 0.396101\tLR: 0.00037487\n",
            "Train Epoch: 6 [97280/123872 (79%)]\tLoss: 0.438674\tLR: 0.00037475\n",
            "Train Epoch: 6 [97280/123872 (79%)]\tLoss: 0.438674\n",
            "Train Epoch: 6 [97536/123872 (79%)]\tLoss: 0.440366\tLR: 0.00037464\n",
            "Train Epoch: 6 [97792/123872 (79%)]\tLoss: 0.387159\tLR: 0.00037452\n",
            "Train Epoch: 6 [98048/123872 (79%)]\tLoss: 0.377773\tLR: 0.00037440\n",
            "Train Epoch: 6 [98304/123872 (79%)]\tLoss: 0.430760\tLR: 0.00037429\n",
            "Train Epoch: 6 [98560/123872 (80%)]\tLoss: 0.382067\tLR: 0.00037417\n",
            "Train Epoch: 6 [98816/123872 (80%)]\tLoss: 0.451724\tLR: 0.00037405\n",
            "Train Epoch: 6 [99072/123872 (80%)]\tLoss: 0.433976\tLR: 0.00037394\n",
            "Train Epoch: 6 [99328/123872 (80%)]\tLoss: 0.442547\tLR: 0.00037382\n",
            "Train Epoch: 6 [99584/123872 (80%)]\tLoss: 0.412566\tLR: 0.00037371\n",
            "Train Epoch: 6 [99840/123872 (81%)]\tLoss: 0.483008\tLR: 0.00037359\n",
            "Train Epoch: 6 [99840/123872 (81%)]\tLoss: 0.483008\n",
            "Train Epoch: 6 [100096/123872 (81%)]\tLoss: 0.420485\tLR: 0.00037347\n",
            "Train Epoch: 6 [100352/123872 (81%)]\tLoss: 0.435288\tLR: 0.00037336\n",
            "Train Epoch: 6 [100608/123872 (81%)]\tLoss: 0.347029\tLR: 0.00037324\n",
            "Train Epoch: 6 [100864/123872 (81%)]\tLoss: 0.460833\tLR: 0.00037312\n",
            "Train Epoch: 6 [101120/123872 (82%)]\tLoss: 0.395015\tLR: 0.00037301\n",
            "Train Epoch: 6 [101376/123872 (82%)]\tLoss: 0.392154\tLR: 0.00037289\n",
            "Train Epoch: 6 [101632/123872 (82%)]\tLoss: 0.390128\tLR: 0.00037277\n",
            "Train Epoch: 6 [101888/123872 (82%)]\tLoss: 0.386356\tLR: 0.00037266\n",
            "Train Epoch: 6 [102144/123872 (82%)]\tLoss: 0.445403\tLR: 0.00037254\n",
            "Train Epoch: 6 [102400/123872 (83%)]\tLoss: 0.413502\tLR: 0.00037242\n",
            "Train Epoch: 6 [102400/123872 (83%)]\tLoss: 0.413502\n",
            "Train Epoch: 6 [102656/123872 (83%)]\tLoss: 0.410076\tLR: 0.00037231\n",
            "Train Epoch: 6 [102912/123872 (83%)]\tLoss: 0.412495\tLR: 0.00037219\n",
            "Train Epoch: 6 [103168/123872 (83%)]\tLoss: 0.420173\tLR: 0.00037207\n",
            "Train Epoch: 6 [103424/123872 (83%)]\tLoss: 0.414210\tLR: 0.00037196\n",
            "Train Epoch: 6 [103680/123872 (84%)]\tLoss: 0.379728\tLR: 0.00037184\n",
            "Train Epoch: 6 [103936/123872 (84%)]\tLoss: 0.407899\tLR: 0.00037172\n",
            "Train Epoch: 6 [104192/123872 (84%)]\tLoss: 0.419623\tLR: 0.00037161\n",
            "Train Epoch: 6 [104448/123872 (84%)]\tLoss: 0.350188\tLR: 0.00037149\n",
            "Train Epoch: 6 [104704/123872 (85%)]\tLoss: 0.402512\tLR: 0.00037137\n",
            "Train Epoch: 6 [104960/123872 (85%)]\tLoss: 0.450028\tLR: 0.00037126\n",
            "Train Epoch: 6 [104960/123872 (85%)]\tLoss: 0.450028\n",
            "Train Epoch: 6 [105216/123872 (85%)]\tLoss: 0.401998\tLR: 0.00037114\n",
            "Train Epoch: 6 [105472/123872 (85%)]\tLoss: 0.394140\tLR: 0.00037102\n",
            "Train Epoch: 6 [105728/123872 (85%)]\tLoss: 0.452534\tLR: 0.00037091\n",
            "Train Epoch: 6 [105984/123872 (86%)]\tLoss: 0.454431\tLR: 0.00037079\n",
            "Train Epoch: 6 [106240/123872 (86%)]\tLoss: 0.441030\tLR: 0.00037067\n",
            "Train Epoch: 6 [106496/123872 (86%)]\tLoss: 0.459853\tLR: 0.00037056\n",
            "Train Epoch: 6 [106752/123872 (86%)]\tLoss: 0.371045\tLR: 0.00037044\n",
            "Train Epoch: 6 [107008/123872 (86%)]\tLoss: 0.354429\tLR: 0.00037032\n",
            "Train Epoch: 6 [107264/123872 (87%)]\tLoss: 0.446065\tLR: 0.00037021\n",
            "Train Epoch: 6 [107520/123872 (87%)]\tLoss: 0.408463\tLR: 0.00037009\n",
            "Train Epoch: 6 [107520/123872 (87%)]\tLoss: 0.408463\n",
            "Train Epoch: 6 [107776/123872 (87%)]\tLoss: 0.419749\tLR: 0.00036997\n",
            "Train Epoch: 6 [108032/123872 (87%)]\tLoss: 0.393604\tLR: 0.00036986\n",
            "Train Epoch: 6 [108288/123872 (87%)]\tLoss: 0.452884\tLR: 0.00036974\n",
            "Train Epoch: 6 [108544/123872 (88%)]\tLoss: 0.338936\tLR: 0.00036962\n",
            "Train Epoch: 6 [108800/123872 (88%)]\tLoss: 0.409906\tLR: 0.00036951\n",
            "Train Epoch: 6 [109056/123872 (88%)]\tLoss: 0.384398\tLR: 0.00036939\n",
            "Train Epoch: 6 [109312/123872 (88%)]\tLoss: 0.412191\tLR: 0.00036927\n",
            "Train Epoch: 6 [109568/123872 (88%)]\tLoss: 0.375980\tLR: 0.00036916\n",
            "Train Epoch: 6 [109824/123872 (89%)]\tLoss: 0.481164\tLR: 0.00036904\n",
            "Train Epoch: 6 [110080/123872 (89%)]\tLoss: 0.450506\tLR: 0.00036892\n",
            "Train Epoch: 6 [110080/123872 (89%)]\tLoss: 0.450506\n",
            "Train Epoch: 6 [110336/123872 (89%)]\tLoss: 0.374820\tLR: 0.00036881\n",
            "Train Epoch: 6 [110592/123872 (89%)]\tLoss: 0.363465\tLR: 0.00036869\n",
            "Train Epoch: 6 [110848/123872 (89%)]\tLoss: 0.436389\tLR: 0.00036857\n",
            "Train Epoch: 6 [111104/123872 (90%)]\tLoss: 0.397744\tLR: 0.00036846\n",
            "Train Epoch: 6 [111360/123872 (90%)]\tLoss: 0.425732\tLR: 0.00036834\n",
            "Train Epoch: 6 [111616/123872 (90%)]\tLoss: 0.394459\tLR: 0.00036822\n",
            "Train Epoch: 6 [111872/123872 (90%)]\tLoss: 0.433153\tLR: 0.00036811\n",
            "Train Epoch: 6 [112128/123872 (90%)]\tLoss: 0.441595\tLR: 0.00036799\n",
            "Train Epoch: 6 [112384/123872 (91%)]\tLoss: 0.445534\tLR: 0.00036787\n",
            "Train Epoch: 6 [112640/123872 (91%)]\tLoss: 0.430711\tLR: 0.00036776\n",
            "Train Epoch: 6 [112640/123872 (91%)]\tLoss: 0.430711\n",
            "Train Epoch: 6 [112896/123872 (91%)]\tLoss: 0.415684\tLR: 0.00036764\n",
            "Train Epoch: 6 [113152/123872 (91%)]\tLoss: 0.356445\tLR: 0.00036752\n",
            "Train Epoch: 6 [113408/123872 (92%)]\tLoss: 0.463522\tLR: 0.00036741\n",
            "Train Epoch: 6 [113664/123872 (92%)]\tLoss: 0.393315\tLR: 0.00036729\n",
            "Train Epoch: 6 [113920/123872 (92%)]\tLoss: 0.414752\tLR: 0.00036717\n",
            "Train Epoch: 6 [114176/123872 (92%)]\tLoss: 0.455198\tLR: 0.00036705\n",
            "Train Epoch: 6 [114432/123872 (92%)]\tLoss: 0.442487\tLR: 0.00036694\n",
            "Train Epoch: 6 [114688/123872 (93%)]\tLoss: 0.367826\tLR: 0.00036682\n",
            "Train Epoch: 6 [114944/123872 (93%)]\tLoss: 0.452185\tLR: 0.00036670\n",
            "Train Epoch: 6 [115200/123872 (93%)]\tLoss: 0.396633\tLR: 0.00036659\n",
            "Train Epoch: 6 [115200/123872 (93%)]\tLoss: 0.396633\n",
            "Train Epoch: 6 [115456/123872 (93%)]\tLoss: 0.485257\tLR: 0.00036647\n",
            "Train Epoch: 6 [115712/123872 (93%)]\tLoss: 0.353441\tLR: 0.00036635\n",
            "Train Epoch: 6 [115968/123872 (94%)]\tLoss: 0.420136\tLR: 0.00036624\n",
            "Train Epoch: 6 [116224/123872 (94%)]\tLoss: 0.434105\tLR: 0.00036612\n",
            "Train Epoch: 6 [116480/123872 (94%)]\tLoss: 0.425460\tLR: 0.00036600\n",
            "Train Epoch: 6 [116736/123872 (94%)]\tLoss: 0.459643\tLR: 0.00036589\n",
            "Train Epoch: 6 [116992/123872 (94%)]\tLoss: 0.374568\tLR: 0.00036577\n",
            "Train Epoch: 6 [117248/123872 (95%)]\tLoss: 0.443284\tLR: 0.00036565\n",
            "Train Epoch: 6 [117504/123872 (95%)]\tLoss: 0.371452\tLR: 0.00036553\n",
            "Train Epoch: 6 [117760/123872 (95%)]\tLoss: 0.437411\tLR: 0.00036542\n",
            "Train Epoch: 6 [117760/123872 (95%)]\tLoss: 0.437411\n",
            "Train Epoch: 6 [118016/123872 (95%)]\tLoss: 0.406983\tLR: 0.00036530\n",
            "Train Epoch: 6 [118272/123872 (95%)]\tLoss: 0.396027\tLR: 0.00036518\n",
            "Train Epoch: 6 [118528/123872 (96%)]\tLoss: 0.386343\tLR: 0.00036507\n",
            "Train Epoch: 6 [118784/123872 (96%)]\tLoss: 0.383497\tLR: 0.00036495\n",
            "Train Epoch: 6 [119040/123872 (96%)]\tLoss: 0.438857\tLR: 0.00036483\n",
            "Train Epoch: 6 [119296/123872 (96%)]\tLoss: 0.407825\tLR: 0.00036472\n",
            "Train Epoch: 6 [119552/123872 (96%)]\tLoss: 0.355828\tLR: 0.00036460\n",
            "Train Epoch: 6 [119808/123872 (97%)]\tLoss: 0.402003\tLR: 0.00036448\n",
            "Train Epoch: 6 [120064/123872 (97%)]\tLoss: 0.349200\tLR: 0.00036436\n",
            "Train Epoch: 6 [120320/123872 (97%)]\tLoss: 0.391646\tLR: 0.00036425\n",
            "Train Epoch: 6 [120320/123872 (97%)]\tLoss: 0.391646\n",
            "Train Epoch: 6 [120576/123872 (97%)]\tLoss: 0.410937\tLR: 0.00036413\n",
            "Train Epoch: 6 [120832/123872 (98%)]\tLoss: 0.394722\tLR: 0.00036401\n",
            "Train Epoch: 6 [121088/123872 (98%)]\tLoss: 0.386904\tLR: 0.00036390\n",
            "Train Epoch: 6 [121344/123872 (98%)]\tLoss: 0.444975\tLR: 0.00036378\n",
            "Train Epoch: 6 [121600/123872 (98%)]\tLoss: 0.449219\tLR: 0.00036366\n",
            "Train Epoch: 6 [121856/123872 (98%)]\tLoss: 0.476444\tLR: 0.00036354\n",
            "Train Epoch: 6 [122112/123872 (99%)]\tLoss: 0.414490\tLR: 0.00036343\n",
            "Train Epoch: 6 [122368/123872 (99%)]\tLoss: 0.391432\tLR: 0.00036331\n",
            "Train Epoch: 6 [122624/123872 (99%)]\tLoss: 0.419128\tLR: 0.00036319\n",
            "Train Epoch: 6 [122880/123872 (99%)]\tLoss: 0.405597\tLR: 0.00036308\n",
            "Train Epoch: 6 [122880/123872 (99%)]\tLoss: 0.405597\n",
            "Train Epoch: 6 [123136/123872 (99%)]\tLoss: 0.409598\tLR: 0.00036296\n",
            "Train Epoch: 6 [123392/123872 (100%)]\tLoss: 0.375220\tLR: 0.00036284\n",
            "Train Epoch: 6 [108192/123872 (100%)]\tLoss: 0.421137\tLR: 0.00036273\n",
            "\n",
            "Test set: Average loss: 0.0016, Accuracy: 24925/30970 (80.48%)\n",
            "\n",
            "Train Epoch: 7 [0/123872 (0%)]\tLoss: 0.464967\tLR: 0.00036261\n",
            "Train Epoch: 7 [0/123872 (0%)]\tLoss: 0.464967\n",
            "Train Epoch: 7 [256/123872 (0%)]\tLoss: 0.413154\tLR: 0.00036249\n",
            "Train Epoch: 7 [512/123872 (0%)]\tLoss: 0.332223\tLR: 0.00036237\n",
            "Train Epoch: 7 [768/123872 (1%)]\tLoss: 0.469258\tLR: 0.00036226\n",
            "Train Epoch: 7 [1024/123872 (1%)]\tLoss: 0.442182\tLR: 0.00036214\n",
            "Train Epoch: 7 [1280/123872 (1%)]\tLoss: 0.384413\tLR: 0.00036202\n",
            "Train Epoch: 7 [1536/123872 (1%)]\tLoss: 0.428299\tLR: 0.00036191\n",
            "Train Epoch: 7 [1792/123872 (1%)]\tLoss: 0.372526\tLR: 0.00036179\n",
            "Train Epoch: 7 [2048/123872 (2%)]\tLoss: 0.413162\tLR: 0.00036167\n",
            "Train Epoch: 7 [2304/123872 (2%)]\tLoss: 0.398592\tLR: 0.00036155\n",
            "Train Epoch: 7 [2560/123872 (2%)]\tLoss: 0.445902\tLR: 0.00036144\n",
            "Train Epoch: 7 [2560/123872 (2%)]\tLoss: 0.445902\n",
            "Train Epoch: 7 [2816/123872 (2%)]\tLoss: 0.390177\tLR: 0.00036132\n",
            "Train Epoch: 7 [3072/123872 (2%)]\tLoss: 0.386739\tLR: 0.00036120\n",
            "Train Epoch: 7 [3328/123872 (3%)]\tLoss: 0.399513\tLR: 0.00036108\n",
            "Train Epoch: 7 [3584/123872 (3%)]\tLoss: 0.364275\tLR: 0.00036097\n",
            "Train Epoch: 7 [3840/123872 (3%)]\tLoss: 0.438151\tLR: 0.00036085\n",
            "Train Epoch: 7 [4096/123872 (3%)]\tLoss: 0.420525\tLR: 0.00036073\n",
            "Train Epoch: 7 [4352/123872 (4%)]\tLoss: 0.426186\tLR: 0.00036062\n",
            "Train Epoch: 7 [4608/123872 (4%)]\tLoss: 0.359429\tLR: 0.00036050\n",
            "Train Epoch: 7 [4864/123872 (4%)]\tLoss: 0.463914\tLR: 0.00036038\n",
            "Train Epoch: 7 [5120/123872 (4%)]\tLoss: 0.459885\tLR: 0.00036026\n",
            "Train Epoch: 7 [5120/123872 (4%)]\tLoss: 0.459885\n",
            "Train Epoch: 7 [5376/123872 (4%)]\tLoss: 0.463498\tLR: 0.00036015\n",
            "Train Epoch: 7 [5632/123872 (5%)]\tLoss: 0.452912\tLR: 0.00036003\n",
            "Train Epoch: 7 [5888/123872 (5%)]\tLoss: 0.397039\tLR: 0.00035991\n",
            "Train Epoch: 7 [6144/123872 (5%)]\tLoss: 0.458672\tLR: 0.00035979\n",
            "Train Epoch: 7 [6400/123872 (5%)]\tLoss: 0.436719\tLR: 0.00035968\n",
            "Train Epoch: 7 [6656/123872 (5%)]\tLoss: 0.404582\tLR: 0.00035956\n",
            "Train Epoch: 7 [6912/123872 (6%)]\tLoss: 0.381233\tLR: 0.00035944\n",
            "Train Epoch: 7 [7168/123872 (6%)]\tLoss: 0.368869\tLR: 0.00035933\n",
            "Train Epoch: 7 [7424/123872 (6%)]\tLoss: 0.421247\tLR: 0.00035921\n",
            "Train Epoch: 7 [7680/123872 (6%)]\tLoss: 0.433453\tLR: 0.00035909\n",
            "Train Epoch: 7 [7680/123872 (6%)]\tLoss: 0.433453\n",
            "Train Epoch: 7 [7936/123872 (6%)]\tLoss: 0.388241\tLR: 0.00035897\n",
            "Train Epoch: 7 [8192/123872 (7%)]\tLoss: 0.429190\tLR: 0.00035886\n",
            "Train Epoch: 7 [8448/123872 (7%)]\tLoss: 0.446001\tLR: 0.00035874\n",
            "Train Epoch: 7 [8704/123872 (7%)]\tLoss: 0.436662\tLR: 0.00035862\n",
            "Train Epoch: 7 [8960/123872 (7%)]\tLoss: 0.417875\tLR: 0.00035850\n",
            "Train Epoch: 7 [9216/123872 (7%)]\tLoss: 0.346057\tLR: 0.00035839\n",
            "Train Epoch: 7 [9472/123872 (8%)]\tLoss: 0.371758\tLR: 0.00035827\n",
            "Train Epoch: 7 [9728/123872 (8%)]\tLoss: 0.398435\tLR: 0.00035815\n",
            "Train Epoch: 7 [9984/123872 (8%)]\tLoss: 0.426059\tLR: 0.00035803\n",
            "Train Epoch: 7 [10240/123872 (8%)]\tLoss: 0.363709\tLR: 0.00035792\n",
            "Train Epoch: 7 [10240/123872 (8%)]\tLoss: 0.363709\n",
            "Train Epoch: 7 [10496/123872 (8%)]\tLoss: 0.437845\tLR: 0.00035780\n",
            "Train Epoch: 7 [10752/123872 (9%)]\tLoss: 0.425206\tLR: 0.00035768\n",
            "Train Epoch: 7 [11008/123872 (9%)]\tLoss: 0.434991\tLR: 0.00035757\n",
            "Train Epoch: 7 [11264/123872 (9%)]\tLoss: 0.424853\tLR: 0.00035745\n",
            "Train Epoch: 7 [11520/123872 (9%)]\tLoss: 0.399629\tLR: 0.00035733\n",
            "Train Epoch: 7 [11776/123872 (10%)]\tLoss: 0.488549\tLR: 0.00035721\n",
            "Train Epoch: 7 [12032/123872 (10%)]\tLoss: 0.471197\tLR: 0.00035710\n",
            "Train Epoch: 7 [12288/123872 (10%)]\tLoss: 0.366782\tLR: 0.00035698\n",
            "Train Epoch: 7 [12544/123872 (10%)]\tLoss: 0.457239\tLR: 0.00035686\n",
            "Train Epoch: 7 [12800/123872 (10%)]\tLoss: 0.373063\tLR: 0.00035674\n",
            "Train Epoch: 7 [12800/123872 (10%)]\tLoss: 0.373063\n",
            "Train Epoch: 7 [13056/123872 (11%)]\tLoss: 0.434552\tLR: 0.00035663\n",
            "Train Epoch: 7 [13312/123872 (11%)]\tLoss: 0.421545\tLR: 0.00035651\n",
            "Train Epoch: 7 [13568/123872 (11%)]\tLoss: 0.421749\tLR: 0.00035639\n",
            "Train Epoch: 7 [13824/123872 (11%)]\tLoss: 0.415113\tLR: 0.00035627\n",
            "Train Epoch: 7 [14080/123872 (11%)]\tLoss: 0.384080\tLR: 0.00035616\n",
            "Train Epoch: 7 [14336/123872 (12%)]\tLoss: 0.448824\tLR: 0.00035604\n",
            "Train Epoch: 7 [14592/123872 (12%)]\tLoss: 0.383108\tLR: 0.00035592\n",
            "Train Epoch: 7 [14848/123872 (12%)]\tLoss: 0.399929\tLR: 0.00035580\n",
            "Train Epoch: 7 [15104/123872 (12%)]\tLoss: 0.460191\tLR: 0.00035569\n",
            "Train Epoch: 7 [15360/123872 (12%)]\tLoss: 0.385319\tLR: 0.00035557\n",
            "Train Epoch: 7 [15360/123872 (12%)]\tLoss: 0.385319\n",
            "Train Epoch: 7 [15616/123872 (13%)]\tLoss: 0.333078\tLR: 0.00035545\n",
            "Train Epoch: 7 [15872/123872 (13%)]\tLoss: 0.369776\tLR: 0.00035533\n",
            "Train Epoch: 7 [16128/123872 (13%)]\tLoss: 0.418709\tLR: 0.00035522\n",
            "Train Epoch: 7 [16384/123872 (13%)]\tLoss: 0.422728\tLR: 0.00035510\n",
            "Train Epoch: 7 [16640/123872 (13%)]\tLoss: 0.372540\tLR: 0.00035498\n",
            "Train Epoch: 7 [16896/123872 (14%)]\tLoss: 0.384483\tLR: 0.00035486\n",
            "Train Epoch: 7 [17152/123872 (14%)]\tLoss: 0.425554\tLR: 0.00035475\n",
            "Train Epoch: 7 [17408/123872 (14%)]\tLoss: 0.355787\tLR: 0.00035463\n",
            "Train Epoch: 7 [17664/123872 (14%)]\tLoss: 0.433743\tLR: 0.00035451\n",
            "Train Epoch: 7 [17920/123872 (14%)]\tLoss: 0.424123\tLR: 0.00035439\n",
            "Train Epoch: 7 [17920/123872 (14%)]\tLoss: 0.424123\n",
            "Train Epoch: 7 [18176/123872 (15%)]\tLoss: 0.421219\tLR: 0.00035428\n",
            "Train Epoch: 7 [18432/123872 (15%)]\tLoss: 0.414997\tLR: 0.00035416\n",
            "Train Epoch: 7 [18688/123872 (15%)]\tLoss: 0.393817\tLR: 0.00035404\n",
            "Train Epoch: 7 [18944/123872 (15%)]\tLoss: 0.386702\tLR: 0.00035392\n",
            "Train Epoch: 7 [19200/123872 (15%)]\tLoss: 0.420997\tLR: 0.00035381\n",
            "Train Epoch: 7 [19456/123872 (16%)]\tLoss: 0.380793\tLR: 0.00035369\n",
            "Train Epoch: 7 [19712/123872 (16%)]\tLoss: 0.406074\tLR: 0.00035357\n",
            "Train Epoch: 7 [19968/123872 (16%)]\tLoss: 0.389447\tLR: 0.00035345\n",
            "Train Epoch: 7 [20224/123872 (16%)]\tLoss: 0.371068\tLR: 0.00035334\n",
            "Train Epoch: 7 [20480/123872 (17%)]\tLoss: 0.390226\tLR: 0.00035322\n",
            "Train Epoch: 7 [20480/123872 (17%)]\tLoss: 0.390226\n",
            "Train Epoch: 7 [20736/123872 (17%)]\tLoss: 0.419477\tLR: 0.00035310\n",
            "Train Epoch: 7 [20992/123872 (17%)]\tLoss: 0.405745\tLR: 0.00035298\n",
            "Train Epoch: 7 [21248/123872 (17%)]\tLoss: 0.442122\tLR: 0.00035287\n",
            "Train Epoch: 7 [21504/123872 (17%)]\tLoss: 0.484820\tLR: 0.00035275\n",
            "Train Epoch: 7 [21760/123872 (18%)]\tLoss: 0.360839\tLR: 0.00035263\n",
            "Train Epoch: 7 [22016/123872 (18%)]\tLoss: 0.424278\tLR: 0.00035251\n",
            "Train Epoch: 7 [22272/123872 (18%)]\tLoss: 0.434215\tLR: 0.00035240\n",
            "Train Epoch: 7 [22528/123872 (18%)]\tLoss: 0.372673\tLR: 0.00035228\n",
            "Train Epoch: 7 [22784/123872 (18%)]\tLoss: 0.383004\tLR: 0.00035216\n",
            "Train Epoch: 7 [23040/123872 (19%)]\tLoss: 0.367837\tLR: 0.00035204\n",
            "Train Epoch: 7 [23040/123872 (19%)]\tLoss: 0.367837\n",
            "Train Epoch: 7 [23296/123872 (19%)]\tLoss: 0.440010\tLR: 0.00035193\n",
            "Train Epoch: 7 [23552/123872 (19%)]\tLoss: 0.456326\tLR: 0.00035181\n",
            "Train Epoch: 7 [23808/123872 (19%)]\tLoss: 0.401782\tLR: 0.00035169\n",
            "Train Epoch: 7 [24064/123872 (19%)]\tLoss: 0.383493\tLR: 0.00035157\n",
            "Train Epoch: 7 [24320/123872 (20%)]\tLoss: 0.413359\tLR: 0.00035145\n",
            "Train Epoch: 7 [24576/123872 (20%)]\tLoss: 0.458353\tLR: 0.00035134\n",
            "Train Epoch: 7 [24832/123872 (20%)]\tLoss: 0.410238\tLR: 0.00035122\n",
            "Train Epoch: 7 [25088/123872 (20%)]\tLoss: 0.382601\tLR: 0.00035110\n",
            "Train Epoch: 7 [25344/123872 (20%)]\tLoss: 0.440846\tLR: 0.00035098\n",
            "Train Epoch: 7 [25600/123872 (21%)]\tLoss: 0.416542\tLR: 0.00035087\n",
            "Train Epoch: 7 [25600/123872 (21%)]\tLoss: 0.416542\n",
            "Train Epoch: 7 [25856/123872 (21%)]\tLoss: 0.457510\tLR: 0.00035075\n",
            "Train Epoch: 7 [26112/123872 (21%)]\tLoss: 0.358978\tLR: 0.00035063\n",
            "Train Epoch: 7 [26368/123872 (21%)]\tLoss: 0.402782\tLR: 0.00035051\n",
            "Train Epoch: 7 [26624/123872 (21%)]\tLoss: 0.430519\tLR: 0.00035040\n",
            "Train Epoch: 7 [26880/123872 (22%)]\tLoss: 0.391272\tLR: 0.00035028\n",
            "Train Epoch: 7 [27136/123872 (22%)]\tLoss: 0.457124\tLR: 0.00035016\n",
            "Train Epoch: 7 [27392/123872 (22%)]\tLoss: 0.441884\tLR: 0.00035004\n",
            "Train Epoch: 7 [27648/123872 (22%)]\tLoss: 0.425617\tLR: 0.00034992\n",
            "Train Epoch: 7 [27904/123872 (23%)]\tLoss: 0.402390\tLR: 0.00034981\n",
            "Train Epoch: 7 [28160/123872 (23%)]\tLoss: 0.408014\tLR: 0.00034969\n",
            "Train Epoch: 7 [28160/123872 (23%)]\tLoss: 0.408014\n",
            "Train Epoch: 7 [28416/123872 (23%)]\tLoss: 0.384884\tLR: 0.00034957\n",
            "Train Epoch: 7 [28672/123872 (23%)]\tLoss: 0.450769\tLR: 0.00034945\n",
            "Train Epoch: 7 [28928/123872 (23%)]\tLoss: 0.413146\tLR: 0.00034934\n",
            "Train Epoch: 7 [29184/123872 (24%)]\tLoss: 0.391708\tLR: 0.00034922\n",
            "Train Epoch: 7 [29440/123872 (24%)]\tLoss: 0.438443\tLR: 0.00034910\n",
            "Train Epoch: 7 [29696/123872 (24%)]\tLoss: 0.389155\tLR: 0.00034898\n",
            "Train Epoch: 7 [29952/123872 (24%)]\tLoss: 0.423149\tLR: 0.00034887\n",
            "Train Epoch: 7 [30208/123872 (24%)]\tLoss: 0.370998\tLR: 0.00034875\n",
            "Train Epoch: 7 [30464/123872 (25%)]\tLoss: 0.402287\tLR: 0.00034863\n",
            "Train Epoch: 7 [30720/123872 (25%)]\tLoss: 0.324286\tLR: 0.00034851\n",
            "Train Epoch: 7 [30720/123872 (25%)]\tLoss: 0.324286\n",
            "Train Epoch: 7 [30976/123872 (25%)]\tLoss: 0.364058\tLR: 0.00034839\n",
            "Train Epoch: 7 [31232/123872 (25%)]\tLoss: 0.430833\tLR: 0.00034828\n",
            "Train Epoch: 7 [31488/123872 (25%)]\tLoss: 0.390086\tLR: 0.00034816\n",
            "Train Epoch: 7 [31744/123872 (26%)]\tLoss: 0.417098\tLR: 0.00034804\n",
            "Train Epoch: 7 [32000/123872 (26%)]\tLoss: 0.457786\tLR: 0.00034792\n",
            "Train Epoch: 7 [32256/123872 (26%)]\tLoss: 0.397887\tLR: 0.00034781\n",
            "Train Epoch: 7 [32512/123872 (26%)]\tLoss: 0.392737\tLR: 0.00034769\n",
            "Train Epoch: 7 [32768/123872 (26%)]\tLoss: 0.406368\tLR: 0.00034757\n",
            "Train Epoch: 7 [33024/123872 (27%)]\tLoss: 0.425733\tLR: 0.00034745\n",
            "Train Epoch: 7 [33280/123872 (27%)]\tLoss: 0.446183\tLR: 0.00034734\n",
            "Train Epoch: 7 [33280/123872 (27%)]\tLoss: 0.446183\n",
            "Train Epoch: 7 [33536/123872 (27%)]\tLoss: 0.408275\tLR: 0.00034722\n",
            "Train Epoch: 7 [33792/123872 (27%)]\tLoss: 0.400114\tLR: 0.00034710\n",
            "Train Epoch: 7 [34048/123872 (27%)]\tLoss: 0.356913\tLR: 0.00034698\n",
            "Train Epoch: 7 [34304/123872 (28%)]\tLoss: 0.358546\tLR: 0.00034686\n",
            "Train Epoch: 7 [34560/123872 (28%)]\tLoss: 0.403117\tLR: 0.00034675\n",
            "Train Epoch: 7 [34816/123872 (28%)]\tLoss: 0.414155\tLR: 0.00034663\n",
            "Train Epoch: 7 [35072/123872 (28%)]\tLoss: 0.413917\tLR: 0.00034651\n",
            "Train Epoch: 7 [35328/123872 (29%)]\tLoss: 0.376306\tLR: 0.00034639\n",
            "Train Epoch: 7 [35584/123872 (29%)]\tLoss: 0.383418\tLR: 0.00034628\n",
            "Train Epoch: 7 [35840/123872 (29%)]\tLoss: 0.362674\tLR: 0.00034616\n",
            "Train Epoch: 7 [35840/123872 (29%)]\tLoss: 0.362674\n",
            "Train Epoch: 7 [36096/123872 (29%)]\tLoss: 0.405987\tLR: 0.00034604\n",
            "Train Epoch: 7 [36352/123872 (29%)]\tLoss: 0.386350\tLR: 0.00034592\n",
            "Train Epoch: 7 [36608/123872 (30%)]\tLoss: 0.412421\tLR: 0.00034580\n",
            "Train Epoch: 7 [36864/123872 (30%)]\tLoss: 0.393657\tLR: 0.00034569\n",
            "Train Epoch: 7 [37120/123872 (30%)]\tLoss: 0.339526\tLR: 0.00034557\n",
            "Train Epoch: 7 [37376/123872 (30%)]\tLoss: 0.388045\tLR: 0.00034545\n",
            "Train Epoch: 7 [37632/123872 (30%)]\tLoss: 0.415183\tLR: 0.00034533\n",
            "Train Epoch: 7 [37888/123872 (31%)]\tLoss: 0.435774\tLR: 0.00034521\n",
            "Train Epoch: 7 [38144/123872 (31%)]\tLoss: 0.450955\tLR: 0.00034510\n",
            "Train Epoch: 7 [38400/123872 (31%)]\tLoss: 0.419999\tLR: 0.00034498\n",
            "Train Epoch: 7 [38400/123872 (31%)]\tLoss: 0.419999\n",
            "Train Epoch: 7 [38656/123872 (31%)]\tLoss: 0.411013\tLR: 0.00034486\n",
            "Train Epoch: 7 [38912/123872 (31%)]\tLoss: 0.410791\tLR: 0.00034474\n",
            "Train Epoch: 7 [39168/123872 (32%)]\tLoss: 0.399355\tLR: 0.00034463\n",
            "Train Epoch: 7 [39424/123872 (32%)]\tLoss: 0.431115\tLR: 0.00034451\n",
            "Train Epoch: 7 [39680/123872 (32%)]\tLoss: 0.385567\tLR: 0.00034439\n",
            "Train Epoch: 7 [39936/123872 (32%)]\tLoss: 0.383986\tLR: 0.00034427\n",
            "Train Epoch: 7 [40192/123872 (32%)]\tLoss: 0.378895\tLR: 0.00034415\n",
            "Train Epoch: 7 [40448/123872 (33%)]\tLoss: 0.376961\tLR: 0.00034404\n",
            "Train Epoch: 7 [40704/123872 (33%)]\tLoss: 0.395507\tLR: 0.00034392\n",
            "Train Epoch: 7 [40960/123872 (33%)]\tLoss: 0.388912\tLR: 0.00034380\n",
            "Train Epoch: 7 [40960/123872 (33%)]\tLoss: 0.388912\n",
            "Train Epoch: 7 [41216/123872 (33%)]\tLoss: 0.400657\tLR: 0.00034368\n",
            "Train Epoch: 7 [41472/123872 (33%)]\tLoss: 0.455877\tLR: 0.00034357\n",
            "Train Epoch: 7 [41728/123872 (34%)]\tLoss: 0.415895\tLR: 0.00034345\n",
            "Train Epoch: 7 [41984/123872 (34%)]\tLoss: 0.422503\tLR: 0.00034333\n",
            "Train Epoch: 7 [42240/123872 (34%)]\tLoss: 0.401043\tLR: 0.00034321\n",
            "Train Epoch: 7 [42496/123872 (34%)]\tLoss: 0.408798\tLR: 0.00034309\n",
            "Train Epoch: 7 [42752/123872 (35%)]\tLoss: 0.363750\tLR: 0.00034298\n",
            "Train Epoch: 7 [43008/123872 (35%)]\tLoss: 0.394591\tLR: 0.00034286\n",
            "Train Epoch: 7 [43264/123872 (35%)]\tLoss: 0.486845\tLR: 0.00034274\n",
            "Train Epoch: 7 [43520/123872 (35%)]\tLoss: 0.429673\tLR: 0.00034262\n",
            "Train Epoch: 7 [43520/123872 (35%)]\tLoss: 0.429673\n",
            "Train Epoch: 7 [43776/123872 (35%)]\tLoss: 0.420127\tLR: 0.00034250\n",
            "Train Epoch: 7 [44032/123872 (36%)]\tLoss: 0.391816\tLR: 0.00034239\n",
            "Train Epoch: 7 [44288/123872 (36%)]\tLoss: 0.412555\tLR: 0.00034227\n",
            "Train Epoch: 7 [44544/123872 (36%)]\tLoss: 0.416955\tLR: 0.00034215\n",
            "Train Epoch: 7 [44800/123872 (36%)]\tLoss: 0.376124\tLR: 0.00034203\n",
            "Train Epoch: 7 [45056/123872 (36%)]\tLoss: 0.408099\tLR: 0.00034191\n",
            "Train Epoch: 7 [45312/123872 (37%)]\tLoss: 0.431145\tLR: 0.00034180\n",
            "Train Epoch: 7 [45568/123872 (37%)]\tLoss: 0.425537\tLR: 0.00034168\n",
            "Train Epoch: 7 [45824/123872 (37%)]\tLoss: 0.398110\tLR: 0.00034156\n",
            "Train Epoch: 7 [46080/123872 (37%)]\tLoss: 0.425618\tLR: 0.00034144\n",
            "Train Epoch: 7 [46080/123872 (37%)]\tLoss: 0.425618\n",
            "Train Epoch: 7 [46336/123872 (37%)]\tLoss: 0.392676\tLR: 0.00034133\n",
            "Train Epoch: 7 [46592/123872 (38%)]\tLoss: 0.309030\tLR: 0.00034121\n",
            "Train Epoch: 7 [46848/123872 (38%)]\tLoss: 0.333655\tLR: 0.00034109\n",
            "Train Epoch: 7 [47104/123872 (38%)]\tLoss: 0.391584\tLR: 0.00034097\n",
            "Train Epoch: 7 [47360/123872 (38%)]\tLoss: 0.430522\tLR: 0.00034085\n",
            "Train Epoch: 7 [47616/123872 (38%)]\tLoss: 0.407478\tLR: 0.00034074\n",
            "Train Epoch: 7 [47872/123872 (39%)]\tLoss: 0.372126\tLR: 0.00034062\n",
            "Train Epoch: 7 [48128/123872 (39%)]\tLoss: 0.413236\tLR: 0.00034050\n",
            "Train Epoch: 7 [48384/123872 (39%)]\tLoss: 0.389730\tLR: 0.00034038\n",
            "Train Epoch: 7 [48640/123872 (39%)]\tLoss: 0.414020\tLR: 0.00034026\n",
            "Train Epoch: 7 [48640/123872 (39%)]\tLoss: 0.414020\n",
            "Train Epoch: 7 [48896/123872 (39%)]\tLoss: 0.445493\tLR: 0.00034015\n",
            "Train Epoch: 7 [49152/123872 (40%)]\tLoss: 0.395947\tLR: 0.00034003\n",
            "Train Epoch: 7 [49408/123872 (40%)]\tLoss: 0.412873\tLR: 0.00033991\n",
            "Train Epoch: 7 [49664/123872 (40%)]\tLoss: 0.433424\tLR: 0.00033979\n",
            "Train Epoch: 7 [49920/123872 (40%)]\tLoss: 0.353292\tLR: 0.00033967\n",
            "Train Epoch: 7 [50176/123872 (40%)]\tLoss: 0.406501\tLR: 0.00033956\n",
            "Train Epoch: 7 [50432/123872 (41%)]\tLoss: 0.428046\tLR: 0.00033944\n",
            "Train Epoch: 7 [50688/123872 (41%)]\tLoss: 0.374392\tLR: 0.00033932\n",
            "Train Epoch: 7 [50944/123872 (41%)]\tLoss: 0.401942\tLR: 0.00033920\n",
            "Train Epoch: 7 [51200/123872 (41%)]\tLoss: 0.374953\tLR: 0.00033908\n",
            "Train Epoch: 7 [51200/123872 (41%)]\tLoss: 0.374953\n",
            "Train Epoch: 7 [51456/123872 (42%)]\tLoss: 0.333320\tLR: 0.00033897\n",
            "Train Epoch: 7 [51712/123872 (42%)]\tLoss: 0.362795\tLR: 0.00033885\n",
            "Train Epoch: 7 [51968/123872 (42%)]\tLoss: 0.352453\tLR: 0.00033873\n",
            "Train Epoch: 7 [52224/123872 (42%)]\tLoss: 0.396907\tLR: 0.00033861\n",
            "Train Epoch: 7 [52480/123872 (42%)]\tLoss: 0.371215\tLR: 0.00033850\n",
            "Train Epoch: 7 [52736/123872 (43%)]\tLoss: 0.466063\tLR: 0.00033838\n",
            "Train Epoch: 7 [52992/123872 (43%)]\tLoss: 0.408169\tLR: 0.00033826\n",
            "Train Epoch: 7 [53248/123872 (43%)]\tLoss: 0.390386\tLR: 0.00033814\n",
            "Train Epoch: 7 [53504/123872 (43%)]\tLoss: 0.427291\tLR: 0.00033802\n",
            "Train Epoch: 7 [53760/123872 (43%)]\tLoss: 0.378628\tLR: 0.00033791\n",
            "Train Epoch: 7 [53760/123872 (43%)]\tLoss: 0.378628\n",
            "Train Epoch: 7 [54016/123872 (44%)]\tLoss: 0.454085\tLR: 0.00033779\n",
            "Train Epoch: 7 [54272/123872 (44%)]\tLoss: 0.444922\tLR: 0.00033767\n",
            "Train Epoch: 7 [54528/123872 (44%)]\tLoss: 0.413330\tLR: 0.00033755\n",
            "Train Epoch: 7 [54784/123872 (44%)]\tLoss: 0.370009\tLR: 0.00033743\n",
            "Train Epoch: 7 [55040/123872 (44%)]\tLoss: 0.381637\tLR: 0.00033732\n",
            "Train Epoch: 7 [55296/123872 (45%)]\tLoss: 0.407481\tLR: 0.00033720\n",
            "Train Epoch: 7 [55552/123872 (45%)]\tLoss: 0.363345\tLR: 0.00033708\n",
            "Train Epoch: 7 [55808/123872 (45%)]\tLoss: 0.378678\tLR: 0.00033696\n",
            "Train Epoch: 7 [56064/123872 (45%)]\tLoss: 0.356190\tLR: 0.00033684\n",
            "Train Epoch: 7 [56320/123872 (45%)]\tLoss: 0.389311\tLR: 0.00033673\n",
            "Train Epoch: 7 [56320/123872 (45%)]\tLoss: 0.389311\n",
            "Train Epoch: 7 [56576/123872 (46%)]\tLoss: 0.435771\tLR: 0.00033661\n",
            "Train Epoch: 7 [56832/123872 (46%)]\tLoss: 0.379158\tLR: 0.00033649\n",
            "Train Epoch: 7 [57088/123872 (46%)]\tLoss: 0.371844\tLR: 0.00033637\n",
            "Train Epoch: 7 [57344/123872 (46%)]\tLoss: 0.396900\tLR: 0.00033625\n",
            "Train Epoch: 7 [57600/123872 (46%)]\tLoss: 0.407949\tLR: 0.00033614\n",
            "Train Epoch: 7 [57856/123872 (47%)]\tLoss: 0.396554\tLR: 0.00033602\n",
            "Train Epoch: 7 [58112/123872 (47%)]\tLoss: 0.467915\tLR: 0.00033590\n",
            "Train Epoch: 7 [58368/123872 (47%)]\tLoss: 0.389659\tLR: 0.00033578\n",
            "Train Epoch: 7 [58624/123872 (47%)]\tLoss: 0.401282\tLR: 0.00033566\n",
            "Train Epoch: 7 [58880/123872 (48%)]\tLoss: 0.442664\tLR: 0.00033555\n",
            "Train Epoch: 7 [58880/123872 (48%)]\tLoss: 0.442664\n",
            "Train Epoch: 7 [59136/123872 (48%)]\tLoss: 0.424550\tLR: 0.00033543\n",
            "Train Epoch: 7 [59392/123872 (48%)]\tLoss: 0.416809\tLR: 0.00033531\n",
            "Train Epoch: 7 [59648/123872 (48%)]\tLoss: 0.420250\tLR: 0.00033519\n",
            "Train Epoch: 7 [59904/123872 (48%)]\tLoss: 0.381673\tLR: 0.00033507\n",
            "Train Epoch: 7 [60160/123872 (49%)]\tLoss: 0.465605\tLR: 0.00033496\n",
            "Train Epoch: 7 [60416/123872 (49%)]\tLoss: 0.401077\tLR: 0.00033484\n",
            "Train Epoch: 7 [60672/123872 (49%)]\tLoss: 0.450551\tLR: 0.00033472\n",
            "Train Epoch: 7 [60928/123872 (49%)]\tLoss: 0.367109\tLR: 0.00033460\n",
            "Train Epoch: 7 [61184/123872 (49%)]\tLoss: 0.370857\tLR: 0.00033448\n",
            "Train Epoch: 7 [61440/123872 (50%)]\tLoss: 0.400301\tLR: 0.00033437\n",
            "Train Epoch: 7 [61440/123872 (50%)]\tLoss: 0.400301\n",
            "Train Epoch: 7 [61696/123872 (50%)]\tLoss: 0.423430\tLR: 0.00033425\n",
            "Train Epoch: 7 [61952/123872 (50%)]\tLoss: 0.419511\tLR: 0.00033413\n",
            "Train Epoch: 7 [62208/123872 (50%)]\tLoss: 0.416306\tLR: 0.00033401\n",
            "Train Epoch: 7 [62464/123872 (50%)]\tLoss: 0.390807\tLR: 0.00033389\n",
            "Train Epoch: 7 [62720/123872 (51%)]\tLoss: 0.412961\tLR: 0.00033378\n",
            "Train Epoch: 7 [62976/123872 (51%)]\tLoss: 0.409053\tLR: 0.00033366\n",
            "Train Epoch: 7 [63232/123872 (51%)]\tLoss: 0.372669\tLR: 0.00033354\n",
            "Train Epoch: 7 [63488/123872 (51%)]\tLoss: 0.456035\tLR: 0.00033342\n",
            "Train Epoch: 7 [63744/123872 (51%)]\tLoss: 0.376506\tLR: 0.00033330\n",
            "Train Epoch: 7 [64000/123872 (52%)]\tLoss: 0.381606\tLR: 0.00033319\n",
            "Train Epoch: 7 [64000/123872 (52%)]\tLoss: 0.381606\n",
            "Train Epoch: 7 [64256/123872 (52%)]\tLoss: 0.378170\tLR: 0.00033307\n",
            "Train Epoch: 7 [64512/123872 (52%)]\tLoss: 0.394737\tLR: 0.00033295\n",
            "Train Epoch: 7 [64768/123872 (52%)]\tLoss: 0.387302\tLR: 0.00033283\n",
            "Train Epoch: 7 [65024/123872 (52%)]\tLoss: 0.434031\tLR: 0.00033271\n",
            "Train Epoch: 7 [65280/123872 (53%)]\tLoss: 0.409830\tLR: 0.00033260\n",
            "Train Epoch: 7 [65536/123872 (53%)]\tLoss: 0.414782\tLR: 0.00033248\n",
            "Train Epoch: 7 [65792/123872 (53%)]\tLoss: 0.375505\tLR: 0.00033236\n",
            "Train Epoch: 7 [66048/123872 (53%)]\tLoss: 0.364080\tLR: 0.00033224\n",
            "Train Epoch: 7 [66304/123872 (54%)]\tLoss: 0.419114\tLR: 0.00033212\n",
            "Train Epoch: 7 [66560/123872 (54%)]\tLoss: 0.456223\tLR: 0.00033201\n",
            "Train Epoch: 7 [66560/123872 (54%)]\tLoss: 0.456223\n",
            "Train Epoch: 7 [66816/123872 (54%)]\tLoss: 0.383839\tLR: 0.00033189\n",
            "Train Epoch: 7 [67072/123872 (54%)]\tLoss: 0.427233\tLR: 0.00033177\n",
            "Train Epoch: 7 [67328/123872 (54%)]\tLoss: 0.394780\tLR: 0.00033165\n",
            "Train Epoch: 7 [67584/123872 (55%)]\tLoss: 0.381286\tLR: 0.00033153\n",
            "Train Epoch: 7 [67840/123872 (55%)]\tLoss: 0.453143\tLR: 0.00033142\n",
            "Train Epoch: 7 [68096/123872 (55%)]\tLoss: 0.361534\tLR: 0.00033130\n",
            "Train Epoch: 7 [68352/123872 (55%)]\tLoss: 0.387503\tLR: 0.00033118\n",
            "Train Epoch: 7 [68608/123872 (55%)]\tLoss: 0.416869\tLR: 0.00033106\n",
            "Train Epoch: 7 [68864/123872 (56%)]\tLoss: 0.396059\tLR: 0.00033094\n",
            "Train Epoch: 7 [69120/123872 (56%)]\tLoss: 0.359418\tLR: 0.00033083\n",
            "Train Epoch: 7 [69120/123872 (56%)]\tLoss: 0.359418\n",
            "Train Epoch: 7 [69376/123872 (56%)]\tLoss: 0.452807\tLR: 0.00033071\n",
            "Train Epoch: 7 [69632/123872 (56%)]\tLoss: 0.359012\tLR: 0.00033059\n",
            "Train Epoch: 7 [69888/123872 (56%)]\tLoss: 0.364709\tLR: 0.00033047\n",
            "Train Epoch: 7 [70144/123872 (57%)]\tLoss: 0.514064\tLR: 0.00033035\n",
            "Train Epoch: 7 [70400/123872 (57%)]\tLoss: 0.391885\tLR: 0.00033024\n",
            "Train Epoch: 7 [70656/123872 (57%)]\tLoss: 0.368310\tLR: 0.00033012\n",
            "Train Epoch: 7 [70912/123872 (57%)]\tLoss: 0.405383\tLR: 0.00033000\n",
            "Train Epoch: 7 [71168/123872 (57%)]\tLoss: 0.376139\tLR: 0.00032988\n",
            "Train Epoch: 7 [71424/123872 (58%)]\tLoss: 0.415958\tLR: 0.00032976\n",
            "Train Epoch: 7 [71680/123872 (58%)]\tLoss: 0.369180\tLR: 0.00032965\n",
            "Train Epoch: 7 [71680/123872 (58%)]\tLoss: 0.369180\n",
            "Train Epoch: 7 [71936/123872 (58%)]\tLoss: 0.397724\tLR: 0.00032953\n",
            "Train Epoch: 7 [72192/123872 (58%)]\tLoss: 0.422880\tLR: 0.00032941\n",
            "Train Epoch: 7 [72448/123872 (58%)]\tLoss: 0.367172\tLR: 0.00032929\n",
            "Train Epoch: 7 [72704/123872 (59%)]\tLoss: 0.417316\tLR: 0.00032917\n",
            "Train Epoch: 7 [72960/123872 (59%)]\tLoss: 0.405488\tLR: 0.00032906\n",
            "Train Epoch: 7 [73216/123872 (59%)]\tLoss: 0.354985\tLR: 0.00032894\n",
            "Train Epoch: 7 [73472/123872 (59%)]\tLoss: 0.441924\tLR: 0.00032882\n",
            "Train Epoch: 7 [73728/123872 (60%)]\tLoss: 0.392341\tLR: 0.00032870\n",
            "Train Epoch: 7 [73984/123872 (60%)]\tLoss: 0.404718\tLR: 0.00032858\n",
            "Train Epoch: 7 [74240/123872 (60%)]\tLoss: 0.345010\tLR: 0.00032847\n",
            "Train Epoch: 7 [74240/123872 (60%)]\tLoss: 0.345010\n",
            "Train Epoch: 7 [74496/123872 (60%)]\tLoss: 0.400069\tLR: 0.00032835\n",
            "Train Epoch: 7 [74752/123872 (60%)]\tLoss: 0.386713\tLR: 0.00032823\n",
            "Train Epoch: 7 [75008/123872 (61%)]\tLoss: 0.431507\tLR: 0.00032811\n",
            "Train Epoch: 7 [75264/123872 (61%)]\tLoss: 0.417417\tLR: 0.00032799\n",
            "Train Epoch: 7 [75520/123872 (61%)]\tLoss: 0.430704\tLR: 0.00032788\n",
            "Train Epoch: 7 [75776/123872 (61%)]\tLoss: 0.374375\tLR: 0.00032776\n",
            "Train Epoch: 7 [76032/123872 (61%)]\tLoss: 0.418909\tLR: 0.00032764\n",
            "Train Epoch: 7 [76288/123872 (62%)]\tLoss: 0.302888\tLR: 0.00032752\n",
            "Train Epoch: 7 [76544/123872 (62%)]\tLoss: 0.401436\tLR: 0.00032740\n",
            "Train Epoch: 7 [76800/123872 (62%)]\tLoss: 0.386059\tLR: 0.00032729\n",
            "Train Epoch: 7 [76800/123872 (62%)]\tLoss: 0.386059\n",
            "Train Epoch: 7 [77056/123872 (62%)]\tLoss: 0.344461\tLR: 0.00032717\n",
            "Train Epoch: 7 [77312/123872 (62%)]\tLoss: 0.442098\tLR: 0.00032705\n",
            "Train Epoch: 7 [77568/123872 (63%)]\tLoss: 0.363048\tLR: 0.00032693\n",
            "Train Epoch: 7 [77824/123872 (63%)]\tLoss: 0.338748\tLR: 0.00032681\n",
            "Train Epoch: 7 [78080/123872 (63%)]\tLoss: 0.350864\tLR: 0.00032670\n",
            "Train Epoch: 7 [78336/123872 (63%)]\tLoss: 0.430095\tLR: 0.00032658\n",
            "Train Epoch: 7 [78592/123872 (63%)]\tLoss: 0.423607\tLR: 0.00032646\n",
            "Train Epoch: 7 [78848/123872 (64%)]\tLoss: 0.510291\tLR: 0.00032634\n",
            "Train Epoch: 7 [79104/123872 (64%)]\tLoss: 0.403292\tLR: 0.00032622\n",
            "Train Epoch: 7 [79360/123872 (64%)]\tLoss: 0.532210\tLR: 0.00032611\n",
            "Train Epoch: 7 [79360/123872 (64%)]\tLoss: 0.532210\n",
            "Train Epoch: 7 [79616/123872 (64%)]\tLoss: 0.361303\tLR: 0.00032599\n",
            "Train Epoch: 7 [79872/123872 (64%)]\tLoss: 0.419788\tLR: 0.00032587\n",
            "Train Epoch: 7 [80128/123872 (65%)]\tLoss: 0.360220\tLR: 0.00032575\n",
            "Train Epoch: 7 [80384/123872 (65%)]\tLoss: 0.392367\tLR: 0.00032563\n",
            "Train Epoch: 7 [80640/123872 (65%)]\tLoss: 0.359701\tLR: 0.00032552\n",
            "Train Epoch: 7 [80896/123872 (65%)]\tLoss: 0.422233\tLR: 0.00032540\n",
            "Train Epoch: 7 [81152/123872 (65%)]\tLoss: 0.390804\tLR: 0.00032528\n",
            "Train Epoch: 7 [81408/123872 (66%)]\tLoss: 0.417123\tLR: 0.00032516\n",
            "Train Epoch: 7 [81664/123872 (66%)]\tLoss: 0.393798\tLR: 0.00032504\n",
            "Train Epoch: 7 [81920/123872 (66%)]\tLoss: 0.371875\tLR: 0.00032493\n",
            "Train Epoch: 7 [81920/123872 (66%)]\tLoss: 0.371875\n",
            "Train Epoch: 7 [82176/123872 (66%)]\tLoss: 0.419469\tLR: 0.00032481\n",
            "Train Epoch: 7 [82432/123872 (67%)]\tLoss: 0.369145\tLR: 0.00032469\n",
            "Train Epoch: 7 [82688/123872 (67%)]\tLoss: 0.437082\tLR: 0.00032457\n",
            "Train Epoch: 7 [82944/123872 (67%)]\tLoss: 0.420245\tLR: 0.00032445\n",
            "Train Epoch: 7 [83200/123872 (67%)]\tLoss: 0.356532\tLR: 0.00032434\n",
            "Train Epoch: 7 [83456/123872 (67%)]\tLoss: 0.438511\tLR: 0.00032422\n",
            "Train Epoch: 7 [83712/123872 (68%)]\tLoss: 0.478348\tLR: 0.00032410\n",
            "Train Epoch: 7 [83968/123872 (68%)]\tLoss: 0.397858\tLR: 0.00032398\n",
            "Train Epoch: 7 [84224/123872 (68%)]\tLoss: 0.329784\tLR: 0.00032386\n",
            "Train Epoch: 7 [84480/123872 (68%)]\tLoss: 0.448091\tLR: 0.00032375\n",
            "Train Epoch: 7 [84480/123872 (68%)]\tLoss: 0.448091\n",
            "Train Epoch: 7 [84736/123872 (68%)]\tLoss: 0.438123\tLR: 0.00032363\n",
            "Train Epoch: 7 [84992/123872 (69%)]\tLoss: 0.420151\tLR: 0.00032351\n",
            "Train Epoch: 7 [85248/123872 (69%)]\tLoss: 0.416928\tLR: 0.00032339\n",
            "Train Epoch: 7 [85504/123872 (69%)]\tLoss: 0.378885\tLR: 0.00032327\n",
            "Train Epoch: 7 [85760/123872 (69%)]\tLoss: 0.463639\tLR: 0.00032316\n",
            "Train Epoch: 7 [86016/123872 (69%)]\tLoss: 0.369532\tLR: 0.00032304\n",
            "Train Epoch: 7 [86272/123872 (70%)]\tLoss: 0.405270\tLR: 0.00032292\n",
            "Train Epoch: 7 [86528/123872 (70%)]\tLoss: 0.405925\tLR: 0.00032280\n",
            "Train Epoch: 7 [86784/123872 (70%)]\tLoss: 0.443530\tLR: 0.00032268\n",
            "Train Epoch: 7 [87040/123872 (70%)]\tLoss: 0.384142\tLR: 0.00032257\n",
            "Train Epoch: 7 [87040/123872 (70%)]\tLoss: 0.384142\n",
            "Train Epoch: 7 [87296/123872 (70%)]\tLoss: 0.428697\tLR: 0.00032245\n",
            "Train Epoch: 7 [87552/123872 (71%)]\tLoss: 0.426490\tLR: 0.00032233\n",
            "Train Epoch: 7 [87808/123872 (71%)]\tLoss: 0.425524\tLR: 0.00032221\n",
            "Train Epoch: 7 [88064/123872 (71%)]\tLoss: 0.437473\tLR: 0.00032209\n",
            "Train Epoch: 7 [88320/123872 (71%)]\tLoss: 0.392969\tLR: 0.00032198\n",
            "Train Epoch: 7 [88576/123872 (71%)]\tLoss: 0.372720\tLR: 0.00032186\n",
            "Train Epoch: 7 [88832/123872 (72%)]\tLoss: 0.428995\tLR: 0.00032174\n",
            "Train Epoch: 7 [89088/123872 (72%)]\tLoss: 0.308405\tLR: 0.00032162\n",
            "Train Epoch: 7 [89344/123872 (72%)]\tLoss: 0.392618\tLR: 0.00032150\n",
            "Train Epoch: 7 [89600/123872 (72%)]\tLoss: 0.453540\tLR: 0.00032139\n",
            "Train Epoch: 7 [89600/123872 (72%)]\tLoss: 0.453540\n",
            "Train Epoch: 7 [89856/123872 (73%)]\tLoss: 0.390042\tLR: 0.00032127\n",
            "Train Epoch: 7 [90112/123872 (73%)]\tLoss: 0.420646\tLR: 0.00032115\n",
            "Train Epoch: 7 [90368/123872 (73%)]\tLoss: 0.400189\tLR: 0.00032103\n",
            "Train Epoch: 7 [90624/123872 (73%)]\tLoss: 0.397422\tLR: 0.00032092\n",
            "Train Epoch: 7 [90880/123872 (73%)]\tLoss: 0.394529\tLR: 0.00032080\n",
            "Train Epoch: 7 [91136/123872 (74%)]\tLoss: 0.357735\tLR: 0.00032068\n",
            "Train Epoch: 7 [91392/123872 (74%)]\tLoss: 0.356470\tLR: 0.00032056\n",
            "Train Epoch: 7 [91648/123872 (74%)]\tLoss: 0.427413\tLR: 0.00032044\n",
            "Train Epoch: 7 [91904/123872 (74%)]\tLoss: 0.428195\tLR: 0.00032033\n",
            "Train Epoch: 7 [92160/123872 (74%)]\tLoss: 0.391663\tLR: 0.00032021\n",
            "Train Epoch: 7 [92160/123872 (74%)]\tLoss: 0.391663\n",
            "Train Epoch: 7 [92416/123872 (75%)]\tLoss: 0.411422\tLR: 0.00032009\n",
            "Train Epoch: 7 [92672/123872 (75%)]\tLoss: 0.392294\tLR: 0.00031997\n",
            "Train Epoch: 7 [92928/123872 (75%)]\tLoss: 0.402910\tLR: 0.00031985\n",
            "Train Epoch: 7 [93184/123872 (75%)]\tLoss: 0.371240\tLR: 0.00031974\n",
            "Train Epoch: 7 [93440/123872 (75%)]\tLoss: 0.427386\tLR: 0.00031962\n",
            "Train Epoch: 7 [93696/123872 (76%)]\tLoss: 0.448610\tLR: 0.00031950\n",
            "Train Epoch: 7 [93952/123872 (76%)]\tLoss: 0.406400\tLR: 0.00031938\n",
            "Train Epoch: 7 [94208/123872 (76%)]\tLoss: 0.389678\tLR: 0.00031926\n",
            "Train Epoch: 7 [94464/123872 (76%)]\tLoss: 0.388254\tLR: 0.00031915\n",
            "Train Epoch: 7 [94720/123872 (76%)]\tLoss: 0.452474\tLR: 0.00031903\n",
            "Train Epoch: 7 [94720/123872 (76%)]\tLoss: 0.452474\n",
            "Train Epoch: 7 [94976/123872 (77%)]\tLoss: 0.411578\tLR: 0.00031891\n",
            "Train Epoch: 7 [95232/123872 (77%)]\tLoss: 0.442086\tLR: 0.00031879\n",
            "Train Epoch: 7 [95488/123872 (77%)]\tLoss: 0.371076\tLR: 0.00031867\n",
            "Train Epoch: 7 [95744/123872 (77%)]\tLoss: 0.376038\tLR: 0.00031856\n",
            "Train Epoch: 7 [96000/123872 (77%)]\tLoss: 0.383691\tLR: 0.00031844\n",
            "Train Epoch: 7 [96256/123872 (78%)]\tLoss: 0.413603\tLR: 0.00031832\n",
            "Train Epoch: 7 [96512/123872 (78%)]\tLoss: 0.359522\tLR: 0.00031820\n",
            "Train Epoch: 7 [96768/123872 (78%)]\tLoss: 0.434123\tLR: 0.00031809\n",
            "Train Epoch: 7 [97024/123872 (78%)]\tLoss: 0.436389\tLR: 0.00031797\n",
            "Train Epoch: 7 [97280/123872 (79%)]\tLoss: 0.414298\tLR: 0.00031785\n",
            "Train Epoch: 7 [97280/123872 (79%)]\tLoss: 0.414298\n",
            "Train Epoch: 7 [97536/123872 (79%)]\tLoss: 0.385850\tLR: 0.00031773\n",
            "Train Epoch: 7 [97792/123872 (79%)]\tLoss: 0.380589\tLR: 0.00031761\n",
            "Train Epoch: 7 [98048/123872 (79%)]\tLoss: 0.413512\tLR: 0.00031750\n",
            "Train Epoch: 7 [98304/123872 (79%)]\tLoss: 0.417422\tLR: 0.00031738\n",
            "Train Epoch: 7 [98560/123872 (80%)]\tLoss: 0.386993\tLR: 0.00031726\n",
            "Train Epoch: 7 [98816/123872 (80%)]\tLoss: 0.394990\tLR: 0.00031714\n",
            "Train Epoch: 7 [99072/123872 (80%)]\tLoss: 0.366239\tLR: 0.00031702\n",
            "Train Epoch: 7 [99328/123872 (80%)]\tLoss: 0.421950\tLR: 0.00031691\n",
            "Train Epoch: 7 [99584/123872 (80%)]\tLoss: 0.439689\tLR: 0.00031679\n",
            "Train Epoch: 7 [99840/123872 (81%)]\tLoss: 0.374962\tLR: 0.00031667\n",
            "Train Epoch: 7 [99840/123872 (81%)]\tLoss: 0.374962\n",
            "Train Epoch: 7 [100096/123872 (81%)]\tLoss: 0.365779\tLR: 0.00031655\n",
            "Train Epoch: 7 [100352/123872 (81%)]\tLoss: 0.360991\tLR: 0.00031643\n",
            "Train Epoch: 7 [100608/123872 (81%)]\tLoss: 0.368694\tLR: 0.00031632\n",
            "Train Epoch: 7 [100864/123872 (81%)]\tLoss: 0.420987\tLR: 0.00031620\n",
            "Train Epoch: 7 [101120/123872 (82%)]\tLoss: 0.338346\tLR: 0.00031608\n",
            "Train Epoch: 7 [101376/123872 (82%)]\tLoss: 0.506859\tLR: 0.00031596\n",
            "Train Epoch: 7 [101632/123872 (82%)]\tLoss: 0.340571\tLR: 0.00031585\n",
            "Train Epoch: 7 [101888/123872 (82%)]\tLoss: 0.440959\tLR: 0.00031573\n",
            "Train Epoch: 7 [102144/123872 (82%)]\tLoss: 0.358752\tLR: 0.00031561\n",
            "Train Epoch: 7 [102400/123872 (83%)]\tLoss: 0.446360\tLR: 0.00031549\n",
            "Train Epoch: 7 [102400/123872 (83%)]\tLoss: 0.446360\n",
            "Train Epoch: 7 [102656/123872 (83%)]\tLoss: 0.387594\tLR: 0.00031537\n",
            "Train Epoch: 7 [102912/123872 (83%)]\tLoss: 0.377736\tLR: 0.00031526\n",
            "Train Epoch: 7 [103168/123872 (83%)]\tLoss: 0.392019\tLR: 0.00031514\n",
            "Train Epoch: 7 [103424/123872 (83%)]\tLoss: 0.407433\tLR: 0.00031502\n",
            "Train Epoch: 7 [103680/123872 (84%)]\tLoss: 0.381388\tLR: 0.00031490\n",
            "Train Epoch: 7 [103936/123872 (84%)]\tLoss: 0.373411\tLR: 0.00031479\n",
            "Train Epoch: 7 [104192/123872 (84%)]\tLoss: 0.404280\tLR: 0.00031467\n",
            "Train Epoch: 7 [104448/123872 (84%)]\tLoss: 0.378804\tLR: 0.00031455\n",
            "Train Epoch: 7 [104704/123872 (85%)]\tLoss: 0.432769\tLR: 0.00031443\n",
            "Train Epoch: 7 [104960/123872 (85%)]\tLoss: 0.418011\tLR: 0.00031431\n",
            "Train Epoch: 7 [104960/123872 (85%)]\tLoss: 0.418011\n",
            "Train Epoch: 7 [105216/123872 (85%)]\tLoss: 0.393002\tLR: 0.00031420\n",
            "Train Epoch: 7 [105472/123872 (85%)]\tLoss: 0.399865\tLR: 0.00031408\n",
            "Train Epoch: 7 [105728/123872 (85%)]\tLoss: 0.434904\tLR: 0.00031396\n",
            "Train Epoch: 7 [105984/123872 (86%)]\tLoss: 0.374690\tLR: 0.00031384\n",
            "Train Epoch: 7 [106240/123872 (86%)]\tLoss: 0.381016\tLR: 0.00031372\n",
            "Train Epoch: 7 [106496/123872 (86%)]\tLoss: 0.431399\tLR: 0.00031361\n",
            "Train Epoch: 7 [106752/123872 (86%)]\tLoss: 0.429978\tLR: 0.00031349\n",
            "Train Epoch: 7 [107008/123872 (86%)]\tLoss: 0.460892\tLR: 0.00031337\n",
            "Train Epoch: 7 [107264/123872 (87%)]\tLoss: 0.427560\tLR: 0.00031325\n",
            "Train Epoch: 7 [107520/123872 (87%)]\tLoss: 0.373646\tLR: 0.00031314\n",
            "Train Epoch: 7 [107520/123872 (87%)]\tLoss: 0.373646\n",
            "Train Epoch: 7 [107776/123872 (87%)]\tLoss: 0.438667\tLR: 0.00031302\n",
            "Train Epoch: 7 [108032/123872 (87%)]\tLoss: 0.410022\tLR: 0.00031290\n",
            "Train Epoch: 7 [108288/123872 (87%)]\tLoss: 0.407588\tLR: 0.00031278\n",
            "Train Epoch: 7 [108544/123872 (88%)]\tLoss: 0.385449\tLR: 0.00031266\n",
            "Train Epoch: 7 [108800/123872 (88%)]\tLoss: 0.386724\tLR: 0.00031255\n",
            "Train Epoch: 7 [109056/123872 (88%)]\tLoss: 0.434878\tLR: 0.00031243\n",
            "Train Epoch: 7 [109312/123872 (88%)]\tLoss: 0.418194\tLR: 0.00031231\n",
            "Train Epoch: 7 [109568/123872 (88%)]\tLoss: 0.411298\tLR: 0.00031219\n",
            "Train Epoch: 7 [109824/123872 (89%)]\tLoss: 0.399838\tLR: 0.00031208\n",
            "Train Epoch: 7 [110080/123872 (89%)]\tLoss: 0.344401\tLR: 0.00031196\n",
            "Train Epoch: 7 [110080/123872 (89%)]\tLoss: 0.344401\n",
            "Train Epoch: 7 [110336/123872 (89%)]\tLoss: 0.399612\tLR: 0.00031184\n",
            "Train Epoch: 7 [110592/123872 (89%)]\tLoss: 0.332849\tLR: 0.00031172\n",
            "Train Epoch: 7 [110848/123872 (89%)]\tLoss: 0.398900\tLR: 0.00031161\n",
            "Train Epoch: 7 [111104/123872 (90%)]\tLoss: 0.439432\tLR: 0.00031149\n",
            "Train Epoch: 7 [111360/123872 (90%)]\tLoss: 0.411755\tLR: 0.00031137\n",
            "Train Epoch: 7 [111616/123872 (90%)]\tLoss: 0.478895\tLR: 0.00031125\n",
            "Train Epoch: 7 [111872/123872 (90%)]\tLoss: 0.450833\tLR: 0.00031113\n",
            "Train Epoch: 7 [112128/123872 (90%)]\tLoss: 0.393540\tLR: 0.00031102\n",
            "Train Epoch: 7 [112384/123872 (91%)]\tLoss: 0.384979\tLR: 0.00031090\n",
            "Train Epoch: 7 [112640/123872 (91%)]\tLoss: 0.422629\tLR: 0.00031078\n",
            "Train Epoch: 7 [112640/123872 (91%)]\tLoss: 0.422629\n",
            "Train Epoch: 7 [112896/123872 (91%)]\tLoss: 0.388939\tLR: 0.00031066\n",
            "Train Epoch: 7 [113152/123872 (91%)]\tLoss: 0.437318\tLR: 0.00031055\n",
            "Train Epoch: 7 [113408/123872 (92%)]\tLoss: 0.385715\tLR: 0.00031043\n",
            "Train Epoch: 7 [113664/123872 (92%)]\tLoss: 0.400179\tLR: 0.00031031\n",
            "Train Epoch: 7 [113920/123872 (92%)]\tLoss: 0.390253\tLR: 0.00031019\n",
            "Train Epoch: 7 [114176/123872 (92%)]\tLoss: 0.373788\tLR: 0.00031008\n",
            "Train Epoch: 7 [114432/123872 (92%)]\tLoss: 0.438140\tLR: 0.00030996\n",
            "Train Epoch: 7 [114688/123872 (93%)]\tLoss: 0.416408\tLR: 0.00030984\n",
            "Train Epoch: 7 [114944/123872 (93%)]\tLoss: 0.349899\tLR: 0.00030972\n",
            "Train Epoch: 7 [115200/123872 (93%)]\tLoss: 0.407782\tLR: 0.00030960\n",
            "Train Epoch: 7 [115200/123872 (93%)]\tLoss: 0.407782\n",
            "Train Epoch: 7 [115456/123872 (93%)]\tLoss: 0.372198\tLR: 0.00030949\n",
            "Train Epoch: 7 [115712/123872 (93%)]\tLoss: 0.373642\tLR: 0.00030937\n",
            "Train Epoch: 7 [115968/123872 (94%)]\tLoss: 0.363403\tLR: 0.00030925\n",
            "Train Epoch: 7 [116224/123872 (94%)]\tLoss: 0.472225\tLR: 0.00030913\n",
            "Train Epoch: 7 [116480/123872 (94%)]\tLoss: 0.355860\tLR: 0.00030902\n",
            "Train Epoch: 7 [116736/123872 (94%)]\tLoss: 0.383690\tLR: 0.00030890\n",
            "Train Epoch: 7 [116992/123872 (94%)]\tLoss: 0.459818\tLR: 0.00030878\n",
            "Train Epoch: 7 [117248/123872 (95%)]\tLoss: 0.343738\tLR: 0.00030866\n",
            "Train Epoch: 7 [117504/123872 (95%)]\tLoss: 0.374119\tLR: 0.00030855\n",
            "Train Epoch: 7 [117760/123872 (95%)]\tLoss: 0.440210\tLR: 0.00030843\n",
            "Train Epoch: 7 [117760/123872 (95%)]\tLoss: 0.440210\n",
            "Train Epoch: 7 [118016/123872 (95%)]\tLoss: 0.412269\tLR: 0.00030831\n",
            "Train Epoch: 7 [118272/123872 (95%)]\tLoss: 0.383124\tLR: 0.00030819\n",
            "Train Epoch: 7 [118528/123872 (96%)]\tLoss: 0.483515\tLR: 0.00030807\n",
            "Train Epoch: 7 [118784/123872 (96%)]\tLoss: 0.445554\tLR: 0.00030796\n",
            "Train Epoch: 7 [119040/123872 (96%)]\tLoss: 0.424058\tLR: 0.00030784\n",
            "Train Epoch: 7 [119296/123872 (96%)]\tLoss: 0.396956\tLR: 0.00030772\n",
            "Train Epoch: 7 [119552/123872 (96%)]\tLoss: 0.388702\tLR: 0.00030760\n",
            "Train Epoch: 7 [119808/123872 (97%)]\tLoss: 0.414620\tLR: 0.00030749\n",
            "Train Epoch: 7 [120064/123872 (97%)]\tLoss: 0.411722\tLR: 0.00030737\n",
            "Train Epoch: 7 [120320/123872 (97%)]\tLoss: 0.389022\tLR: 0.00030725\n",
            "Train Epoch: 7 [120320/123872 (97%)]\tLoss: 0.389022\n",
            "Train Epoch: 7 [120576/123872 (97%)]\tLoss: 0.415987\tLR: 0.00030713\n",
            "Train Epoch: 7 [120832/123872 (98%)]\tLoss: 0.425790\tLR: 0.00030702\n",
            "Train Epoch: 7 [121088/123872 (98%)]\tLoss: 0.358436\tLR: 0.00030690\n",
            "Train Epoch: 7 [121344/123872 (98%)]\tLoss: 0.388556\tLR: 0.00030678\n",
            "Train Epoch: 7 [121600/123872 (98%)]\tLoss: 0.393795\tLR: 0.00030666\n",
            "Train Epoch: 7 [121856/123872 (98%)]\tLoss: 0.417294\tLR: 0.00030655\n",
            "Train Epoch: 7 [122112/123872 (99%)]\tLoss: 0.369417\tLR: 0.00030643\n",
            "Train Epoch: 7 [122368/123872 (99%)]\tLoss: 0.381503\tLR: 0.00030631\n",
            "Train Epoch: 7 [122624/123872 (99%)]\tLoss: 0.441653\tLR: 0.00030619\n",
            "Train Epoch: 7 [122880/123872 (99%)]\tLoss: 0.371010\tLR: 0.00030608\n",
            "Train Epoch: 7 [122880/123872 (99%)]\tLoss: 0.371010\n",
            "Train Epoch: 7 [123136/123872 (99%)]\tLoss: 0.355265\tLR: 0.00030596\n",
            "Train Epoch: 7 [123392/123872 (100%)]\tLoss: 0.414536\tLR: 0.00030584\n",
            "Train Epoch: 7 [108192/123872 (100%)]\tLoss: 0.409141\tLR: 0.00030572\n",
            "\n",
            "Test set: Average loss: 0.0016, Accuracy: 25356/30970 (81.87%)\n",
            "\n",
            "Train Epoch: 8 [0/123872 (0%)]\tLoss: 0.428768\tLR: 0.00030561\n",
            "Train Epoch: 8 [0/123872 (0%)]\tLoss: 0.428768\n",
            "Train Epoch: 8 [256/123872 (0%)]\tLoss: 0.339882\tLR: 0.00030549\n",
            "Train Epoch: 8 [512/123872 (0%)]\tLoss: 0.361316\tLR: 0.00030537\n",
            "Train Epoch: 8 [768/123872 (1%)]\tLoss: 0.370783\tLR: 0.00030525\n",
            "Train Epoch: 8 [1024/123872 (1%)]\tLoss: 0.389377\tLR: 0.00030514\n",
            "Train Epoch: 8 [1280/123872 (1%)]\tLoss: 0.400357\tLR: 0.00030502\n",
            "Train Epoch: 8 [1536/123872 (1%)]\tLoss: 0.415140\tLR: 0.00030490\n",
            "Train Epoch: 8 [1792/123872 (1%)]\tLoss: 0.446289\tLR: 0.00030478\n",
            "Train Epoch: 8 [2048/123872 (2%)]\tLoss: 0.413328\tLR: 0.00030467\n",
            "Train Epoch: 8 [2304/123872 (2%)]\tLoss: 0.470771\tLR: 0.00030455\n",
            "Train Epoch: 8 [2560/123872 (2%)]\tLoss: 0.394767\tLR: 0.00030443\n",
            "Train Epoch: 8 [2560/123872 (2%)]\tLoss: 0.394767\n",
            "Train Epoch: 8 [2816/123872 (2%)]\tLoss: 0.450508\tLR: 0.00030431\n",
            "Train Epoch: 8 [3072/123872 (2%)]\tLoss: 0.341535\tLR: 0.00030420\n",
            "Train Epoch: 8 [3328/123872 (3%)]\tLoss: 0.362598\tLR: 0.00030408\n",
            "Train Epoch: 8 [3584/123872 (3%)]\tLoss: 0.351187\tLR: 0.00030396\n",
            "Train Epoch: 8 [3840/123872 (3%)]\tLoss: 0.402085\tLR: 0.00030384\n",
            "Train Epoch: 8 [4096/123872 (3%)]\tLoss: 0.391369\tLR: 0.00030373\n",
            "Train Epoch: 8 [4352/123872 (4%)]\tLoss: 0.393814\tLR: 0.00030361\n",
            "Train Epoch: 8 [4608/123872 (4%)]\tLoss: 0.398504\tLR: 0.00030349\n",
            "Train Epoch: 8 [4864/123872 (4%)]\tLoss: 0.431767\tLR: 0.00030337\n",
            "Train Epoch: 8 [5120/123872 (4%)]\tLoss: 0.381427\tLR: 0.00030326\n",
            "Train Epoch: 8 [5120/123872 (4%)]\tLoss: 0.381427\n",
            "Train Epoch: 8 [5376/123872 (4%)]\tLoss: 0.404873\tLR: 0.00030314\n",
            "Train Epoch: 8 [5632/123872 (5%)]\tLoss: 0.448278\tLR: 0.00030302\n",
            "Train Epoch: 8 [5888/123872 (5%)]\tLoss: 0.456714\tLR: 0.00030290\n",
            "Train Epoch: 8 [6144/123872 (5%)]\tLoss: 0.437410\tLR: 0.00030279\n",
            "Train Epoch: 8 [6400/123872 (5%)]\tLoss: 0.381262\tLR: 0.00030267\n",
            "Train Epoch: 8 [6656/123872 (5%)]\tLoss: 0.374546\tLR: 0.00030255\n",
            "Train Epoch: 8 [6912/123872 (6%)]\tLoss: 0.383614\tLR: 0.00030243\n",
            "Train Epoch: 8 [7168/123872 (6%)]\tLoss: 0.424820\tLR: 0.00030232\n",
            "Train Epoch: 8 [7424/123872 (6%)]\tLoss: 0.361702\tLR: 0.00030220\n",
            "Train Epoch: 8 [7680/123872 (6%)]\tLoss: 0.366828\tLR: 0.00030208\n",
            "Train Epoch: 8 [7680/123872 (6%)]\tLoss: 0.366828\n",
            "Train Epoch: 8 [7936/123872 (6%)]\tLoss: 0.362831\tLR: 0.00030197\n",
            "Train Epoch: 8 [8192/123872 (7%)]\tLoss: 0.379178\tLR: 0.00030185\n",
            "Train Epoch: 8 [8448/123872 (7%)]\tLoss: 0.404845\tLR: 0.00030173\n",
            "Train Epoch: 8 [8704/123872 (7%)]\tLoss: 0.351112\tLR: 0.00030161\n",
            "Train Epoch: 8 [8960/123872 (7%)]\tLoss: 0.396232\tLR: 0.00030150\n",
            "Train Epoch: 8 [9216/123872 (7%)]\tLoss: 0.441701\tLR: 0.00030138\n",
            "Train Epoch: 8 [9472/123872 (8%)]\tLoss: 0.412836\tLR: 0.00030126\n",
            "Train Epoch: 8 [9728/123872 (8%)]\tLoss: 0.370525\tLR: 0.00030114\n",
            "Train Epoch: 8 [9984/123872 (8%)]\tLoss: 0.350720\tLR: 0.00030103\n",
            "Train Epoch: 8 [10240/123872 (8%)]\tLoss: 0.362972\tLR: 0.00030091\n",
            "Train Epoch: 8 [10240/123872 (8%)]\tLoss: 0.362972\n",
            "Train Epoch: 8 [10496/123872 (8%)]\tLoss: 0.351654\tLR: 0.00030079\n",
            "Train Epoch: 8 [10752/123872 (9%)]\tLoss: 0.382935\tLR: 0.00030067\n",
            "Train Epoch: 8 [11008/123872 (9%)]\tLoss: 0.386802\tLR: 0.00030056\n",
            "Train Epoch: 8 [11264/123872 (9%)]\tLoss: 0.386161\tLR: 0.00030044\n",
            "Train Epoch: 8 [11520/123872 (9%)]\tLoss: 0.358323\tLR: 0.00030032\n",
            "Train Epoch: 8 [11776/123872 (10%)]\tLoss: 0.438506\tLR: 0.00030021\n",
            "Train Epoch: 8 [12032/123872 (10%)]\tLoss: 0.438885\tLR: 0.00030009\n",
            "Train Epoch: 8 [12288/123872 (10%)]\tLoss: 0.372558\tLR: 0.00029997\n",
            "Train Epoch: 8 [12544/123872 (10%)]\tLoss: 0.444895\tLR: 0.00029985\n",
            "Train Epoch: 8 [12800/123872 (10%)]\tLoss: 0.411195\tLR: 0.00029974\n",
            "Train Epoch: 8 [12800/123872 (10%)]\tLoss: 0.411195\n",
            "Train Epoch: 8 [13056/123872 (11%)]\tLoss: 0.437062\tLR: 0.00029962\n",
            "Train Epoch: 8 [13312/123872 (11%)]\tLoss: 0.418248\tLR: 0.00029950\n",
            "Train Epoch: 8 [13568/123872 (11%)]\tLoss: 0.441496\tLR: 0.00029938\n",
            "Train Epoch: 8 [13824/123872 (11%)]\tLoss: 0.329118\tLR: 0.00029927\n",
            "Train Epoch: 8 [14080/123872 (11%)]\tLoss: 0.354876\tLR: 0.00029915\n",
            "Train Epoch: 8 [14336/123872 (12%)]\tLoss: 0.381112\tLR: 0.00029903\n",
            "Train Epoch: 8 [14592/123872 (12%)]\tLoss: 0.484799\tLR: 0.00029892\n",
            "Train Epoch: 8 [14848/123872 (12%)]\tLoss: 0.384807\tLR: 0.00029880\n",
            "Train Epoch: 8 [15104/123872 (12%)]\tLoss: 0.438697\tLR: 0.00029868\n",
            "Train Epoch: 8 [15360/123872 (12%)]\tLoss: 0.384087\tLR: 0.00029856\n",
            "Train Epoch: 8 [15360/123872 (12%)]\tLoss: 0.384087\n",
            "Train Epoch: 8 [15616/123872 (13%)]\tLoss: 0.379040\tLR: 0.00029845\n",
            "Train Epoch: 8 [15872/123872 (13%)]\tLoss: 0.375528\tLR: 0.00029833\n",
            "Train Epoch: 8 [16128/123872 (13%)]\tLoss: 0.379815\tLR: 0.00029821\n",
            "Train Epoch: 8 [16384/123872 (13%)]\tLoss: 0.369526\tLR: 0.00029809\n",
            "Train Epoch: 8 [16640/123872 (13%)]\tLoss: 0.428569\tLR: 0.00029798\n",
            "Train Epoch: 8 [16896/123872 (14%)]\tLoss: 0.355623\tLR: 0.00029786\n",
            "Train Epoch: 8 [17152/123872 (14%)]\tLoss: 0.373498\tLR: 0.00029774\n",
            "Train Epoch: 8 [17408/123872 (14%)]\tLoss: 0.361438\tLR: 0.00029763\n",
            "Train Epoch: 8 [17664/123872 (14%)]\tLoss: 0.436545\tLR: 0.00029751\n",
            "Train Epoch: 8 [17920/123872 (14%)]\tLoss: 0.410299\tLR: 0.00029739\n",
            "Train Epoch: 8 [17920/123872 (14%)]\tLoss: 0.410299\n",
            "Train Epoch: 8 [18176/123872 (15%)]\tLoss: 0.410008\tLR: 0.00029727\n",
            "Train Epoch: 8 [18432/123872 (15%)]\tLoss: 0.399171\tLR: 0.00029716\n",
            "Train Epoch: 8 [18688/123872 (15%)]\tLoss: 0.324719\tLR: 0.00029704\n",
            "Train Epoch: 8 [18944/123872 (15%)]\tLoss: 0.416327\tLR: 0.00029692\n",
            "Train Epoch: 8 [19200/123872 (15%)]\tLoss: 0.373905\tLR: 0.00029681\n",
            "Train Epoch: 8 [19456/123872 (16%)]\tLoss: 0.407019\tLR: 0.00029669\n",
            "Train Epoch: 8 [19712/123872 (16%)]\tLoss: 0.321010\tLR: 0.00029657\n",
            "Train Epoch: 8 [19968/123872 (16%)]\tLoss: 0.363308\tLR: 0.00029646\n",
            "Train Epoch: 8 [20224/123872 (16%)]\tLoss: 0.380626\tLR: 0.00029634\n",
            "Train Epoch: 8 [20480/123872 (17%)]\tLoss: 0.347881\tLR: 0.00029622\n",
            "Train Epoch: 8 [20480/123872 (17%)]\tLoss: 0.347881\n",
            "Train Epoch: 8 [20736/123872 (17%)]\tLoss: 0.442945\tLR: 0.00029610\n",
            "Train Epoch: 8 [20992/123872 (17%)]\tLoss: 0.431701\tLR: 0.00029599\n",
            "Train Epoch: 8 [21248/123872 (17%)]\tLoss: 0.395580\tLR: 0.00029587\n",
            "Train Epoch: 8 [21504/123872 (17%)]\tLoss: 0.393487\tLR: 0.00029575\n",
            "Train Epoch: 8 [21760/123872 (18%)]\tLoss: 0.438994\tLR: 0.00029564\n",
            "Train Epoch: 8 [22016/123872 (18%)]\tLoss: 0.469892\tLR: 0.00029552\n",
            "Train Epoch: 8 [22272/123872 (18%)]\tLoss: 0.367019\tLR: 0.00029540\n",
            "Train Epoch: 8 [22528/123872 (18%)]\tLoss: 0.398264\tLR: 0.00029528\n",
            "Train Epoch: 8 [22784/123872 (18%)]\tLoss: 0.334706\tLR: 0.00029517\n",
            "Train Epoch: 8 [23040/123872 (19%)]\tLoss: 0.344724\tLR: 0.00029505\n",
            "Train Epoch: 8 [23040/123872 (19%)]\tLoss: 0.344724\n",
            "Train Epoch: 8 [23296/123872 (19%)]\tLoss: 0.367944\tLR: 0.00029493\n",
            "Train Epoch: 8 [23552/123872 (19%)]\tLoss: 0.453413\tLR: 0.00029482\n",
            "Train Epoch: 8 [23808/123872 (19%)]\tLoss: 0.412976\tLR: 0.00029470\n",
            "Train Epoch: 8 [24064/123872 (19%)]\tLoss: 0.389437\tLR: 0.00029458\n",
            "Train Epoch: 8 [24320/123872 (20%)]\tLoss: 0.377797\tLR: 0.00029447\n",
            "Train Epoch: 8 [24576/123872 (20%)]\tLoss: 0.440379\tLR: 0.00029435\n",
            "Train Epoch: 8 [24832/123872 (20%)]\tLoss: 0.333533\tLR: 0.00029423\n",
            "Train Epoch: 8 [25088/123872 (20%)]\tLoss: 0.465901\tLR: 0.00029411\n",
            "Train Epoch: 8 [25344/123872 (20%)]\tLoss: 0.375743\tLR: 0.00029400\n",
            "Train Epoch: 8 [25600/123872 (21%)]\tLoss: 0.412685\tLR: 0.00029388\n",
            "Train Epoch: 8 [25600/123872 (21%)]\tLoss: 0.412685\n",
            "Train Epoch: 8 [25856/123872 (21%)]\tLoss: 0.449989\tLR: 0.00029376\n",
            "Train Epoch: 8 [26112/123872 (21%)]\tLoss: 0.378287\tLR: 0.00029365\n",
            "Train Epoch: 8 [26368/123872 (21%)]\tLoss: 0.399029\tLR: 0.00029353\n",
            "Train Epoch: 8 [26624/123872 (21%)]\tLoss: 0.416922\tLR: 0.00029341\n",
            "Train Epoch: 8 [26880/123872 (22%)]\tLoss: 0.395673\tLR: 0.00029330\n",
            "Train Epoch: 8 [27136/123872 (22%)]\tLoss: 0.350362\tLR: 0.00029318\n",
            "Train Epoch: 8 [27392/123872 (22%)]\tLoss: 0.445157\tLR: 0.00029306\n",
            "Train Epoch: 8 [27648/123872 (22%)]\tLoss: 0.355287\tLR: 0.00029295\n",
            "Train Epoch: 8 [27904/123872 (23%)]\tLoss: 0.362116\tLR: 0.00029283\n",
            "Train Epoch: 8 [28160/123872 (23%)]\tLoss: 0.354915\tLR: 0.00029271\n",
            "Train Epoch: 8 [28160/123872 (23%)]\tLoss: 0.354915\n",
            "Train Epoch: 8 [28416/123872 (23%)]\tLoss: 0.408086\tLR: 0.00029259\n",
            "Train Epoch: 8 [28672/123872 (23%)]\tLoss: 0.357700\tLR: 0.00029248\n",
            "Train Epoch: 8 [28928/123872 (23%)]\tLoss: 0.355778\tLR: 0.00029236\n",
            "Train Epoch: 8 [29184/123872 (24%)]\tLoss: 0.382305\tLR: 0.00029224\n",
            "Train Epoch: 8 [29440/123872 (24%)]\tLoss: 0.360490\tLR: 0.00029213\n",
            "Train Epoch: 8 [29696/123872 (24%)]\tLoss: 0.411083\tLR: 0.00029201\n",
            "Train Epoch: 8 [29952/123872 (24%)]\tLoss: 0.452636\tLR: 0.00029189\n",
            "Train Epoch: 8 [30208/123872 (24%)]\tLoss: 0.409747\tLR: 0.00029178\n",
            "Train Epoch: 8 [30464/123872 (25%)]\tLoss: 0.439021\tLR: 0.00029166\n",
            "Train Epoch: 8 [30720/123872 (25%)]\tLoss: 0.380132\tLR: 0.00029154\n",
            "Train Epoch: 8 [30720/123872 (25%)]\tLoss: 0.380132\n",
            "Train Epoch: 8 [30976/123872 (25%)]\tLoss: 0.466923\tLR: 0.00029143\n",
            "Train Epoch: 8 [31232/123872 (25%)]\tLoss: 0.371698\tLR: 0.00029131\n",
            "Train Epoch: 8 [31488/123872 (25%)]\tLoss: 0.410743\tLR: 0.00029119\n",
            "Train Epoch: 8 [31744/123872 (26%)]\tLoss: 0.393111\tLR: 0.00029108\n",
            "Train Epoch: 8 [32000/123872 (26%)]\tLoss: 0.369668\tLR: 0.00029096\n",
            "Train Epoch: 8 [32256/123872 (26%)]\tLoss: 0.384488\tLR: 0.00029084\n",
            "Train Epoch: 8 [32512/123872 (26%)]\tLoss: 0.390702\tLR: 0.00029073\n",
            "Train Epoch: 8 [32768/123872 (26%)]\tLoss: 0.294789\tLR: 0.00029061\n",
            "Train Epoch: 8 [33024/123872 (27%)]\tLoss: 0.356248\tLR: 0.00029049\n",
            "Train Epoch: 8 [33280/123872 (27%)]\tLoss: 0.406218\tLR: 0.00029038\n",
            "Train Epoch: 8 [33280/123872 (27%)]\tLoss: 0.406218\n",
            "Train Epoch: 8 [33536/123872 (27%)]\tLoss: 0.451908\tLR: 0.00029026\n",
            "Train Epoch: 8 [33792/123872 (27%)]\tLoss: 0.379041\tLR: 0.00029014\n",
            "Train Epoch: 8 [34048/123872 (27%)]\tLoss: 0.394725\tLR: 0.00029003\n",
            "Train Epoch: 8 [34304/123872 (28%)]\tLoss: 0.374834\tLR: 0.00028991\n",
            "Train Epoch: 8 [34560/123872 (28%)]\tLoss: 0.365981\tLR: 0.00028979\n",
            "Train Epoch: 8 [34816/123872 (28%)]\tLoss: 0.337075\tLR: 0.00028968\n",
            "Train Epoch: 8 [35072/123872 (28%)]\tLoss: 0.425558\tLR: 0.00028956\n",
            "Train Epoch: 8 [35328/123872 (29%)]\tLoss: 0.413318\tLR: 0.00028944\n",
            "Train Epoch: 8 [35584/123872 (29%)]\tLoss: 0.427915\tLR: 0.00028933\n",
            "Train Epoch: 8 [35840/123872 (29%)]\tLoss: 0.369965\tLR: 0.00028921\n",
            "Train Epoch: 8 [35840/123872 (29%)]\tLoss: 0.369965\n",
            "Train Epoch: 8 [36096/123872 (29%)]\tLoss: 0.434692\tLR: 0.00028909\n",
            "Train Epoch: 8 [36352/123872 (29%)]\tLoss: 0.402681\tLR: 0.00028898\n",
            "Train Epoch: 8 [36608/123872 (30%)]\tLoss: 0.443105\tLR: 0.00028886\n",
            "Train Epoch: 8 [36864/123872 (30%)]\tLoss: 0.351811\tLR: 0.00028874\n",
            "Train Epoch: 8 [37120/123872 (30%)]\tLoss: 0.433345\tLR: 0.00028863\n",
            "Train Epoch: 8 [37376/123872 (30%)]\tLoss: 0.389020\tLR: 0.00028851\n",
            "Train Epoch: 8 [37632/123872 (30%)]\tLoss: 0.388245\tLR: 0.00028839\n",
            "Train Epoch: 8 [37888/123872 (31%)]\tLoss: 0.372155\tLR: 0.00028828\n",
            "Train Epoch: 8 [38144/123872 (31%)]\tLoss: 0.436173\tLR: 0.00028816\n",
            "Train Epoch: 8 [38400/123872 (31%)]\tLoss: 0.374458\tLR: 0.00028804\n",
            "Train Epoch: 8 [38400/123872 (31%)]\tLoss: 0.374458\n",
            "Train Epoch: 8 [38656/123872 (31%)]\tLoss: 0.375856\tLR: 0.00028793\n",
            "Train Epoch: 8 [38912/123872 (31%)]\tLoss: 0.406699\tLR: 0.00028781\n",
            "Train Epoch: 8 [39168/123872 (32%)]\tLoss: 0.383457\tLR: 0.00028769\n",
            "Train Epoch: 8 [39424/123872 (32%)]\tLoss: 0.387369\tLR: 0.00028758\n",
            "Train Epoch: 8 [39680/123872 (32%)]\tLoss: 0.370035\tLR: 0.00028746\n",
            "Train Epoch: 8 [39936/123872 (32%)]\tLoss: 0.418614\tLR: 0.00028734\n",
            "Train Epoch: 8 [40192/123872 (32%)]\tLoss: 0.388057\tLR: 0.00028723\n",
            "Train Epoch: 8 [40448/123872 (33%)]\tLoss: 0.370320\tLR: 0.00028711\n",
            "Train Epoch: 8 [40704/123872 (33%)]\tLoss: 0.374080\tLR: 0.00028699\n",
            "Train Epoch: 8 [40960/123872 (33%)]\tLoss: 0.374240\tLR: 0.00028688\n",
            "Train Epoch: 8 [40960/123872 (33%)]\tLoss: 0.374240\n",
            "Train Epoch: 8 [41216/123872 (33%)]\tLoss: 0.400575\tLR: 0.00028676\n",
            "Train Epoch: 8 [41472/123872 (33%)]\tLoss: 0.406505\tLR: 0.00028664\n",
            "Train Epoch: 8 [41728/123872 (34%)]\tLoss: 0.406618\tLR: 0.00028653\n",
            "Train Epoch: 8 [41984/123872 (34%)]\tLoss: 0.376470\tLR: 0.00028641\n",
            "Train Epoch: 8 [42240/123872 (34%)]\tLoss: 0.355899\tLR: 0.00028629\n",
            "Train Epoch: 8 [42496/123872 (34%)]\tLoss: 0.449481\tLR: 0.00028618\n",
            "Train Epoch: 8 [42752/123872 (35%)]\tLoss: 0.427249\tLR: 0.00028606\n",
            "Train Epoch: 8 [43008/123872 (35%)]\tLoss: 0.354118\tLR: 0.00028595\n",
            "Train Epoch: 8 [43264/123872 (35%)]\tLoss: 0.371376\tLR: 0.00028583\n",
            "Train Epoch: 8 [43520/123872 (35%)]\tLoss: 0.412205\tLR: 0.00028571\n",
            "Train Epoch: 8 [43520/123872 (35%)]\tLoss: 0.412205\n",
            "Train Epoch: 8 [43776/123872 (35%)]\tLoss: 0.459380\tLR: 0.00028560\n",
            "Train Epoch: 8 [44032/123872 (36%)]\tLoss: 0.402906\tLR: 0.00028548\n",
            "Train Epoch: 8 [44288/123872 (36%)]\tLoss: 0.406458\tLR: 0.00028536\n",
            "Train Epoch: 8 [44544/123872 (36%)]\tLoss: 0.369869\tLR: 0.00028525\n",
            "Train Epoch: 8 [44800/123872 (36%)]\tLoss: 0.411207\tLR: 0.00028513\n",
            "Train Epoch: 8 [45056/123872 (36%)]\tLoss: 0.316928\tLR: 0.00028501\n",
            "Train Epoch: 8 [45312/123872 (37%)]\tLoss: 0.404451\tLR: 0.00028490\n",
            "Train Epoch: 8 [45568/123872 (37%)]\tLoss: 0.403688\tLR: 0.00028478\n",
            "Train Epoch: 8 [45824/123872 (37%)]\tLoss: 0.343851\tLR: 0.00028467\n",
            "Train Epoch: 8 [46080/123872 (37%)]\tLoss: 0.411998\tLR: 0.00028455\n",
            "Train Epoch: 8 [46080/123872 (37%)]\tLoss: 0.411998\n",
            "Train Epoch: 8 [46336/123872 (37%)]\tLoss: 0.381791\tLR: 0.00028443\n",
            "Train Epoch: 8 [46592/123872 (38%)]\tLoss: 0.378664\tLR: 0.00028432\n",
            "Train Epoch: 8 [46848/123872 (38%)]\tLoss: 0.387745\tLR: 0.00028420\n",
            "Train Epoch: 8 [47104/123872 (38%)]\tLoss: 0.386607\tLR: 0.00028408\n",
            "Train Epoch: 8 [47360/123872 (38%)]\tLoss: 0.432015\tLR: 0.00028397\n",
            "Train Epoch: 8 [47616/123872 (38%)]\tLoss: 0.415618\tLR: 0.00028385\n",
            "Train Epoch: 8 [47872/123872 (39%)]\tLoss: 0.364704\tLR: 0.00028373\n",
            "Train Epoch: 8 [48128/123872 (39%)]\tLoss: 0.479722\tLR: 0.00028362\n",
            "Train Epoch: 8 [48384/123872 (39%)]\tLoss: 0.342351\tLR: 0.00028350\n",
            "Train Epoch: 8 [48640/123872 (39%)]\tLoss: 0.408067\tLR: 0.00028339\n",
            "Train Epoch: 8 [48640/123872 (39%)]\tLoss: 0.408067\n",
            "Train Epoch: 8 [48896/123872 (39%)]\tLoss: 0.342777\tLR: 0.00028327\n",
            "Train Epoch: 8 [49152/123872 (40%)]\tLoss: 0.363681\tLR: 0.00028315\n",
            "Train Epoch: 8 [49408/123872 (40%)]\tLoss: 0.397118\tLR: 0.00028304\n",
            "Train Epoch: 8 [49664/123872 (40%)]\tLoss: 0.382930\tLR: 0.00028292\n",
            "Train Epoch: 8 [49920/123872 (40%)]\tLoss: 0.404876\tLR: 0.00028281\n",
            "Train Epoch: 8 [50176/123872 (40%)]\tLoss: 0.396464\tLR: 0.00028269\n",
            "Train Epoch: 8 [50432/123872 (41%)]\tLoss: 0.392082\tLR: 0.00028257\n",
            "Train Epoch: 8 [50688/123872 (41%)]\tLoss: 0.360471\tLR: 0.00028246\n",
            "Train Epoch: 8 [50944/123872 (41%)]\tLoss: 0.341755\tLR: 0.00028234\n",
            "Train Epoch: 8 [51200/123872 (41%)]\tLoss: 0.418996\tLR: 0.00028222\n",
            "Train Epoch: 8 [51200/123872 (41%)]\tLoss: 0.418996\n",
            "Train Epoch: 8 [51456/123872 (42%)]\tLoss: 0.411092\tLR: 0.00028211\n",
            "Train Epoch: 8 [51712/123872 (42%)]\tLoss: 0.402079\tLR: 0.00028199\n",
            "Train Epoch: 8 [51968/123872 (42%)]\tLoss: 0.386968\tLR: 0.00028188\n",
            "Train Epoch: 8 [52224/123872 (42%)]\tLoss: 0.382876\tLR: 0.00028176\n",
            "Train Epoch: 8 [52480/123872 (42%)]\tLoss: 0.358709\tLR: 0.00028164\n",
            "Train Epoch: 8 [52736/123872 (43%)]\tLoss: 0.392155\tLR: 0.00028153\n",
            "Train Epoch: 8 [52992/123872 (43%)]\tLoss: 0.438656\tLR: 0.00028141\n",
            "Train Epoch: 8 [53248/123872 (43%)]\tLoss: 0.398318\tLR: 0.00028130\n",
            "Train Epoch: 8 [53504/123872 (43%)]\tLoss: 0.416580\tLR: 0.00028118\n",
            "Train Epoch: 8 [53760/123872 (43%)]\tLoss: 0.388800\tLR: 0.00028106\n",
            "Train Epoch: 8 [53760/123872 (43%)]\tLoss: 0.388800\n",
            "Train Epoch: 8 [54016/123872 (44%)]\tLoss: 0.417752\tLR: 0.00028095\n",
            "Train Epoch: 8 [54272/123872 (44%)]\tLoss: 0.423142\tLR: 0.00028083\n",
            "Train Epoch: 8 [54528/123872 (44%)]\tLoss: 0.401770\tLR: 0.00028072\n",
            "Train Epoch: 8 [54784/123872 (44%)]\tLoss: 0.343971\tLR: 0.00028060\n",
            "Train Epoch: 8 [55040/123872 (44%)]\tLoss: 0.400851\tLR: 0.00028048\n",
            "Train Epoch: 8 [55296/123872 (45%)]\tLoss: 0.452046\tLR: 0.00028037\n",
            "Train Epoch: 8 [55552/123872 (45%)]\tLoss: 0.341505\tLR: 0.00028025\n",
            "Train Epoch: 8 [55808/123872 (45%)]\tLoss: 0.390741\tLR: 0.00028014\n",
            "Train Epoch: 8 [56064/123872 (45%)]\tLoss: 0.386973\tLR: 0.00028002\n",
            "Train Epoch: 8 [56320/123872 (45%)]\tLoss: 0.394301\tLR: 0.00027990\n",
            "Train Epoch: 8 [56320/123872 (45%)]\tLoss: 0.394301\n",
            "Train Epoch: 8 [56576/123872 (46%)]\tLoss: 0.402215\tLR: 0.00027979\n",
            "Train Epoch: 8 [56832/123872 (46%)]\tLoss: 0.406867\tLR: 0.00027967\n",
            "Train Epoch: 8 [57088/123872 (46%)]\tLoss: 0.366840\tLR: 0.00027956\n",
            "Train Epoch: 8 [57344/123872 (46%)]\tLoss: 0.399665\tLR: 0.00027944\n",
            "Train Epoch: 8 [57600/123872 (46%)]\tLoss: 0.412856\tLR: 0.00027932\n",
            "Train Epoch: 8 [57856/123872 (47%)]\tLoss: 0.387882\tLR: 0.00027921\n",
            "Train Epoch: 8 [58112/123872 (47%)]\tLoss: 0.441184\tLR: 0.00027909\n",
            "Train Epoch: 8 [58368/123872 (47%)]\tLoss: 0.433561\tLR: 0.00027898\n",
            "Train Epoch: 8 [58624/123872 (47%)]\tLoss: 0.346132\tLR: 0.00027886\n",
            "Train Epoch: 8 [58880/123872 (48%)]\tLoss: 0.407951\tLR: 0.00027874\n",
            "Train Epoch: 8 [58880/123872 (48%)]\tLoss: 0.407951\n",
            "Train Epoch: 8 [59136/123872 (48%)]\tLoss: 0.402108\tLR: 0.00027863\n",
            "Train Epoch: 8 [59392/123872 (48%)]\tLoss: 0.389847\tLR: 0.00027851\n",
            "Train Epoch: 8 [59648/123872 (48%)]\tLoss: 0.410011\tLR: 0.00027840\n",
            "Train Epoch: 8 [59904/123872 (48%)]\tLoss: 0.373831\tLR: 0.00027828\n",
            "Train Epoch: 8 [60160/123872 (49%)]\tLoss: 0.369816\tLR: 0.00027816\n",
            "Train Epoch: 8 [60416/123872 (49%)]\tLoss: 0.371428\tLR: 0.00027805\n",
            "Train Epoch: 8 [60672/123872 (49%)]\tLoss: 0.349254\tLR: 0.00027793\n",
            "Train Epoch: 8 [60928/123872 (49%)]\tLoss: 0.383108\tLR: 0.00027782\n",
            "Train Epoch: 8 [61184/123872 (49%)]\tLoss: 0.400773\tLR: 0.00027770\n",
            "Train Epoch: 8 [61440/123872 (50%)]\tLoss: 0.399180\tLR: 0.00027759\n",
            "Train Epoch: 8 [61440/123872 (50%)]\tLoss: 0.399180\n",
            "Train Epoch: 8 [61696/123872 (50%)]\tLoss: 0.363212\tLR: 0.00027747\n",
            "Train Epoch: 8 [61952/123872 (50%)]\tLoss: 0.376843\tLR: 0.00027735\n",
            "Train Epoch: 8 [62208/123872 (50%)]\tLoss: 0.433800\tLR: 0.00027724\n",
            "Train Epoch: 8 [62464/123872 (50%)]\tLoss: 0.346457\tLR: 0.00027712\n",
            "Train Epoch: 8 [62720/123872 (51%)]\tLoss: 0.412778\tLR: 0.00027701\n",
            "Train Epoch: 8 [62976/123872 (51%)]\tLoss: 0.328748\tLR: 0.00027689\n",
            "Train Epoch: 8 [63232/123872 (51%)]\tLoss: 0.366899\tLR: 0.00027678\n",
            "Train Epoch: 8 [63488/123872 (51%)]\tLoss: 0.432021\tLR: 0.00027666\n",
            "Train Epoch: 8 [63744/123872 (51%)]\tLoss: 0.359727\tLR: 0.00027654\n",
            "Train Epoch: 8 [64000/123872 (52%)]\tLoss: 0.397893\tLR: 0.00027643\n",
            "Train Epoch: 8 [64000/123872 (52%)]\tLoss: 0.397893\n",
            "Train Epoch: 8 [64256/123872 (52%)]\tLoss: 0.353885\tLR: 0.00027631\n",
            "Train Epoch: 8 [64512/123872 (52%)]\tLoss: 0.379170\tLR: 0.00027620\n",
            "Train Epoch: 8 [64768/123872 (52%)]\tLoss: 0.340263\tLR: 0.00027608\n",
            "Train Epoch: 8 [65024/123872 (52%)]\tLoss: 0.433716\tLR: 0.00027597\n",
            "Train Epoch: 8 [65280/123872 (53%)]\tLoss: 0.359825\tLR: 0.00027585\n",
            "Train Epoch: 8 [65536/123872 (53%)]\tLoss: 0.384146\tLR: 0.00027574\n",
            "Train Epoch: 8 [65792/123872 (53%)]\tLoss: 0.392589\tLR: 0.00027562\n",
            "Train Epoch: 8 [66048/123872 (53%)]\tLoss: 0.411684\tLR: 0.00027550\n",
            "Train Epoch: 8 [66304/123872 (54%)]\tLoss: 0.381907\tLR: 0.00027539\n",
            "Train Epoch: 8 [66560/123872 (54%)]\tLoss: 0.405619\tLR: 0.00027527\n",
            "Train Epoch: 8 [66560/123872 (54%)]\tLoss: 0.405619\n",
            "Train Epoch: 8 [66816/123872 (54%)]\tLoss: 0.455275\tLR: 0.00027516\n",
            "Train Epoch: 8 [67072/123872 (54%)]\tLoss: 0.397393\tLR: 0.00027504\n",
            "Train Epoch: 8 [67328/123872 (54%)]\tLoss: 0.393426\tLR: 0.00027493\n",
            "Train Epoch: 8 [67584/123872 (55%)]\tLoss: 0.345414\tLR: 0.00027481\n",
            "Train Epoch: 8 [67840/123872 (55%)]\tLoss: 0.337340\tLR: 0.00027470\n",
            "Train Epoch: 8 [68096/123872 (55%)]\tLoss: 0.389127\tLR: 0.00027458\n",
            "Train Epoch: 8 [68352/123872 (55%)]\tLoss: 0.416965\tLR: 0.00027446\n",
            "Train Epoch: 8 [68608/123872 (55%)]\tLoss: 0.402093\tLR: 0.00027435\n",
            "Train Epoch: 8 [68864/123872 (56%)]\tLoss: 0.393659\tLR: 0.00027423\n",
            "Train Epoch: 8 [69120/123872 (56%)]\tLoss: 0.396218\tLR: 0.00027412\n",
            "Train Epoch: 8 [69120/123872 (56%)]\tLoss: 0.396218\n",
            "Train Epoch: 8 [69376/123872 (56%)]\tLoss: 0.372155\tLR: 0.00027400\n",
            "Train Epoch: 8 [69632/123872 (56%)]\tLoss: 0.373728\tLR: 0.00027389\n",
            "Train Epoch: 8 [69888/123872 (56%)]\tLoss: 0.425700\tLR: 0.00027377\n",
            "Train Epoch: 8 [70144/123872 (57%)]\tLoss: 0.398199\tLR: 0.00027366\n",
            "Train Epoch: 8 [70400/123872 (57%)]\tLoss: 0.391570\tLR: 0.00027354\n",
            "Train Epoch: 8 [70656/123872 (57%)]\tLoss: 0.327704\tLR: 0.00027343\n",
            "Train Epoch: 8 [70912/123872 (57%)]\tLoss: 0.326981\tLR: 0.00027331\n",
            "Train Epoch: 8 [71168/123872 (57%)]\tLoss: 0.375239\tLR: 0.00027319\n",
            "Train Epoch: 8 [71424/123872 (58%)]\tLoss: 0.376512\tLR: 0.00027308\n",
            "Train Epoch: 8 [71680/123872 (58%)]\tLoss: 0.392413\tLR: 0.00027296\n",
            "Train Epoch: 8 [71680/123872 (58%)]\tLoss: 0.392413\n",
            "Train Epoch: 8 [71936/123872 (58%)]\tLoss: 0.435338\tLR: 0.00027285\n",
            "Train Epoch: 8 [72192/123872 (58%)]\tLoss: 0.369758\tLR: 0.00027273\n",
            "Train Epoch: 8 [72448/123872 (58%)]\tLoss: 0.477453\tLR: 0.00027262\n",
            "Train Epoch: 8 [72704/123872 (59%)]\tLoss: 0.422539\tLR: 0.00027250\n",
            "Train Epoch: 8 [72960/123872 (59%)]\tLoss: 0.457184\tLR: 0.00027239\n",
            "Train Epoch: 8 [73216/123872 (59%)]\tLoss: 0.410344\tLR: 0.00027227\n",
            "Train Epoch: 8 [73472/123872 (59%)]\tLoss: 0.446776\tLR: 0.00027216\n",
            "Train Epoch: 8 [73728/123872 (60%)]\tLoss: 0.399933\tLR: 0.00027204\n",
            "Train Epoch: 8 [73984/123872 (60%)]\tLoss: 0.377148\tLR: 0.00027193\n",
            "Train Epoch: 8 [74240/123872 (60%)]\tLoss: 0.390078\tLR: 0.00027181\n",
            "Train Epoch: 8 [74240/123872 (60%)]\tLoss: 0.390078\n",
            "Train Epoch: 8 [74496/123872 (60%)]\tLoss: 0.393109\tLR: 0.00027170\n",
            "Train Epoch: 8 [74752/123872 (60%)]\tLoss: 0.395260\tLR: 0.00027158\n",
            "Train Epoch: 8 [75008/123872 (61%)]\tLoss: 0.418130\tLR: 0.00027147\n",
            "Train Epoch: 8 [75264/123872 (61%)]\tLoss: 0.382761\tLR: 0.00027135\n",
            "Train Epoch: 8 [75520/123872 (61%)]\tLoss: 0.457143\tLR: 0.00027123\n",
            "Train Epoch: 8 [75776/123872 (61%)]\tLoss: 0.375686\tLR: 0.00027112\n",
            "Train Epoch: 8 [76032/123872 (61%)]\tLoss: 0.408561\tLR: 0.00027100\n",
            "Train Epoch: 8 [76288/123872 (62%)]\tLoss: 0.386861\tLR: 0.00027089\n",
            "Train Epoch: 8 [76544/123872 (62%)]\tLoss: 0.392179\tLR: 0.00027077\n",
            "Train Epoch: 8 [76800/123872 (62%)]\tLoss: 0.436064\tLR: 0.00027066\n",
            "Train Epoch: 8 [76800/123872 (62%)]\tLoss: 0.436064\n",
            "Train Epoch: 8 [77056/123872 (62%)]\tLoss: 0.409487\tLR: 0.00027054\n",
            "Train Epoch: 8 [77312/123872 (62%)]\tLoss: 0.396085\tLR: 0.00027043\n",
            "Train Epoch: 8 [77568/123872 (63%)]\tLoss: 0.316022\tLR: 0.00027031\n",
            "Train Epoch: 8 [77824/123872 (63%)]\tLoss: 0.397895\tLR: 0.00027020\n",
            "Train Epoch: 8 [78080/123872 (63%)]\tLoss: 0.455228\tLR: 0.00027008\n",
            "Train Epoch: 8 [78336/123872 (63%)]\tLoss: 0.345279\tLR: 0.00026997\n",
            "Train Epoch: 8 [78592/123872 (63%)]\tLoss: 0.406155\tLR: 0.00026985\n",
            "Train Epoch: 8 [78848/123872 (64%)]\tLoss: 0.427703\tLR: 0.00026974\n",
            "Train Epoch: 8 [79104/123872 (64%)]\tLoss: 0.341686\tLR: 0.00026962\n",
            "Train Epoch: 8 [79360/123872 (64%)]\tLoss: 0.348375\tLR: 0.00026951\n",
            "Train Epoch: 8 [79360/123872 (64%)]\tLoss: 0.348375\n",
            "Train Epoch: 8 [79616/123872 (64%)]\tLoss: 0.384552\tLR: 0.00026939\n",
            "Train Epoch: 8 [79872/123872 (64%)]\tLoss: 0.386176\tLR: 0.00026928\n",
            "Train Epoch: 8 [80128/123872 (65%)]\tLoss: 0.334812\tLR: 0.00026916\n",
            "Train Epoch: 8 [80384/123872 (65%)]\tLoss: 0.409401\tLR: 0.00026905\n",
            "Train Epoch: 8 [80640/123872 (65%)]\tLoss: 0.420231\tLR: 0.00026893\n",
            "Train Epoch: 8 [80896/123872 (65%)]\tLoss: 0.429526\tLR: 0.00026882\n",
            "Train Epoch: 8 [81152/123872 (65%)]\tLoss: 0.381567\tLR: 0.00026870\n",
            "Train Epoch: 8 [81408/123872 (66%)]\tLoss: 0.385636\tLR: 0.00026859\n",
            "Train Epoch: 8 [81664/123872 (66%)]\tLoss: 0.356231\tLR: 0.00026847\n",
            "Train Epoch: 8 [81920/123872 (66%)]\tLoss: 0.375771\tLR: 0.00026836\n",
            "Train Epoch: 8 [81920/123872 (66%)]\tLoss: 0.375771\n",
            "Train Epoch: 8 [82176/123872 (66%)]\tLoss: 0.367291\tLR: 0.00026824\n",
            "Train Epoch: 8 [82432/123872 (67%)]\tLoss: 0.409075\tLR: 0.00026813\n",
            "Train Epoch: 8 [82688/123872 (67%)]\tLoss: 0.417364\tLR: 0.00026801\n",
            "Train Epoch: 8 [82944/123872 (67%)]\tLoss: 0.357686\tLR: 0.00026790\n",
            "Train Epoch: 8 [83200/123872 (67%)]\tLoss: 0.413038\tLR: 0.00026778\n",
            "Train Epoch: 8 [83456/123872 (67%)]\tLoss: 0.380620\tLR: 0.00026767\n",
            "Train Epoch: 8 [83712/123872 (68%)]\tLoss: 0.491257\tLR: 0.00026756\n",
            "Train Epoch: 8 [83968/123872 (68%)]\tLoss: 0.407455\tLR: 0.00026744\n",
            "Train Epoch: 8 [84224/123872 (68%)]\tLoss: 0.431295\tLR: 0.00026733\n",
            "Train Epoch: 8 [84480/123872 (68%)]\tLoss: 0.376726\tLR: 0.00026721\n",
            "Train Epoch: 8 [84480/123872 (68%)]\tLoss: 0.376726\n",
            "Train Epoch: 8 [84736/123872 (68%)]\tLoss: 0.397978\tLR: 0.00026710\n",
            "Train Epoch: 8 [84992/123872 (69%)]\tLoss: 0.444220\tLR: 0.00026698\n",
            "Train Epoch: 8 [85248/123872 (69%)]\tLoss: 0.379713\tLR: 0.00026687\n",
            "Train Epoch: 8 [85504/123872 (69%)]\tLoss: 0.373068\tLR: 0.00026675\n",
            "Train Epoch: 8 [85760/123872 (69%)]\tLoss: 0.350111\tLR: 0.00026664\n",
            "Train Epoch: 8 [86016/123872 (69%)]\tLoss: 0.388809\tLR: 0.00026652\n",
            "Train Epoch: 8 [86272/123872 (70%)]\tLoss: 0.384973\tLR: 0.00026641\n",
            "Train Epoch: 8 [86528/123872 (70%)]\tLoss: 0.401460\tLR: 0.00026629\n",
            "Train Epoch: 8 [86784/123872 (70%)]\tLoss: 0.349984\tLR: 0.00026618\n",
            "Train Epoch: 8 [87040/123872 (70%)]\tLoss: 0.420321\tLR: 0.00026606\n",
            "Train Epoch: 8 [87040/123872 (70%)]\tLoss: 0.420321\n",
            "Train Epoch: 8 [87296/123872 (70%)]\tLoss: 0.412449\tLR: 0.00026595\n",
            "Train Epoch: 8 [87552/123872 (71%)]\tLoss: 0.366515\tLR: 0.00026583\n",
            "Train Epoch: 8 [87808/123872 (71%)]\tLoss: 0.357355\tLR: 0.00026572\n",
            "Train Epoch: 8 [88064/123872 (71%)]\tLoss: 0.367058\tLR: 0.00026561\n",
            "Train Epoch: 8 [88320/123872 (71%)]\tLoss: 0.444395\tLR: 0.00026549\n",
            "Train Epoch: 8 [88576/123872 (71%)]\tLoss: 0.448067\tLR: 0.00026538\n",
            "Train Epoch: 8 [88832/123872 (72%)]\tLoss: 0.352979\tLR: 0.00026526\n",
            "Train Epoch: 8 [89088/123872 (72%)]\tLoss: 0.417125\tLR: 0.00026515\n",
            "Train Epoch: 8 [89344/123872 (72%)]\tLoss: 0.405564\tLR: 0.00026503\n",
            "Train Epoch: 8 [89600/123872 (72%)]\tLoss: 0.332295\tLR: 0.00026492\n",
            "Train Epoch: 8 [89600/123872 (72%)]\tLoss: 0.332295\n",
            "Train Epoch: 8 [89856/123872 (73%)]\tLoss: 0.379973\tLR: 0.00026480\n",
            "Train Epoch: 8 [90112/123872 (73%)]\tLoss: 0.376432\tLR: 0.00026469\n",
            "Train Epoch: 8 [90368/123872 (73%)]\tLoss: 0.378830\tLR: 0.00026457\n",
            "Train Epoch: 8 [90624/123872 (73%)]\tLoss: 0.379598\tLR: 0.00026446\n",
            "Train Epoch: 8 [90880/123872 (73%)]\tLoss: 0.354994\tLR: 0.00026435\n",
            "Train Epoch: 8 [91136/123872 (74%)]\tLoss: 0.418286\tLR: 0.00026423\n",
            "Train Epoch: 8 [91392/123872 (74%)]\tLoss: 0.383027\tLR: 0.00026412\n",
            "Train Epoch: 8 [91648/123872 (74%)]\tLoss: 0.376496\tLR: 0.00026400\n",
            "Train Epoch: 8 [91904/123872 (74%)]\tLoss: 0.351150\tLR: 0.00026389\n",
            "Train Epoch: 8 [92160/123872 (74%)]\tLoss: 0.410821\tLR: 0.00026377\n",
            "Train Epoch: 8 [92160/123872 (74%)]\tLoss: 0.410821\n",
            "Train Epoch: 8 [92416/123872 (75%)]\tLoss: 0.370272\tLR: 0.00026366\n",
            "Train Epoch: 8 [92672/123872 (75%)]\tLoss: 0.337573\tLR: 0.00026354\n",
            "Train Epoch: 8 [92928/123872 (75%)]\tLoss: 0.370616\tLR: 0.00026343\n",
            "Train Epoch: 8 [93184/123872 (75%)]\tLoss: 0.390789\tLR: 0.00026332\n",
            "Train Epoch: 8 [93440/123872 (75%)]\tLoss: 0.391569\tLR: 0.00026320\n",
            "Train Epoch: 8 [93696/123872 (76%)]\tLoss: 0.407220\tLR: 0.00026309\n",
            "Train Epoch: 8 [93952/123872 (76%)]\tLoss: 0.344154\tLR: 0.00026297\n",
            "Train Epoch: 8 [94208/123872 (76%)]\tLoss: 0.356144\tLR: 0.00026286\n",
            "Train Epoch: 8 [94464/123872 (76%)]\tLoss: 0.412480\tLR: 0.00026274\n",
            "Train Epoch: 8 [94720/123872 (76%)]\tLoss: 0.401520\tLR: 0.00026263\n",
            "Train Epoch: 8 [94720/123872 (76%)]\tLoss: 0.401520\n",
            "Train Epoch: 8 [94976/123872 (77%)]\tLoss: 0.381026\tLR: 0.00026252\n",
            "Train Epoch: 8 [95232/123872 (77%)]\tLoss: 0.371903\tLR: 0.00026240\n",
            "Train Epoch: 8 [95488/123872 (77%)]\tLoss: 0.368349\tLR: 0.00026229\n",
            "Train Epoch: 8 [95744/123872 (77%)]\tLoss: 0.392007\tLR: 0.00026217\n",
            "Train Epoch: 8 [96000/123872 (77%)]\tLoss: 0.401915\tLR: 0.00026206\n",
            "Train Epoch: 8 [96256/123872 (78%)]\tLoss: 0.386803\tLR: 0.00026194\n",
            "Train Epoch: 8 [96512/123872 (78%)]\tLoss: 0.364845\tLR: 0.00026183\n",
            "Train Epoch: 8 [96768/123872 (78%)]\tLoss: 0.444354\tLR: 0.00026172\n",
            "Train Epoch: 8 [97024/123872 (78%)]\tLoss: 0.356577\tLR: 0.00026160\n",
            "Train Epoch: 8 [97280/123872 (79%)]\tLoss: 0.371588\tLR: 0.00026149\n",
            "Train Epoch: 8 [97280/123872 (79%)]\tLoss: 0.371588\n",
            "Train Epoch: 8 [97536/123872 (79%)]\tLoss: 0.389261\tLR: 0.00026137\n",
            "Train Epoch: 8 [97792/123872 (79%)]\tLoss: 0.377863\tLR: 0.00026126\n",
            "Train Epoch: 8 [98048/123872 (79%)]\tLoss: 0.465680\tLR: 0.00026115\n",
            "Train Epoch: 8 [98304/123872 (79%)]\tLoss: 0.397570\tLR: 0.00026103\n",
            "Train Epoch: 8 [98560/123872 (80%)]\tLoss: 0.406549\tLR: 0.00026092\n",
            "Train Epoch: 8 [98816/123872 (80%)]\tLoss: 0.329952\tLR: 0.00026080\n",
            "Train Epoch: 8 [99072/123872 (80%)]\tLoss: 0.412803\tLR: 0.00026069\n",
            "Train Epoch: 8 [99328/123872 (80%)]\tLoss: 0.373060\tLR: 0.00026057\n",
            "Train Epoch: 8 [99584/123872 (80%)]\tLoss: 0.413682\tLR: 0.00026046\n",
            "Train Epoch: 8 [99840/123872 (81%)]\tLoss: 0.411821\tLR: 0.00026035\n",
            "Train Epoch: 8 [99840/123872 (81%)]\tLoss: 0.411821\n",
            "Train Epoch: 8 [100096/123872 (81%)]\tLoss: 0.362063\tLR: 0.00026023\n",
            "Train Epoch: 8 [100352/123872 (81%)]\tLoss: 0.379125\tLR: 0.00026012\n",
            "Train Epoch: 8 [100608/123872 (81%)]\tLoss: 0.397144\tLR: 0.00026000\n",
            "Train Epoch: 8 [100864/123872 (81%)]\tLoss: 0.361810\tLR: 0.00025989\n",
            "Train Epoch: 8 [101120/123872 (82%)]\tLoss: 0.376612\tLR: 0.00025978\n",
            "Train Epoch: 8 [101376/123872 (82%)]\tLoss: 0.356060\tLR: 0.00025966\n",
            "Train Epoch: 8 [101632/123872 (82%)]\tLoss: 0.341742\tLR: 0.00025955\n",
            "Train Epoch: 8 [101888/123872 (82%)]\tLoss: 0.394293\tLR: 0.00025944\n",
            "Train Epoch: 8 [102144/123872 (82%)]\tLoss: 0.404611\tLR: 0.00025932\n",
            "Train Epoch: 8 [102400/123872 (83%)]\tLoss: 0.391110\tLR: 0.00025921\n",
            "Train Epoch: 8 [102400/123872 (83%)]\tLoss: 0.391110\n",
            "Train Epoch: 8 [102656/123872 (83%)]\tLoss: 0.377583\tLR: 0.00025909\n",
            "Train Epoch: 8 [102912/123872 (83%)]\tLoss: 0.399255\tLR: 0.00025898\n",
            "Train Epoch: 8 [103168/123872 (83%)]\tLoss: 0.428741\tLR: 0.00025887\n",
            "Train Epoch: 8 [103424/123872 (83%)]\tLoss: 0.414603\tLR: 0.00025875\n",
            "Train Epoch: 8 [103680/123872 (84%)]\tLoss: 0.403201\tLR: 0.00025864\n",
            "Train Epoch: 8 [103936/123872 (84%)]\tLoss: 0.388096\tLR: 0.00025852\n",
            "Train Epoch: 8 [104192/123872 (84%)]\tLoss: 0.397637\tLR: 0.00025841\n",
            "Train Epoch: 8 [104448/123872 (84%)]\tLoss: 0.333999\tLR: 0.00025830\n",
            "Train Epoch: 8 [104704/123872 (85%)]\tLoss: 0.302878\tLR: 0.00025818\n",
            "Train Epoch: 8 [104960/123872 (85%)]\tLoss: 0.403890\tLR: 0.00025807\n",
            "Train Epoch: 8 [104960/123872 (85%)]\tLoss: 0.403890\n",
            "Train Epoch: 8 [105216/123872 (85%)]\tLoss: 0.387618\tLR: 0.00025796\n",
            "Train Epoch: 8 [105472/123872 (85%)]\tLoss: 0.351131\tLR: 0.00025784\n",
            "Train Epoch: 8 [105728/123872 (85%)]\tLoss: 0.464811\tLR: 0.00025773\n",
            "Train Epoch: 8 [105984/123872 (86%)]\tLoss: 0.493338\tLR: 0.00025761\n",
            "Train Epoch: 8 [106240/123872 (86%)]\tLoss: 0.374927\tLR: 0.00025750\n",
            "Train Epoch: 8 [106496/123872 (86%)]\tLoss: 0.351169\tLR: 0.00025739\n",
            "Train Epoch: 8 [106752/123872 (86%)]\tLoss: 0.454487\tLR: 0.00025727\n",
            "Train Epoch: 8 [107008/123872 (86%)]\tLoss: 0.371125\tLR: 0.00025716\n",
            "Train Epoch: 8 [107264/123872 (87%)]\tLoss: 0.332481\tLR: 0.00025705\n",
            "Train Epoch: 8 [107520/123872 (87%)]\tLoss: 0.346141\tLR: 0.00025693\n",
            "Train Epoch: 8 [107520/123872 (87%)]\tLoss: 0.346141\n",
            "Train Epoch: 8 [107776/123872 (87%)]\tLoss: 0.388252\tLR: 0.00025682\n",
            "Train Epoch: 8 [108032/123872 (87%)]\tLoss: 0.404001\tLR: 0.00025671\n",
            "Train Epoch: 8 [108288/123872 (87%)]\tLoss: 0.425040\tLR: 0.00025659\n",
            "Train Epoch: 8 [108544/123872 (88%)]\tLoss: 0.424283\tLR: 0.00025648\n",
            "Train Epoch: 8 [108800/123872 (88%)]\tLoss: 0.399721\tLR: 0.00025636\n",
            "Train Epoch: 8 [109056/123872 (88%)]\tLoss: 0.409496\tLR: 0.00025625\n",
            "Train Epoch: 8 [109312/123872 (88%)]\tLoss: 0.405869\tLR: 0.00025614\n",
            "Train Epoch: 8 [109568/123872 (88%)]\tLoss: 0.439915\tLR: 0.00025602\n",
            "Train Epoch: 8 [109824/123872 (89%)]\tLoss: 0.355048\tLR: 0.00025591\n",
            "Train Epoch: 8 [110080/123872 (89%)]\tLoss: 0.392260\tLR: 0.00025580\n",
            "Train Epoch: 8 [110080/123872 (89%)]\tLoss: 0.392260\n",
            "Train Epoch: 8 [110336/123872 (89%)]\tLoss: 0.461459\tLR: 0.00025568\n",
            "Train Epoch: 8 [110592/123872 (89%)]\tLoss: 0.435683\tLR: 0.00025557\n",
            "Train Epoch: 8 [110848/123872 (89%)]\tLoss: 0.402984\tLR: 0.00025546\n",
            "Train Epoch: 8 [111104/123872 (90%)]\tLoss: 0.463425\tLR: 0.00025534\n",
            "Train Epoch: 8 [111360/123872 (90%)]\tLoss: 0.408680\tLR: 0.00025523\n",
            "Train Epoch: 8 [111616/123872 (90%)]\tLoss: 0.412661\tLR: 0.00025512\n",
            "Train Epoch: 8 [111872/123872 (90%)]\tLoss: 0.438039\tLR: 0.00025500\n",
            "Train Epoch: 8 [112128/123872 (90%)]\tLoss: 0.377532\tLR: 0.00025489\n",
            "Train Epoch: 8 [112384/123872 (91%)]\tLoss: 0.367335\tLR: 0.00025478\n",
            "Train Epoch: 8 [112640/123872 (91%)]\tLoss: 0.409703\tLR: 0.00025466\n",
            "Train Epoch: 8 [112640/123872 (91%)]\tLoss: 0.409703\n",
            "Train Epoch: 8 [112896/123872 (91%)]\tLoss: 0.381730\tLR: 0.00025455\n",
            "Train Epoch: 8 [113152/123872 (91%)]\tLoss: 0.345376\tLR: 0.00025444\n",
            "Train Epoch: 8 [113408/123872 (92%)]\tLoss: 0.342164\tLR: 0.00025432\n",
            "Train Epoch: 8 [113664/123872 (92%)]\tLoss: 0.356020\tLR: 0.00025421\n",
            "Train Epoch: 8 [113920/123872 (92%)]\tLoss: 0.390517\tLR: 0.00025410\n",
            "Train Epoch: 8 [114176/123872 (92%)]\tLoss: 0.423367\tLR: 0.00025398\n",
            "Train Epoch: 8 [114432/123872 (92%)]\tLoss: 0.379579\tLR: 0.00025387\n",
            "Train Epoch: 8 [114688/123872 (93%)]\tLoss: 0.486120\tLR: 0.00025376\n",
            "Train Epoch: 8 [114944/123872 (93%)]\tLoss: 0.349445\tLR: 0.00025364\n",
            "Train Epoch: 8 [115200/123872 (93%)]\tLoss: 0.453029\tLR: 0.00025353\n",
            "Train Epoch: 8 [115200/123872 (93%)]\tLoss: 0.453029\n",
            "Train Epoch: 8 [115456/123872 (93%)]\tLoss: 0.341920\tLR: 0.00025342\n",
            "Train Epoch: 8 [115712/123872 (93%)]\tLoss: 0.422727\tLR: 0.00025330\n",
            "Train Epoch: 8 [115968/123872 (94%)]\tLoss: 0.383594\tLR: 0.00025319\n",
            "Train Epoch: 8 [116224/123872 (94%)]\tLoss: 0.347807\tLR: 0.00025308\n",
            "Train Epoch: 8 [116480/123872 (94%)]\tLoss: 0.401354\tLR: 0.00025297\n",
            "Train Epoch: 8 [116736/123872 (94%)]\tLoss: 0.365052\tLR: 0.00025285\n",
            "Train Epoch: 8 [116992/123872 (94%)]\tLoss: 0.403314\tLR: 0.00025274\n",
            "Train Epoch: 8 [117248/123872 (95%)]\tLoss: 0.482451\tLR: 0.00025263\n",
            "Train Epoch: 8 [117504/123872 (95%)]\tLoss: 0.370699\tLR: 0.00025251\n",
            "Train Epoch: 8 [117760/123872 (95%)]\tLoss: 0.366779\tLR: 0.00025240\n",
            "Train Epoch: 8 [117760/123872 (95%)]\tLoss: 0.366779\n",
            "Train Epoch: 8 [118016/123872 (95%)]\tLoss: 0.431662\tLR: 0.00025229\n",
            "Train Epoch: 8 [118272/123872 (95%)]\tLoss: 0.415111\tLR: 0.00025217\n",
            "Train Epoch: 8 [118528/123872 (96%)]\tLoss: 0.427226\tLR: 0.00025206\n",
            "Train Epoch: 8 [118784/123872 (96%)]\tLoss: 0.365596\tLR: 0.00025195\n",
            "Train Epoch: 8 [119040/123872 (96%)]\tLoss: 0.463147\tLR: 0.00025183\n",
            "Train Epoch: 8 [119296/123872 (96%)]\tLoss: 0.343528\tLR: 0.00025172\n",
            "Train Epoch: 8 [119552/123872 (96%)]\tLoss: 0.370659\tLR: 0.00025161\n",
            "Train Epoch: 8 [119808/123872 (97%)]\tLoss: 0.353874\tLR: 0.00025150\n",
            "Train Epoch: 8 [120064/123872 (97%)]\tLoss: 0.344036\tLR: 0.00025138\n",
            "Train Epoch: 8 [120320/123872 (97%)]\tLoss: 0.420064\tLR: 0.00025127\n",
            "Train Epoch: 8 [120320/123872 (97%)]\tLoss: 0.420064\n",
            "Train Epoch: 8 [120576/123872 (97%)]\tLoss: 0.427624\tLR: 0.00025116\n",
            "Train Epoch: 8 [120832/123872 (98%)]\tLoss: 0.340947\tLR: 0.00025104\n",
            "Train Epoch: 8 [121088/123872 (98%)]\tLoss: 0.366472\tLR: 0.00025093\n",
            "Train Epoch: 8 [121344/123872 (98%)]\tLoss: 0.457180\tLR: 0.00025082\n",
            "Train Epoch: 8 [121600/123872 (98%)]\tLoss: 0.351885\tLR: 0.00025071\n",
            "Train Epoch: 8 [121856/123872 (98%)]\tLoss: 0.378160\tLR: 0.00025059\n",
            "Train Epoch: 8 [122112/123872 (99%)]\tLoss: 0.366996\tLR: 0.00025048\n",
            "Train Epoch: 8 [122368/123872 (99%)]\tLoss: 0.363645\tLR: 0.00025037\n",
            "Train Epoch: 8 [122624/123872 (99%)]\tLoss: 0.335293\tLR: 0.00025026\n",
            "Train Epoch: 8 [122880/123872 (99%)]\tLoss: 0.439846\tLR: 0.00025014\n",
            "Train Epoch: 8 [122880/123872 (99%)]\tLoss: 0.439846\n",
            "Train Epoch: 8 [123136/123872 (99%)]\tLoss: 0.497854\tLR: 0.00025003\n",
            "Train Epoch: 8 [123392/123872 (100%)]\tLoss: 0.408931\tLR: 0.00024992\n",
            "Train Epoch: 8 [108192/123872 (100%)]\tLoss: 0.397810\tLR: 0.00024980\n",
            "\n",
            "Test set: Average loss: 0.0016, Accuracy: 25317/30970 (81.75%)\n",
            "\n",
            "Train Epoch: 9 [0/123872 (0%)]\tLoss: 0.374270\tLR: 0.00024969\n",
            "Train Epoch: 9 [0/123872 (0%)]\tLoss: 0.374270\n",
            "Train Epoch: 9 [256/123872 (0%)]\tLoss: 0.408486\tLR: 0.00024958\n",
            "Train Epoch: 9 [512/123872 (0%)]\tLoss: 0.378988\tLR: 0.00024947\n",
            "Train Epoch: 9 [768/123872 (1%)]\tLoss: 0.369998\tLR: 0.00024935\n",
            "Train Epoch: 9 [1024/123872 (1%)]\tLoss: 0.374834\tLR: 0.00024924\n",
            "Train Epoch: 9 [1280/123872 (1%)]\tLoss: 0.317670\tLR: 0.00024913\n",
            "Train Epoch: 9 [1536/123872 (1%)]\tLoss: 0.403154\tLR: 0.00024902\n",
            "Train Epoch: 9 [1792/123872 (1%)]\tLoss: 0.430428\tLR: 0.00024890\n",
            "Train Epoch: 9 [2048/123872 (2%)]\tLoss: 0.354798\tLR: 0.00024879\n",
            "Train Epoch: 9 [2304/123872 (2%)]\tLoss: 0.339474\tLR: 0.00024868\n",
            "Train Epoch: 9 [2560/123872 (2%)]\tLoss: 0.378464\tLR: 0.00024857\n",
            "Train Epoch: 9 [2560/123872 (2%)]\tLoss: 0.378464\n",
            "Train Epoch: 9 [2816/123872 (2%)]\tLoss: 0.422981\tLR: 0.00024845\n",
            "Train Epoch: 9 [3072/123872 (2%)]\tLoss: 0.425363\tLR: 0.00024834\n",
            "Train Epoch: 9 [3328/123872 (3%)]\tLoss: 0.369946\tLR: 0.00024823\n",
            "Train Epoch: 9 [3584/123872 (3%)]\tLoss: 0.398792\tLR: 0.00024812\n",
            "Train Epoch: 9 [3840/123872 (3%)]\tLoss: 0.380938\tLR: 0.00024800\n",
            "Train Epoch: 9 [4096/123872 (3%)]\tLoss: 0.332690\tLR: 0.00024789\n",
            "Train Epoch: 9 [4352/123872 (4%)]\tLoss: 0.390144\tLR: 0.00024778\n",
            "Train Epoch: 9 [4608/123872 (4%)]\tLoss: 0.363540\tLR: 0.00024767\n",
            "Train Epoch: 9 [4864/123872 (4%)]\tLoss: 0.381617\tLR: 0.00024755\n",
            "Train Epoch: 9 [5120/123872 (4%)]\tLoss: 0.384376\tLR: 0.00024744\n",
            "Train Epoch: 9 [5120/123872 (4%)]\tLoss: 0.384376\n",
            "Train Epoch: 9 [5376/123872 (4%)]\tLoss: 0.377091\tLR: 0.00024733\n",
            "Train Epoch: 9 [5632/123872 (5%)]\tLoss: 0.483160\tLR: 0.00024722\n",
            "Train Epoch: 9 [5888/123872 (5%)]\tLoss: 0.455269\tLR: 0.00024710\n",
            "Train Epoch: 9 [6144/123872 (5%)]\tLoss: 0.400149\tLR: 0.00024699\n",
            "Train Epoch: 9 [6400/123872 (5%)]\tLoss: 0.361434\tLR: 0.00024688\n",
            "Train Epoch: 9 [6656/123872 (5%)]\tLoss: 0.434463\tLR: 0.00024677\n",
            "Train Epoch: 9 [6912/123872 (6%)]\tLoss: 0.349295\tLR: 0.00024666\n",
            "Train Epoch: 9 [7168/123872 (6%)]\tLoss: 0.362809\tLR: 0.00024654\n",
            "Train Epoch: 9 [7424/123872 (6%)]\tLoss: 0.410978\tLR: 0.00024643\n",
            "Train Epoch: 9 [7680/123872 (6%)]\tLoss: 0.373555\tLR: 0.00024632\n",
            "Train Epoch: 9 [7680/123872 (6%)]\tLoss: 0.373555\n",
            "Train Epoch: 9 [7936/123872 (6%)]\tLoss: 0.366211\tLR: 0.00024621\n",
            "Train Epoch: 9 [8192/123872 (7%)]\tLoss: 0.438688\tLR: 0.00024609\n",
            "Train Epoch: 9 [8448/123872 (7%)]\tLoss: 0.378433\tLR: 0.00024598\n",
            "Train Epoch: 9 [8704/123872 (7%)]\tLoss: 0.373654\tLR: 0.00024587\n",
            "Train Epoch: 9 [8960/123872 (7%)]\tLoss: 0.387521\tLR: 0.00024576\n",
            "Train Epoch: 9 [9216/123872 (7%)]\tLoss: 0.382102\tLR: 0.00024565\n",
            "Train Epoch: 9 [9472/123872 (8%)]\tLoss: 0.383469\tLR: 0.00024553\n",
            "Train Epoch: 9 [9728/123872 (8%)]\tLoss: 0.351091\tLR: 0.00024542\n",
            "Train Epoch: 9 [9984/123872 (8%)]\tLoss: 0.393337\tLR: 0.00024531\n",
            "Train Epoch: 9 [10240/123872 (8%)]\tLoss: 0.397361\tLR: 0.00024520\n",
            "Train Epoch: 9 [10240/123872 (8%)]\tLoss: 0.397361\n",
            "Train Epoch: 9 [10496/123872 (8%)]\tLoss: 0.403258\tLR: 0.00024509\n",
            "Train Epoch: 9 [10752/123872 (9%)]\tLoss: 0.360125\tLR: 0.00024497\n",
            "Train Epoch: 9 [11008/123872 (9%)]\tLoss: 0.398355\tLR: 0.00024486\n",
            "Train Epoch: 9 [11264/123872 (9%)]\tLoss: 0.405881\tLR: 0.00024475\n",
            "Train Epoch: 9 [11520/123872 (9%)]\tLoss: 0.341089\tLR: 0.00024464\n",
            "Train Epoch: 9 [11776/123872 (10%)]\tLoss: 0.380079\tLR: 0.00024453\n",
            "Train Epoch: 9 [12032/123872 (10%)]\tLoss: 0.391885\tLR: 0.00024441\n",
            "Train Epoch: 9 [12288/123872 (10%)]\tLoss: 0.431910\tLR: 0.00024430\n",
            "Train Epoch: 9 [12544/123872 (10%)]\tLoss: 0.403419\tLR: 0.00024419\n",
            "Train Epoch: 9 [12800/123872 (10%)]\tLoss: 0.389109\tLR: 0.00024408\n",
            "Train Epoch: 9 [12800/123872 (10%)]\tLoss: 0.389109\n",
            "Train Epoch: 9 [13056/123872 (11%)]\tLoss: 0.355501\tLR: 0.00024397\n",
            "Train Epoch: 9 [13312/123872 (11%)]\tLoss: 0.372297\tLR: 0.00024385\n",
            "Train Epoch: 9 [13568/123872 (11%)]\tLoss: 0.391903\tLR: 0.00024374\n",
            "Train Epoch: 9 [13824/123872 (11%)]\tLoss: 0.387213\tLR: 0.00024363\n",
            "Train Epoch: 9 [14080/123872 (11%)]\tLoss: 0.356303\tLR: 0.00024352\n",
            "Train Epoch: 9 [14336/123872 (12%)]\tLoss: 0.448358\tLR: 0.00024341\n",
            "Train Epoch: 9 [14592/123872 (12%)]\tLoss: 0.361378\tLR: 0.00024330\n",
            "Train Epoch: 9 [14848/123872 (12%)]\tLoss: 0.450827\tLR: 0.00024318\n",
            "Train Epoch: 9 [15104/123872 (12%)]\tLoss: 0.349660\tLR: 0.00024307\n",
            "Train Epoch: 9 [15360/123872 (12%)]\tLoss: 0.374331\tLR: 0.00024296\n",
            "Train Epoch: 9 [15360/123872 (12%)]\tLoss: 0.374331\n",
            "Train Epoch: 9 [15616/123872 (13%)]\tLoss: 0.380871\tLR: 0.00024285\n",
            "Train Epoch: 9 [15872/123872 (13%)]\tLoss: 0.369278\tLR: 0.00024274\n",
            "Train Epoch: 9 [16128/123872 (13%)]\tLoss: 0.395813\tLR: 0.00024262\n",
            "Train Epoch: 9 [16384/123872 (13%)]\tLoss: 0.335951\tLR: 0.00024251\n",
            "Train Epoch: 9 [16640/123872 (13%)]\tLoss: 0.407373\tLR: 0.00024240\n",
            "Train Epoch: 9 [16896/123872 (14%)]\tLoss: 0.355934\tLR: 0.00024229\n",
            "Train Epoch: 9 [17152/123872 (14%)]\tLoss: 0.407259\tLR: 0.00024218\n",
            "Train Epoch: 9 [17408/123872 (14%)]\tLoss: 0.472465\tLR: 0.00024207\n",
            "Train Epoch: 9 [17664/123872 (14%)]\tLoss: 0.325709\tLR: 0.00024196\n",
            "Train Epoch: 9 [17920/123872 (14%)]\tLoss: 0.336414\tLR: 0.00024184\n",
            "Train Epoch: 9 [17920/123872 (14%)]\tLoss: 0.336414\n",
            "Train Epoch: 9 [18176/123872 (15%)]\tLoss: 0.368737\tLR: 0.00024173\n",
            "Train Epoch: 9 [18432/123872 (15%)]\tLoss: 0.363048\tLR: 0.00024162\n",
            "Train Epoch: 9 [18688/123872 (15%)]\tLoss: 0.385099\tLR: 0.00024151\n",
            "Train Epoch: 9 [18944/123872 (15%)]\tLoss: 0.415400\tLR: 0.00024140\n",
            "Train Epoch: 9 [19200/123872 (15%)]\tLoss: 0.394111\tLR: 0.00024129\n",
            "Train Epoch: 9 [19456/123872 (16%)]\tLoss: 0.463261\tLR: 0.00024117\n",
            "Train Epoch: 9 [19712/123872 (16%)]\tLoss: 0.369875\tLR: 0.00024106\n",
            "Train Epoch: 9 [19968/123872 (16%)]\tLoss: 0.313205\tLR: 0.00024095\n",
            "Train Epoch: 9 [20224/123872 (16%)]\tLoss: 0.373209\tLR: 0.00024084\n",
            "Train Epoch: 9 [20480/123872 (17%)]\tLoss: 0.367767\tLR: 0.00024073\n",
            "Train Epoch: 9 [20480/123872 (17%)]\tLoss: 0.367767\n",
            "Train Epoch: 9 [20736/123872 (17%)]\tLoss: 0.362176\tLR: 0.00024062\n",
            "Train Epoch: 9 [20992/123872 (17%)]\tLoss: 0.417230\tLR: 0.00024051\n",
            "Train Epoch: 9 [21248/123872 (17%)]\tLoss: 0.325837\tLR: 0.00024040\n",
            "Train Epoch: 9 [21504/123872 (17%)]\tLoss: 0.354071\tLR: 0.00024028\n",
            "Train Epoch: 9 [21760/123872 (18%)]\tLoss: 0.378652\tLR: 0.00024017\n",
            "Train Epoch: 9 [22016/123872 (18%)]\tLoss: 0.414239\tLR: 0.00024006\n",
            "Train Epoch: 9 [22272/123872 (18%)]\tLoss: 0.405212\tLR: 0.00023995\n",
            "Train Epoch: 9 [22528/123872 (18%)]\tLoss: 0.377869\tLR: 0.00023984\n",
            "Train Epoch: 9 [22784/123872 (18%)]\tLoss: 0.387440\tLR: 0.00023973\n",
            "Train Epoch: 9 [23040/123872 (19%)]\tLoss: 0.385748\tLR: 0.00023962\n",
            "Train Epoch: 9 [23040/123872 (19%)]\tLoss: 0.385748\n",
            "Train Epoch: 9 [23296/123872 (19%)]\tLoss: 0.371993\tLR: 0.00023951\n",
            "Train Epoch: 9 [23552/123872 (19%)]\tLoss: 0.446506\tLR: 0.00023939\n",
            "Train Epoch: 9 [23808/123872 (19%)]\tLoss: 0.365598\tLR: 0.00023928\n",
            "Train Epoch: 9 [24064/123872 (19%)]\tLoss: 0.455657\tLR: 0.00023917\n",
            "Train Epoch: 9 [24320/123872 (20%)]\tLoss: 0.334767\tLR: 0.00023906\n",
            "Train Epoch: 9 [24576/123872 (20%)]\tLoss: 0.335520\tLR: 0.00023895\n",
            "Train Epoch: 9 [24832/123872 (20%)]\tLoss: 0.385641\tLR: 0.00023884\n",
            "Train Epoch: 9 [25088/123872 (20%)]\tLoss: 0.392377\tLR: 0.00023873\n",
            "Train Epoch: 9 [25344/123872 (20%)]\tLoss: 0.394367\tLR: 0.00023862\n",
            "Train Epoch: 9 [25600/123872 (21%)]\tLoss: 0.371205\tLR: 0.00023851\n",
            "Train Epoch: 9 [25600/123872 (21%)]\tLoss: 0.371205\n",
            "Train Epoch: 9 [25856/123872 (21%)]\tLoss: 0.339352\tLR: 0.00023839\n",
            "Train Epoch: 9 [26112/123872 (21%)]\tLoss: 0.462953\tLR: 0.00023828\n",
            "Train Epoch: 9 [26368/123872 (21%)]\tLoss: 0.453929\tLR: 0.00023817\n",
            "Train Epoch: 9 [26624/123872 (21%)]\tLoss: 0.415699\tLR: 0.00023806\n",
            "Train Epoch: 9 [26880/123872 (22%)]\tLoss: 0.381640\tLR: 0.00023795\n",
            "Train Epoch: 9 [27136/123872 (22%)]\tLoss: 0.408307\tLR: 0.00023784\n",
            "Train Epoch: 9 [27392/123872 (22%)]\tLoss: 0.384203\tLR: 0.00023773\n",
            "Train Epoch: 9 [27648/123872 (22%)]\tLoss: 0.410952\tLR: 0.00023762\n",
            "Train Epoch: 9 [27904/123872 (23%)]\tLoss: 0.353271\tLR: 0.00023751\n",
            "Train Epoch: 9 [28160/123872 (23%)]\tLoss: 0.371535\tLR: 0.00023740\n",
            "Train Epoch: 9 [28160/123872 (23%)]\tLoss: 0.371535\n",
            "Train Epoch: 9 [28416/123872 (23%)]\tLoss: 0.440209\tLR: 0.00023729\n",
            "Train Epoch: 9 [28672/123872 (23%)]\tLoss: 0.449373\tLR: 0.00023717\n",
            "Train Epoch: 9 [28928/123872 (23%)]\tLoss: 0.369875\tLR: 0.00023706\n",
            "Train Epoch: 9 [29184/123872 (24%)]\tLoss: 0.369956\tLR: 0.00023695\n",
            "Train Epoch: 9 [29440/123872 (24%)]\tLoss: 0.395017\tLR: 0.00023684\n",
            "Train Epoch: 9 [29696/123872 (24%)]\tLoss: 0.380133\tLR: 0.00023673\n",
            "Train Epoch: 9 [29952/123872 (24%)]\tLoss: 0.406792\tLR: 0.00023662\n",
            "Train Epoch: 9 [30208/123872 (24%)]\tLoss: 0.374394\tLR: 0.00023651\n",
            "Train Epoch: 9 [30464/123872 (25%)]\tLoss: 0.378274\tLR: 0.00023640\n",
            "Train Epoch: 9 [30720/123872 (25%)]\tLoss: 0.364803\tLR: 0.00023629\n",
            "Train Epoch: 9 [30720/123872 (25%)]\tLoss: 0.364803\n",
            "Train Epoch: 9 [30976/123872 (25%)]\tLoss: 0.367761\tLR: 0.00023618\n",
            "Train Epoch: 9 [31232/123872 (25%)]\tLoss: 0.474539\tLR: 0.00023607\n",
            "Train Epoch: 9 [31488/123872 (25%)]\tLoss: 0.393366\tLR: 0.00023596\n",
            "Train Epoch: 9 [31744/123872 (26%)]\tLoss: 0.372435\tLR: 0.00023585\n",
            "Train Epoch: 9 [32000/123872 (26%)]\tLoss: 0.420939\tLR: 0.00023574\n",
            "Train Epoch: 9 [32256/123872 (26%)]\tLoss: 0.386410\tLR: 0.00023562\n",
            "Train Epoch: 9 [32512/123872 (26%)]\tLoss: 0.372024\tLR: 0.00023551\n",
            "Train Epoch: 9 [32768/123872 (26%)]\tLoss: 0.382794\tLR: 0.00023540\n",
            "Train Epoch: 9 [33024/123872 (27%)]\tLoss: 0.392644\tLR: 0.00023529\n",
            "Train Epoch: 9 [33280/123872 (27%)]\tLoss: 0.424224\tLR: 0.00023518\n",
            "Train Epoch: 9 [33280/123872 (27%)]\tLoss: 0.424224\n",
            "Train Epoch: 9 [33536/123872 (27%)]\tLoss: 0.403211\tLR: 0.00023507\n",
            "Train Epoch: 9 [33792/123872 (27%)]\tLoss: 0.360101\tLR: 0.00023496\n",
            "Train Epoch: 9 [34048/123872 (27%)]\tLoss: 0.358485\tLR: 0.00023485\n",
            "Train Epoch: 9 [34304/123872 (28%)]\tLoss: 0.466432\tLR: 0.00023474\n",
            "Train Epoch: 9 [34560/123872 (28%)]\tLoss: 0.410084\tLR: 0.00023463\n",
            "Train Epoch: 9 [34816/123872 (28%)]\tLoss: 0.330184\tLR: 0.00023452\n",
            "Train Epoch: 9 [35072/123872 (28%)]\tLoss: 0.346237\tLR: 0.00023441\n",
            "Train Epoch: 9 [35328/123872 (29%)]\tLoss: 0.386230\tLR: 0.00023430\n",
            "Train Epoch: 9 [35584/123872 (29%)]\tLoss: 0.392554\tLR: 0.00023419\n",
            "Train Epoch: 9 [35840/123872 (29%)]\tLoss: 0.441326\tLR: 0.00023408\n",
            "Train Epoch: 9 [35840/123872 (29%)]\tLoss: 0.441326\n",
            "Train Epoch: 9 [36096/123872 (29%)]\tLoss: 0.395677\tLR: 0.00023397\n",
            "Train Epoch: 9 [36352/123872 (29%)]\tLoss: 0.424720\tLR: 0.00023386\n",
            "Train Epoch: 9 [36608/123872 (30%)]\tLoss: 0.409525\tLR: 0.00023375\n",
            "Train Epoch: 9 [36864/123872 (30%)]\tLoss: 0.361406\tLR: 0.00023364\n",
            "Train Epoch: 9 [37120/123872 (30%)]\tLoss: 0.349632\tLR: 0.00023353\n",
            "Train Epoch: 9 [37376/123872 (30%)]\tLoss: 0.400949\tLR: 0.00023342\n",
            "Train Epoch: 9 [37632/123872 (30%)]\tLoss: 0.366803\tLR: 0.00023331\n",
            "Train Epoch: 9 [37888/123872 (31%)]\tLoss: 0.395853\tLR: 0.00023320\n",
            "Train Epoch: 9 [38144/123872 (31%)]\tLoss: 0.446376\tLR: 0.00023309\n",
            "Train Epoch: 9 [38400/123872 (31%)]\tLoss: 0.393296\tLR: 0.00023298\n",
            "Train Epoch: 9 [38400/123872 (31%)]\tLoss: 0.393296\n",
            "Train Epoch: 9 [38656/123872 (31%)]\tLoss: 0.370789\tLR: 0.00023287\n",
            "Train Epoch: 9 [38912/123872 (31%)]\tLoss: 0.349121\tLR: 0.00023276\n",
            "Train Epoch: 9 [39168/123872 (32%)]\tLoss: 0.434134\tLR: 0.00023265\n",
            "Train Epoch: 9 [39424/123872 (32%)]\tLoss: 0.335659\tLR: 0.00023254\n",
            "Train Epoch: 9 [39680/123872 (32%)]\tLoss: 0.434666\tLR: 0.00023243\n",
            "Train Epoch: 9 [39936/123872 (32%)]\tLoss: 0.449699\tLR: 0.00023232\n",
            "Train Epoch: 9 [40192/123872 (32%)]\tLoss: 0.390868\tLR: 0.00023221\n",
            "Train Epoch: 9 [40448/123872 (33%)]\tLoss: 0.382341\tLR: 0.00023210\n",
            "Train Epoch: 9 [40704/123872 (33%)]\tLoss: 0.346719\tLR: 0.00023199\n",
            "Train Epoch: 9 [40960/123872 (33%)]\tLoss: 0.400884\tLR: 0.00023188\n",
            "Train Epoch: 9 [40960/123872 (33%)]\tLoss: 0.400884\n",
            "Train Epoch: 9 [41216/123872 (33%)]\tLoss: 0.425832\tLR: 0.00023177\n",
            "Train Epoch: 9 [41472/123872 (33%)]\tLoss: 0.330710\tLR: 0.00023166\n",
            "Train Epoch: 9 [41728/123872 (34%)]\tLoss: 0.364205\tLR: 0.00023155\n",
            "Train Epoch: 9 [41984/123872 (34%)]\tLoss: 0.387221\tLR: 0.00023144\n",
            "Train Epoch: 9 [42240/123872 (34%)]\tLoss: 0.331414\tLR: 0.00023133\n",
            "Train Epoch: 9 [42496/123872 (34%)]\tLoss: 0.405389\tLR: 0.00023122\n",
            "Train Epoch: 9 [42752/123872 (35%)]\tLoss: 0.419092\tLR: 0.00023111\n",
            "Train Epoch: 9 [43008/123872 (35%)]\tLoss: 0.376890\tLR: 0.00023100\n",
            "Train Epoch: 9 [43264/123872 (35%)]\tLoss: 0.408275\tLR: 0.00023089\n",
            "Train Epoch: 9 [43520/123872 (35%)]\tLoss: 0.375000\tLR: 0.00023078\n",
            "Train Epoch: 9 [43520/123872 (35%)]\tLoss: 0.375000\n",
            "Train Epoch: 9 [43776/123872 (35%)]\tLoss: 0.346919\tLR: 0.00023067\n",
            "Train Epoch: 9 [44032/123872 (36%)]\tLoss: 0.401035\tLR: 0.00023056\n",
            "Train Epoch: 9 [44288/123872 (36%)]\tLoss: 0.407997\tLR: 0.00023045\n",
            "Train Epoch: 9 [44544/123872 (36%)]\tLoss: 0.378059\tLR: 0.00023034\n",
            "Train Epoch: 9 [44800/123872 (36%)]\tLoss: 0.375409\tLR: 0.00023023\n",
            "Train Epoch: 9 [45056/123872 (36%)]\tLoss: 0.351786\tLR: 0.00023012\n",
            "Train Epoch: 9 [45312/123872 (37%)]\tLoss: 0.387882\tLR: 0.00023001\n",
            "Train Epoch: 9 [45568/123872 (37%)]\tLoss: 0.375389\tLR: 0.00022990\n",
            "Train Epoch: 9 [45824/123872 (37%)]\tLoss: 0.421018\tLR: 0.00022979\n",
            "Train Epoch: 9 [46080/123872 (37%)]\tLoss: 0.435421\tLR: 0.00022968\n",
            "Train Epoch: 9 [46080/123872 (37%)]\tLoss: 0.435421\n",
            "Train Epoch: 9 [46336/123872 (37%)]\tLoss: 0.385075\tLR: 0.00022957\n",
            "Train Epoch: 9 [46592/123872 (38%)]\tLoss: 0.384629\tLR: 0.00022946\n",
            "Train Epoch: 9 [46848/123872 (38%)]\tLoss: 0.350481\tLR: 0.00022935\n",
            "Train Epoch: 9 [47104/123872 (38%)]\tLoss: 0.410876\tLR: 0.00022924\n",
            "Train Epoch: 9 [47360/123872 (38%)]\tLoss: 0.345487\tLR: 0.00022913\n",
            "Train Epoch: 9 [47616/123872 (38%)]\tLoss: 0.344573\tLR: 0.00022902\n",
            "Train Epoch: 9 [47872/123872 (39%)]\tLoss: 0.373761\tLR: 0.00022891\n",
            "Train Epoch: 9 [48128/123872 (39%)]\tLoss: 0.394774\tLR: 0.00022881\n",
            "Train Epoch: 9 [48384/123872 (39%)]\tLoss: 0.347657\tLR: 0.00022870\n",
            "Train Epoch: 9 [48640/123872 (39%)]\tLoss: 0.372798\tLR: 0.00022859\n",
            "Train Epoch: 9 [48640/123872 (39%)]\tLoss: 0.372798\n",
            "Train Epoch: 9 [48896/123872 (39%)]\tLoss: 0.322679\tLR: 0.00022848\n",
            "Train Epoch: 9 [49152/123872 (40%)]\tLoss: 0.377253\tLR: 0.00022837\n",
            "Train Epoch: 9 [49408/123872 (40%)]\tLoss: 0.377845\tLR: 0.00022826\n",
            "Train Epoch: 9 [49664/123872 (40%)]\tLoss: 0.355399\tLR: 0.00022815\n",
            "Train Epoch: 9 [49920/123872 (40%)]\tLoss: 0.416574\tLR: 0.00022804\n",
            "Train Epoch: 9 [50176/123872 (40%)]\tLoss: 0.320784\tLR: 0.00022793\n",
            "Train Epoch: 9 [50432/123872 (41%)]\tLoss: 0.388482\tLR: 0.00022782\n",
            "Train Epoch: 9 [50688/123872 (41%)]\tLoss: 0.361700\tLR: 0.00022771\n",
            "Train Epoch: 9 [50944/123872 (41%)]\tLoss: 0.348030\tLR: 0.00022760\n",
            "Train Epoch: 9 [51200/123872 (41%)]\tLoss: 0.354171\tLR: 0.00022749\n",
            "Train Epoch: 9 [51200/123872 (41%)]\tLoss: 0.354171\n",
            "Train Epoch: 9 [51456/123872 (42%)]\tLoss: 0.345228\tLR: 0.00022738\n",
            "Train Epoch: 9 [51712/123872 (42%)]\tLoss: 0.409473\tLR: 0.00022728\n",
            "Train Epoch: 9 [51968/123872 (42%)]\tLoss: 0.443885\tLR: 0.00022717\n",
            "Train Epoch: 9 [52224/123872 (42%)]\tLoss: 0.347901\tLR: 0.00022706\n",
            "Train Epoch: 9 [52480/123872 (42%)]\tLoss: 0.423384\tLR: 0.00022695\n",
            "Train Epoch: 9 [52736/123872 (43%)]\tLoss: 0.334758\tLR: 0.00022684\n",
            "Train Epoch: 9 [52992/123872 (43%)]\tLoss: 0.415228\tLR: 0.00022673\n",
            "Train Epoch: 9 [53248/123872 (43%)]\tLoss: 0.342464\tLR: 0.00022662\n",
            "Train Epoch: 9 [53504/123872 (43%)]\tLoss: 0.434745\tLR: 0.00022651\n",
            "Train Epoch: 9 [53760/123872 (43%)]\tLoss: 0.381806\tLR: 0.00022640\n",
            "Train Epoch: 9 [53760/123872 (43%)]\tLoss: 0.381806\n",
            "Train Epoch: 9 [54016/123872 (44%)]\tLoss: 0.327578\tLR: 0.00022629\n",
            "Train Epoch: 9 [54272/123872 (44%)]\tLoss: 0.424720\tLR: 0.00022619\n",
            "Train Epoch: 9 [54528/123872 (44%)]\tLoss: 0.343600\tLR: 0.00022608\n",
            "Train Epoch: 9 [54784/123872 (44%)]\tLoss: 0.381839\tLR: 0.00022597\n",
            "Train Epoch: 9 [55040/123872 (44%)]\tLoss: 0.391186\tLR: 0.00022586\n",
            "Train Epoch: 9 [55296/123872 (45%)]\tLoss: 0.403574\tLR: 0.00022575\n",
            "Train Epoch: 9 [55552/123872 (45%)]\tLoss: 0.375813\tLR: 0.00022564\n",
            "Train Epoch: 9 [55808/123872 (45%)]\tLoss: 0.341528\tLR: 0.00022553\n",
            "Train Epoch: 9 [56064/123872 (45%)]\tLoss: 0.408148\tLR: 0.00022542\n",
            "Train Epoch: 9 [56320/123872 (45%)]\tLoss: 0.429772\tLR: 0.00022531\n",
            "Train Epoch: 9 [56320/123872 (45%)]\tLoss: 0.429772\n",
            "Train Epoch: 9 [56576/123872 (46%)]\tLoss: 0.424057\tLR: 0.00022521\n",
            "Train Epoch: 9 [56832/123872 (46%)]\tLoss: 0.390810\tLR: 0.00022510\n",
            "Train Epoch: 9 [57088/123872 (46%)]\tLoss: 0.357277\tLR: 0.00022499\n",
            "Train Epoch: 9 [57344/123872 (46%)]\tLoss: 0.325873\tLR: 0.00022488\n",
            "Train Epoch: 9 [57600/123872 (46%)]\tLoss: 0.391447\tLR: 0.00022477\n",
            "Train Epoch: 9 [57856/123872 (47%)]\tLoss: 0.428385\tLR: 0.00022466\n",
            "Train Epoch: 9 [58112/123872 (47%)]\tLoss: 0.342752\tLR: 0.00022455\n",
            "Train Epoch: 9 [58368/123872 (47%)]\tLoss: 0.428050\tLR: 0.00022444\n",
            "Train Epoch: 9 [58624/123872 (47%)]\tLoss: 0.378747\tLR: 0.00022434\n",
            "Train Epoch: 9 [58880/123872 (48%)]\tLoss: 0.411710\tLR: 0.00022423\n",
            "Train Epoch: 9 [58880/123872 (48%)]\tLoss: 0.411710\n",
            "Train Epoch: 9 [59136/123872 (48%)]\tLoss: 0.370477\tLR: 0.00022412\n",
            "Train Epoch: 9 [59392/123872 (48%)]\tLoss: 0.374454\tLR: 0.00022401\n",
            "Train Epoch: 9 [59648/123872 (48%)]\tLoss: 0.407208\tLR: 0.00022390\n",
            "Train Epoch: 9 [59904/123872 (48%)]\tLoss: 0.375316\tLR: 0.00022379\n",
            "Train Epoch: 9 [60160/123872 (49%)]\tLoss: 0.415565\tLR: 0.00022368\n",
            "Train Epoch: 9 [60416/123872 (49%)]\tLoss: 0.343390\tLR: 0.00022358\n",
            "Train Epoch: 9 [60672/123872 (49%)]\tLoss: 0.358582\tLR: 0.00022347\n",
            "Train Epoch: 9 [60928/123872 (49%)]\tLoss: 0.370972\tLR: 0.00022336\n",
            "Train Epoch: 9 [61184/123872 (49%)]\tLoss: 0.382444\tLR: 0.00022325\n",
            "Train Epoch: 9 [61440/123872 (50%)]\tLoss: 0.344389\tLR: 0.00022314\n",
            "Train Epoch: 9 [61440/123872 (50%)]\tLoss: 0.344389\n",
            "Train Epoch: 9 [61696/123872 (50%)]\tLoss: 0.354869\tLR: 0.00022303\n",
            "Train Epoch: 9 [61952/123872 (50%)]\tLoss: 0.393478\tLR: 0.00022293\n",
            "Train Epoch: 9 [62208/123872 (50%)]\tLoss: 0.421751\tLR: 0.00022282\n",
            "Train Epoch: 9 [62464/123872 (50%)]\tLoss: 0.370571\tLR: 0.00022271\n",
            "Train Epoch: 9 [62720/123872 (51%)]\tLoss: 0.380237\tLR: 0.00022260\n",
            "Train Epoch: 9 [62976/123872 (51%)]\tLoss: 0.325049\tLR: 0.00022249\n",
            "Train Epoch: 9 [63232/123872 (51%)]\tLoss: 0.350744\tLR: 0.00022238\n",
            "Train Epoch: 9 [63488/123872 (51%)]\tLoss: 0.404013\tLR: 0.00022228\n",
            "Train Epoch: 9 [63744/123872 (51%)]\tLoss: 0.459186\tLR: 0.00022217\n",
            "Train Epoch: 9 [64000/123872 (52%)]\tLoss: 0.318000\tLR: 0.00022206\n",
            "Train Epoch: 9 [64000/123872 (52%)]\tLoss: 0.318000\n",
            "Train Epoch: 9 [64256/123872 (52%)]\tLoss: 0.340630\tLR: 0.00022195\n",
            "Train Epoch: 9 [64512/123872 (52%)]\tLoss: 0.411248\tLR: 0.00022184\n",
            "Train Epoch: 9 [64768/123872 (52%)]\tLoss: 0.346188\tLR: 0.00022174\n",
            "Train Epoch: 9 [65024/123872 (52%)]\tLoss: 0.386578\tLR: 0.00022163\n",
            "Train Epoch: 9 [65280/123872 (53%)]\tLoss: 0.391355\tLR: 0.00022152\n",
            "Train Epoch: 9 [65536/123872 (53%)]\tLoss: 0.376821\tLR: 0.00022141\n",
            "Train Epoch: 9 [65792/123872 (53%)]\tLoss: 0.327895\tLR: 0.00022130\n",
            "Train Epoch: 9 [66048/123872 (53%)]\tLoss: 0.404517\tLR: 0.00022120\n",
            "Train Epoch: 9 [66304/123872 (54%)]\tLoss: 0.390477\tLR: 0.00022109\n",
            "Train Epoch: 9 [66560/123872 (54%)]\tLoss: 0.356275\tLR: 0.00022098\n",
            "Train Epoch: 9 [66560/123872 (54%)]\tLoss: 0.356275\n",
            "Train Epoch: 9 [66816/123872 (54%)]\tLoss: 0.353405\tLR: 0.00022087\n",
            "Train Epoch: 9 [67072/123872 (54%)]\tLoss: 0.387744\tLR: 0.00022076\n",
            "Train Epoch: 9 [67328/123872 (54%)]\tLoss: 0.332164\tLR: 0.00022066\n",
            "Train Epoch: 9 [67584/123872 (55%)]\tLoss: 0.358038\tLR: 0.00022055\n",
            "Train Epoch: 9 [67840/123872 (55%)]\tLoss: 0.356878\tLR: 0.00022044\n",
            "Train Epoch: 9 [68096/123872 (55%)]\tLoss: 0.397311\tLR: 0.00022033\n",
            "Train Epoch: 9 [68352/123872 (55%)]\tLoss: 0.390053\tLR: 0.00022022\n",
            "Train Epoch: 9 [68608/123872 (55%)]\tLoss: 0.377491\tLR: 0.00022012\n",
            "Train Epoch: 9 [68864/123872 (56%)]\tLoss: 0.386194\tLR: 0.00022001\n",
            "Train Epoch: 9 [69120/123872 (56%)]\tLoss: 0.458228\tLR: 0.00021990\n",
            "Train Epoch: 9 [69120/123872 (56%)]\tLoss: 0.458228\n",
            "Train Epoch: 9 [69376/123872 (56%)]\tLoss: 0.383804\tLR: 0.00021979\n",
            "Train Epoch: 9 [69632/123872 (56%)]\tLoss: 0.377760\tLR: 0.00021969\n",
            "Train Epoch: 9 [69888/123872 (56%)]\tLoss: 0.344954\tLR: 0.00021958\n",
            "Train Epoch: 9 [70144/123872 (57%)]\tLoss: 0.308883\tLR: 0.00021947\n",
            "Train Epoch: 9 [70400/123872 (57%)]\tLoss: 0.366360\tLR: 0.00021936\n",
            "Train Epoch: 9 [70656/123872 (57%)]\tLoss: 0.423811\tLR: 0.00021925\n",
            "Train Epoch: 9 [70912/123872 (57%)]\tLoss: 0.441542\tLR: 0.00021915\n",
            "Train Epoch: 9 [71168/123872 (57%)]\tLoss: 0.397411\tLR: 0.00021904\n",
            "Train Epoch: 9 [71424/123872 (58%)]\tLoss: 0.387444\tLR: 0.00021893\n",
            "Train Epoch: 9 [71680/123872 (58%)]\tLoss: 0.316756\tLR: 0.00021882\n",
            "Train Epoch: 9 [71680/123872 (58%)]\tLoss: 0.316756\n",
            "Train Epoch: 9 [71936/123872 (58%)]\tLoss: 0.427652\tLR: 0.00021872\n",
            "Train Epoch: 9 [72192/123872 (58%)]\tLoss: 0.343933\tLR: 0.00021861\n",
            "Train Epoch: 9 [72448/123872 (58%)]\tLoss: 0.397323\tLR: 0.00021850\n",
            "Train Epoch: 9 [72704/123872 (59%)]\tLoss: 0.408151\tLR: 0.00021839\n",
            "Train Epoch: 9 [72960/123872 (59%)]\tLoss: 0.371646\tLR: 0.00021829\n",
            "Train Epoch: 9 [73216/123872 (59%)]\tLoss: 0.385929\tLR: 0.00021818\n",
            "Train Epoch: 9 [73472/123872 (59%)]\tLoss: 0.435501\tLR: 0.00021807\n",
            "Train Epoch: 9 [73728/123872 (60%)]\tLoss: 0.346547\tLR: 0.00021796\n",
            "Train Epoch: 9 [73984/123872 (60%)]\tLoss: 0.353529\tLR: 0.00021786\n",
            "Train Epoch: 9 [74240/123872 (60%)]\tLoss: 0.436005\tLR: 0.00021775\n",
            "Train Epoch: 9 [74240/123872 (60%)]\tLoss: 0.436005\n",
            "Train Epoch: 9 [74496/123872 (60%)]\tLoss: 0.327234\tLR: 0.00021764\n",
            "Train Epoch: 9 [74752/123872 (60%)]\tLoss: 0.366428\tLR: 0.00021754\n",
            "Train Epoch: 9 [75008/123872 (61%)]\tLoss: 0.405069\tLR: 0.00021743\n",
            "Train Epoch: 9 [75264/123872 (61%)]\tLoss: 0.376009\tLR: 0.00021732\n",
            "Train Epoch: 9 [75520/123872 (61%)]\tLoss: 0.403006\tLR: 0.00021721\n",
            "Train Epoch: 9 [75776/123872 (61%)]\tLoss: 0.359793\tLR: 0.00021711\n",
            "Train Epoch: 9 [76032/123872 (61%)]\tLoss: 0.391952\tLR: 0.00021700\n",
            "Train Epoch: 9 [76288/123872 (62%)]\tLoss: 0.390624\tLR: 0.00021689\n",
            "Train Epoch: 9 [76544/123872 (62%)]\tLoss: 0.373006\tLR: 0.00021679\n",
            "Train Epoch: 9 [76800/123872 (62%)]\tLoss: 0.351282\tLR: 0.00021668\n",
            "Train Epoch: 9 [76800/123872 (62%)]\tLoss: 0.351282\n",
            "Train Epoch: 9 [77056/123872 (62%)]\tLoss: 0.401565\tLR: 0.00021657\n",
            "Train Epoch: 9 [77312/123872 (62%)]\tLoss: 0.334509\tLR: 0.00021646\n",
            "Train Epoch: 9 [77568/123872 (63%)]\tLoss: 0.378346\tLR: 0.00021636\n",
            "Train Epoch: 9 [77824/123872 (63%)]\tLoss: 0.487956\tLR: 0.00021625\n",
            "Train Epoch: 9 [78080/123872 (63%)]\tLoss: 0.497843\tLR: 0.00021614\n",
            "Train Epoch: 9 [78336/123872 (63%)]\tLoss: 0.367327\tLR: 0.00021604\n",
            "Train Epoch: 9 [78592/123872 (63%)]\tLoss: 0.359936\tLR: 0.00021593\n",
            "Train Epoch: 9 [78848/123872 (64%)]\tLoss: 0.399071\tLR: 0.00021582\n",
            "Train Epoch: 9 [79104/123872 (64%)]\tLoss: 0.336581\tLR: 0.00021571\n",
            "Train Epoch: 9 [79360/123872 (64%)]\tLoss: 0.377534\tLR: 0.00021561\n",
            "Train Epoch: 9 [79360/123872 (64%)]\tLoss: 0.377534\n",
            "Train Epoch: 9 [79616/123872 (64%)]\tLoss: 0.407577\tLR: 0.00021550\n",
            "Train Epoch: 9 [79872/123872 (64%)]\tLoss: 0.367475\tLR: 0.00021539\n",
            "Train Epoch: 9 [80128/123872 (65%)]\tLoss: 0.444267\tLR: 0.00021529\n",
            "Train Epoch: 9 [80384/123872 (65%)]\tLoss: 0.422694\tLR: 0.00021518\n",
            "Train Epoch: 9 [80640/123872 (65%)]\tLoss: 0.364760\tLR: 0.00021507\n",
            "Train Epoch: 9 [80896/123872 (65%)]\tLoss: 0.374294\tLR: 0.00021497\n",
            "Train Epoch: 9 [81152/123872 (65%)]\tLoss: 0.348531\tLR: 0.00021486\n",
            "Train Epoch: 9 [81408/123872 (66%)]\tLoss: 0.357166\tLR: 0.00021475\n",
            "Train Epoch: 9 [81664/123872 (66%)]\tLoss: 0.409333\tLR: 0.00021465\n",
            "Train Epoch: 9 [81920/123872 (66%)]\tLoss: 0.370715\tLR: 0.00021454\n",
            "Train Epoch: 9 [81920/123872 (66%)]\tLoss: 0.370715\n",
            "Train Epoch: 9 [82176/123872 (66%)]\tLoss: 0.370169\tLR: 0.00021443\n",
            "Train Epoch: 9 [82432/123872 (67%)]\tLoss: 0.394122\tLR: 0.00021433\n",
            "Train Epoch: 9 [82688/123872 (67%)]\tLoss: 0.335733\tLR: 0.00021422\n",
            "Train Epoch: 9 [82944/123872 (67%)]\tLoss: 0.393702\tLR: 0.00021411\n",
            "Train Epoch: 9 [83200/123872 (67%)]\tLoss: 0.417045\tLR: 0.00021401\n",
            "Train Epoch: 9 [83456/123872 (67%)]\tLoss: 0.382145\tLR: 0.00021390\n",
            "Train Epoch: 9 [83712/123872 (68%)]\tLoss: 0.388879\tLR: 0.00021379\n",
            "Train Epoch: 9 [83968/123872 (68%)]\tLoss: 0.434205\tLR: 0.00021369\n",
            "Train Epoch: 9 [84224/123872 (68%)]\tLoss: 0.383206\tLR: 0.00021358\n",
            "Train Epoch: 9 [84480/123872 (68%)]\tLoss: 0.324473\tLR: 0.00021347\n",
            "Train Epoch: 9 [84480/123872 (68%)]\tLoss: 0.324473\n",
            "Train Epoch: 9 [84736/123872 (68%)]\tLoss: 0.353131\tLR: 0.00021337\n",
            "Train Epoch: 9 [84992/123872 (69%)]\tLoss: 0.349792\tLR: 0.00021326\n",
            "Train Epoch: 9 [85248/123872 (69%)]\tLoss: 0.405134\tLR: 0.00021316\n",
            "Train Epoch: 9 [85504/123872 (69%)]\tLoss: 0.436739\tLR: 0.00021305\n",
            "Train Epoch: 9 [85760/123872 (69%)]\tLoss: 0.382626\tLR: 0.00021294\n",
            "Train Epoch: 9 [86016/123872 (69%)]\tLoss: 0.423804\tLR: 0.00021284\n",
            "Train Epoch: 9 [86272/123872 (70%)]\tLoss: 0.394502\tLR: 0.00021273\n",
            "Train Epoch: 9 [86528/123872 (70%)]\tLoss: 0.351026\tLR: 0.00021262\n",
            "Train Epoch: 9 [86784/123872 (70%)]\tLoss: 0.366104\tLR: 0.00021252\n",
            "Train Epoch: 9 [87040/123872 (70%)]\tLoss: 0.362120\tLR: 0.00021241\n",
            "Train Epoch: 9 [87040/123872 (70%)]\tLoss: 0.362120\n",
            "Train Epoch: 9 [87296/123872 (70%)]\tLoss: 0.349274\tLR: 0.00021230\n",
            "Train Epoch: 9 [87552/123872 (71%)]\tLoss: 0.292711\tLR: 0.00021220\n",
            "Train Epoch: 9 [87808/123872 (71%)]\tLoss: 0.410408\tLR: 0.00021209\n",
            "Train Epoch: 9 [88064/123872 (71%)]\tLoss: 0.404187\tLR: 0.00021199\n",
            "Train Epoch: 9 [88320/123872 (71%)]\tLoss: 0.364356\tLR: 0.00021188\n",
            "Train Epoch: 9 [88576/123872 (71%)]\tLoss: 0.392551\tLR: 0.00021177\n",
            "Train Epoch: 9 [88832/123872 (72%)]\tLoss: 0.356512\tLR: 0.00021167\n",
            "Train Epoch: 9 [89088/123872 (72%)]\tLoss: 0.435197\tLR: 0.00021156\n",
            "Train Epoch: 9 [89344/123872 (72%)]\tLoss: 0.392575\tLR: 0.00021146\n",
            "Train Epoch: 9 [89600/123872 (72%)]\tLoss: 0.374356\tLR: 0.00021135\n",
            "Train Epoch: 9 [89600/123872 (72%)]\tLoss: 0.374356\n",
            "Train Epoch: 9 [89856/123872 (73%)]\tLoss: 0.347826\tLR: 0.00021124\n",
            "Train Epoch: 9 [90112/123872 (73%)]\tLoss: 0.381903\tLR: 0.00021114\n",
            "Train Epoch: 9 [90368/123872 (73%)]\tLoss: 0.360514\tLR: 0.00021103\n",
            "Train Epoch: 9 [90624/123872 (73%)]\tLoss: 0.332508\tLR: 0.00021093\n",
            "Train Epoch: 9 [90880/123872 (73%)]\tLoss: 0.367615\tLR: 0.00021082\n",
            "Train Epoch: 9 [91136/123872 (74%)]\tLoss: 0.379696\tLR: 0.00021071\n",
            "Train Epoch: 9 [91392/123872 (74%)]\tLoss: 0.356293\tLR: 0.00021061\n",
            "Train Epoch: 9 [91648/123872 (74%)]\tLoss: 0.431513\tLR: 0.00021050\n",
            "Train Epoch: 9 [91904/123872 (74%)]\tLoss: 0.352172\tLR: 0.00021040\n",
            "Train Epoch: 9 [92160/123872 (74%)]\tLoss: 0.341847\tLR: 0.00021029\n",
            "Train Epoch: 9 [92160/123872 (74%)]\tLoss: 0.341847\n",
            "Train Epoch: 9 [92416/123872 (75%)]\tLoss: 0.419224\tLR: 0.00021019\n",
            "Train Epoch: 9 [92672/123872 (75%)]\tLoss: 0.328396\tLR: 0.00021008\n",
            "Train Epoch: 9 [92928/123872 (75%)]\tLoss: 0.374954\tLR: 0.00020997\n",
            "Train Epoch: 9 [93184/123872 (75%)]\tLoss: 0.385818\tLR: 0.00020987\n",
            "Train Epoch: 9 [93440/123872 (75%)]\tLoss: 0.459141\tLR: 0.00020976\n",
            "Train Epoch: 9 [93696/123872 (76%)]\tLoss: 0.377094\tLR: 0.00020966\n",
            "Train Epoch: 9 [93952/123872 (76%)]\tLoss: 0.381876\tLR: 0.00020955\n",
            "Train Epoch: 9 [94208/123872 (76%)]\tLoss: 0.384326\tLR: 0.00020945\n",
            "Train Epoch: 9 [94464/123872 (76%)]\tLoss: 0.350281\tLR: 0.00020934\n",
            "Train Epoch: 9 [94720/123872 (76%)]\tLoss: 0.437678\tLR: 0.00020923\n",
            "Train Epoch: 9 [94720/123872 (76%)]\tLoss: 0.437678\n",
            "Train Epoch: 9 [94976/123872 (77%)]\tLoss: 0.310397\tLR: 0.00020913\n",
            "Train Epoch: 9 [95232/123872 (77%)]\tLoss: 0.409131\tLR: 0.00020902\n",
            "Train Epoch: 9 [95488/123872 (77%)]\tLoss: 0.356221\tLR: 0.00020892\n",
            "Train Epoch: 9 [95744/123872 (77%)]\tLoss: 0.367014\tLR: 0.00020881\n",
            "Train Epoch: 9 [96000/123872 (77%)]\tLoss: 0.366379\tLR: 0.00020871\n",
            "Train Epoch: 9 [96256/123872 (78%)]\tLoss: 0.367841\tLR: 0.00020860\n",
            "Train Epoch: 9 [96512/123872 (78%)]\tLoss: 0.360409\tLR: 0.00020850\n",
            "Train Epoch: 9 [96768/123872 (78%)]\tLoss: 0.381141\tLR: 0.00020839\n",
            "Train Epoch: 9 [97024/123872 (78%)]\tLoss: 0.411858\tLR: 0.00020829\n",
            "Train Epoch: 9 [97280/123872 (79%)]\tLoss: 0.385119\tLR: 0.00020818\n",
            "Train Epoch: 9 [97280/123872 (79%)]\tLoss: 0.385119\n",
            "Train Epoch: 9 [97536/123872 (79%)]\tLoss: 0.379580\tLR: 0.00020807\n",
            "Train Epoch: 9 [97792/123872 (79%)]\tLoss: 0.394660\tLR: 0.00020797\n",
            "Train Epoch: 9 [98048/123872 (79%)]\tLoss: 0.400159\tLR: 0.00020786\n",
            "Train Epoch: 9 [98304/123872 (79%)]\tLoss: 0.385305\tLR: 0.00020776\n",
            "Train Epoch: 9 [98560/123872 (80%)]\tLoss: 0.371982\tLR: 0.00020765\n",
            "Train Epoch: 9 [98816/123872 (80%)]\tLoss: 0.380379\tLR: 0.00020755\n",
            "Train Epoch: 9 [99072/123872 (80%)]\tLoss: 0.385800\tLR: 0.00020744\n",
            "Train Epoch: 9 [99328/123872 (80%)]\tLoss: 0.380521\tLR: 0.00020734\n",
            "Train Epoch: 9 [99584/123872 (80%)]\tLoss: 0.336155\tLR: 0.00020723\n",
            "Train Epoch: 9 [99840/123872 (81%)]\tLoss: 0.359852\tLR: 0.00020713\n",
            "Train Epoch: 9 [99840/123872 (81%)]\tLoss: 0.359852\n",
            "Train Epoch: 9 [100096/123872 (81%)]\tLoss: 0.372860\tLR: 0.00020702\n",
            "Train Epoch: 9 [100352/123872 (81%)]\tLoss: 0.350839\tLR: 0.00020692\n",
            "Train Epoch: 9 [100608/123872 (81%)]\tLoss: 0.370977\tLR: 0.00020681\n",
            "Train Epoch: 9 [100864/123872 (81%)]\tLoss: 0.353352\tLR: 0.00020671\n",
            "Train Epoch: 9 [101120/123872 (82%)]\tLoss: 0.410795\tLR: 0.00020660\n",
            "Train Epoch: 9 [101376/123872 (82%)]\tLoss: 0.426527\tLR: 0.00020650\n",
            "Train Epoch: 9 [101632/123872 (82%)]\tLoss: 0.397850\tLR: 0.00020639\n",
            "Train Epoch: 9 [101888/123872 (82%)]\tLoss: 0.366434\tLR: 0.00020629\n",
            "Train Epoch: 9 [102144/123872 (82%)]\tLoss: 0.417367\tLR: 0.00020618\n",
            "Train Epoch: 9 [102400/123872 (83%)]\tLoss: 0.401878\tLR: 0.00020608\n",
            "Train Epoch: 9 [102400/123872 (83%)]\tLoss: 0.401878\n",
            "Train Epoch: 9 [102656/123872 (83%)]\tLoss: 0.369020\tLR: 0.00020597\n",
            "Train Epoch: 9 [102912/123872 (83%)]\tLoss: 0.445499\tLR: 0.00020587\n",
            "Train Epoch: 9 [103168/123872 (83%)]\tLoss: 0.324942\tLR: 0.00020576\n",
            "Train Epoch: 9 [103424/123872 (83%)]\tLoss: 0.384933\tLR: 0.00020566\n",
            "Train Epoch: 9 [103680/123872 (84%)]\tLoss: 0.402100\tLR: 0.00020555\n",
            "Train Epoch: 9 [103936/123872 (84%)]\tLoss: 0.435402\tLR: 0.00020545\n",
            "Train Epoch: 9 [104192/123872 (84%)]\tLoss: 0.359347\tLR: 0.00020535\n",
            "Train Epoch: 9 [104448/123872 (84%)]\tLoss: 0.390588\tLR: 0.00020524\n",
            "Train Epoch: 9 [104704/123872 (85%)]\tLoss: 0.409678\tLR: 0.00020514\n",
            "Train Epoch: 9 [104960/123872 (85%)]\tLoss: 0.396857\tLR: 0.00020503\n",
            "Train Epoch: 9 [104960/123872 (85%)]\tLoss: 0.396857\n",
            "Train Epoch: 9 [105216/123872 (85%)]\tLoss: 0.422055\tLR: 0.00020493\n",
            "Train Epoch: 9 [105472/123872 (85%)]\tLoss: 0.422344\tLR: 0.00020482\n",
            "Train Epoch: 9 [105728/123872 (85%)]\tLoss: 0.361541\tLR: 0.00020472\n",
            "Train Epoch: 9 [105984/123872 (86%)]\tLoss: 0.376848\tLR: 0.00020461\n",
            "Train Epoch: 9 [106240/123872 (86%)]\tLoss: 0.430693\tLR: 0.00020451\n",
            "Train Epoch: 9 [106496/123872 (86%)]\tLoss: 0.412073\tLR: 0.00020440\n",
            "Train Epoch: 9 [106752/123872 (86%)]\tLoss: 0.401343\tLR: 0.00020430\n",
            "Train Epoch: 9 [107008/123872 (86%)]\tLoss: 0.397787\tLR: 0.00020420\n",
            "Train Epoch: 9 [107264/123872 (87%)]\tLoss: 0.445223\tLR: 0.00020409\n",
            "Train Epoch: 9 [107520/123872 (87%)]\tLoss: 0.376057\tLR: 0.00020399\n",
            "Train Epoch: 9 [107520/123872 (87%)]\tLoss: 0.376057\n",
            "Train Epoch: 9 [107776/123872 (87%)]\tLoss: 0.401479\tLR: 0.00020388\n",
            "Train Epoch: 9 [108032/123872 (87%)]\tLoss: 0.434436\tLR: 0.00020378\n",
            "Train Epoch: 9 [108288/123872 (87%)]\tLoss: 0.415268\tLR: 0.00020367\n",
            "Train Epoch: 9 [108544/123872 (88%)]\tLoss: 0.395496\tLR: 0.00020357\n",
            "Train Epoch: 9 [108800/123872 (88%)]\tLoss: 0.372293\tLR: 0.00020347\n",
            "Train Epoch: 9 [109056/123872 (88%)]\tLoss: 0.338883\tLR: 0.00020336\n",
            "Train Epoch: 9 [109312/123872 (88%)]\tLoss: 0.376730\tLR: 0.00020326\n",
            "Train Epoch: 9 [109568/123872 (88%)]\tLoss: 0.377625\tLR: 0.00020315\n",
            "Train Epoch: 9 [109824/123872 (89%)]\tLoss: 0.347762\tLR: 0.00020305\n",
            "Train Epoch: 9 [110080/123872 (89%)]\tLoss: 0.381947\tLR: 0.00020294\n",
            "Train Epoch: 9 [110080/123872 (89%)]\tLoss: 0.381947\n",
            "Train Epoch: 9 [110336/123872 (89%)]\tLoss: 0.404894\tLR: 0.00020284\n",
            "Train Epoch: 9 [110592/123872 (89%)]\tLoss: 0.344296\tLR: 0.00020274\n",
            "Train Epoch: 9 [110848/123872 (89%)]\tLoss: 0.409273\tLR: 0.00020263\n",
            "Train Epoch: 9 [111104/123872 (90%)]\tLoss: 0.402810\tLR: 0.00020253\n",
            "Train Epoch: 9 [111360/123872 (90%)]\tLoss: 0.342534\tLR: 0.00020242\n",
            "Train Epoch: 9 [111616/123872 (90%)]\tLoss: 0.369432\tLR: 0.00020232\n",
            "Train Epoch: 9 [111872/123872 (90%)]\tLoss: 0.317078\tLR: 0.00020222\n",
            "Train Epoch: 9 [112128/123872 (90%)]\tLoss: 0.297825\tLR: 0.00020211\n",
            "Train Epoch: 9 [112384/123872 (91%)]\tLoss: 0.393496\tLR: 0.00020201\n",
            "Train Epoch: 9 [112640/123872 (91%)]\tLoss: 0.440228\tLR: 0.00020190\n",
            "Train Epoch: 9 [112640/123872 (91%)]\tLoss: 0.440228\n",
            "Train Epoch: 9 [112896/123872 (91%)]\tLoss: 0.409639\tLR: 0.00020180\n",
            "Train Epoch: 9 [113152/123872 (91%)]\tLoss: 0.361019\tLR: 0.00020170\n",
            "Train Epoch: 9 [113408/123872 (92%)]\tLoss: 0.360695\tLR: 0.00020159\n",
            "Train Epoch: 9 [113664/123872 (92%)]\tLoss: 0.373737\tLR: 0.00020149\n",
            "Train Epoch: 9 [113920/123872 (92%)]\tLoss: 0.334772\tLR: 0.00020138\n",
            "Train Epoch: 9 [114176/123872 (92%)]\tLoss: 0.329522\tLR: 0.00020128\n",
            "Train Epoch: 9 [114432/123872 (92%)]\tLoss: 0.435214\tLR: 0.00020118\n",
            "Train Epoch: 9 [114688/123872 (93%)]\tLoss: 0.400965\tLR: 0.00020107\n",
            "Train Epoch: 9 [114944/123872 (93%)]\tLoss: 0.390938\tLR: 0.00020097\n",
            "Train Epoch: 9 [115200/123872 (93%)]\tLoss: 0.433733\tLR: 0.00020087\n",
            "Train Epoch: 9 [115200/123872 (93%)]\tLoss: 0.433733\n",
            "Train Epoch: 9 [115456/123872 (93%)]\tLoss: 0.356751\tLR: 0.00020076\n",
            "Train Epoch: 9 [115712/123872 (93%)]\tLoss: 0.346685\tLR: 0.00020066\n",
            "Train Epoch: 9 [115968/123872 (94%)]\tLoss: 0.323716\tLR: 0.00020056\n",
            "Train Epoch: 9 [116224/123872 (94%)]\tLoss: 0.343325\tLR: 0.00020045\n",
            "Train Epoch: 9 [116480/123872 (94%)]\tLoss: 0.371035\tLR: 0.00020035\n",
            "Train Epoch: 9 [116736/123872 (94%)]\tLoss: 0.325837\tLR: 0.00020025\n",
            "Train Epoch: 9 [116992/123872 (94%)]\tLoss: 0.462560\tLR: 0.00020014\n",
            "Train Epoch: 9 [117248/123872 (95%)]\tLoss: 0.418968\tLR: 0.00020004\n",
            "Train Epoch: 9 [117504/123872 (95%)]\tLoss: 0.389951\tLR: 0.00019993\n",
            "Train Epoch: 9 [117760/123872 (95%)]\tLoss: 0.338959\tLR: 0.00019983\n",
            "Train Epoch: 9 [117760/123872 (95%)]\tLoss: 0.338959\n",
            "Train Epoch: 9 [118016/123872 (95%)]\tLoss: 0.463973\tLR: 0.00019973\n",
            "Train Epoch: 9 [118272/123872 (95%)]\tLoss: 0.358461\tLR: 0.00019962\n",
            "Train Epoch: 9 [118528/123872 (96%)]\tLoss: 0.351477\tLR: 0.00019952\n",
            "Train Epoch: 9 [118784/123872 (96%)]\tLoss: 0.386980\tLR: 0.00019942\n",
            "Train Epoch: 9 [119040/123872 (96%)]\tLoss: 0.381814\tLR: 0.00019931\n",
            "Train Epoch: 9 [119296/123872 (96%)]\tLoss: 0.371358\tLR: 0.00019921\n",
            "Train Epoch: 9 [119552/123872 (96%)]\tLoss: 0.332918\tLR: 0.00019911\n",
            "Train Epoch: 9 [119808/123872 (97%)]\tLoss: 0.296224\tLR: 0.00019901\n",
            "Train Epoch: 9 [120064/123872 (97%)]\tLoss: 0.389827\tLR: 0.00019890\n",
            "Train Epoch: 9 [120320/123872 (97%)]\tLoss: 0.350852\tLR: 0.00019880\n",
            "Train Epoch: 9 [120320/123872 (97%)]\tLoss: 0.350852\n",
            "Train Epoch: 9 [120576/123872 (97%)]\tLoss: 0.399491\tLR: 0.00019870\n",
            "Train Epoch: 9 [120832/123872 (98%)]\tLoss: 0.362390\tLR: 0.00019859\n",
            "Train Epoch: 9 [121088/123872 (98%)]\tLoss: 0.448563\tLR: 0.00019849\n",
            "Train Epoch: 9 [121344/123872 (98%)]\tLoss: 0.373844\tLR: 0.00019839\n",
            "Train Epoch: 9 [121600/123872 (98%)]\tLoss: 0.450156\tLR: 0.00019828\n",
            "Train Epoch: 9 [121856/123872 (98%)]\tLoss: 0.331137\tLR: 0.00019818\n",
            "Train Epoch: 9 [122112/123872 (99%)]\tLoss: 0.346830\tLR: 0.00019808\n",
            "Train Epoch: 9 [122368/123872 (99%)]\tLoss: 0.426615\tLR: 0.00019797\n",
            "Train Epoch: 9 [122624/123872 (99%)]\tLoss: 0.358394\tLR: 0.00019787\n",
            "Train Epoch: 9 [122880/123872 (99%)]\tLoss: 0.359771\tLR: 0.00019777\n",
            "Train Epoch: 9 [122880/123872 (99%)]\tLoss: 0.359771\n",
            "Train Epoch: 9 [123136/123872 (99%)]\tLoss: 0.399296\tLR: 0.00019767\n",
            "Train Epoch: 9 [123392/123872 (100%)]\tLoss: 0.334148\tLR: 0.00019756\n",
            "Train Epoch: 9 [108192/123872 (100%)]\tLoss: 0.452555\tLR: 0.00019746\n",
            "\n",
            "Test set: Average loss: 0.0016, Accuracy: 25474/30970 (82.25%)\n",
            "\n",
            "Train Epoch: 10 [0/123872 (0%)]\tLoss: 0.365250\tLR: 0.00019736\n",
            "Train Epoch: 10 [0/123872 (0%)]\tLoss: 0.365250\n",
            "Train Epoch: 10 [256/123872 (0%)]\tLoss: 0.378201\tLR: 0.00019725\n",
            "Train Epoch: 10 [512/123872 (0%)]\tLoss: 0.400888\tLR: 0.00019715\n",
            "Train Epoch: 10 [768/123872 (1%)]\tLoss: 0.339366\tLR: 0.00019705\n",
            "Train Epoch: 10 [1024/123872 (1%)]\tLoss: 0.401803\tLR: 0.00019695\n",
            "Train Epoch: 10 [1280/123872 (1%)]\tLoss: 0.357999\tLR: 0.00019684\n",
            "Train Epoch: 10 [1536/123872 (1%)]\tLoss: 0.287837\tLR: 0.00019674\n",
            "Train Epoch: 10 [1792/123872 (1%)]\tLoss: 0.351397\tLR: 0.00019664\n",
            "Train Epoch: 10 [2048/123872 (2%)]\tLoss: 0.419550\tLR: 0.00019654\n",
            "Train Epoch: 10 [2304/123872 (2%)]\tLoss: 0.414639\tLR: 0.00019643\n",
            "Train Epoch: 10 [2560/123872 (2%)]\tLoss: 0.329759\tLR: 0.00019633\n",
            "Train Epoch: 10 [2560/123872 (2%)]\tLoss: 0.329759\n",
            "Train Epoch: 10 [2816/123872 (2%)]\tLoss: 0.344901\tLR: 0.00019623\n",
            "Train Epoch: 10 [3072/123872 (2%)]\tLoss: 0.365499\tLR: 0.00019613\n",
            "Train Epoch: 10 [3328/123872 (3%)]\tLoss: 0.455334\tLR: 0.00019602\n",
            "Train Epoch: 10 [3584/123872 (3%)]\tLoss: 0.370595\tLR: 0.00019592\n",
            "Train Epoch: 10 [3840/123872 (3%)]\tLoss: 0.340348\tLR: 0.00019582\n",
            "Train Epoch: 10 [4096/123872 (3%)]\tLoss: 0.373207\tLR: 0.00019572\n",
            "Train Epoch: 10 [4352/123872 (4%)]\tLoss: 0.341143\tLR: 0.00019561\n",
            "Train Epoch: 10 [4608/123872 (4%)]\tLoss: 0.349583\tLR: 0.00019551\n",
            "Train Epoch: 10 [4864/123872 (4%)]\tLoss: 0.424959\tLR: 0.00019541\n",
            "Train Epoch: 10 [5120/123872 (4%)]\tLoss: 0.397671\tLR: 0.00019531\n",
            "Train Epoch: 10 [5120/123872 (4%)]\tLoss: 0.397671\n",
            "Train Epoch: 10 [5376/123872 (4%)]\tLoss: 0.366579\tLR: 0.00019520\n",
            "Train Epoch: 10 [5632/123872 (5%)]\tLoss: 0.304100\tLR: 0.00019510\n",
            "Train Epoch: 10 [5888/123872 (5%)]\tLoss: 0.396941\tLR: 0.00019500\n",
            "Train Epoch: 10 [6144/123872 (5%)]\tLoss: 0.381101\tLR: 0.00019490\n",
            "Train Epoch: 10 [6400/123872 (5%)]\tLoss: 0.396791\tLR: 0.00019480\n",
            "Train Epoch: 10 [6656/123872 (5%)]\tLoss: 0.337387\tLR: 0.00019469\n",
            "Train Epoch: 10 [6912/123872 (6%)]\tLoss: 0.401538\tLR: 0.00019459\n",
            "Train Epoch: 10 [7168/123872 (6%)]\tLoss: 0.393419\tLR: 0.00019449\n",
            "Train Epoch: 10 [7424/123872 (6%)]\tLoss: 0.382075\tLR: 0.00019439\n",
            "Train Epoch: 10 [7680/123872 (6%)]\tLoss: 0.312768\tLR: 0.00019429\n",
            "Train Epoch: 10 [7680/123872 (6%)]\tLoss: 0.312768\n",
            "Train Epoch: 10 [7936/123872 (6%)]\tLoss: 0.392841\tLR: 0.00019418\n",
            "Train Epoch: 10 [8192/123872 (7%)]\tLoss: 0.374137\tLR: 0.00019408\n",
            "Train Epoch: 10 [8448/123872 (7%)]\tLoss: 0.357587\tLR: 0.00019398\n",
            "Train Epoch: 10 [8704/123872 (7%)]\tLoss: 0.359159\tLR: 0.00019388\n",
            "Train Epoch: 10 [8960/123872 (7%)]\tLoss: 0.396377\tLR: 0.00019378\n",
            "Train Epoch: 10 [9216/123872 (7%)]\tLoss: 0.375988\tLR: 0.00019367\n",
            "Train Epoch: 10 [9472/123872 (8%)]\tLoss: 0.367665\tLR: 0.00019357\n",
            "Train Epoch: 10 [9728/123872 (8%)]\tLoss: 0.414469\tLR: 0.00019347\n",
            "Train Epoch: 10 [9984/123872 (8%)]\tLoss: 0.383418\tLR: 0.00019337\n",
            "Train Epoch: 10 [10240/123872 (8%)]\tLoss: 0.364663\tLR: 0.00019327\n",
            "Train Epoch: 10 [10240/123872 (8%)]\tLoss: 0.364663\n",
            "Train Epoch: 10 [10496/123872 (8%)]\tLoss: 0.349948\tLR: 0.00019316\n",
            "Train Epoch: 10 [10752/123872 (9%)]\tLoss: 0.429715\tLR: 0.00019306\n",
            "Train Epoch: 10 [11008/123872 (9%)]\tLoss: 0.336298\tLR: 0.00019296\n",
            "Train Epoch: 10 [11264/123872 (9%)]\tLoss: 0.374806\tLR: 0.00019286\n",
            "Train Epoch: 10 [11520/123872 (9%)]\tLoss: 0.315648\tLR: 0.00019276\n",
            "Train Epoch: 10 [11776/123872 (10%)]\tLoss: 0.364702\tLR: 0.00019266\n",
            "Train Epoch: 10 [12032/123872 (10%)]\tLoss: 0.377408\tLR: 0.00019255\n",
            "Train Epoch: 10 [12288/123872 (10%)]\tLoss: 0.413182\tLR: 0.00019245\n",
            "Train Epoch: 10 [12544/123872 (10%)]\tLoss: 0.389701\tLR: 0.00019235\n",
            "Train Epoch: 10 [12800/123872 (10%)]\tLoss: 0.394957\tLR: 0.00019225\n",
            "Train Epoch: 10 [12800/123872 (10%)]\tLoss: 0.394957\n",
            "Train Epoch: 10 [13056/123872 (11%)]\tLoss: 0.394681\tLR: 0.00019215\n",
            "Train Epoch: 10 [13312/123872 (11%)]\tLoss: 0.363200\tLR: 0.00019205\n",
            "Train Epoch: 10 [13568/123872 (11%)]\tLoss: 0.381686\tLR: 0.00019195\n",
            "Train Epoch: 10 [13824/123872 (11%)]\tLoss: 0.408007\tLR: 0.00019184\n",
            "Train Epoch: 10 [14080/123872 (11%)]\tLoss: 0.375002\tLR: 0.00019174\n",
            "Train Epoch: 10 [14336/123872 (12%)]\tLoss: 0.402593\tLR: 0.00019164\n",
            "Train Epoch: 10 [14592/123872 (12%)]\tLoss: 0.364859\tLR: 0.00019154\n",
            "Train Epoch: 10 [14848/123872 (12%)]\tLoss: 0.381383\tLR: 0.00019144\n",
            "Train Epoch: 10 [15104/123872 (12%)]\tLoss: 0.378254\tLR: 0.00019134\n",
            "Train Epoch: 10 [15360/123872 (12%)]\tLoss: 0.438259\tLR: 0.00019124\n",
            "Train Epoch: 10 [15360/123872 (12%)]\tLoss: 0.438259\n",
            "Train Epoch: 10 [15616/123872 (13%)]\tLoss: 0.380399\tLR: 0.00019114\n",
            "Train Epoch: 10 [15872/123872 (13%)]\tLoss: 0.352378\tLR: 0.00019103\n",
            "Train Epoch: 10 [16128/123872 (13%)]\tLoss: 0.422617\tLR: 0.00019093\n",
            "Train Epoch: 10 [16384/123872 (13%)]\tLoss: 0.375611\tLR: 0.00019083\n",
            "Train Epoch: 10 [16640/123872 (13%)]\tLoss: 0.381733\tLR: 0.00019073\n",
            "Train Epoch: 10 [16896/123872 (14%)]\tLoss: 0.338104\tLR: 0.00019063\n",
            "Train Epoch: 10 [17152/123872 (14%)]\tLoss: 0.368077\tLR: 0.00019053\n",
            "Train Epoch: 10 [17408/123872 (14%)]\tLoss: 0.403425\tLR: 0.00019043\n",
            "Train Epoch: 10 [17664/123872 (14%)]\tLoss: 0.386781\tLR: 0.00019033\n",
            "Train Epoch: 10 [17920/123872 (14%)]\tLoss: 0.327955\tLR: 0.00019023\n",
            "Train Epoch: 10 [17920/123872 (14%)]\tLoss: 0.327955\n",
            "Train Epoch: 10 [18176/123872 (15%)]\tLoss: 0.337936\tLR: 0.00019012\n",
            "Train Epoch: 10 [18432/123872 (15%)]\tLoss: 0.339985\tLR: 0.00019002\n",
            "Train Epoch: 10 [18688/123872 (15%)]\tLoss: 0.372252\tLR: 0.00018992\n",
            "Train Epoch: 10 [18944/123872 (15%)]\tLoss: 0.400559\tLR: 0.00018982\n",
            "Train Epoch: 10 [19200/123872 (15%)]\tLoss: 0.350159\tLR: 0.00018972\n",
            "Train Epoch: 10 [19456/123872 (16%)]\tLoss: 0.344924\tLR: 0.00018962\n",
            "Train Epoch: 10 [19712/123872 (16%)]\tLoss: 0.346188\tLR: 0.00018952\n",
            "Train Epoch: 10 [19968/123872 (16%)]\tLoss: 0.438765\tLR: 0.00018942\n",
            "Train Epoch: 10 [20224/123872 (16%)]\tLoss: 0.391524\tLR: 0.00018932\n",
            "Train Epoch: 10 [20480/123872 (17%)]\tLoss: 0.395194\tLR: 0.00018922\n",
            "Train Epoch: 10 [20480/123872 (17%)]\tLoss: 0.395194\n",
            "Train Epoch: 10 [20736/123872 (17%)]\tLoss: 0.433847\tLR: 0.00018912\n",
            "Train Epoch: 10 [20992/123872 (17%)]\tLoss: 0.343998\tLR: 0.00018902\n",
            "Train Epoch: 10 [21248/123872 (17%)]\tLoss: 0.400226\tLR: 0.00018892\n",
            "Train Epoch: 10 [21504/123872 (17%)]\tLoss: 0.406517\tLR: 0.00018881\n",
            "Train Epoch: 10 [21760/123872 (18%)]\tLoss: 0.433913\tLR: 0.00018871\n",
            "Train Epoch: 10 [22016/123872 (18%)]\tLoss: 0.379369\tLR: 0.00018861\n",
            "Train Epoch: 10 [22272/123872 (18%)]\tLoss: 0.334374\tLR: 0.00018851\n",
            "Train Epoch: 10 [22528/123872 (18%)]\tLoss: 0.350555\tLR: 0.00018841\n",
            "Train Epoch: 10 [22784/123872 (18%)]\tLoss: 0.360706\tLR: 0.00018831\n",
            "Train Epoch: 10 [23040/123872 (19%)]\tLoss: 0.331656\tLR: 0.00018821\n",
            "Train Epoch: 10 [23040/123872 (19%)]\tLoss: 0.331656\n",
            "Train Epoch: 10 [23296/123872 (19%)]\tLoss: 0.387631\tLR: 0.00018811\n",
            "Train Epoch: 10 [23552/123872 (19%)]\tLoss: 0.386529\tLR: 0.00018801\n",
            "Train Epoch: 10 [23808/123872 (19%)]\tLoss: 0.389852\tLR: 0.00018791\n",
            "Train Epoch: 10 [24064/123872 (19%)]\tLoss: 0.311732\tLR: 0.00018781\n",
            "Train Epoch: 10 [24320/123872 (20%)]\tLoss: 0.376711\tLR: 0.00018771\n",
            "Train Epoch: 10 [24576/123872 (20%)]\tLoss: 0.415414\tLR: 0.00018761\n",
            "Train Epoch: 10 [24832/123872 (20%)]\tLoss: 0.392340\tLR: 0.00018751\n",
            "Train Epoch: 10 [25088/123872 (20%)]\tLoss: 0.382291\tLR: 0.00018741\n",
            "Train Epoch: 10 [25344/123872 (20%)]\tLoss: 0.402298\tLR: 0.00018731\n",
            "Train Epoch: 10 [25600/123872 (21%)]\tLoss: 0.327798\tLR: 0.00018721\n",
            "Train Epoch: 10 [25600/123872 (21%)]\tLoss: 0.327798\n",
            "Train Epoch: 10 [25856/123872 (21%)]\tLoss: 0.359648\tLR: 0.00018711\n",
            "Train Epoch: 10 [26112/123872 (21%)]\tLoss: 0.428460\tLR: 0.00018701\n",
            "Train Epoch: 10 [26368/123872 (21%)]\tLoss: 0.347252\tLR: 0.00018691\n",
            "Train Epoch: 10 [26624/123872 (21%)]\tLoss: 0.349332\tLR: 0.00018681\n",
            "Train Epoch: 10 [26880/123872 (22%)]\tLoss: 0.313008\tLR: 0.00018671\n",
            "Train Epoch: 10 [27136/123872 (22%)]\tLoss: 0.445651\tLR: 0.00018661\n",
            "Train Epoch: 10 [27392/123872 (22%)]\tLoss: 0.376208\tLR: 0.00018651\n",
            "Train Epoch: 10 [27648/123872 (22%)]\tLoss: 0.382349\tLR: 0.00018641\n",
            "Train Epoch: 10 [27904/123872 (23%)]\tLoss: 0.305794\tLR: 0.00018631\n",
            "Train Epoch: 10 [28160/123872 (23%)]\tLoss: 0.317300\tLR: 0.00018621\n",
            "Train Epoch: 10 [28160/123872 (23%)]\tLoss: 0.317300\n",
            "Train Epoch: 10 [28416/123872 (23%)]\tLoss: 0.340414\tLR: 0.00018611\n",
            "Train Epoch: 10 [28672/123872 (23%)]\tLoss: 0.424846\tLR: 0.00018601\n",
            "Train Epoch: 10 [28928/123872 (23%)]\tLoss: 0.397272\tLR: 0.00018591\n",
            "Train Epoch: 10 [29184/123872 (24%)]\tLoss: 0.359919\tLR: 0.00018581\n",
            "Train Epoch: 10 [29440/123872 (24%)]\tLoss: 0.319554\tLR: 0.00018571\n",
            "Train Epoch: 10 [29696/123872 (24%)]\tLoss: 0.335146\tLR: 0.00018561\n",
            "Train Epoch: 10 [29952/123872 (24%)]\tLoss: 0.386853\tLR: 0.00018551\n",
            "Train Epoch: 10 [30208/123872 (24%)]\tLoss: 0.354956\tLR: 0.00018541\n",
            "Train Epoch: 10 [30464/123872 (25%)]\tLoss: 0.382510\tLR: 0.00018531\n",
            "Train Epoch: 10 [30720/123872 (25%)]\tLoss: 0.381406\tLR: 0.00018521\n",
            "Train Epoch: 10 [30720/123872 (25%)]\tLoss: 0.381406\n",
            "Train Epoch: 10 [30976/123872 (25%)]\tLoss: 0.345915\tLR: 0.00018511\n",
            "Train Epoch: 10 [31232/123872 (25%)]\tLoss: 0.388045\tLR: 0.00018501\n",
            "Train Epoch: 10 [31488/123872 (25%)]\tLoss: 0.386873\tLR: 0.00018491\n",
            "Train Epoch: 10 [31744/123872 (26%)]\tLoss: 0.358096\tLR: 0.00018481\n",
            "Train Epoch: 10 [32000/123872 (26%)]\tLoss: 0.347774\tLR: 0.00018471\n",
            "Train Epoch: 10 [32256/123872 (26%)]\tLoss: 0.356133\tLR: 0.00018461\n",
            "Train Epoch: 10 [32512/123872 (26%)]\tLoss: 0.384213\tLR: 0.00018451\n",
            "Train Epoch: 10 [32768/123872 (26%)]\tLoss: 0.406728\tLR: 0.00018442\n",
            "Train Epoch: 10 [33024/123872 (27%)]\tLoss: 0.350285\tLR: 0.00018432\n",
            "Train Epoch: 10 [33280/123872 (27%)]\tLoss: 0.379133\tLR: 0.00018422\n",
            "Train Epoch: 10 [33280/123872 (27%)]\tLoss: 0.379133\n",
            "Train Epoch: 10 [33536/123872 (27%)]\tLoss: 0.408264\tLR: 0.00018412\n",
            "Train Epoch: 10 [33792/123872 (27%)]\tLoss: 0.409407\tLR: 0.00018402\n",
            "Train Epoch: 10 [34048/123872 (27%)]\tLoss: 0.361857\tLR: 0.00018392\n",
            "Train Epoch: 10 [34304/123872 (28%)]\tLoss: 0.364357\tLR: 0.00018382\n",
            "Train Epoch: 10 [34560/123872 (28%)]\tLoss: 0.352835\tLR: 0.00018372\n",
            "Train Epoch: 10 [34816/123872 (28%)]\tLoss: 0.361613\tLR: 0.00018362\n",
            "Train Epoch: 10 [35072/123872 (28%)]\tLoss: 0.353275\tLR: 0.00018352\n",
            "Train Epoch: 10 [35328/123872 (29%)]\tLoss: 0.393171\tLR: 0.00018342\n",
            "Train Epoch: 10 [35584/123872 (29%)]\tLoss: 0.376166\tLR: 0.00018332\n",
            "Train Epoch: 10 [35840/123872 (29%)]\tLoss: 0.393608\tLR: 0.00018322\n",
            "Train Epoch: 10 [35840/123872 (29%)]\tLoss: 0.393608\n",
            "Train Epoch: 10 [36096/123872 (29%)]\tLoss: 0.400832\tLR: 0.00018313\n",
            "Train Epoch: 10 [36352/123872 (29%)]\tLoss: 0.305027\tLR: 0.00018303\n",
            "Train Epoch: 10 [36608/123872 (30%)]\tLoss: 0.399030\tLR: 0.00018293\n",
            "Train Epoch: 10 [36864/123872 (30%)]\tLoss: 0.388685\tLR: 0.00018283\n",
            "Train Epoch: 10 [37120/123872 (30%)]\tLoss: 0.378028\tLR: 0.00018273\n",
            "Train Epoch: 10 [37376/123872 (30%)]\tLoss: 0.385875\tLR: 0.00018263\n",
            "Train Epoch: 10 [37632/123872 (30%)]\tLoss: 0.425003\tLR: 0.00018253\n",
            "Train Epoch: 10 [37888/123872 (31%)]\tLoss: 0.349319\tLR: 0.00018243\n",
            "Train Epoch: 10 [38144/123872 (31%)]\tLoss: 0.378235\tLR: 0.00018233\n",
            "Train Epoch: 10 [38400/123872 (31%)]\tLoss: 0.378243\tLR: 0.00018224\n",
            "Train Epoch: 10 [38400/123872 (31%)]\tLoss: 0.378243\n",
            "Train Epoch: 10 [38656/123872 (31%)]\tLoss: 0.348712\tLR: 0.00018214\n",
            "Train Epoch: 10 [38912/123872 (31%)]\tLoss: 0.400379\tLR: 0.00018204\n",
            "Train Epoch: 10 [39168/123872 (32%)]\tLoss: 0.379442\tLR: 0.00018194\n",
            "Train Epoch: 10 [39424/123872 (32%)]\tLoss: 0.393419\tLR: 0.00018184\n",
            "Train Epoch: 10 [39680/123872 (32%)]\tLoss: 0.377473\tLR: 0.00018174\n",
            "Train Epoch: 10 [39936/123872 (32%)]\tLoss: 0.402001\tLR: 0.00018164\n",
            "Train Epoch: 10 [40192/123872 (32%)]\tLoss: 0.362311\tLR: 0.00018154\n",
            "Train Epoch: 10 [40448/123872 (33%)]\tLoss: 0.376427\tLR: 0.00018145\n",
            "Train Epoch: 10 [40704/123872 (33%)]\tLoss: 0.326777\tLR: 0.00018135\n",
            "Train Epoch: 10 [40960/123872 (33%)]\tLoss: 0.346385\tLR: 0.00018125\n",
            "Train Epoch: 10 [40960/123872 (33%)]\tLoss: 0.346385\n",
            "Train Epoch: 10 [41216/123872 (33%)]\tLoss: 0.379203\tLR: 0.00018115\n",
            "Train Epoch: 10 [41472/123872 (33%)]\tLoss: 0.377806\tLR: 0.00018105\n",
            "Train Epoch: 10 [41728/123872 (34%)]\tLoss: 0.389417\tLR: 0.00018095\n",
            "Train Epoch: 10 [41984/123872 (34%)]\tLoss: 0.373156\tLR: 0.00018086\n",
            "Train Epoch: 10 [42240/123872 (34%)]\tLoss: 0.338597\tLR: 0.00018076\n",
            "Train Epoch: 10 [42496/123872 (34%)]\tLoss: 0.393272\tLR: 0.00018066\n",
            "Train Epoch: 10 [42752/123872 (35%)]\tLoss: 0.416261\tLR: 0.00018056\n",
            "Train Epoch: 10 [43008/123872 (35%)]\tLoss: 0.332792\tLR: 0.00018046\n",
            "Train Epoch: 10 [43264/123872 (35%)]\tLoss: 0.344190\tLR: 0.00018036\n",
            "Train Epoch: 10 [43520/123872 (35%)]\tLoss: 0.466749\tLR: 0.00018027\n",
            "Train Epoch: 10 [43520/123872 (35%)]\tLoss: 0.466749\n",
            "Train Epoch: 10 [43776/123872 (35%)]\tLoss: 0.408799\tLR: 0.00018017\n",
            "Train Epoch: 10 [44032/123872 (36%)]\tLoss: 0.383161\tLR: 0.00018007\n",
            "Train Epoch: 10 [44288/123872 (36%)]\tLoss: 0.314581\tLR: 0.00017997\n",
            "Train Epoch: 10 [44544/123872 (36%)]\tLoss: 0.399606\tLR: 0.00017987\n",
            "Train Epoch: 10 [44800/123872 (36%)]\tLoss: 0.331735\tLR: 0.00017978\n",
            "Train Epoch: 10 [45056/123872 (36%)]\tLoss: 0.369385\tLR: 0.00017968\n",
            "Train Epoch: 10 [45312/123872 (37%)]\tLoss: 0.347368\tLR: 0.00017958\n",
            "Train Epoch: 10 [45568/123872 (37%)]\tLoss: 0.373450\tLR: 0.00017948\n",
            "Train Epoch: 10 [45824/123872 (37%)]\tLoss: 0.321885\tLR: 0.00017938\n",
            "Train Epoch: 10 [46080/123872 (37%)]\tLoss: 0.385358\tLR: 0.00017929\n",
            "Train Epoch: 10 [46080/123872 (37%)]\tLoss: 0.385358\n",
            "Train Epoch: 10 [46336/123872 (37%)]\tLoss: 0.312389\tLR: 0.00017919\n",
            "Train Epoch: 10 [46592/123872 (38%)]\tLoss: 0.392994\tLR: 0.00017909\n",
            "Train Epoch: 10 [46848/123872 (38%)]\tLoss: 0.433093\tLR: 0.00017899\n",
            "Train Epoch: 10 [47104/123872 (38%)]\tLoss: 0.329761\tLR: 0.00017889\n",
            "Train Epoch: 10 [47360/123872 (38%)]\tLoss: 0.405796\tLR: 0.00017880\n",
            "Train Epoch: 10 [47616/123872 (38%)]\tLoss: 0.340230\tLR: 0.00017870\n",
            "Train Epoch: 10 [47872/123872 (39%)]\tLoss: 0.358293\tLR: 0.00017860\n",
            "Train Epoch: 10 [48128/123872 (39%)]\tLoss: 0.349538\tLR: 0.00017850\n",
            "Train Epoch: 10 [48384/123872 (39%)]\tLoss: 0.405226\tLR: 0.00017841\n",
            "Train Epoch: 10 [48640/123872 (39%)]\tLoss: 0.406882\tLR: 0.00017831\n",
            "Train Epoch: 10 [48640/123872 (39%)]\tLoss: 0.406882\n",
            "Train Epoch: 10 [48896/123872 (39%)]\tLoss: 0.455147\tLR: 0.00017821\n",
            "Train Epoch: 10 [49152/123872 (40%)]\tLoss: 0.402251\tLR: 0.00017811\n",
            "Train Epoch: 10 [49408/123872 (40%)]\tLoss: 0.403243\tLR: 0.00017802\n",
            "Train Epoch: 10 [49664/123872 (40%)]\tLoss: 0.417658\tLR: 0.00017792\n",
            "Train Epoch: 10 [49920/123872 (40%)]\tLoss: 0.333249\tLR: 0.00017782\n",
            "Train Epoch: 10 [50176/123872 (40%)]\tLoss: 0.354278\tLR: 0.00017772\n",
            "Train Epoch: 10 [50432/123872 (41%)]\tLoss: 0.400341\tLR: 0.00017763\n",
            "Train Epoch: 10 [50688/123872 (41%)]\tLoss: 0.435301\tLR: 0.00017753\n",
            "Train Epoch: 10 [50944/123872 (41%)]\tLoss: 0.329484\tLR: 0.00017743\n",
            "Train Epoch: 10 [51200/123872 (41%)]\tLoss: 0.346399\tLR: 0.00017733\n",
            "Train Epoch: 10 [51200/123872 (41%)]\tLoss: 0.346399\n",
            "Train Epoch: 10 [51456/123872 (42%)]\tLoss: 0.344358\tLR: 0.00017724\n",
            "Train Epoch: 10 [51712/123872 (42%)]\tLoss: 0.371979\tLR: 0.00017714\n",
            "Train Epoch: 10 [51968/123872 (42%)]\tLoss: 0.389054\tLR: 0.00017704\n",
            "Train Epoch: 10 [52224/123872 (42%)]\tLoss: 0.340912\tLR: 0.00017694\n",
            "Train Epoch: 10 [52480/123872 (42%)]\tLoss: 0.405306\tLR: 0.00017685\n",
            "Train Epoch: 10 [52736/123872 (43%)]\tLoss: 0.389270\tLR: 0.00017675\n",
            "Train Epoch: 10 [52992/123872 (43%)]\tLoss: 0.380236\tLR: 0.00017665\n",
            "Train Epoch: 10 [53248/123872 (43%)]\tLoss: 0.344971\tLR: 0.00017656\n",
            "Train Epoch: 10 [53504/123872 (43%)]\tLoss: 0.343482\tLR: 0.00017646\n",
            "Train Epoch: 10 [53760/123872 (43%)]\tLoss: 0.427114\tLR: 0.00017636\n",
            "Train Epoch: 10 [53760/123872 (43%)]\tLoss: 0.427114\n",
            "Train Epoch: 10 [54016/123872 (44%)]\tLoss: 0.453638\tLR: 0.00017626\n",
            "Train Epoch: 10 [54272/123872 (44%)]\tLoss: 0.340724\tLR: 0.00017617\n",
            "Train Epoch: 10 [54528/123872 (44%)]\tLoss: 0.330020\tLR: 0.00017607\n",
            "Train Epoch: 10 [54784/123872 (44%)]\tLoss: 0.398844\tLR: 0.00017597\n",
            "Train Epoch: 10 [55040/123872 (44%)]\tLoss: 0.356367\tLR: 0.00017588\n",
            "Train Epoch: 10 [55296/123872 (45%)]\tLoss: 0.359702\tLR: 0.00017578\n",
            "Train Epoch: 10 [55552/123872 (45%)]\tLoss: 0.363151\tLR: 0.00017568\n",
            "Train Epoch: 10 [55808/123872 (45%)]\tLoss: 0.389489\tLR: 0.00017559\n",
            "Train Epoch: 10 [56064/123872 (45%)]\tLoss: 0.399982\tLR: 0.00017549\n",
            "Train Epoch: 10 [56320/123872 (45%)]\tLoss: 0.318014\tLR: 0.00017539\n",
            "Train Epoch: 10 [56320/123872 (45%)]\tLoss: 0.318014\n",
            "Train Epoch: 10 [56576/123872 (46%)]\tLoss: 0.432416\tLR: 0.00017530\n",
            "Train Epoch: 10 [56832/123872 (46%)]\tLoss: 0.393927\tLR: 0.00017520\n",
            "Train Epoch: 10 [57088/123872 (46%)]\tLoss: 0.361666\tLR: 0.00017510\n",
            "Train Epoch: 10 [57344/123872 (46%)]\tLoss: 0.383970\tLR: 0.00017501\n",
            "Train Epoch: 10 [57600/123872 (46%)]\tLoss: 0.376839\tLR: 0.00017491\n",
            "Train Epoch: 10 [57856/123872 (47%)]\tLoss: 0.389332\tLR: 0.00017481\n",
            "Train Epoch: 10 [58112/123872 (47%)]\tLoss: 0.377868\tLR: 0.00017472\n",
            "Train Epoch: 10 [58368/123872 (47%)]\tLoss: 0.373626\tLR: 0.00017462\n",
            "Train Epoch: 10 [58624/123872 (47%)]\tLoss: 0.326607\tLR: 0.00017452\n",
            "Train Epoch: 10 [58880/123872 (48%)]\tLoss: 0.368660\tLR: 0.00017443\n",
            "Train Epoch: 10 [58880/123872 (48%)]\tLoss: 0.368660\n",
            "Train Epoch: 10 [59136/123872 (48%)]\tLoss: 0.339236\tLR: 0.00017433\n",
            "Train Epoch: 10 [59392/123872 (48%)]\tLoss: 0.429042\tLR: 0.00017423\n",
            "Train Epoch: 10 [59648/123872 (48%)]\tLoss: 0.278805\tLR: 0.00017414\n",
            "Train Epoch: 10 [59904/123872 (48%)]\tLoss: 0.366597\tLR: 0.00017404\n",
            "Train Epoch: 10 [60160/123872 (49%)]\tLoss: 0.354011\tLR: 0.00017394\n",
            "Train Epoch: 10 [60416/123872 (49%)]\tLoss: 0.374426\tLR: 0.00017385\n",
            "Train Epoch: 10 [60672/123872 (49%)]\tLoss: 0.344226\tLR: 0.00017375\n",
            "Train Epoch: 10 [60928/123872 (49%)]\tLoss: 0.420123\tLR: 0.00017366\n",
            "Train Epoch: 10 [61184/123872 (49%)]\tLoss: 0.357503\tLR: 0.00017356\n",
            "Train Epoch: 10 [61440/123872 (50%)]\tLoss: 0.431523\tLR: 0.00017346\n",
            "Train Epoch: 10 [61440/123872 (50%)]\tLoss: 0.431523\n",
            "Train Epoch: 10 [61696/123872 (50%)]\tLoss: 0.370378\tLR: 0.00017337\n",
            "Train Epoch: 10 [61952/123872 (50%)]\tLoss: 0.325577\tLR: 0.00017327\n",
            "Train Epoch: 10 [62208/123872 (50%)]\tLoss: 0.385447\tLR: 0.00017317\n",
            "Train Epoch: 10 [62464/123872 (50%)]\tLoss: 0.375118\tLR: 0.00017308\n",
            "Train Epoch: 10 [62720/123872 (51%)]\tLoss: 0.320045\tLR: 0.00017298\n",
            "Train Epoch: 10 [62976/123872 (51%)]\tLoss: 0.380110\tLR: 0.00017289\n",
            "Train Epoch: 10 [63232/123872 (51%)]\tLoss: 0.361140\tLR: 0.00017279\n",
            "Train Epoch: 10 [63488/123872 (51%)]\tLoss: 0.367576\tLR: 0.00017270\n",
            "Train Epoch: 10 [63744/123872 (51%)]\tLoss: 0.380470\tLR: 0.00017260\n",
            "Train Epoch: 10 [64000/123872 (52%)]\tLoss: 0.352421\tLR: 0.00017250\n",
            "Train Epoch: 10 [64000/123872 (52%)]\tLoss: 0.352421\n",
            "Train Epoch: 10 [64256/123872 (52%)]\tLoss: 0.390633\tLR: 0.00017241\n",
            "Train Epoch: 10 [64512/123872 (52%)]\tLoss: 0.336017\tLR: 0.00017231\n",
            "Train Epoch: 10 [64768/123872 (52%)]\tLoss: 0.405385\tLR: 0.00017222\n",
            "Train Epoch: 10 [65024/123872 (52%)]\tLoss: 0.399784\tLR: 0.00017212\n",
            "Train Epoch: 10 [65280/123872 (53%)]\tLoss: 0.397248\tLR: 0.00017202\n",
            "Train Epoch: 10 [65536/123872 (53%)]\tLoss: 0.324217\tLR: 0.00017193\n",
            "Train Epoch: 10 [65792/123872 (53%)]\tLoss: 0.346587\tLR: 0.00017183\n",
            "Train Epoch: 10 [66048/123872 (53%)]\tLoss: 0.311165\tLR: 0.00017174\n",
            "Train Epoch: 10 [66304/123872 (54%)]\tLoss: 0.369791\tLR: 0.00017164\n",
            "Train Epoch: 10 [66560/123872 (54%)]\tLoss: 0.355841\tLR: 0.00017155\n",
            "Train Epoch: 10 [66560/123872 (54%)]\tLoss: 0.355841\n",
            "Train Epoch: 10 [66816/123872 (54%)]\tLoss: 0.294619\tLR: 0.00017145\n",
            "Train Epoch: 10 [67072/123872 (54%)]\tLoss: 0.413334\tLR: 0.00017136\n",
            "Train Epoch: 10 [67328/123872 (54%)]\tLoss: 0.450294\tLR: 0.00017126\n",
            "Train Epoch: 10 [67584/123872 (55%)]\tLoss: 0.407809\tLR: 0.00017116\n",
            "Train Epoch: 10 [67840/123872 (55%)]\tLoss: 0.385217\tLR: 0.00017107\n",
            "Train Epoch: 10 [68096/123872 (55%)]\tLoss: 0.360841\tLR: 0.00017097\n",
            "Train Epoch: 10 [68352/123872 (55%)]\tLoss: 0.419283\tLR: 0.00017088\n",
            "Train Epoch: 10 [68608/123872 (55%)]\tLoss: 0.379251\tLR: 0.00017078\n",
            "Train Epoch: 10 [68864/123872 (56%)]\tLoss: 0.455768\tLR: 0.00017069\n",
            "Train Epoch: 10 [69120/123872 (56%)]\tLoss: 0.370926\tLR: 0.00017059\n",
            "Train Epoch: 10 [69120/123872 (56%)]\tLoss: 0.370926\n",
            "Train Epoch: 10 [69376/123872 (56%)]\tLoss: 0.375546\tLR: 0.00017050\n",
            "Train Epoch: 10 [69632/123872 (56%)]\tLoss: 0.374600\tLR: 0.00017040\n",
            "Train Epoch: 10 [69888/123872 (56%)]\tLoss: 0.301746\tLR: 0.00017031\n",
            "Train Epoch: 10 [70144/123872 (57%)]\tLoss: 0.446079\tLR: 0.00017021\n",
            "Train Epoch: 10 [70400/123872 (57%)]\tLoss: 0.384175\tLR: 0.00017012\n",
            "Train Epoch: 10 [70656/123872 (57%)]\tLoss: 0.333373\tLR: 0.00017002\n",
            "Train Epoch: 10 [70912/123872 (57%)]\tLoss: 0.426532\tLR: 0.00016993\n",
            "Train Epoch: 10 [71168/123872 (57%)]\tLoss: 0.343703\tLR: 0.00016983\n",
            "Train Epoch: 10 [71424/123872 (58%)]\tLoss: 0.311816\tLR: 0.00016974\n",
            "Train Epoch: 10 [71680/123872 (58%)]\tLoss: 0.364456\tLR: 0.00016964\n",
            "Train Epoch: 10 [71680/123872 (58%)]\tLoss: 0.364456\n",
            "Train Epoch: 10 [71936/123872 (58%)]\tLoss: 0.340539\tLR: 0.00016955\n",
            "Train Epoch: 10 [72192/123872 (58%)]\tLoss: 0.345760\tLR: 0.00016945\n",
            "Train Epoch: 10 [72448/123872 (58%)]\tLoss: 0.384756\tLR: 0.00016936\n",
            "Train Epoch: 10 [72704/123872 (59%)]\tLoss: 0.405061\tLR: 0.00016926\n",
            "Train Epoch: 10 [72960/123872 (59%)]\tLoss: 0.376119\tLR: 0.00016917\n",
            "Train Epoch: 10 [73216/123872 (59%)]\tLoss: 0.423885\tLR: 0.00016907\n",
            "Train Epoch: 10 [73472/123872 (59%)]\tLoss: 0.430996\tLR: 0.00016898\n",
            "Train Epoch: 10 [73728/123872 (60%)]\tLoss: 0.364297\tLR: 0.00016888\n",
            "Train Epoch: 10 [73984/123872 (60%)]\tLoss: 0.348316\tLR: 0.00016879\n",
            "Train Epoch: 10 [74240/123872 (60%)]\tLoss: 0.387517\tLR: 0.00016869\n",
            "Train Epoch: 10 [74240/123872 (60%)]\tLoss: 0.387517\n",
            "Train Epoch: 10 [74496/123872 (60%)]\tLoss: 0.411842\tLR: 0.00016860\n",
            "Train Epoch: 10 [74752/123872 (60%)]\tLoss: 0.440732\tLR: 0.00016850\n",
            "Train Epoch: 10 [75008/123872 (61%)]\tLoss: 0.453614\tLR: 0.00016841\n",
            "Train Epoch: 10 [75264/123872 (61%)]\tLoss: 0.412950\tLR: 0.00016832\n",
            "Train Epoch: 10 [75520/123872 (61%)]\tLoss: 0.354339\tLR: 0.00016822\n",
            "Train Epoch: 10 [75776/123872 (61%)]\tLoss: 0.425365\tLR: 0.00016813\n",
            "Train Epoch: 10 [76032/123872 (61%)]\tLoss: 0.368557\tLR: 0.00016803\n",
            "Train Epoch: 10 [76288/123872 (62%)]\tLoss: 0.376030\tLR: 0.00016794\n",
            "Train Epoch: 10 [76544/123872 (62%)]\tLoss: 0.355331\tLR: 0.00016784\n",
            "Train Epoch: 10 [76800/123872 (62%)]\tLoss: 0.336440\tLR: 0.00016775\n",
            "Train Epoch: 10 [76800/123872 (62%)]\tLoss: 0.336440\n",
            "Train Epoch: 10 [77056/123872 (62%)]\tLoss: 0.358065\tLR: 0.00016765\n",
            "Train Epoch: 10 [77312/123872 (62%)]\tLoss: 0.392123\tLR: 0.00016756\n",
            "Train Epoch: 10 [77568/123872 (63%)]\tLoss: 0.368955\tLR: 0.00016747\n",
            "Train Epoch: 10 [77824/123872 (63%)]\tLoss: 0.389942\tLR: 0.00016737\n",
            "Train Epoch: 10 [78080/123872 (63%)]\tLoss: 0.340981\tLR: 0.00016728\n",
            "Train Epoch: 10 [78336/123872 (63%)]\tLoss: 0.360652\tLR: 0.00016718\n",
            "Train Epoch: 10 [78592/123872 (63%)]\tLoss: 0.362913\tLR: 0.00016709\n",
            "Train Epoch: 10 [78848/123872 (64%)]\tLoss: 0.401173\tLR: 0.00016700\n",
            "Train Epoch: 10 [79104/123872 (64%)]\tLoss: 0.350283\tLR: 0.00016690\n",
            "Train Epoch: 10 [79360/123872 (64%)]\tLoss: 0.360946\tLR: 0.00016681\n",
            "Train Epoch: 10 [79360/123872 (64%)]\tLoss: 0.360946\n",
            "Train Epoch: 10 [79616/123872 (64%)]\tLoss: 0.322111\tLR: 0.00016671\n",
            "Train Epoch: 10 [79872/123872 (64%)]\tLoss: 0.370803\tLR: 0.00016662\n",
            "Train Epoch: 10 [80128/123872 (65%)]\tLoss: 0.390855\tLR: 0.00016653\n",
            "Train Epoch: 10 [80384/123872 (65%)]\tLoss: 0.399320\tLR: 0.00016643\n",
            "Train Epoch: 10 [80640/123872 (65%)]\tLoss: 0.382503\tLR: 0.00016634\n",
            "Train Epoch: 10 [80896/123872 (65%)]\tLoss: 0.368121\tLR: 0.00016624\n",
            "Train Epoch: 10 [81152/123872 (65%)]\tLoss: 0.380870\tLR: 0.00016615\n",
            "Train Epoch: 10 [81408/123872 (66%)]\tLoss: 0.341043\tLR: 0.00016606\n",
            "Train Epoch: 10 [81664/123872 (66%)]\tLoss: 0.416419\tLR: 0.00016596\n",
            "Train Epoch: 10 [81920/123872 (66%)]\tLoss: 0.439647\tLR: 0.00016587\n",
            "Train Epoch: 10 [81920/123872 (66%)]\tLoss: 0.439647\n",
            "Train Epoch: 10 [82176/123872 (66%)]\tLoss: 0.387835\tLR: 0.00016577\n",
            "Train Epoch: 10 [82432/123872 (67%)]\tLoss: 0.387243\tLR: 0.00016568\n",
            "Train Epoch: 10 [82688/123872 (67%)]\tLoss: 0.381018\tLR: 0.00016559\n",
            "Train Epoch: 10 [82944/123872 (67%)]\tLoss: 0.375454\tLR: 0.00016549\n",
            "Train Epoch: 10 [83200/123872 (67%)]\tLoss: 0.350112\tLR: 0.00016540\n",
            "Train Epoch: 10 [83456/123872 (67%)]\tLoss: 0.360667\tLR: 0.00016531\n",
            "Train Epoch: 10 [83712/123872 (68%)]\tLoss: 0.433725\tLR: 0.00016521\n",
            "Train Epoch: 10 [83968/123872 (68%)]\tLoss: 0.357010\tLR: 0.00016512\n",
            "Train Epoch: 10 [84224/123872 (68%)]\tLoss: 0.349032\tLR: 0.00016503\n",
            "Train Epoch: 10 [84480/123872 (68%)]\tLoss: 0.391981\tLR: 0.00016493\n",
            "Train Epoch: 10 [84480/123872 (68%)]\tLoss: 0.391981\n",
            "Train Epoch: 10 [84736/123872 (68%)]\tLoss: 0.411126\tLR: 0.00016484\n",
            "Train Epoch: 10 [84992/123872 (69%)]\tLoss: 0.350062\tLR: 0.00016475\n",
            "Train Epoch: 10 [85248/123872 (69%)]\tLoss: 0.341708\tLR: 0.00016465\n",
            "Train Epoch: 10 [85504/123872 (69%)]\tLoss: 0.344519\tLR: 0.00016456\n",
            "Train Epoch: 10 [85760/123872 (69%)]\tLoss: 0.419830\tLR: 0.00016447\n",
            "Train Epoch: 10 [86016/123872 (69%)]\tLoss: 0.387827\tLR: 0.00016437\n",
            "Train Epoch: 10 [86272/123872 (70%)]\tLoss: 0.329070\tLR: 0.00016428\n",
            "Train Epoch: 10 [86528/123872 (70%)]\tLoss: 0.360865\tLR: 0.00016419\n",
            "Train Epoch: 10 [86784/123872 (70%)]\tLoss: 0.404571\tLR: 0.00016409\n",
            "Train Epoch: 10 [87040/123872 (70%)]\tLoss: 0.344257\tLR: 0.00016400\n",
            "Train Epoch: 10 [87040/123872 (70%)]\tLoss: 0.344257\n",
            "Train Epoch: 10 [87296/123872 (70%)]\tLoss: 0.398178\tLR: 0.00016391\n",
            "Train Epoch: 10 [87552/123872 (71%)]\tLoss: 0.358339\tLR: 0.00016381\n",
            "Train Epoch: 10 [87808/123872 (71%)]\tLoss: 0.319283\tLR: 0.00016372\n",
            "Train Epoch: 10 [88064/123872 (71%)]\tLoss: 0.389580\tLR: 0.00016363\n",
            "Train Epoch: 10 [88320/123872 (71%)]\tLoss: 0.346128\tLR: 0.00016354\n",
            "Train Epoch: 10 [88576/123872 (71%)]\tLoss: 0.368759\tLR: 0.00016344\n",
            "Train Epoch: 10 [88832/123872 (72%)]\tLoss: 0.453532\tLR: 0.00016335\n",
            "Train Epoch: 10 [89088/123872 (72%)]\tLoss: 0.397303\tLR: 0.00016326\n",
            "Train Epoch: 10 [89344/123872 (72%)]\tLoss: 0.321313\tLR: 0.00016316\n",
            "Train Epoch: 10 [89600/123872 (72%)]\tLoss: 0.398812\tLR: 0.00016307\n",
            "Train Epoch: 10 [89600/123872 (72%)]\tLoss: 0.398812\n",
            "Train Epoch: 10 [89856/123872 (73%)]\tLoss: 0.313465\tLR: 0.00016298\n",
            "Train Epoch: 10 [90112/123872 (73%)]\tLoss: 0.378616\tLR: 0.00016289\n",
            "Train Epoch: 10 [90368/123872 (73%)]\tLoss: 0.362865\tLR: 0.00016279\n",
            "Train Epoch: 10 [90624/123872 (73%)]\tLoss: 0.384573\tLR: 0.00016270\n",
            "Train Epoch: 10 [90880/123872 (73%)]\tLoss: 0.356530\tLR: 0.00016261\n",
            "Train Epoch: 10 [91136/123872 (74%)]\tLoss: 0.356691\tLR: 0.00016252\n",
            "Train Epoch: 10 [91392/123872 (74%)]\tLoss: 0.396634\tLR: 0.00016242\n",
            "Train Epoch: 10 [91648/123872 (74%)]\tLoss: 0.291433\tLR: 0.00016233\n",
            "Train Epoch: 10 [91904/123872 (74%)]\tLoss: 0.424918\tLR: 0.00016224\n",
            "Train Epoch: 10 [92160/123872 (74%)]\tLoss: 0.362556\tLR: 0.00016215\n",
            "Train Epoch: 10 [92160/123872 (74%)]\tLoss: 0.362556\n",
            "Train Epoch: 10 [92416/123872 (75%)]\tLoss: 0.367882\tLR: 0.00016205\n",
            "Train Epoch: 10 [92672/123872 (75%)]\tLoss: 0.360924\tLR: 0.00016196\n",
            "Train Epoch: 10 [92928/123872 (75%)]\tLoss: 0.361682\tLR: 0.00016187\n",
            "Train Epoch: 10 [93184/123872 (75%)]\tLoss: 0.347241\tLR: 0.00016178\n",
            "Train Epoch: 10 [93440/123872 (75%)]\tLoss: 0.365357\tLR: 0.00016168\n",
            "Train Epoch: 10 [93696/123872 (76%)]\tLoss: 0.445628\tLR: 0.00016159\n",
            "Train Epoch: 10 [93952/123872 (76%)]\tLoss: 0.351085\tLR: 0.00016150\n",
            "Train Epoch: 10 [94208/123872 (76%)]\tLoss: 0.361565\tLR: 0.00016141\n",
            "Train Epoch: 10 [94464/123872 (76%)]\tLoss: 0.348223\tLR: 0.00016132\n",
            "Train Epoch: 10 [94720/123872 (76%)]\tLoss: 0.385878\tLR: 0.00016122\n",
            "Train Epoch: 10 [94720/123872 (76%)]\tLoss: 0.385878\n",
            "Train Epoch: 10 [94976/123872 (77%)]\tLoss: 0.370570\tLR: 0.00016113\n",
            "Train Epoch: 10 [95232/123872 (77%)]\tLoss: 0.369369\tLR: 0.00016104\n",
            "Train Epoch: 10 [95488/123872 (77%)]\tLoss: 0.399783\tLR: 0.00016095\n",
            "Train Epoch: 10 [95744/123872 (77%)]\tLoss: 0.341620\tLR: 0.00016085\n",
            "Train Epoch: 10 [96000/123872 (77%)]\tLoss: 0.406550\tLR: 0.00016076\n",
            "Train Epoch: 10 [96256/123872 (78%)]\tLoss: 0.338494\tLR: 0.00016067\n",
            "Train Epoch: 10 [96512/123872 (78%)]\tLoss: 0.373816\tLR: 0.00016058\n",
            "Train Epoch: 10 [96768/123872 (78%)]\tLoss: 0.410820\tLR: 0.00016049\n",
            "Train Epoch: 10 [97024/123872 (78%)]\tLoss: 0.354360\tLR: 0.00016040\n",
            "Train Epoch: 10 [97280/123872 (79%)]\tLoss: 0.380836\tLR: 0.00016030\n",
            "Train Epoch: 10 [97280/123872 (79%)]\tLoss: 0.380836\n",
            "Train Epoch: 10 [97536/123872 (79%)]\tLoss: 0.386003\tLR: 0.00016021\n",
            "Train Epoch: 10 [97792/123872 (79%)]\tLoss: 0.362036\tLR: 0.00016012\n",
            "Train Epoch: 10 [98048/123872 (79%)]\tLoss: 0.410256\tLR: 0.00016003\n",
            "Train Epoch: 10 [98304/123872 (79%)]\tLoss: 0.370699\tLR: 0.00015994\n",
            "Train Epoch: 10 [98560/123872 (80%)]\tLoss: 0.422035\tLR: 0.00015985\n",
            "Train Epoch: 10 [98816/123872 (80%)]\tLoss: 0.329301\tLR: 0.00015975\n",
            "Train Epoch: 10 [99072/123872 (80%)]\tLoss: 0.356246\tLR: 0.00015966\n",
            "Train Epoch: 10 [99328/123872 (80%)]\tLoss: 0.387377\tLR: 0.00015957\n",
            "Train Epoch: 10 [99584/123872 (80%)]\tLoss: 0.393345\tLR: 0.00015948\n",
            "Train Epoch: 10 [99840/123872 (81%)]\tLoss: 0.347564\tLR: 0.00015939\n",
            "Train Epoch: 10 [99840/123872 (81%)]\tLoss: 0.347564\n",
            "Train Epoch: 10 [100096/123872 (81%)]\tLoss: 0.366301\tLR: 0.00015930\n",
            "Train Epoch: 10 [100352/123872 (81%)]\tLoss: 0.407733\tLR: 0.00015920\n",
            "Train Epoch: 10 [100608/123872 (81%)]\tLoss: 0.351702\tLR: 0.00015911\n",
            "Train Epoch: 10 [100864/123872 (81%)]\tLoss: 0.432335\tLR: 0.00015902\n",
            "Train Epoch: 10 [101120/123872 (82%)]\tLoss: 0.402550\tLR: 0.00015893\n",
            "Train Epoch: 10 [101376/123872 (82%)]\tLoss: 0.388167\tLR: 0.00015884\n",
            "Train Epoch: 10 [101632/123872 (82%)]\tLoss: 0.379906\tLR: 0.00015875\n",
            "Train Epoch: 10 [101888/123872 (82%)]\tLoss: 0.401943\tLR: 0.00015866\n",
            "Train Epoch: 10 [102144/123872 (82%)]\tLoss: 0.350853\tLR: 0.00015857\n",
            "Train Epoch: 10 [102400/123872 (83%)]\tLoss: 0.366405\tLR: 0.00015847\n",
            "Train Epoch: 10 [102400/123872 (83%)]\tLoss: 0.366405\n",
            "Train Epoch: 10 [102656/123872 (83%)]\tLoss: 0.353424\tLR: 0.00015838\n",
            "Train Epoch: 10 [102912/123872 (83%)]\tLoss: 0.390481\tLR: 0.00015829\n",
            "Train Epoch: 10 [103168/123872 (83%)]\tLoss: 0.401109\tLR: 0.00015820\n",
            "Train Epoch: 10 [103424/123872 (83%)]\tLoss: 0.347178\tLR: 0.00015811\n",
            "Train Epoch: 10 [103680/123872 (84%)]\tLoss: 0.343961\tLR: 0.00015802\n",
            "Train Epoch: 10 [103936/123872 (84%)]\tLoss: 0.355801\tLR: 0.00015793\n",
            "Train Epoch: 10 [104192/123872 (84%)]\tLoss: 0.397044\tLR: 0.00015784\n",
            "Train Epoch: 10 [104448/123872 (84%)]\tLoss: 0.408164\tLR: 0.00015775\n",
            "Train Epoch: 10 [104704/123872 (85%)]\tLoss: 0.391358\tLR: 0.00015766\n",
            "Train Epoch: 10 [104960/123872 (85%)]\tLoss: 0.359277\tLR: 0.00015756\n",
            "Train Epoch: 10 [104960/123872 (85%)]\tLoss: 0.359277\n",
            "Train Epoch: 10 [105216/123872 (85%)]\tLoss: 0.313682\tLR: 0.00015747\n",
            "Train Epoch: 10 [105472/123872 (85%)]\tLoss: 0.361925\tLR: 0.00015738\n",
            "Train Epoch: 10 [105728/123872 (85%)]\tLoss: 0.356861\tLR: 0.00015729\n",
            "Train Epoch: 10 [105984/123872 (86%)]\tLoss: 0.301520\tLR: 0.00015720\n",
            "Train Epoch: 10 [106240/123872 (86%)]\tLoss: 0.378363\tLR: 0.00015711\n",
            "Train Epoch: 10 [106496/123872 (86%)]\tLoss: 0.392044\tLR: 0.00015702\n",
            "Train Epoch: 10 [106752/123872 (86%)]\tLoss: 0.319568\tLR: 0.00015693\n",
            "Train Epoch: 10 [107008/123872 (86%)]\tLoss: 0.379182\tLR: 0.00015684\n",
            "Train Epoch: 10 [107264/123872 (87%)]\tLoss: 0.331104\tLR: 0.00015675\n",
            "Train Epoch: 10 [107520/123872 (87%)]\tLoss: 0.365872\tLR: 0.00015666\n",
            "Train Epoch: 10 [107520/123872 (87%)]\tLoss: 0.365872\n",
            "Train Epoch: 10 [107776/123872 (87%)]\tLoss: 0.390797\tLR: 0.00015657\n",
            "Train Epoch: 10 [108032/123872 (87%)]\tLoss: 0.420584\tLR: 0.00015648\n",
            "Train Epoch: 10 [108288/123872 (87%)]\tLoss: 0.414181\tLR: 0.00015639\n",
            "Train Epoch: 10 [108544/123872 (88%)]\tLoss: 0.340260\tLR: 0.00015630\n",
            "Train Epoch: 10 [108800/123872 (88%)]\tLoss: 0.362729\tLR: 0.00015621\n",
            "Train Epoch: 10 [109056/123872 (88%)]\tLoss: 0.367829\tLR: 0.00015612\n",
            "Train Epoch: 10 [109312/123872 (88%)]\tLoss: 0.425106\tLR: 0.00015603\n",
            "Train Epoch: 10 [109568/123872 (88%)]\tLoss: 0.387797\tLR: 0.00015594\n",
            "Train Epoch: 10 [109824/123872 (89%)]\tLoss: 0.382214\tLR: 0.00015585\n",
            "Train Epoch: 10 [110080/123872 (89%)]\tLoss: 0.319930\tLR: 0.00015576\n",
            "Train Epoch: 10 [110080/123872 (89%)]\tLoss: 0.319930\n",
            "Train Epoch: 10 [110336/123872 (89%)]\tLoss: 0.341882\tLR: 0.00015567\n",
            "Train Epoch: 10 [110592/123872 (89%)]\tLoss: 0.341682\tLR: 0.00015558\n",
            "Train Epoch: 10 [110848/123872 (89%)]\tLoss: 0.336684\tLR: 0.00015548\n",
            "Train Epoch: 10 [111104/123872 (90%)]\tLoss: 0.365916\tLR: 0.00015539\n",
            "Train Epoch: 10 [111360/123872 (90%)]\tLoss: 0.347363\tLR: 0.00015530\n",
            "Train Epoch: 10 [111616/123872 (90%)]\tLoss: 0.387315\tLR: 0.00015522\n",
            "Train Epoch: 10 [111872/123872 (90%)]\tLoss: 0.433999\tLR: 0.00015513\n",
            "Train Epoch: 10 [112128/123872 (90%)]\tLoss: 0.397443\tLR: 0.00015504\n",
            "Train Epoch: 10 [112384/123872 (91%)]\tLoss: 0.391369\tLR: 0.00015495\n",
            "Train Epoch: 10 [112640/123872 (91%)]\tLoss: 0.364563\tLR: 0.00015486\n",
            "Train Epoch: 10 [112640/123872 (91%)]\tLoss: 0.364563\n",
            "Train Epoch: 10 [112896/123872 (91%)]\tLoss: 0.361005\tLR: 0.00015477\n",
            "Train Epoch: 10 [113152/123872 (91%)]\tLoss: 0.337431\tLR: 0.00015468\n",
            "Train Epoch: 10 [113408/123872 (92%)]\tLoss: 0.354868\tLR: 0.00015459\n",
            "Train Epoch: 10 [113664/123872 (92%)]\tLoss: 0.323494\tLR: 0.00015450\n",
            "Train Epoch: 10 [113920/123872 (92%)]\tLoss: 0.371103\tLR: 0.00015441\n",
            "Train Epoch: 10 [114176/123872 (92%)]\tLoss: 0.336436\tLR: 0.00015432\n",
            "Train Epoch: 10 [114432/123872 (92%)]\tLoss: 0.331254\tLR: 0.00015423\n",
            "Train Epoch: 10 [114688/123872 (93%)]\tLoss: 0.365381\tLR: 0.00015414\n",
            "Train Epoch: 10 [114944/123872 (93%)]\tLoss: 0.334884\tLR: 0.00015405\n",
            "Train Epoch: 10 [115200/123872 (93%)]\tLoss: 0.453634\tLR: 0.00015396\n",
            "Train Epoch: 10 [115200/123872 (93%)]\tLoss: 0.453634\n",
            "Train Epoch: 10 [115456/123872 (93%)]\tLoss: 0.328832\tLR: 0.00015387\n",
            "Train Epoch: 10 [115712/123872 (93%)]\tLoss: 0.420181\tLR: 0.00015378\n",
            "Train Epoch: 10 [115968/123872 (94%)]\tLoss: 0.373076\tLR: 0.00015369\n",
            "Train Epoch: 10 [116224/123872 (94%)]\tLoss: 0.355947\tLR: 0.00015360\n",
            "Train Epoch: 10 [116480/123872 (94%)]\tLoss: 0.391361\tLR: 0.00015351\n",
            "Train Epoch: 10 [116736/123872 (94%)]\tLoss: 0.379566\tLR: 0.00015342\n",
            "Train Epoch: 10 [116992/123872 (94%)]\tLoss: 0.351479\tLR: 0.00015333\n",
            "Train Epoch: 10 [117248/123872 (95%)]\tLoss: 0.313229\tLR: 0.00015324\n",
            "Train Epoch: 10 [117504/123872 (95%)]\tLoss: 0.352479\tLR: 0.00015316\n",
            "Train Epoch: 10 [117760/123872 (95%)]\tLoss: 0.370991\tLR: 0.00015307\n",
            "Train Epoch: 10 [117760/123872 (95%)]\tLoss: 0.370991\n",
            "Train Epoch: 10 [118016/123872 (95%)]\tLoss: 0.329819\tLR: 0.00015298\n",
            "Train Epoch: 10 [118272/123872 (95%)]\tLoss: 0.347235\tLR: 0.00015289\n",
            "Train Epoch: 10 [118528/123872 (96%)]\tLoss: 0.491988\tLR: 0.00015280\n",
            "Train Epoch: 10 [118784/123872 (96%)]\tLoss: 0.410406\tLR: 0.00015271\n",
            "Train Epoch: 10 [119040/123872 (96%)]\tLoss: 0.390552\tLR: 0.00015262\n",
            "Train Epoch: 10 [119296/123872 (96%)]\tLoss: 0.412511\tLR: 0.00015253\n",
            "Train Epoch: 10 [119552/123872 (96%)]\tLoss: 0.348358\tLR: 0.00015244\n",
            "Train Epoch: 10 [119808/123872 (97%)]\tLoss: 0.383249\tLR: 0.00015235\n",
            "Train Epoch: 10 [120064/123872 (97%)]\tLoss: 0.401906\tLR: 0.00015227\n",
            "Train Epoch: 10 [120320/123872 (97%)]\tLoss: 0.323135\tLR: 0.00015218\n",
            "Train Epoch: 10 [120320/123872 (97%)]\tLoss: 0.323135\n",
            "Train Epoch: 10 [120576/123872 (97%)]\tLoss: 0.371564\tLR: 0.00015209\n",
            "Train Epoch: 10 [120832/123872 (98%)]\tLoss: 0.327064\tLR: 0.00015200\n",
            "Train Epoch: 10 [121088/123872 (98%)]\tLoss: 0.372306\tLR: 0.00015191\n",
            "Train Epoch: 10 [121344/123872 (98%)]\tLoss: 0.356884\tLR: 0.00015182\n",
            "Train Epoch: 10 [121600/123872 (98%)]\tLoss: 0.338368\tLR: 0.00015173\n",
            "Train Epoch: 10 [121856/123872 (98%)]\tLoss: 0.322063\tLR: 0.00015164\n",
            "Train Epoch: 10 [122112/123872 (99%)]\tLoss: 0.353703\tLR: 0.00015156\n",
            "Train Epoch: 10 [122368/123872 (99%)]\tLoss: 0.395128\tLR: 0.00015147\n",
            "Train Epoch: 10 [122624/123872 (99%)]\tLoss: 0.376372\tLR: 0.00015138\n",
            "Train Epoch: 10 [122880/123872 (99%)]\tLoss: 0.385438\tLR: 0.00015129\n",
            "Train Epoch: 10 [122880/123872 (99%)]\tLoss: 0.385438\n",
            "Train Epoch: 10 [123136/123872 (99%)]\tLoss: 0.308157\tLR: 0.00015120\n",
            "Train Epoch: 10 [123392/123872 (100%)]\tLoss: 0.404871\tLR: 0.00015111\n",
            "Train Epoch: 10 [108192/123872 (100%)]\tLoss: 0.386197\tLR: 0.00015102\n",
            "\n",
            "Test set: Average loss: 0.0015, Accuracy: 25574/30970 (82.58%)\n",
            "\n",
            "Train Epoch: 11 [0/123872 (0%)]\tLoss: 0.409054\tLR: 0.00015094\n",
            "Train Epoch: 11 [0/123872 (0%)]\tLoss: 0.409054\n",
            "Train Epoch: 11 [256/123872 (0%)]\tLoss: 0.393671\tLR: 0.00015085\n",
            "Train Epoch: 11 [512/123872 (0%)]\tLoss: 0.370366\tLR: 0.00015076\n",
            "Train Epoch: 11 [768/123872 (1%)]\tLoss: 0.379286\tLR: 0.00015067\n",
            "Train Epoch: 11 [1024/123872 (1%)]\tLoss: 0.436223\tLR: 0.00015058\n",
            "Train Epoch: 11 [1280/123872 (1%)]\tLoss: 0.350294\tLR: 0.00015050\n",
            "Train Epoch: 11 [1536/123872 (1%)]\tLoss: 0.320094\tLR: 0.00015041\n",
            "Train Epoch: 11 [1792/123872 (1%)]\tLoss: 0.438876\tLR: 0.00015032\n",
            "Train Epoch: 11 [2048/123872 (2%)]\tLoss: 0.391534\tLR: 0.00015023\n",
            "Train Epoch: 11 [2304/123872 (2%)]\tLoss: 0.375404\tLR: 0.00015014\n",
            "Train Epoch: 11 [2560/123872 (2%)]\tLoss: 0.368813\tLR: 0.00015006\n",
            "Train Epoch: 11 [2560/123872 (2%)]\tLoss: 0.368813\n",
            "Train Epoch: 11 [2816/123872 (2%)]\tLoss: 0.340090\tLR: 0.00014997\n",
            "Train Epoch: 11 [3072/123872 (2%)]\tLoss: 0.359313\tLR: 0.00014988\n",
            "Train Epoch: 11 [3328/123872 (3%)]\tLoss: 0.394993\tLR: 0.00014979\n",
            "Train Epoch: 11 [3584/123872 (3%)]\tLoss: 0.388072\tLR: 0.00014970\n",
            "Train Epoch: 11 [3840/123872 (3%)]\tLoss: 0.334249\tLR: 0.00014962\n",
            "Train Epoch: 11 [4096/123872 (3%)]\tLoss: 0.399068\tLR: 0.00014953\n",
            "Train Epoch: 11 [4352/123872 (4%)]\tLoss: 0.374397\tLR: 0.00014944\n",
            "Train Epoch: 11 [4608/123872 (4%)]\tLoss: 0.351824\tLR: 0.00014935\n",
            "Train Epoch: 11 [4864/123872 (4%)]\tLoss: 0.367570\tLR: 0.00014926\n",
            "Train Epoch: 11 [5120/123872 (4%)]\tLoss: 0.395382\tLR: 0.00014918\n",
            "Train Epoch: 11 [5120/123872 (4%)]\tLoss: 0.395382\n",
            "Train Epoch: 11 [5376/123872 (4%)]\tLoss: 0.372571\tLR: 0.00014909\n",
            "Train Epoch: 11 [5632/123872 (5%)]\tLoss: 0.397958\tLR: 0.00014900\n",
            "Train Epoch: 11 [5888/123872 (5%)]\tLoss: 0.345440\tLR: 0.00014891\n",
            "Train Epoch: 11 [6144/123872 (5%)]\tLoss: 0.337123\tLR: 0.00014883\n",
            "Train Epoch: 11 [6400/123872 (5%)]\tLoss: 0.385209\tLR: 0.00014874\n",
            "Train Epoch: 11 [6656/123872 (5%)]\tLoss: 0.377675\tLR: 0.00014865\n",
            "Train Epoch: 11 [6912/123872 (6%)]\tLoss: 0.362784\tLR: 0.00014856\n",
            "Train Epoch: 11 [7168/123872 (6%)]\tLoss: 0.317776\tLR: 0.00014848\n",
            "Train Epoch: 11 [7424/123872 (6%)]\tLoss: 0.393153\tLR: 0.00014839\n",
            "Train Epoch: 11 [7680/123872 (6%)]\tLoss: 0.358411\tLR: 0.00014830\n",
            "Train Epoch: 11 [7680/123872 (6%)]\tLoss: 0.358411\n",
            "Train Epoch: 11 [7936/123872 (6%)]\tLoss: 0.376715\tLR: 0.00014822\n",
            "Train Epoch: 11 [8192/123872 (7%)]\tLoss: 0.354453\tLR: 0.00014813\n",
            "Train Epoch: 11 [8448/123872 (7%)]\tLoss: 0.351806\tLR: 0.00014804\n",
            "Train Epoch: 11 [8704/123872 (7%)]\tLoss: 0.335792\tLR: 0.00014795\n",
            "Train Epoch: 11 [8960/123872 (7%)]\tLoss: 0.279201\tLR: 0.00014787\n",
            "Train Epoch: 11 [9216/123872 (7%)]\tLoss: 0.392422\tLR: 0.00014778\n",
            "Train Epoch: 11 [9472/123872 (8%)]\tLoss: 0.335236\tLR: 0.00014769\n",
            "Train Epoch: 11 [9728/123872 (8%)]\tLoss: 0.377674\tLR: 0.00014761\n",
            "Train Epoch: 11 [9984/123872 (8%)]\tLoss: 0.320979\tLR: 0.00014752\n",
            "Train Epoch: 11 [10240/123872 (8%)]\tLoss: 0.364041\tLR: 0.00014743\n",
            "Train Epoch: 11 [10240/123872 (8%)]\tLoss: 0.364041\n",
            "Train Epoch: 11 [10496/123872 (8%)]\tLoss: 0.382558\tLR: 0.00014734\n",
            "Train Epoch: 11 [10752/123872 (9%)]\tLoss: 0.403113\tLR: 0.00014726\n",
            "Train Epoch: 11 [11008/123872 (9%)]\tLoss: 0.387746\tLR: 0.00014717\n",
            "Train Epoch: 11 [11264/123872 (9%)]\tLoss: 0.346988\tLR: 0.00014708\n",
            "Train Epoch: 11 [11520/123872 (9%)]\tLoss: 0.405393\tLR: 0.00014700\n",
            "Train Epoch: 11 [11776/123872 (10%)]\tLoss: 0.407817\tLR: 0.00014691\n",
            "Train Epoch: 11 [12032/123872 (10%)]\tLoss: 0.327483\tLR: 0.00014682\n",
            "Train Epoch: 11 [12288/123872 (10%)]\tLoss: 0.394696\tLR: 0.00014674\n",
            "Train Epoch: 11 [12544/123872 (10%)]\tLoss: 0.369346\tLR: 0.00014665\n",
            "Train Epoch: 11 [12800/123872 (10%)]\tLoss: 0.361987\tLR: 0.00014656\n",
            "Train Epoch: 11 [12800/123872 (10%)]\tLoss: 0.361987\n",
            "Train Epoch: 11 [13056/123872 (11%)]\tLoss: 0.381296\tLR: 0.00014648\n",
            "Train Epoch: 11 [13312/123872 (11%)]\tLoss: 0.317502\tLR: 0.00014639\n",
            "Train Epoch: 11 [13568/123872 (11%)]\tLoss: 0.347292\tLR: 0.00014630\n",
            "Train Epoch: 11 [13824/123872 (11%)]\tLoss: 0.340766\tLR: 0.00014622\n",
            "Train Epoch: 11 [14080/123872 (11%)]\tLoss: 0.364818\tLR: 0.00014613\n",
            "Train Epoch: 11 [14336/123872 (12%)]\tLoss: 0.346406\tLR: 0.00014604\n",
            "Train Epoch: 11 [14592/123872 (12%)]\tLoss: 0.373682\tLR: 0.00014596\n",
            "Train Epoch: 11 [14848/123872 (12%)]\tLoss: 0.387516\tLR: 0.00014587\n",
            "Train Epoch: 11 [15104/123872 (12%)]\tLoss: 0.329460\tLR: 0.00014579\n",
            "Train Epoch: 11 [15360/123872 (12%)]\tLoss: 0.285933\tLR: 0.00014570\n",
            "Train Epoch: 11 [15360/123872 (12%)]\tLoss: 0.285933\n",
            "Train Epoch: 11 [15616/123872 (13%)]\tLoss: 0.371423\tLR: 0.00014561\n",
            "Train Epoch: 11 [15872/123872 (13%)]\tLoss: 0.376239\tLR: 0.00014553\n",
            "Train Epoch: 11 [16128/123872 (13%)]\tLoss: 0.389601\tLR: 0.00014544\n",
            "Train Epoch: 11 [16384/123872 (13%)]\tLoss: 0.393161\tLR: 0.00014535\n",
            "Train Epoch: 11 [16640/123872 (13%)]\tLoss: 0.319283\tLR: 0.00014527\n",
            "Train Epoch: 11 [16896/123872 (14%)]\tLoss: 0.323541\tLR: 0.00014518\n",
            "Train Epoch: 11 [17152/123872 (14%)]\tLoss: 0.377991\tLR: 0.00014510\n",
            "Train Epoch: 11 [17408/123872 (14%)]\tLoss: 0.404330\tLR: 0.00014501\n",
            "Train Epoch: 11 [17664/123872 (14%)]\tLoss: 0.375777\tLR: 0.00014492\n",
            "Train Epoch: 11 [17920/123872 (14%)]\tLoss: 0.394484\tLR: 0.00014484\n",
            "Train Epoch: 11 [17920/123872 (14%)]\tLoss: 0.394484\n",
            "Train Epoch: 11 [18176/123872 (15%)]\tLoss: 0.362504\tLR: 0.00014475\n",
            "Train Epoch: 11 [18432/123872 (15%)]\tLoss: 0.350305\tLR: 0.00014467\n",
            "Train Epoch: 11 [18688/123872 (15%)]\tLoss: 0.406028\tLR: 0.00014458\n",
            "Train Epoch: 11 [18944/123872 (15%)]\tLoss: 0.360519\tLR: 0.00014450\n",
            "Train Epoch: 11 [19200/123872 (15%)]\tLoss: 0.371714\tLR: 0.00014441\n",
            "Train Epoch: 11 [19456/123872 (16%)]\tLoss: 0.322398\tLR: 0.00014432\n",
            "Train Epoch: 11 [19712/123872 (16%)]\tLoss: 0.349541\tLR: 0.00014424\n",
            "Train Epoch: 11 [19968/123872 (16%)]\tLoss: 0.389040\tLR: 0.00014415\n",
            "Train Epoch: 11 [20224/123872 (16%)]\tLoss: 0.342298\tLR: 0.00014407\n",
            "Train Epoch: 11 [20480/123872 (17%)]\tLoss: 0.312748\tLR: 0.00014398\n",
            "Train Epoch: 11 [20480/123872 (17%)]\tLoss: 0.312748\n",
            "Train Epoch: 11 [20736/123872 (17%)]\tLoss: 0.371277\tLR: 0.00014390\n",
            "Train Epoch: 11 [20992/123872 (17%)]\tLoss: 0.412195\tLR: 0.00014381\n",
            "Train Epoch: 11 [21248/123872 (17%)]\tLoss: 0.411363\tLR: 0.00014373\n",
            "Train Epoch: 11 [21504/123872 (17%)]\tLoss: 0.363913\tLR: 0.00014364\n",
            "Train Epoch: 11 [21760/123872 (18%)]\tLoss: 0.341981\tLR: 0.00014355\n",
            "Train Epoch: 11 [22016/123872 (18%)]\tLoss: 0.393104\tLR: 0.00014347\n",
            "Train Epoch: 11 [22272/123872 (18%)]\tLoss: 0.409102\tLR: 0.00014338\n",
            "Train Epoch: 11 [22528/123872 (18%)]\tLoss: 0.365004\tLR: 0.00014330\n",
            "Train Epoch: 11 [22784/123872 (18%)]\tLoss: 0.421008\tLR: 0.00014321\n",
            "Train Epoch: 11 [23040/123872 (19%)]\tLoss: 0.442805\tLR: 0.00014313\n",
            "Train Epoch: 11 [23040/123872 (19%)]\tLoss: 0.442805\n",
            "Train Epoch: 11 [23296/123872 (19%)]\tLoss: 0.419145\tLR: 0.00014304\n",
            "Train Epoch: 11 [23552/123872 (19%)]\tLoss: 0.340615\tLR: 0.00014296\n",
            "Train Epoch: 11 [23808/123872 (19%)]\tLoss: 0.365717\tLR: 0.00014287\n",
            "Train Epoch: 11 [24064/123872 (19%)]\tLoss: 0.405008\tLR: 0.00014279\n",
            "Train Epoch: 11 [24320/123872 (20%)]\tLoss: 0.338239\tLR: 0.00014270\n",
            "Train Epoch: 11 [24576/123872 (20%)]\tLoss: 0.401799\tLR: 0.00014262\n",
            "Train Epoch: 11 [24832/123872 (20%)]\tLoss: 0.359953\tLR: 0.00014253\n",
            "Train Epoch: 11 [25088/123872 (20%)]\tLoss: 0.381019\tLR: 0.00014245\n",
            "Train Epoch: 11 [25344/123872 (20%)]\tLoss: 0.320243\tLR: 0.00014236\n",
            "Train Epoch: 11 [25600/123872 (21%)]\tLoss: 0.340588\tLR: 0.00014228\n",
            "Train Epoch: 11 [25600/123872 (21%)]\tLoss: 0.340588\n",
            "Train Epoch: 11 [25856/123872 (21%)]\tLoss: 0.344376\tLR: 0.00014219\n",
            "Train Epoch: 11 [26112/123872 (21%)]\tLoss: 0.323570\tLR: 0.00014211\n",
            "Train Epoch: 11 [26368/123872 (21%)]\tLoss: 0.315854\tLR: 0.00014202\n",
            "Train Epoch: 11 [26624/123872 (21%)]\tLoss: 0.342673\tLR: 0.00014194\n",
            "Train Epoch: 11 [26880/123872 (22%)]\tLoss: 0.357640\tLR: 0.00014185\n",
            "Train Epoch: 11 [27136/123872 (22%)]\tLoss: 0.332867\tLR: 0.00014177\n",
            "Train Epoch: 11 [27392/123872 (22%)]\tLoss: 0.337526\tLR: 0.00014169\n",
            "Train Epoch: 11 [27648/123872 (22%)]\tLoss: 0.387978\tLR: 0.00014160\n",
            "Train Epoch: 11 [27904/123872 (23%)]\tLoss: 0.316314\tLR: 0.00014152\n",
            "Train Epoch: 11 [28160/123872 (23%)]\tLoss: 0.360584\tLR: 0.00014143\n",
            "Train Epoch: 11 [28160/123872 (23%)]\tLoss: 0.360584\n",
            "Train Epoch: 11 [28416/123872 (23%)]\tLoss: 0.333275\tLR: 0.00014135\n",
            "Train Epoch: 11 [28672/123872 (23%)]\tLoss: 0.348810\tLR: 0.00014126\n",
            "Train Epoch: 11 [28928/123872 (23%)]\tLoss: 0.374455\tLR: 0.00014118\n",
            "Train Epoch: 11 [29184/123872 (24%)]\tLoss: 0.466572\tLR: 0.00014109\n",
            "Train Epoch: 11 [29440/123872 (24%)]\tLoss: 0.375742\tLR: 0.00014101\n",
            "Train Epoch: 11 [29696/123872 (24%)]\tLoss: 0.313773\tLR: 0.00014093\n",
            "Train Epoch: 11 [29952/123872 (24%)]\tLoss: 0.362070\tLR: 0.00014084\n",
            "Train Epoch: 11 [30208/123872 (24%)]\tLoss: 0.329586\tLR: 0.00014076\n",
            "Train Epoch: 11 [30464/123872 (25%)]\tLoss: 0.361860\tLR: 0.00014067\n",
            "Train Epoch: 11 [30720/123872 (25%)]\tLoss: 0.395398\tLR: 0.00014059\n",
            "Train Epoch: 11 [30720/123872 (25%)]\tLoss: 0.395398\n",
            "Train Epoch: 11 [30976/123872 (25%)]\tLoss: 0.364289\tLR: 0.00014050\n",
            "Train Epoch: 11 [31232/123872 (25%)]\tLoss: 0.333614\tLR: 0.00014042\n",
            "Train Epoch: 11 [31488/123872 (25%)]\tLoss: 0.382235\tLR: 0.00014034\n",
            "Train Epoch: 11 [31744/123872 (26%)]\tLoss: 0.346538\tLR: 0.00014025\n",
            "Train Epoch: 11 [32000/123872 (26%)]\tLoss: 0.363000\tLR: 0.00014017\n",
            "Train Epoch: 11 [32256/123872 (26%)]\tLoss: 0.373957\tLR: 0.00014009\n",
            "Train Epoch: 11 [32512/123872 (26%)]\tLoss: 0.360976\tLR: 0.00014000\n",
            "Train Epoch: 11 [32768/123872 (26%)]\tLoss: 0.355051\tLR: 0.00013992\n",
            "Train Epoch: 11 [33024/123872 (27%)]\tLoss: 0.346181\tLR: 0.00013983\n",
            "Train Epoch: 11 [33280/123872 (27%)]\tLoss: 0.341188\tLR: 0.00013975\n",
            "Train Epoch: 11 [33280/123872 (27%)]\tLoss: 0.341188\n",
            "Train Epoch: 11 [33536/123872 (27%)]\tLoss: 0.399336\tLR: 0.00013967\n",
            "Train Epoch: 11 [33792/123872 (27%)]\tLoss: 0.314930\tLR: 0.00013958\n",
            "Train Epoch: 11 [34048/123872 (27%)]\tLoss: 0.340095\tLR: 0.00013950\n",
            "Train Epoch: 11 [34304/123872 (28%)]\tLoss: 0.368464\tLR: 0.00013942\n",
            "Train Epoch: 11 [34560/123872 (28%)]\tLoss: 0.343921\tLR: 0.00013933\n",
            "Train Epoch: 11 [34816/123872 (28%)]\tLoss: 0.335909\tLR: 0.00013925\n",
            "Train Epoch: 11 [35072/123872 (28%)]\tLoss: 0.372148\tLR: 0.00013916\n",
            "Train Epoch: 11 [35328/123872 (29%)]\tLoss: 0.359750\tLR: 0.00013908\n",
            "Train Epoch: 11 [35584/123872 (29%)]\tLoss: 0.421912\tLR: 0.00013900\n",
            "Train Epoch: 11 [35840/123872 (29%)]\tLoss: 0.408918\tLR: 0.00013891\n",
            "Train Epoch: 11 [35840/123872 (29%)]\tLoss: 0.408918\n",
            "Train Epoch: 11 [36096/123872 (29%)]\tLoss: 0.310350\tLR: 0.00013883\n",
            "Train Epoch: 11 [36352/123872 (29%)]\tLoss: 0.275617\tLR: 0.00013875\n",
            "Train Epoch: 11 [36608/123872 (30%)]\tLoss: 0.345589\tLR: 0.00013866\n",
            "Train Epoch: 11 [36864/123872 (30%)]\tLoss: 0.371554\tLR: 0.00013858\n",
            "Train Epoch: 11 [37120/123872 (30%)]\tLoss: 0.426376\tLR: 0.00013850\n",
            "Train Epoch: 11 [37376/123872 (30%)]\tLoss: 0.341345\tLR: 0.00013841\n",
            "Train Epoch: 11 [37632/123872 (30%)]\tLoss: 0.324035\tLR: 0.00013833\n",
            "Train Epoch: 11 [37888/123872 (31%)]\tLoss: 0.388067\tLR: 0.00013825\n",
            "Train Epoch: 11 [38144/123872 (31%)]\tLoss: 0.366034\tLR: 0.00013817\n",
            "Train Epoch: 11 [38400/123872 (31%)]\tLoss: 0.341986\tLR: 0.00013808\n",
            "Train Epoch: 11 [38400/123872 (31%)]\tLoss: 0.341986\n",
            "Train Epoch: 11 [38656/123872 (31%)]\tLoss: 0.370032\tLR: 0.00013800\n",
            "Train Epoch: 11 [38912/123872 (31%)]\tLoss: 0.342565\tLR: 0.00013792\n",
            "Train Epoch: 11 [39168/123872 (32%)]\tLoss: 0.331143\tLR: 0.00013783\n",
            "Train Epoch: 11 [39424/123872 (32%)]\tLoss: 0.377704\tLR: 0.00013775\n",
            "Train Epoch: 11 [39680/123872 (32%)]\tLoss: 0.347499\tLR: 0.00013767\n",
            "Train Epoch: 11 [39936/123872 (32%)]\tLoss: 0.382621\tLR: 0.00013759\n",
            "Train Epoch: 11 [40192/123872 (32%)]\tLoss: 0.393920\tLR: 0.00013750\n",
            "Train Epoch: 11 [40448/123872 (33%)]\tLoss: 0.372587\tLR: 0.00013742\n",
            "Train Epoch: 11 [40704/123872 (33%)]\tLoss: 0.333170\tLR: 0.00013734\n",
            "Train Epoch: 11 [40960/123872 (33%)]\tLoss: 0.377699\tLR: 0.00013725\n",
            "Train Epoch: 11 [40960/123872 (33%)]\tLoss: 0.377699\n",
            "Train Epoch: 11 [41216/123872 (33%)]\tLoss: 0.332688\tLR: 0.00013717\n",
            "Train Epoch: 11 [41472/123872 (33%)]\tLoss: 0.374326\tLR: 0.00013709\n",
            "Train Epoch: 11 [41728/123872 (34%)]\tLoss: 0.411067\tLR: 0.00013701\n",
            "Train Epoch: 11 [41984/123872 (34%)]\tLoss: 0.351045\tLR: 0.00013692\n",
            "Train Epoch: 11 [42240/123872 (34%)]\tLoss: 0.337133\tLR: 0.00013684\n",
            "Train Epoch: 11 [42496/123872 (34%)]\tLoss: 0.378220\tLR: 0.00013676\n",
            "Train Epoch: 11 [42752/123872 (35%)]\tLoss: 0.351809\tLR: 0.00013668\n",
            "Train Epoch: 11 [43008/123872 (35%)]\tLoss: 0.350132\tLR: 0.00013659\n",
            "Train Epoch: 11 [43264/123872 (35%)]\tLoss: 0.401155\tLR: 0.00013651\n",
            "Train Epoch: 11 [43520/123872 (35%)]\tLoss: 0.393613\tLR: 0.00013643\n",
            "Train Epoch: 11 [43520/123872 (35%)]\tLoss: 0.393613\n",
            "Train Epoch: 11 [43776/123872 (35%)]\tLoss: 0.392917\tLR: 0.00013635\n",
            "Train Epoch: 11 [44032/123872 (36%)]\tLoss: 0.369745\tLR: 0.00013627\n",
            "Train Epoch: 11 [44288/123872 (36%)]\tLoss: 0.415079\tLR: 0.00013618\n",
            "Train Epoch: 11 [44544/123872 (36%)]\tLoss: 0.313569\tLR: 0.00013610\n",
            "Train Epoch: 11 [44800/123872 (36%)]\tLoss: 0.398974\tLR: 0.00013602\n",
            "Train Epoch: 11 [45056/123872 (36%)]\tLoss: 0.346649\tLR: 0.00013594\n",
            "Train Epoch: 11 [45312/123872 (37%)]\tLoss: 0.411706\tLR: 0.00013585\n",
            "Train Epoch: 11 [45568/123872 (37%)]\tLoss: 0.360984\tLR: 0.00013577\n",
            "Train Epoch: 11 [45824/123872 (37%)]\tLoss: 0.412693\tLR: 0.00013569\n",
            "Train Epoch: 11 [46080/123872 (37%)]\tLoss: 0.406320\tLR: 0.00013561\n",
            "Train Epoch: 11 [46080/123872 (37%)]\tLoss: 0.406320\n",
            "Train Epoch: 11 [46336/123872 (37%)]\tLoss: 0.371513\tLR: 0.00013553\n",
            "Train Epoch: 11 [46592/123872 (38%)]\tLoss: 0.322694\tLR: 0.00013545\n",
            "Train Epoch: 11 [46848/123872 (38%)]\tLoss: 0.364532\tLR: 0.00013536\n",
            "Train Epoch: 11 [47104/123872 (38%)]\tLoss: 0.396723\tLR: 0.00013528\n",
            "Train Epoch: 11 [47360/123872 (38%)]\tLoss: 0.381247\tLR: 0.00013520\n",
            "Train Epoch: 11 [47616/123872 (38%)]\tLoss: 0.350801\tLR: 0.00013512\n",
            "Train Epoch: 11 [47872/123872 (39%)]\tLoss: 0.320362\tLR: 0.00013504\n",
            "Train Epoch: 11 [48128/123872 (39%)]\tLoss: 0.371558\tLR: 0.00013495\n",
            "Train Epoch: 11 [48384/123872 (39%)]\tLoss: 0.318914\tLR: 0.00013487\n",
            "Train Epoch: 11 [48640/123872 (39%)]\tLoss: 0.431587\tLR: 0.00013479\n",
            "Train Epoch: 11 [48640/123872 (39%)]\tLoss: 0.431587\n",
            "Train Epoch: 11 [48896/123872 (39%)]\tLoss: 0.489011\tLR: 0.00013471\n",
            "Train Epoch: 11 [49152/123872 (40%)]\tLoss: 0.421235\tLR: 0.00013463\n",
            "Train Epoch: 11 [49408/123872 (40%)]\tLoss: 0.331395\tLR: 0.00013455\n",
            "Train Epoch: 11 [49664/123872 (40%)]\tLoss: 0.315927\tLR: 0.00013447\n",
            "Train Epoch: 11 [49920/123872 (40%)]\tLoss: 0.456355\tLR: 0.00013438\n",
            "Train Epoch: 11 [50176/123872 (40%)]\tLoss: 0.345147\tLR: 0.00013430\n",
            "Train Epoch: 11 [50432/123872 (41%)]\tLoss: 0.342215\tLR: 0.00013422\n",
            "Train Epoch: 11 [50688/123872 (41%)]\tLoss: 0.384697\tLR: 0.00013414\n",
            "Train Epoch: 11 [50944/123872 (41%)]\tLoss: 0.395070\tLR: 0.00013406\n",
            "Train Epoch: 11 [51200/123872 (41%)]\tLoss: 0.357846\tLR: 0.00013398\n",
            "Train Epoch: 11 [51200/123872 (41%)]\tLoss: 0.357846\n",
            "Train Epoch: 11 [51456/123872 (42%)]\tLoss: 0.320058\tLR: 0.00013390\n",
            "Train Epoch: 11 [51712/123872 (42%)]\tLoss: 0.371493\tLR: 0.00013382\n",
            "Train Epoch: 11 [51968/123872 (42%)]\tLoss: 0.350891\tLR: 0.00013374\n",
            "Train Epoch: 11 [52224/123872 (42%)]\tLoss: 0.377110\tLR: 0.00013365\n",
            "Train Epoch: 11 [52480/123872 (42%)]\tLoss: 0.356291\tLR: 0.00013357\n",
            "Train Epoch: 11 [52736/123872 (43%)]\tLoss: 0.333950\tLR: 0.00013349\n",
            "Train Epoch: 11 [52992/123872 (43%)]\tLoss: 0.314492\tLR: 0.00013341\n",
            "Train Epoch: 11 [53248/123872 (43%)]\tLoss: 0.383703\tLR: 0.00013333\n",
            "Train Epoch: 11 [53504/123872 (43%)]\tLoss: 0.359050\tLR: 0.00013325\n",
            "Train Epoch: 11 [53760/123872 (43%)]\tLoss: 0.377781\tLR: 0.00013317\n",
            "Train Epoch: 11 [53760/123872 (43%)]\tLoss: 0.377781\n",
            "Train Epoch: 11 [54016/123872 (44%)]\tLoss: 0.380437\tLR: 0.00013309\n",
            "Train Epoch: 11 [54272/123872 (44%)]\tLoss: 0.399340\tLR: 0.00013301\n",
            "Train Epoch: 11 [54528/123872 (44%)]\tLoss: 0.351530\tLR: 0.00013293\n",
            "Train Epoch: 11 [54784/123872 (44%)]\tLoss: 0.321198\tLR: 0.00013285\n",
            "Train Epoch: 11 [55040/123872 (44%)]\tLoss: 0.393520\tLR: 0.00013277\n",
            "Train Epoch: 11 [55296/123872 (45%)]\tLoss: 0.363010\tLR: 0.00013268\n",
            "Train Epoch: 11 [55552/123872 (45%)]\tLoss: 0.399993\tLR: 0.00013260\n",
            "Train Epoch: 11 [55808/123872 (45%)]\tLoss: 0.413431\tLR: 0.00013252\n",
            "Train Epoch: 11 [56064/123872 (45%)]\tLoss: 0.384590\tLR: 0.00013244\n",
            "Train Epoch: 11 [56320/123872 (45%)]\tLoss: 0.408324\tLR: 0.00013236\n",
            "Train Epoch: 11 [56320/123872 (45%)]\tLoss: 0.408324\n",
            "Train Epoch: 11 [56576/123872 (46%)]\tLoss: 0.338285\tLR: 0.00013228\n",
            "Train Epoch: 11 [56832/123872 (46%)]\tLoss: 0.344958\tLR: 0.00013220\n",
            "Train Epoch: 11 [57088/123872 (46%)]\tLoss: 0.358100\tLR: 0.00013212\n",
            "Train Epoch: 11 [57344/123872 (46%)]\tLoss: 0.336973\tLR: 0.00013204\n",
            "Train Epoch: 11 [57600/123872 (46%)]\tLoss: 0.393220\tLR: 0.00013196\n",
            "Train Epoch: 11 [57856/123872 (47%)]\tLoss: 0.390351\tLR: 0.00013188\n",
            "Train Epoch: 11 [58112/123872 (47%)]\tLoss: 0.378565\tLR: 0.00013180\n",
            "Train Epoch: 11 [58368/123872 (47%)]\tLoss: 0.357443\tLR: 0.00013172\n",
            "Train Epoch: 11 [58624/123872 (47%)]\tLoss: 0.332875\tLR: 0.00013164\n",
            "Train Epoch: 11 [58880/123872 (48%)]\tLoss: 0.350822\tLR: 0.00013156\n",
            "Train Epoch: 11 [58880/123872 (48%)]\tLoss: 0.350822\n",
            "Train Epoch: 11 [59136/123872 (48%)]\tLoss: 0.380162\tLR: 0.00013148\n",
            "Train Epoch: 11 [59392/123872 (48%)]\tLoss: 0.330088\tLR: 0.00013140\n",
            "Train Epoch: 11 [59648/123872 (48%)]\tLoss: 0.367983\tLR: 0.00013132\n",
            "Train Epoch: 11 [59904/123872 (48%)]\tLoss: 0.431847\tLR: 0.00013124\n",
            "Train Epoch: 11 [60160/123872 (49%)]\tLoss: 0.354498\tLR: 0.00013116\n",
            "Train Epoch: 11 [60416/123872 (49%)]\tLoss: 0.401406\tLR: 0.00013108\n",
            "Train Epoch: 11 [60672/123872 (49%)]\tLoss: 0.370065\tLR: 0.00013100\n",
            "Train Epoch: 11 [60928/123872 (49%)]\tLoss: 0.304119\tLR: 0.00013092\n",
            "Train Epoch: 11 [61184/123872 (49%)]\tLoss: 0.499255\tLR: 0.00013084\n",
            "Train Epoch: 11 [61440/123872 (50%)]\tLoss: 0.369505\tLR: 0.00013076\n",
            "Train Epoch: 11 [61440/123872 (50%)]\tLoss: 0.369505\n",
            "Train Epoch: 11 [61696/123872 (50%)]\tLoss: 0.387273\tLR: 0.00013068\n",
            "Train Epoch: 11 [61952/123872 (50%)]\tLoss: 0.318110\tLR: 0.00013060\n",
            "Train Epoch: 11 [62208/123872 (50%)]\tLoss: 0.348556\tLR: 0.00013052\n",
            "Train Epoch: 11 [62464/123872 (50%)]\tLoss: 0.349492\tLR: 0.00013044\n",
            "Train Epoch: 11 [62720/123872 (51%)]\tLoss: 0.364551\tLR: 0.00013036\n",
            "Train Epoch: 11 [62976/123872 (51%)]\tLoss: 0.411556\tLR: 0.00013029\n",
            "Train Epoch: 11 [63232/123872 (51%)]\tLoss: 0.344986\tLR: 0.00013021\n",
            "Train Epoch: 11 [63488/123872 (51%)]\tLoss: 0.349207\tLR: 0.00013013\n",
            "Train Epoch: 11 [63744/123872 (51%)]\tLoss: 0.361937\tLR: 0.00013005\n",
            "Train Epoch: 11 [64000/123872 (52%)]\tLoss: 0.303621\tLR: 0.00012997\n",
            "Train Epoch: 11 [64000/123872 (52%)]\tLoss: 0.303621\n",
            "Train Epoch: 11 [64256/123872 (52%)]\tLoss: 0.377194\tLR: 0.00012989\n",
            "Train Epoch: 11 [64512/123872 (52%)]\tLoss: 0.388914\tLR: 0.00012981\n",
            "Train Epoch: 11 [64768/123872 (52%)]\tLoss: 0.330003\tLR: 0.00012973\n",
            "Train Epoch: 11 [65024/123872 (52%)]\tLoss: 0.351839\tLR: 0.00012965\n",
            "Train Epoch: 11 [65280/123872 (53%)]\tLoss: 0.372632\tLR: 0.00012957\n",
            "Train Epoch: 11 [65536/123872 (53%)]\tLoss: 0.380389\tLR: 0.00012949\n",
            "Train Epoch: 11 [65792/123872 (53%)]\tLoss: 0.355534\tLR: 0.00012941\n",
            "Train Epoch: 11 [66048/123872 (53%)]\tLoss: 0.375323\tLR: 0.00012934\n",
            "Train Epoch: 11 [66304/123872 (54%)]\tLoss: 0.322595\tLR: 0.00012926\n",
            "Train Epoch: 11 [66560/123872 (54%)]\tLoss: 0.311304\tLR: 0.00012918\n",
            "Train Epoch: 11 [66560/123872 (54%)]\tLoss: 0.311304\n",
            "Train Epoch: 11 [66816/123872 (54%)]\tLoss: 0.378405\tLR: 0.00012910\n",
            "Train Epoch: 11 [67072/123872 (54%)]\tLoss: 0.377689\tLR: 0.00012902\n",
            "Train Epoch: 11 [67328/123872 (54%)]\tLoss: 0.348418\tLR: 0.00012894\n",
            "Train Epoch: 11 [67584/123872 (55%)]\tLoss: 0.356882\tLR: 0.00012886\n",
            "Train Epoch: 11 [67840/123872 (55%)]\tLoss: 0.369148\tLR: 0.00012878\n",
            "Train Epoch: 11 [68096/123872 (55%)]\tLoss: 0.338904\tLR: 0.00012870\n",
            "Train Epoch: 11 [68352/123872 (55%)]\tLoss: 0.333226\tLR: 0.00012863\n",
            "Train Epoch: 11 [68608/123872 (55%)]\tLoss: 0.410652\tLR: 0.00012855\n",
            "Train Epoch: 11 [68864/123872 (56%)]\tLoss: 0.316506\tLR: 0.00012847\n",
            "Train Epoch: 11 [69120/123872 (56%)]\tLoss: 0.357051\tLR: 0.00012839\n",
            "Train Epoch: 11 [69120/123872 (56%)]\tLoss: 0.357051\n",
            "Train Epoch: 11 [69376/123872 (56%)]\tLoss: 0.352931\tLR: 0.00012831\n",
            "Train Epoch: 11 [69632/123872 (56%)]\tLoss: 0.354827\tLR: 0.00012823\n",
            "Train Epoch: 11 [69888/123872 (56%)]\tLoss: 0.342032\tLR: 0.00012816\n",
            "Train Epoch: 11 [70144/123872 (57%)]\tLoss: 0.413796\tLR: 0.00012808\n",
            "Train Epoch: 11 [70400/123872 (57%)]\tLoss: 0.382467\tLR: 0.00012800\n",
            "Train Epoch: 11 [70656/123872 (57%)]\tLoss: 0.338631\tLR: 0.00012792\n",
            "Train Epoch: 11 [70912/123872 (57%)]\tLoss: 0.357211\tLR: 0.00012784\n",
            "Train Epoch: 11 [71168/123872 (57%)]\tLoss: 0.304307\tLR: 0.00012776\n",
            "Train Epoch: 11 [71424/123872 (58%)]\tLoss: 0.377989\tLR: 0.00012769\n",
            "Train Epoch: 11 [71680/123872 (58%)]\tLoss: 0.445845\tLR: 0.00012761\n",
            "Train Epoch: 11 [71680/123872 (58%)]\tLoss: 0.445845\n",
            "Train Epoch: 11 [71936/123872 (58%)]\tLoss: 0.405289\tLR: 0.00012753\n",
            "Train Epoch: 11 [72192/123872 (58%)]\tLoss: 0.373788\tLR: 0.00012745\n",
            "Train Epoch: 11 [72448/123872 (58%)]\tLoss: 0.359339\tLR: 0.00012737\n",
            "Train Epoch: 11 [72704/123872 (59%)]\tLoss: 0.348284\tLR: 0.00012730\n",
            "Train Epoch: 11 [72960/123872 (59%)]\tLoss: 0.395946\tLR: 0.00012722\n",
            "Train Epoch: 11 [73216/123872 (59%)]\tLoss: 0.364985\tLR: 0.00012714\n",
            "Train Epoch: 11 [73472/123872 (59%)]\tLoss: 0.335839\tLR: 0.00012706\n",
            "Train Epoch: 11 [73728/123872 (60%)]\tLoss: 0.407779\tLR: 0.00012698\n",
            "Train Epoch: 11 [73984/123872 (60%)]\tLoss: 0.343224\tLR: 0.00012691\n",
            "Train Epoch: 11 [74240/123872 (60%)]\tLoss: 0.379358\tLR: 0.00012683\n",
            "Train Epoch: 11 [74240/123872 (60%)]\tLoss: 0.379358\n",
            "Train Epoch: 11 [74496/123872 (60%)]\tLoss: 0.350235\tLR: 0.00012675\n",
            "Train Epoch: 11 [74752/123872 (60%)]\tLoss: 0.470267\tLR: 0.00012667\n",
            "Train Epoch: 11 [75008/123872 (61%)]\tLoss: 0.346468\tLR: 0.00012660\n",
            "Train Epoch: 11 [75264/123872 (61%)]\tLoss: 0.329103\tLR: 0.00012652\n",
            "Train Epoch: 11 [75520/123872 (61%)]\tLoss: 0.409543\tLR: 0.00012644\n",
            "Train Epoch: 11 [75776/123872 (61%)]\tLoss: 0.369756\tLR: 0.00012636\n",
            "Train Epoch: 11 [76032/123872 (61%)]\tLoss: 0.390408\tLR: 0.00012629\n",
            "Train Epoch: 11 [76288/123872 (62%)]\tLoss: 0.373262\tLR: 0.00012621\n",
            "Train Epoch: 11 [76544/123872 (62%)]\tLoss: 0.387263\tLR: 0.00012613\n",
            "Train Epoch: 11 [76800/123872 (62%)]\tLoss: 0.360035\tLR: 0.00012605\n",
            "Train Epoch: 11 [76800/123872 (62%)]\tLoss: 0.360035\n",
            "Train Epoch: 11 [77056/123872 (62%)]\tLoss: 0.386559\tLR: 0.00012598\n",
            "Train Epoch: 11 [77312/123872 (62%)]\tLoss: 0.398985\tLR: 0.00012590\n",
            "Train Epoch: 11 [77568/123872 (63%)]\tLoss: 0.403325\tLR: 0.00012582\n",
            "Train Epoch: 11 [77824/123872 (63%)]\tLoss: 0.348656\tLR: 0.00012574\n",
            "Train Epoch: 11 [78080/123872 (63%)]\tLoss: 0.416678\tLR: 0.00012567\n",
            "Train Epoch: 11 [78336/123872 (63%)]\tLoss: 0.316361\tLR: 0.00012559\n",
            "Train Epoch: 11 [78592/123872 (63%)]\tLoss: 0.293833\tLR: 0.00012551\n",
            "Train Epoch: 11 [78848/123872 (64%)]\tLoss: 0.359156\tLR: 0.00012544\n",
            "Train Epoch: 11 [79104/123872 (64%)]\tLoss: 0.336800\tLR: 0.00012536\n",
            "Train Epoch: 11 [79360/123872 (64%)]\tLoss: 0.396610\tLR: 0.00012528\n",
            "Train Epoch: 11 [79360/123872 (64%)]\tLoss: 0.396610\n",
            "Train Epoch: 11 [79616/123872 (64%)]\tLoss: 0.414836\tLR: 0.00012520\n",
            "Train Epoch: 11 [79872/123872 (64%)]\tLoss: 0.402083\tLR: 0.00012513\n",
            "Train Epoch: 11 [80128/123872 (65%)]\tLoss: 0.327409\tLR: 0.00012505\n",
            "Train Epoch: 11 [80384/123872 (65%)]\tLoss: 0.421053\tLR: 0.00012497\n",
            "Train Epoch: 11 [80640/123872 (65%)]\tLoss: 0.370840\tLR: 0.00012490\n",
            "Train Epoch: 11 [80896/123872 (65%)]\tLoss: 0.333724\tLR: 0.00012482\n",
            "Train Epoch: 11 [81152/123872 (65%)]\tLoss: 0.324665\tLR: 0.00012474\n",
            "Train Epoch: 11 [81408/123872 (66%)]\tLoss: 0.411655\tLR: 0.00012467\n",
            "Train Epoch: 11 [81664/123872 (66%)]\tLoss: 0.313668\tLR: 0.00012459\n",
            "Train Epoch: 11 [81920/123872 (66%)]\tLoss: 0.368141\tLR: 0.00012451\n",
            "Train Epoch: 11 [81920/123872 (66%)]\tLoss: 0.368141\n",
            "Train Epoch: 11 [82176/123872 (66%)]\tLoss: 0.341611\tLR: 0.00012444\n",
            "Train Epoch: 11 [82432/123872 (67%)]\tLoss: 0.369073\tLR: 0.00012436\n",
            "Train Epoch: 11 [82688/123872 (67%)]\tLoss: 0.357857\tLR: 0.00012428\n",
            "Train Epoch: 11 [82944/123872 (67%)]\tLoss: 0.395295\tLR: 0.00012421\n",
            "Train Epoch: 11 [83200/123872 (67%)]\tLoss: 0.327119\tLR: 0.00012413\n",
            "Train Epoch: 11 [83456/123872 (67%)]\tLoss: 0.344110\tLR: 0.00012406\n",
            "Train Epoch: 11 [83712/123872 (68%)]\tLoss: 0.336041\tLR: 0.00012398\n",
            "Train Epoch: 11 [83968/123872 (68%)]\tLoss: 0.353095\tLR: 0.00012390\n",
            "Train Epoch: 11 [84224/123872 (68%)]\tLoss: 0.360874\tLR: 0.00012383\n",
            "Train Epoch: 11 [84480/123872 (68%)]\tLoss: 0.325761\tLR: 0.00012375\n",
            "Train Epoch: 11 [84480/123872 (68%)]\tLoss: 0.325761\n",
            "Train Epoch: 11 [84736/123872 (68%)]\tLoss: 0.385760\tLR: 0.00012367\n",
            "Train Epoch: 11 [84992/123872 (69%)]\tLoss: 0.390952\tLR: 0.00012360\n",
            "Train Epoch: 11 [85248/123872 (69%)]\tLoss: 0.348125\tLR: 0.00012352\n",
            "Train Epoch: 11 [85504/123872 (69%)]\tLoss: 0.375754\tLR: 0.00012345\n",
            "Train Epoch: 11 [85760/123872 (69%)]\tLoss: 0.414814\tLR: 0.00012337\n",
            "Train Epoch: 11 [86016/123872 (69%)]\tLoss: 0.384031\tLR: 0.00012329\n",
            "Train Epoch: 11 [86272/123872 (70%)]\tLoss: 0.347006\tLR: 0.00012322\n",
            "Train Epoch: 11 [86528/123872 (70%)]\tLoss: 0.380289\tLR: 0.00012314\n",
            "Train Epoch: 11 [86784/123872 (70%)]\tLoss: 0.366136\tLR: 0.00012307\n",
            "Train Epoch: 11 [87040/123872 (70%)]\tLoss: 0.359668\tLR: 0.00012299\n",
            "Train Epoch: 11 [87040/123872 (70%)]\tLoss: 0.359668\n",
            "Train Epoch: 11 [87296/123872 (70%)]\tLoss: 0.345441\tLR: 0.00012292\n",
            "Train Epoch: 11 [87552/123872 (71%)]\tLoss: 0.374015\tLR: 0.00012284\n",
            "Train Epoch: 11 [87808/123872 (71%)]\tLoss: 0.386232\tLR: 0.00012276\n",
            "Train Epoch: 11 [88064/123872 (71%)]\tLoss: 0.392960\tLR: 0.00012269\n",
            "Train Epoch: 11 [88320/123872 (71%)]\tLoss: 0.410828\tLR: 0.00012261\n",
            "Train Epoch: 11 [88576/123872 (71%)]\tLoss: 0.323611\tLR: 0.00012254\n",
            "Train Epoch: 11 [88832/123872 (72%)]\tLoss: 0.346266\tLR: 0.00012246\n",
            "Train Epoch: 11 [89088/123872 (72%)]\tLoss: 0.416862\tLR: 0.00012239\n",
            "Train Epoch: 11 [89344/123872 (72%)]\tLoss: 0.343779\tLR: 0.00012231\n",
            "Train Epoch: 11 [89600/123872 (72%)]\tLoss: 0.351876\tLR: 0.00012224\n",
            "Train Epoch: 11 [89600/123872 (72%)]\tLoss: 0.351876\n",
            "Train Epoch: 11 [89856/123872 (73%)]\tLoss: 0.373471\tLR: 0.00012216\n",
            "Train Epoch: 11 [90112/123872 (73%)]\tLoss: 0.413509\tLR: 0.00012208\n",
            "Train Epoch: 11 [90368/123872 (73%)]\tLoss: 0.378651\tLR: 0.00012201\n",
            "Train Epoch: 11 [90624/123872 (73%)]\tLoss: 0.389608\tLR: 0.00012193\n",
            "Train Epoch: 11 [90880/123872 (73%)]\tLoss: 0.376062\tLR: 0.00012186\n",
            "Train Epoch: 11 [91136/123872 (74%)]\tLoss: 0.387754\tLR: 0.00012178\n",
            "Train Epoch: 11 [91392/123872 (74%)]\tLoss: 0.406891\tLR: 0.00012171\n",
            "Train Epoch: 11 [91648/123872 (74%)]\tLoss: 0.395573\tLR: 0.00012163\n",
            "Train Epoch: 11 [91904/123872 (74%)]\tLoss: 0.401644\tLR: 0.00012156\n",
            "Train Epoch: 11 [92160/123872 (74%)]\tLoss: 0.421882\tLR: 0.00012148\n",
            "Train Epoch: 11 [92160/123872 (74%)]\tLoss: 0.421882\n",
            "Train Epoch: 11 [92416/123872 (75%)]\tLoss: 0.396911\tLR: 0.00012141\n",
            "Train Epoch: 11 [92672/123872 (75%)]\tLoss: 0.364694\tLR: 0.00012133\n",
            "Train Epoch: 11 [92928/123872 (75%)]\tLoss: 0.419816\tLR: 0.00012126\n",
            "Train Epoch: 11 [93184/123872 (75%)]\tLoss: 0.417826\tLR: 0.00012118\n",
            "Train Epoch: 11 [93440/123872 (75%)]\tLoss: 0.376677\tLR: 0.00012111\n",
            "Train Epoch: 11 [93696/123872 (76%)]\tLoss: 0.312674\tLR: 0.00012103\n",
            "Train Epoch: 11 [93952/123872 (76%)]\tLoss: 0.425889\tLR: 0.00012096\n",
            "Train Epoch: 11 [94208/123872 (76%)]\tLoss: 0.348346\tLR: 0.00012089\n",
            "Train Epoch: 11 [94464/123872 (76%)]\tLoss: 0.390620\tLR: 0.00012081\n",
            "Train Epoch: 11 [94720/123872 (76%)]\tLoss: 0.341750\tLR: 0.00012074\n",
            "Train Epoch: 11 [94720/123872 (76%)]\tLoss: 0.341750\n",
            "Train Epoch: 11 [94976/123872 (77%)]\tLoss: 0.335935\tLR: 0.00012066\n",
            "Train Epoch: 11 [95232/123872 (77%)]\tLoss: 0.413136\tLR: 0.00012059\n",
            "Train Epoch: 11 [95488/123872 (77%)]\tLoss: 0.313324\tLR: 0.00012051\n",
            "Train Epoch: 11 [95744/123872 (77%)]\tLoss: 0.393055\tLR: 0.00012044\n",
            "Train Epoch: 11 [96000/123872 (77%)]\tLoss: 0.393675\tLR: 0.00012036\n",
            "Train Epoch: 11 [96256/123872 (78%)]\tLoss: 0.331727\tLR: 0.00012029\n",
            "Train Epoch: 11 [96512/123872 (78%)]\tLoss: 0.377490\tLR: 0.00012022\n",
            "Train Epoch: 11 [96768/123872 (78%)]\tLoss: 0.297531\tLR: 0.00012014\n",
            "Train Epoch: 11 [97024/123872 (78%)]\tLoss: 0.372564\tLR: 0.00012007\n",
            "Train Epoch: 11 [97280/123872 (79%)]\tLoss: 0.355691\tLR: 0.00011999\n",
            "Train Epoch: 11 [97280/123872 (79%)]\tLoss: 0.355691\n",
            "Train Epoch: 11 [97536/123872 (79%)]\tLoss: 0.327084\tLR: 0.00011992\n",
            "Train Epoch: 11 [97792/123872 (79%)]\tLoss: 0.364657\tLR: 0.00011984\n",
            "Train Epoch: 11 [98048/123872 (79%)]\tLoss: 0.334926\tLR: 0.00011977\n",
            "Train Epoch: 11 [98304/123872 (79%)]\tLoss: 0.399191\tLR: 0.00011970\n",
            "Train Epoch: 11 [98560/123872 (80%)]\tLoss: 0.322884\tLR: 0.00011962\n",
            "Train Epoch: 11 [98816/123872 (80%)]\tLoss: 0.380398\tLR: 0.00011955\n",
            "Train Epoch: 11 [99072/123872 (80%)]\tLoss: 0.357412\tLR: 0.00011947\n",
            "Train Epoch: 11 [99328/123872 (80%)]\tLoss: 0.379789\tLR: 0.00011940\n",
            "Train Epoch: 11 [99584/123872 (80%)]\tLoss: 0.340727\tLR: 0.00011933\n",
            "Train Epoch: 11 [99840/123872 (81%)]\tLoss: 0.389980\tLR: 0.00011925\n",
            "Train Epoch: 11 [99840/123872 (81%)]\tLoss: 0.389980\n",
            "Train Epoch: 11 [100096/123872 (81%)]\tLoss: 0.326719\tLR: 0.00011918\n",
            "Train Epoch: 11 [100352/123872 (81%)]\tLoss: 0.399727\tLR: 0.00011911\n",
            "Train Epoch: 11 [100608/123872 (81%)]\tLoss: 0.401442\tLR: 0.00011903\n",
            "Train Epoch: 11 [100864/123872 (81%)]\tLoss: 0.384547\tLR: 0.00011896\n",
            "Train Epoch: 11 [101120/123872 (82%)]\tLoss: 0.333620\tLR: 0.00011888\n",
            "Train Epoch: 11 [101376/123872 (82%)]\tLoss: 0.326940\tLR: 0.00011881\n",
            "Train Epoch: 11 [101632/123872 (82%)]\tLoss: 0.353492\tLR: 0.00011874\n",
            "Train Epoch: 11 [101888/123872 (82%)]\tLoss: 0.399051\tLR: 0.00011866\n",
            "Train Epoch: 11 [102144/123872 (82%)]\tLoss: 0.341136\tLR: 0.00011859\n",
            "Train Epoch: 11 [102400/123872 (83%)]\tLoss: 0.370514\tLR: 0.00011852\n",
            "Train Epoch: 11 [102400/123872 (83%)]\tLoss: 0.370514\n",
            "Train Epoch: 11 [102656/123872 (83%)]\tLoss: 0.358041\tLR: 0.00011844\n",
            "Train Epoch: 11 [102912/123872 (83%)]\tLoss: 0.302572\tLR: 0.00011837\n",
            "Train Epoch: 11 [103168/123872 (83%)]\tLoss: 0.363146\tLR: 0.00011830\n",
            "Train Epoch: 11 [103424/123872 (83%)]\tLoss: 0.378679\tLR: 0.00011822\n",
            "Train Epoch: 11 [103680/123872 (84%)]\tLoss: 0.355035\tLR: 0.00011815\n",
            "Train Epoch: 11 [103936/123872 (84%)]\tLoss: 0.344294\tLR: 0.00011808\n",
            "Train Epoch: 11 [104192/123872 (84%)]\tLoss: 0.413747\tLR: 0.00011800\n",
            "Train Epoch: 11 [104448/123872 (84%)]\tLoss: 0.377419\tLR: 0.00011793\n",
            "Train Epoch: 11 [104704/123872 (85%)]\tLoss: 0.382276\tLR: 0.00011786\n",
            "Train Epoch: 11 [104960/123872 (85%)]\tLoss: 0.401176\tLR: 0.00011779\n",
            "Train Epoch: 11 [104960/123872 (85%)]\tLoss: 0.401176\n",
            "Train Epoch: 11 [105216/123872 (85%)]\tLoss: 0.370115\tLR: 0.00011771\n",
            "Train Epoch: 11 [105472/123872 (85%)]\tLoss: 0.357762\tLR: 0.00011764\n",
            "Train Epoch: 11 [105728/123872 (85%)]\tLoss: 0.357756\tLR: 0.00011757\n",
            "Train Epoch: 11 [105984/123872 (86%)]\tLoss: 0.403335\tLR: 0.00011749\n",
            "Train Epoch: 11 [106240/123872 (86%)]\tLoss: 0.419392\tLR: 0.00011742\n",
            "Train Epoch: 11 [106496/123872 (86%)]\tLoss: 0.384805\tLR: 0.00011735\n",
            "Train Epoch: 11 [106752/123872 (86%)]\tLoss: 0.370429\tLR: 0.00011728\n",
            "Train Epoch: 11 [107008/123872 (86%)]\tLoss: 0.320676\tLR: 0.00011720\n",
            "Train Epoch: 11 [107264/123872 (87%)]\tLoss: 0.365882\tLR: 0.00011713\n",
            "Train Epoch: 11 [107520/123872 (87%)]\tLoss: 0.345680\tLR: 0.00011706\n",
            "Train Epoch: 11 [107520/123872 (87%)]\tLoss: 0.345680\n",
            "Train Epoch: 11 [107776/123872 (87%)]\tLoss: 0.395798\tLR: 0.00011699\n",
            "Train Epoch: 11 [108032/123872 (87%)]\tLoss: 0.334516\tLR: 0.00011691\n",
            "Train Epoch: 11 [108288/123872 (87%)]\tLoss: 0.359896\tLR: 0.00011684\n",
            "Train Epoch: 11 [108544/123872 (88%)]\tLoss: 0.376294\tLR: 0.00011677\n",
            "Train Epoch: 11 [108800/123872 (88%)]\tLoss: 0.404374\tLR: 0.00011670\n",
            "Train Epoch: 11 [109056/123872 (88%)]\tLoss: 0.369029\tLR: 0.00011662\n",
            "Train Epoch: 11 [109312/123872 (88%)]\tLoss: 0.328047\tLR: 0.00011655\n",
            "Train Epoch: 11 [109568/123872 (88%)]\tLoss: 0.336956\tLR: 0.00011648\n",
            "Train Epoch: 11 [109824/123872 (89%)]\tLoss: 0.411625\tLR: 0.00011641\n",
            "Train Epoch: 11 [110080/123872 (89%)]\tLoss: 0.337076\tLR: 0.00011633\n",
            "Train Epoch: 11 [110080/123872 (89%)]\tLoss: 0.337076\n",
            "Train Epoch: 11 [110336/123872 (89%)]\tLoss: 0.373972\tLR: 0.00011626\n",
            "Train Epoch: 11 [110592/123872 (89%)]\tLoss: 0.451430\tLR: 0.00011619\n",
            "Train Epoch: 11 [110848/123872 (89%)]\tLoss: 0.337410\tLR: 0.00011612\n",
            "Train Epoch: 11 [111104/123872 (90%)]\tLoss: 0.347511\tLR: 0.00011605\n",
            "Train Epoch: 11 [111360/123872 (90%)]\tLoss: 0.330834\tLR: 0.00011597\n",
            "Train Epoch: 11 [111616/123872 (90%)]\tLoss: 0.319638\tLR: 0.00011590\n",
            "Train Epoch: 11 [111872/123872 (90%)]\tLoss: 0.376592\tLR: 0.00011583\n",
            "Train Epoch: 11 [112128/123872 (90%)]\tLoss: 0.329495\tLR: 0.00011576\n",
            "Train Epoch: 11 [112384/123872 (91%)]\tLoss: 0.325656\tLR: 0.00011569\n",
            "Train Epoch: 11 [112640/123872 (91%)]\tLoss: 0.367524\tLR: 0.00011562\n",
            "Train Epoch: 11 [112640/123872 (91%)]\tLoss: 0.367524\n",
            "Train Epoch: 11 [112896/123872 (91%)]\tLoss: 0.323403\tLR: 0.00011554\n",
            "Train Epoch: 11 [113152/123872 (91%)]\tLoss: 0.393402\tLR: 0.00011547\n",
            "Train Epoch: 11 [113408/123872 (92%)]\tLoss: 0.308066\tLR: 0.00011540\n",
            "Train Epoch: 11 [113664/123872 (92%)]\tLoss: 0.364968\tLR: 0.00011533\n",
            "Train Epoch: 11 [113920/123872 (92%)]\tLoss: 0.412585\tLR: 0.00011526\n",
            "Train Epoch: 11 [114176/123872 (92%)]\tLoss: 0.308552\tLR: 0.00011519\n",
            "Train Epoch: 11 [114432/123872 (92%)]\tLoss: 0.354873\tLR: 0.00011511\n",
            "Train Epoch: 11 [114688/123872 (93%)]\tLoss: 0.342439\tLR: 0.00011504\n",
            "Train Epoch: 11 [114944/123872 (93%)]\tLoss: 0.374248\tLR: 0.00011497\n",
            "Train Epoch: 11 [115200/123872 (93%)]\tLoss: 0.325638\tLR: 0.00011490\n",
            "Train Epoch: 11 [115200/123872 (93%)]\tLoss: 0.325638\n",
            "Train Epoch: 11 [115456/123872 (93%)]\tLoss: 0.402188\tLR: 0.00011483\n",
            "Train Epoch: 11 [115712/123872 (93%)]\tLoss: 0.375953\tLR: 0.00011476\n",
            "Train Epoch: 11 [115968/123872 (94%)]\tLoss: 0.338702\tLR: 0.00011469\n",
            "Train Epoch: 11 [116224/123872 (94%)]\tLoss: 0.409556\tLR: 0.00011461\n",
            "Train Epoch: 11 [116480/123872 (94%)]\tLoss: 0.406704\tLR: 0.00011454\n",
            "Train Epoch: 11 [116736/123872 (94%)]\tLoss: 0.316428\tLR: 0.00011447\n",
            "Train Epoch: 11 [116992/123872 (94%)]\tLoss: 0.396290\tLR: 0.00011440\n",
            "Train Epoch: 11 [117248/123872 (95%)]\tLoss: 0.304411\tLR: 0.00011433\n",
            "Train Epoch: 11 [117504/123872 (95%)]\tLoss: 0.375513\tLR: 0.00011426\n",
            "Train Epoch: 11 [117760/123872 (95%)]\tLoss: 0.400076\tLR: 0.00011419\n",
            "Train Epoch: 11 [117760/123872 (95%)]\tLoss: 0.400076\n",
            "Train Epoch: 11 [118016/123872 (95%)]\tLoss: 0.440705\tLR: 0.00011412\n",
            "Train Epoch: 11 [118272/123872 (95%)]\tLoss: 0.456954\tLR: 0.00011405\n",
            "Train Epoch: 11 [118528/123872 (96%)]\tLoss: 0.370619\tLR: 0.00011398\n",
            "Train Epoch: 11 [118784/123872 (96%)]\tLoss: 0.380479\tLR: 0.00011391\n",
            "Train Epoch: 11 [119040/123872 (96%)]\tLoss: 0.387964\tLR: 0.00011383\n",
            "Train Epoch: 11 [119296/123872 (96%)]\tLoss: 0.363660\tLR: 0.00011376\n",
            "Train Epoch: 11 [119552/123872 (96%)]\tLoss: 0.429159\tLR: 0.00011369\n",
            "Train Epoch: 11 [119808/123872 (97%)]\tLoss: 0.337935\tLR: 0.00011362\n",
            "Train Epoch: 11 [120064/123872 (97%)]\tLoss: 0.337289\tLR: 0.00011355\n",
            "Train Epoch: 11 [120320/123872 (97%)]\tLoss: 0.333647\tLR: 0.00011348\n",
            "Train Epoch: 11 [120320/123872 (97%)]\tLoss: 0.333647\n",
            "Train Epoch: 11 [120576/123872 (97%)]\tLoss: 0.354628\tLR: 0.00011341\n",
            "Train Epoch: 11 [120832/123872 (98%)]\tLoss: 0.394649\tLR: 0.00011334\n",
            "Train Epoch: 11 [121088/123872 (98%)]\tLoss: 0.374093\tLR: 0.00011327\n",
            "Train Epoch: 11 [121344/123872 (98%)]\tLoss: 0.373790\tLR: 0.00011320\n",
            "Train Epoch: 11 [121600/123872 (98%)]\tLoss: 0.395132\tLR: 0.00011313\n",
            "Train Epoch: 11 [121856/123872 (98%)]\tLoss: 0.306936\tLR: 0.00011306\n",
            "Train Epoch: 11 [122112/123872 (99%)]\tLoss: 0.353987\tLR: 0.00011299\n",
            "Train Epoch: 11 [122368/123872 (99%)]\tLoss: 0.413010\tLR: 0.00011292\n",
            "Train Epoch: 11 [122624/123872 (99%)]\tLoss: 0.380387\tLR: 0.00011285\n",
            "Train Epoch: 11 [122880/123872 (99%)]\tLoss: 0.364100\tLR: 0.00011278\n",
            "Train Epoch: 11 [122880/123872 (99%)]\tLoss: 0.364100\n",
            "Train Epoch: 11 [123136/123872 (99%)]\tLoss: 0.311630\tLR: 0.00011271\n",
            "Train Epoch: 11 [123392/123872 (100%)]\tLoss: 0.368018\tLR: 0.00011264\n",
            "Train Epoch: 11 [108192/123872 (100%)]\tLoss: 0.313663\tLR: 0.00011257\n",
            "\n",
            "Test set: Average loss: 0.0015, Accuracy: 25680/30970 (82.92%)\n",
            "\n",
            "Train Epoch: 12 [0/123872 (0%)]\tLoss: 0.350480\tLR: 0.00011250\n",
            "Train Epoch: 12 [0/123872 (0%)]\tLoss: 0.350480\n",
            "Train Epoch: 12 [256/123872 (0%)]\tLoss: 0.362758\tLR: 0.00011243\n",
            "Train Epoch: 12 [512/123872 (0%)]\tLoss: 0.468649\tLR: 0.00011236\n",
            "Train Epoch: 12 [768/123872 (1%)]\tLoss: 0.332265\tLR: 0.00011229\n",
            "Train Epoch: 12 [1024/123872 (1%)]\tLoss: 0.326719\tLR: 0.00011222\n",
            "Train Epoch: 12 [1280/123872 (1%)]\tLoss: 0.424502\tLR: 0.00011215\n",
            "Train Epoch: 12 [1536/123872 (1%)]\tLoss: 0.335048\tLR: 0.00011208\n",
            "Train Epoch: 12 [1792/123872 (1%)]\tLoss: 0.401309\tLR: 0.00011201\n",
            "Train Epoch: 12 [2048/123872 (2%)]\tLoss: 0.407592\tLR: 0.00011194\n",
            "Train Epoch: 12 [2304/123872 (2%)]\tLoss: 0.308357\tLR: 0.00011187\n",
            "Train Epoch: 12 [2560/123872 (2%)]\tLoss: 0.385679\tLR: 0.00011180\n",
            "Train Epoch: 12 [2560/123872 (2%)]\tLoss: 0.385679\n",
            "Train Epoch: 12 [2816/123872 (2%)]\tLoss: 0.318548\tLR: 0.00011173\n",
            "Train Epoch: 12 [3072/123872 (2%)]\tLoss: 0.393366\tLR: 0.00011166\n",
            "Train Epoch: 12 [3328/123872 (3%)]\tLoss: 0.339465\tLR: 0.00011159\n",
            "Train Epoch: 12 [3584/123872 (3%)]\tLoss: 0.301625\tLR: 0.00011152\n",
            "Train Epoch: 12 [3840/123872 (3%)]\tLoss: 0.320165\tLR: 0.00011145\n",
            "Train Epoch: 12 [4096/123872 (3%)]\tLoss: 0.386674\tLR: 0.00011139\n",
            "Train Epoch: 12 [4352/123872 (4%)]\tLoss: 0.388936\tLR: 0.00011132\n",
            "Train Epoch: 12 [4608/123872 (4%)]\tLoss: 0.353932\tLR: 0.00011125\n",
            "Train Epoch: 12 [4864/123872 (4%)]\tLoss: 0.394493\tLR: 0.00011118\n",
            "Train Epoch: 12 [5120/123872 (4%)]\tLoss: 0.354370\tLR: 0.00011111\n",
            "Train Epoch: 12 [5120/123872 (4%)]\tLoss: 0.354370\n",
            "Train Epoch: 12 [5376/123872 (4%)]\tLoss: 0.378768\tLR: 0.00011104\n",
            "Train Epoch: 12 [5632/123872 (5%)]\tLoss: 0.314840\tLR: 0.00011097\n",
            "Train Epoch: 12 [5888/123872 (5%)]\tLoss: 0.379703\tLR: 0.00011090\n",
            "Train Epoch: 12 [6144/123872 (5%)]\tLoss: 0.294828\tLR: 0.00011083\n",
            "Train Epoch: 12 [6400/123872 (5%)]\tLoss: 0.282671\tLR: 0.00011076\n",
            "Train Epoch: 12 [6656/123872 (5%)]\tLoss: 0.338333\tLR: 0.00011069\n",
            "Train Epoch: 12 [6912/123872 (6%)]\tLoss: 0.365359\tLR: 0.00011063\n",
            "Train Epoch: 12 [7168/123872 (6%)]\tLoss: 0.355473\tLR: 0.00011056\n",
            "Train Epoch: 12 [7424/123872 (6%)]\tLoss: 0.344944\tLR: 0.00011049\n",
            "Train Epoch: 12 [7680/123872 (6%)]\tLoss: 0.369247\tLR: 0.00011042\n",
            "Train Epoch: 12 [7680/123872 (6%)]\tLoss: 0.369247\n",
            "Train Epoch: 12 [7936/123872 (6%)]\tLoss: 0.341663\tLR: 0.00011035\n",
            "Train Epoch: 12 [8192/123872 (7%)]\tLoss: 0.410978\tLR: 0.00011028\n",
            "Train Epoch: 12 [8448/123872 (7%)]\tLoss: 0.316379\tLR: 0.00011021\n",
            "Train Epoch: 12 [8704/123872 (7%)]\tLoss: 0.404560\tLR: 0.00011015\n",
            "Train Epoch: 12 [8960/123872 (7%)]\tLoss: 0.350685\tLR: 0.00011008\n",
            "Train Epoch: 12 [9216/123872 (7%)]\tLoss: 0.355040\tLR: 0.00011001\n",
            "Train Epoch: 12 [9472/123872 (8%)]\tLoss: 0.294657\tLR: 0.00010994\n",
            "Train Epoch: 12 [9728/123872 (8%)]\tLoss: 0.402903\tLR: 0.00010987\n",
            "Train Epoch: 12 [9984/123872 (8%)]\tLoss: 0.378896\tLR: 0.00010980\n",
            "Train Epoch: 12 [10240/123872 (8%)]\tLoss: 0.309205\tLR: 0.00010974\n",
            "Train Epoch: 12 [10240/123872 (8%)]\tLoss: 0.309205\n",
            "Train Epoch: 12 [10496/123872 (8%)]\tLoss: 0.383109\tLR: 0.00010967\n",
            "Train Epoch: 12 [10752/123872 (9%)]\tLoss: 0.301731\tLR: 0.00010960\n",
            "Train Epoch: 12 [11008/123872 (9%)]\tLoss: 0.332340\tLR: 0.00010953\n",
            "Train Epoch: 12 [11264/123872 (9%)]\tLoss: 0.335853\tLR: 0.00010946\n",
            "Train Epoch: 12 [11520/123872 (9%)]\tLoss: 0.314034\tLR: 0.00010939\n",
            "Train Epoch: 12 [11776/123872 (10%)]\tLoss: 0.377966\tLR: 0.00010933\n",
            "Train Epoch: 12 [12032/123872 (10%)]\tLoss: 0.415892\tLR: 0.00010926\n",
            "Train Epoch: 12 [12288/123872 (10%)]\tLoss: 0.362146\tLR: 0.00010919\n",
            "Train Epoch: 12 [12544/123872 (10%)]\tLoss: 0.387368\tLR: 0.00010912\n",
            "Train Epoch: 12 [12800/123872 (10%)]\tLoss: 0.336632\tLR: 0.00010905\n",
            "Train Epoch: 12 [12800/123872 (10%)]\tLoss: 0.336632\n",
            "Train Epoch: 12 [13056/123872 (11%)]\tLoss: 0.326533\tLR: 0.00010899\n",
            "Train Epoch: 12 [13312/123872 (11%)]\tLoss: 0.346298\tLR: 0.00010892\n",
            "Train Epoch: 12 [13568/123872 (11%)]\tLoss: 0.349775\tLR: 0.00010885\n",
            "Train Epoch: 12 [13824/123872 (11%)]\tLoss: 0.437846\tLR: 0.00010878\n",
            "Train Epoch: 12 [14080/123872 (11%)]\tLoss: 0.428410\tLR: 0.00010872\n",
            "Train Epoch: 12 [14336/123872 (12%)]\tLoss: 0.354460\tLR: 0.00010865\n",
            "Train Epoch: 12 [14592/123872 (12%)]\tLoss: 0.343087\tLR: 0.00010858\n",
            "Train Epoch: 12 [14848/123872 (12%)]\tLoss: 0.350610\tLR: 0.00010851\n",
            "Train Epoch: 12 [15104/123872 (12%)]\tLoss: 0.406880\tLR: 0.00010845\n",
            "Train Epoch: 12 [15360/123872 (12%)]\tLoss: 0.405320\tLR: 0.00010838\n",
            "Train Epoch: 12 [15360/123872 (12%)]\tLoss: 0.405320\n",
            "Train Epoch: 12 [15616/123872 (13%)]\tLoss: 0.355547\tLR: 0.00010831\n",
            "Train Epoch: 12 [15872/123872 (13%)]\tLoss: 0.335024\tLR: 0.00010824\n",
            "Train Epoch: 12 [16128/123872 (13%)]\tLoss: 0.353570\tLR: 0.00010818\n",
            "Train Epoch: 12 [16384/123872 (13%)]\tLoss: 0.355516\tLR: 0.00010811\n",
            "Train Epoch: 12 [16640/123872 (13%)]\tLoss: 0.304240\tLR: 0.00010804\n",
            "Train Epoch: 12 [16896/123872 (14%)]\tLoss: 0.459251\tLR: 0.00010797\n",
            "Train Epoch: 12 [17152/123872 (14%)]\tLoss: 0.374284\tLR: 0.00010791\n",
            "Train Epoch: 12 [17408/123872 (14%)]\tLoss: 0.343077\tLR: 0.00010784\n",
            "Train Epoch: 12 [17664/123872 (14%)]\tLoss: 0.348217\tLR: 0.00010777\n",
            "Train Epoch: 12 [17920/123872 (14%)]\tLoss: 0.378468\tLR: 0.00010771\n",
            "Train Epoch: 12 [17920/123872 (14%)]\tLoss: 0.378468\n",
            "Train Epoch: 12 [18176/123872 (15%)]\tLoss: 0.379928\tLR: 0.00010764\n",
            "Train Epoch: 12 [18432/123872 (15%)]\tLoss: 0.445016\tLR: 0.00010757\n",
            "Train Epoch: 12 [18688/123872 (15%)]\tLoss: 0.319518\tLR: 0.00010751\n",
            "Train Epoch: 12 [18944/123872 (15%)]\tLoss: 0.367746\tLR: 0.00010744\n",
            "Train Epoch: 12 [19200/123872 (15%)]\tLoss: 0.352705\tLR: 0.00010737\n",
            "Train Epoch: 12 [19456/123872 (16%)]\tLoss: 0.333980\tLR: 0.00010731\n",
            "Train Epoch: 12 [19712/123872 (16%)]\tLoss: 0.426872\tLR: 0.00010724\n",
            "Train Epoch: 12 [19968/123872 (16%)]\tLoss: 0.375006\tLR: 0.00010717\n",
            "Train Epoch: 12 [20224/123872 (16%)]\tLoss: 0.392800\tLR: 0.00010711\n",
            "Train Epoch: 12 [20480/123872 (17%)]\tLoss: 0.384449\tLR: 0.00010704\n",
            "Train Epoch: 12 [20480/123872 (17%)]\tLoss: 0.384449\n",
            "Train Epoch: 12 [20736/123872 (17%)]\tLoss: 0.336039\tLR: 0.00010697\n",
            "Train Epoch: 12 [20992/123872 (17%)]\tLoss: 0.337977\tLR: 0.00010691\n",
            "Train Epoch: 12 [21248/123872 (17%)]\tLoss: 0.314645\tLR: 0.00010684\n",
            "Train Epoch: 12 [21504/123872 (17%)]\tLoss: 0.366897\tLR: 0.00010677\n",
            "Train Epoch: 12 [21760/123872 (18%)]\tLoss: 0.372798\tLR: 0.00010671\n",
            "Train Epoch: 12 [22016/123872 (18%)]\tLoss: 0.364599\tLR: 0.00010664\n",
            "Train Epoch: 12 [22272/123872 (18%)]\tLoss: 0.386382\tLR: 0.00010657\n",
            "Train Epoch: 12 [22528/123872 (18%)]\tLoss: 0.361405\tLR: 0.00010651\n",
            "Train Epoch: 12 [22784/123872 (18%)]\tLoss: 0.317098\tLR: 0.00010644\n",
            "Train Epoch: 12 [23040/123872 (19%)]\tLoss: 0.331933\tLR: 0.00010638\n",
            "Train Epoch: 12 [23040/123872 (19%)]\tLoss: 0.331933\n",
            "Train Epoch: 12 [23296/123872 (19%)]\tLoss: 0.396801\tLR: 0.00010631\n",
            "Train Epoch: 12 [23552/123872 (19%)]\tLoss: 0.359134\tLR: 0.00010624\n",
            "Train Epoch: 12 [23808/123872 (19%)]\tLoss: 0.392110\tLR: 0.00010618\n",
            "Train Epoch: 12 [24064/123872 (19%)]\tLoss: 0.344998\tLR: 0.00010611\n",
            "Train Epoch: 12 [24320/123872 (20%)]\tLoss: 0.327345\tLR: 0.00010605\n",
            "Train Epoch: 12 [24576/123872 (20%)]\tLoss: 0.282779\tLR: 0.00010598\n",
            "Train Epoch: 12 [24832/123872 (20%)]\tLoss: 0.312132\tLR: 0.00010591\n",
            "Train Epoch: 12 [25088/123872 (20%)]\tLoss: 0.331356\tLR: 0.00010585\n",
            "Train Epoch: 12 [25344/123872 (20%)]\tLoss: 0.366292\tLR: 0.00010578\n",
            "Train Epoch: 12 [25600/123872 (21%)]\tLoss: 0.338473\tLR: 0.00010572\n",
            "Train Epoch: 12 [25600/123872 (21%)]\tLoss: 0.338473\n",
            "Train Epoch: 12 [25856/123872 (21%)]\tLoss: 0.427921\tLR: 0.00010565\n",
            "Train Epoch: 12 [26112/123872 (21%)]\tLoss: 0.364114\tLR: 0.00010559\n",
            "Train Epoch: 12 [26368/123872 (21%)]\tLoss: 0.388246\tLR: 0.00010552\n",
            "Train Epoch: 12 [26624/123872 (21%)]\tLoss: 0.381793\tLR: 0.00010545\n",
            "Train Epoch: 12 [26880/123872 (22%)]\tLoss: 0.388615\tLR: 0.00010539\n",
            "Train Epoch: 12 [27136/123872 (22%)]\tLoss: 0.334950\tLR: 0.00010532\n",
            "Train Epoch: 12 [27392/123872 (22%)]\tLoss: 0.369586\tLR: 0.00010526\n",
            "Train Epoch: 12 [27648/123872 (22%)]\tLoss: 0.330921\tLR: 0.00010519\n",
            "Train Epoch: 12 [27904/123872 (23%)]\tLoss: 0.357153\tLR: 0.00010513\n",
            "Train Epoch: 12 [28160/123872 (23%)]\tLoss: 0.381928\tLR: 0.00010506\n",
            "Train Epoch: 12 [28160/123872 (23%)]\tLoss: 0.381928\n",
            "Train Epoch: 12 [28416/123872 (23%)]\tLoss: 0.385374\tLR: 0.00010500\n",
            "Train Epoch: 12 [28672/123872 (23%)]\tLoss: 0.329215\tLR: 0.00010493\n",
            "Train Epoch: 12 [28928/123872 (23%)]\tLoss: 0.326221\tLR: 0.00010487\n",
            "Train Epoch: 12 [29184/123872 (24%)]\tLoss: 0.355449\tLR: 0.00010480\n",
            "Train Epoch: 12 [29440/123872 (24%)]\tLoss: 0.328422\tLR: 0.00010474\n",
            "Train Epoch: 12 [29696/123872 (24%)]\tLoss: 0.348401\tLR: 0.00010467\n",
            "Train Epoch: 12 [29952/123872 (24%)]\tLoss: 0.372477\tLR: 0.00010461\n",
            "Train Epoch: 12 [30208/123872 (24%)]\tLoss: 0.362195\tLR: 0.00010454\n",
            "Train Epoch: 12 [30464/123872 (25%)]\tLoss: 0.352200\tLR: 0.00010448\n",
            "Train Epoch: 12 [30720/123872 (25%)]\tLoss: 0.401176\tLR: 0.00010441\n",
            "Train Epoch: 12 [30720/123872 (25%)]\tLoss: 0.401176\n",
            "Train Epoch: 12 [30976/123872 (25%)]\tLoss: 0.340144\tLR: 0.00010435\n",
            "Train Epoch: 12 [31232/123872 (25%)]\tLoss: 0.347255\tLR: 0.00010428\n",
            "Train Epoch: 12 [31488/123872 (25%)]\tLoss: 0.385699\tLR: 0.00010422\n",
            "Train Epoch: 12 [31744/123872 (26%)]\tLoss: 0.363157\tLR: 0.00010415\n",
            "Train Epoch: 12 [32000/123872 (26%)]\tLoss: 0.375501\tLR: 0.00010409\n",
            "Train Epoch: 12 [32256/123872 (26%)]\tLoss: 0.332902\tLR: 0.00010402\n",
            "Train Epoch: 12 [32512/123872 (26%)]\tLoss: 0.375942\tLR: 0.00010396\n",
            "Train Epoch: 12 [32768/123872 (26%)]\tLoss: 0.309479\tLR: 0.00010389\n",
            "Train Epoch: 12 [33024/123872 (27%)]\tLoss: 0.349070\tLR: 0.00010383\n",
            "Train Epoch: 12 [33280/123872 (27%)]\tLoss: 0.414848\tLR: 0.00010376\n",
            "Train Epoch: 12 [33280/123872 (27%)]\tLoss: 0.414848\n",
            "Train Epoch: 12 [33536/123872 (27%)]\tLoss: 0.325354\tLR: 0.00010370\n",
            "Train Epoch: 12 [33792/123872 (27%)]\tLoss: 0.367960\tLR: 0.00010364\n",
            "Train Epoch: 12 [34048/123872 (27%)]\tLoss: 0.336709\tLR: 0.00010357\n",
            "Train Epoch: 12 [34304/123872 (28%)]\tLoss: 0.405960\tLR: 0.00010351\n",
            "Train Epoch: 12 [34560/123872 (28%)]\tLoss: 0.364365\tLR: 0.00010344\n",
            "Train Epoch: 12 [34816/123872 (28%)]\tLoss: 0.339534\tLR: 0.00010338\n",
            "Train Epoch: 12 [35072/123872 (28%)]\tLoss: 0.331242\tLR: 0.00010332\n",
            "Train Epoch: 12 [35328/123872 (29%)]\tLoss: 0.341331\tLR: 0.00010325\n",
            "Train Epoch: 12 [35584/123872 (29%)]\tLoss: 0.383828\tLR: 0.00010319\n",
            "Train Epoch: 12 [35840/123872 (29%)]\tLoss: 0.341406\tLR: 0.00010312\n",
            "Train Epoch: 12 [35840/123872 (29%)]\tLoss: 0.341406\n",
            "Train Epoch: 12 [36096/123872 (29%)]\tLoss: 0.364568\tLR: 0.00010306\n",
            "Train Epoch: 12 [36352/123872 (29%)]\tLoss: 0.346790\tLR: 0.00010300\n",
            "Train Epoch: 12 [36608/123872 (30%)]\tLoss: 0.367770\tLR: 0.00010293\n",
            "Train Epoch: 12 [36864/123872 (30%)]\tLoss: 0.419418\tLR: 0.00010287\n",
            "Train Epoch: 12 [37120/123872 (30%)]\tLoss: 0.350338\tLR: 0.00010280\n",
            "Train Epoch: 12 [37376/123872 (30%)]\tLoss: 0.360594\tLR: 0.00010274\n",
            "Train Epoch: 12 [37632/123872 (30%)]\tLoss: 0.356743\tLR: 0.00010268\n",
            "Train Epoch: 12 [37888/123872 (31%)]\tLoss: 0.345077\tLR: 0.00010261\n",
            "Train Epoch: 12 [38144/123872 (31%)]\tLoss: 0.360690\tLR: 0.00010255\n",
            "Train Epoch: 12 [38400/123872 (31%)]\tLoss: 0.358559\tLR: 0.00010249\n",
            "Train Epoch: 12 [38400/123872 (31%)]\tLoss: 0.358559\n",
            "Train Epoch: 12 [38656/123872 (31%)]\tLoss: 0.357061\tLR: 0.00010242\n",
            "Train Epoch: 12 [38912/123872 (31%)]\tLoss: 0.458925\tLR: 0.00010236\n",
            "Train Epoch: 12 [39168/123872 (32%)]\tLoss: 0.340242\tLR: 0.00010229\n",
            "Train Epoch: 12 [39424/123872 (32%)]\tLoss: 0.326572\tLR: 0.00010223\n",
            "Train Epoch: 12 [39680/123872 (32%)]\tLoss: 0.358357\tLR: 0.00010217\n",
            "Train Epoch: 12 [39936/123872 (32%)]\tLoss: 0.362362\tLR: 0.00010210\n",
            "Train Epoch: 12 [40192/123872 (32%)]\tLoss: 0.372076\tLR: 0.00010204\n",
            "Train Epoch: 12 [40448/123872 (33%)]\tLoss: 0.338378\tLR: 0.00010198\n",
            "Train Epoch: 12 [40704/123872 (33%)]\tLoss: 0.362470\tLR: 0.00010192\n",
            "Train Epoch: 12 [40960/123872 (33%)]\tLoss: 0.379389\tLR: 0.00010185\n",
            "Train Epoch: 12 [40960/123872 (33%)]\tLoss: 0.379389\n",
            "Train Epoch: 12 [41216/123872 (33%)]\tLoss: 0.294625\tLR: 0.00010179\n",
            "Train Epoch: 12 [41472/123872 (33%)]\tLoss: 0.315746\tLR: 0.00010173\n",
            "Train Epoch: 12 [41728/123872 (34%)]\tLoss: 0.374113\tLR: 0.00010166\n",
            "Train Epoch: 12 [41984/123872 (34%)]\tLoss: 0.380425\tLR: 0.00010160\n",
            "Train Epoch: 12 [42240/123872 (34%)]\tLoss: 0.383786\tLR: 0.00010154\n",
            "Train Epoch: 12 [42496/123872 (34%)]\tLoss: 0.331734\tLR: 0.00010147\n",
            "Train Epoch: 12 [42752/123872 (35%)]\tLoss: 0.338086\tLR: 0.00010141\n",
            "Train Epoch: 12 [43008/123872 (35%)]\tLoss: 0.380619\tLR: 0.00010135\n",
            "Train Epoch: 12 [43264/123872 (35%)]\tLoss: 0.309283\tLR: 0.00010129\n",
            "Train Epoch: 12 [43520/123872 (35%)]\tLoss: 0.397038\tLR: 0.00010122\n",
            "Train Epoch: 12 [43520/123872 (35%)]\tLoss: 0.397038\n",
            "Train Epoch: 12 [43776/123872 (35%)]\tLoss: 0.304648\tLR: 0.00010116\n",
            "Train Epoch: 12 [44032/123872 (36%)]\tLoss: 0.369760\tLR: 0.00010110\n",
            "Train Epoch: 12 [44288/123872 (36%)]\tLoss: 0.306422\tLR: 0.00010104\n",
            "Train Epoch: 12 [44544/123872 (36%)]\tLoss: 0.384576\tLR: 0.00010097\n",
            "Train Epoch: 12 [44800/123872 (36%)]\tLoss: 0.375274\tLR: 0.00010091\n",
            "Train Epoch: 12 [45056/123872 (36%)]\tLoss: 0.361199\tLR: 0.00010085\n",
            "Train Epoch: 12 [45312/123872 (37%)]\tLoss: 0.375282\tLR: 0.00010079\n",
            "Train Epoch: 12 [45568/123872 (37%)]\tLoss: 0.389527\tLR: 0.00010072\n",
            "Train Epoch: 12 [45824/123872 (37%)]\tLoss: 0.323364\tLR: 0.00010066\n",
            "Train Epoch: 12 [46080/123872 (37%)]\tLoss: 0.351265\tLR: 0.00010060\n",
            "Train Epoch: 12 [46080/123872 (37%)]\tLoss: 0.351265\n",
            "Train Epoch: 12 [46336/123872 (37%)]\tLoss: 0.357000\tLR: 0.00010054\n",
            "Train Epoch: 12 [46592/123872 (38%)]\tLoss: 0.407378\tLR: 0.00010047\n",
            "Train Epoch: 12 [46848/123872 (38%)]\tLoss: 0.337283\tLR: 0.00010041\n",
            "Train Epoch: 12 [47104/123872 (38%)]\tLoss: 0.364566\tLR: 0.00010035\n",
            "Train Epoch: 12 [47360/123872 (38%)]\tLoss: 0.331148\tLR: 0.00010029\n",
            "Train Epoch: 12 [47616/123872 (38%)]\tLoss: 0.321065\tLR: 0.00010023\n",
            "Train Epoch: 12 [47872/123872 (39%)]\tLoss: 0.315752\tLR: 0.00010016\n",
            "Train Epoch: 12 [48128/123872 (39%)]\tLoss: 0.338048\tLR: 0.00010010\n",
            "Train Epoch: 12 [48384/123872 (39%)]\tLoss: 0.355606\tLR: 0.00010004\n",
            "Train Epoch: 12 [48640/123872 (39%)]\tLoss: 0.335382\tLR: 0.00009998\n",
            "Train Epoch: 12 [48640/123872 (39%)]\tLoss: 0.335382\n",
            "Train Epoch: 12 [48896/123872 (39%)]\tLoss: 0.420919\tLR: 0.00009992\n",
            "Train Epoch: 12 [49152/123872 (40%)]\tLoss: 0.407326\tLR: 0.00009986\n",
            "Train Epoch: 12 [49408/123872 (40%)]\tLoss: 0.314738\tLR: 0.00009979\n",
            "Train Epoch: 12 [49664/123872 (40%)]\tLoss: 0.351413\tLR: 0.00009973\n",
            "Train Epoch: 12 [49920/123872 (40%)]\tLoss: 0.348294\tLR: 0.00009967\n",
            "Train Epoch: 12 [50176/123872 (40%)]\tLoss: 0.356227\tLR: 0.00009961\n",
            "Train Epoch: 12 [50432/123872 (41%)]\tLoss: 0.434230\tLR: 0.00009955\n",
            "Train Epoch: 12 [50688/123872 (41%)]\tLoss: 0.418176\tLR: 0.00009949\n",
            "Train Epoch: 12 [50944/123872 (41%)]\tLoss: 0.322266\tLR: 0.00009942\n",
            "Train Epoch: 12 [51200/123872 (41%)]\tLoss: 0.376456\tLR: 0.00009936\n",
            "Train Epoch: 12 [51200/123872 (41%)]\tLoss: 0.376456\n",
            "Train Epoch: 12 [51456/123872 (42%)]\tLoss: 0.437361\tLR: 0.00009930\n",
            "Train Epoch: 12 [51712/123872 (42%)]\tLoss: 0.295480\tLR: 0.00009924\n",
            "Train Epoch: 12 [51968/123872 (42%)]\tLoss: 0.349407\tLR: 0.00009918\n",
            "Train Epoch: 12 [52224/123872 (42%)]\tLoss: 0.326216\tLR: 0.00009912\n",
            "Train Epoch: 12 [52480/123872 (42%)]\tLoss: 0.414180\tLR: 0.00009906\n",
            "Train Epoch: 12 [52736/123872 (43%)]\tLoss: 0.331019\tLR: 0.00009900\n",
            "Train Epoch: 12 [52992/123872 (43%)]\tLoss: 0.364912\tLR: 0.00009893\n",
            "Train Epoch: 12 [53248/123872 (43%)]\tLoss: 0.357897\tLR: 0.00009887\n",
            "Train Epoch: 12 [53504/123872 (43%)]\tLoss: 0.345369\tLR: 0.00009881\n",
            "Train Epoch: 12 [53760/123872 (43%)]\tLoss: 0.316163\tLR: 0.00009875\n",
            "Train Epoch: 12 [53760/123872 (43%)]\tLoss: 0.316163\n",
            "Train Epoch: 12 [54016/123872 (44%)]\tLoss: 0.366098\tLR: 0.00009869\n",
            "Train Epoch: 12 [54272/123872 (44%)]\tLoss: 0.382667\tLR: 0.00009863\n",
            "Train Epoch: 12 [54528/123872 (44%)]\tLoss: 0.405093\tLR: 0.00009857\n",
            "Train Epoch: 12 [54784/123872 (44%)]\tLoss: 0.333219\tLR: 0.00009851\n",
            "Train Epoch: 12 [55040/123872 (44%)]\tLoss: 0.395792\tLR: 0.00009845\n",
            "Train Epoch: 12 [55296/123872 (45%)]\tLoss: 0.334166\tLR: 0.00009839\n",
            "Train Epoch: 12 [55552/123872 (45%)]\tLoss: 0.315376\tLR: 0.00009833\n",
            "Train Epoch: 12 [55808/123872 (45%)]\tLoss: 0.345340\tLR: 0.00009827\n",
            "Train Epoch: 12 [56064/123872 (45%)]\tLoss: 0.425591\tLR: 0.00009820\n",
            "Train Epoch: 12 [56320/123872 (45%)]\tLoss: 0.362370\tLR: 0.00009814\n",
            "Train Epoch: 12 [56320/123872 (45%)]\tLoss: 0.362370\n",
            "Train Epoch: 12 [56576/123872 (46%)]\tLoss: 0.335247\tLR: 0.00009808\n",
            "Train Epoch: 12 [56832/123872 (46%)]\tLoss: 0.400525\tLR: 0.00009802\n",
            "Train Epoch: 12 [57088/123872 (46%)]\tLoss: 0.354660\tLR: 0.00009796\n",
            "Train Epoch: 12 [57344/123872 (46%)]\tLoss: 0.299606\tLR: 0.00009790\n",
            "Train Epoch: 12 [57600/123872 (46%)]\tLoss: 0.381772\tLR: 0.00009784\n",
            "Train Epoch: 12 [57856/123872 (47%)]\tLoss: 0.302312\tLR: 0.00009778\n",
            "Train Epoch: 12 [58112/123872 (47%)]\tLoss: 0.299174\tLR: 0.00009772\n",
            "Train Epoch: 12 [58368/123872 (47%)]\tLoss: 0.282633\tLR: 0.00009766\n",
            "Train Epoch: 12 [58624/123872 (47%)]\tLoss: 0.395817\tLR: 0.00009760\n",
            "Train Epoch: 12 [58880/123872 (48%)]\tLoss: 0.362512\tLR: 0.00009754\n",
            "Train Epoch: 12 [58880/123872 (48%)]\tLoss: 0.362512\n",
            "Train Epoch: 12 [59136/123872 (48%)]\tLoss: 0.357443\tLR: 0.00009748\n",
            "Train Epoch: 12 [59392/123872 (48%)]\tLoss: 0.391784\tLR: 0.00009742\n",
            "Train Epoch: 12 [59648/123872 (48%)]\tLoss: 0.401040\tLR: 0.00009736\n",
            "Train Epoch: 12 [59904/123872 (48%)]\tLoss: 0.372354\tLR: 0.00009730\n",
            "Train Epoch: 12 [60160/123872 (49%)]\tLoss: 0.350611\tLR: 0.00009724\n",
            "Train Epoch: 12 [60416/123872 (49%)]\tLoss: 0.346808\tLR: 0.00009718\n",
            "Train Epoch: 12 [60672/123872 (49%)]\tLoss: 0.393009\tLR: 0.00009712\n",
            "Train Epoch: 12 [60928/123872 (49%)]\tLoss: 0.309638\tLR: 0.00009706\n",
            "Train Epoch: 12 [61184/123872 (49%)]\tLoss: 0.349323\tLR: 0.00009700\n",
            "Train Epoch: 12 [61440/123872 (50%)]\tLoss: 0.312360\tLR: 0.00009694\n",
            "Train Epoch: 12 [61440/123872 (50%)]\tLoss: 0.312360\n",
            "Train Epoch: 12 [61696/123872 (50%)]\tLoss: 0.354583\tLR: 0.00009688\n",
            "Train Epoch: 12 [61952/123872 (50%)]\tLoss: 0.366634\tLR: 0.00009682\n",
            "Train Epoch: 12 [62208/123872 (50%)]\tLoss: 0.329957\tLR: 0.00009677\n",
            "Train Epoch: 12 [62464/123872 (50%)]\tLoss: 0.387751\tLR: 0.00009671\n",
            "Train Epoch: 12 [62720/123872 (51%)]\tLoss: 0.345940\tLR: 0.00009665\n",
            "Train Epoch: 12 [62976/123872 (51%)]\tLoss: 0.356674\tLR: 0.00009659\n",
            "Train Epoch: 12 [63232/123872 (51%)]\tLoss: 0.363367\tLR: 0.00009653\n",
            "Train Epoch: 12 [63488/123872 (51%)]\tLoss: 0.352228\tLR: 0.00009647\n",
            "Train Epoch: 12 [63744/123872 (51%)]\tLoss: 0.403697\tLR: 0.00009641\n",
            "Train Epoch: 12 [64000/123872 (52%)]\tLoss: 0.387160\tLR: 0.00009635\n",
            "Train Epoch: 12 [64000/123872 (52%)]\tLoss: 0.387160\n",
            "Train Epoch: 12 [64256/123872 (52%)]\tLoss: 0.380058\tLR: 0.00009629\n",
            "Train Epoch: 12 [64512/123872 (52%)]\tLoss: 0.338591\tLR: 0.00009623\n",
            "Train Epoch: 12 [64768/123872 (52%)]\tLoss: 0.346535\tLR: 0.00009617\n",
            "Train Epoch: 12 [65024/123872 (52%)]\tLoss: 0.316897\tLR: 0.00009611\n",
            "Train Epoch: 12 [65280/123872 (53%)]\tLoss: 0.365455\tLR: 0.00009606\n",
            "Train Epoch: 12 [65536/123872 (53%)]\tLoss: 0.377667\tLR: 0.00009600\n",
            "Train Epoch: 12 [65792/123872 (53%)]\tLoss: 0.317533\tLR: 0.00009594\n",
            "Train Epoch: 12 [66048/123872 (53%)]\tLoss: 0.320317\tLR: 0.00009588\n",
            "Train Epoch: 12 [66304/123872 (54%)]\tLoss: 0.290148\tLR: 0.00009582\n",
            "Train Epoch: 12 [66560/123872 (54%)]\tLoss: 0.343810\tLR: 0.00009576\n",
            "Train Epoch: 12 [66560/123872 (54%)]\tLoss: 0.343810\n",
            "Train Epoch: 12 [66816/123872 (54%)]\tLoss: 0.375923\tLR: 0.00009570\n",
            "Train Epoch: 12 [67072/123872 (54%)]\tLoss: 0.340122\tLR: 0.00009564\n",
            "Train Epoch: 12 [67328/123872 (54%)]\tLoss: 0.328975\tLR: 0.00009559\n",
            "Train Epoch: 12 [67584/123872 (55%)]\tLoss: 0.411666\tLR: 0.00009553\n",
            "Train Epoch: 12 [67840/123872 (55%)]\tLoss: 0.383994\tLR: 0.00009547\n",
            "Train Epoch: 12 [68096/123872 (55%)]\tLoss: 0.315985\tLR: 0.00009541\n",
            "Train Epoch: 12 [68352/123872 (55%)]\tLoss: 0.298382\tLR: 0.00009535\n",
            "Train Epoch: 12 [68608/123872 (55%)]\tLoss: 0.386564\tLR: 0.00009529\n",
            "Train Epoch: 12 [68864/123872 (56%)]\tLoss: 0.311432\tLR: 0.00009523\n",
            "Train Epoch: 12 [69120/123872 (56%)]\tLoss: 0.412793\tLR: 0.00009518\n",
            "Train Epoch: 12 [69120/123872 (56%)]\tLoss: 0.412793\n",
            "Train Epoch: 12 [69376/123872 (56%)]\tLoss: 0.381488\tLR: 0.00009512\n",
            "Train Epoch: 12 [69632/123872 (56%)]\tLoss: 0.358730\tLR: 0.00009506\n",
            "Train Epoch: 12 [69888/123872 (56%)]\tLoss: 0.390761\tLR: 0.00009500\n",
            "Train Epoch: 12 [70144/123872 (57%)]\tLoss: 0.339325\tLR: 0.00009494\n",
            "Train Epoch: 12 [70400/123872 (57%)]\tLoss: 0.400437\tLR: 0.00009489\n",
            "Train Epoch: 12 [70656/123872 (57%)]\tLoss: 0.430013\tLR: 0.00009483\n",
            "Train Epoch: 12 [70912/123872 (57%)]\tLoss: 0.367217\tLR: 0.00009477\n",
            "Train Epoch: 12 [71168/123872 (57%)]\tLoss: 0.328569\tLR: 0.00009471\n",
            "Train Epoch: 12 [71424/123872 (58%)]\tLoss: 0.325148\tLR: 0.00009465\n",
            "Train Epoch: 12 [71680/123872 (58%)]\tLoss: 0.348905\tLR: 0.00009460\n",
            "Train Epoch: 12 [71680/123872 (58%)]\tLoss: 0.348905\n",
            "Train Epoch: 12 [71936/123872 (58%)]\tLoss: 0.335573\tLR: 0.00009454\n",
            "Train Epoch: 12 [72192/123872 (58%)]\tLoss: 0.395846\tLR: 0.00009448\n",
            "Train Epoch: 12 [72448/123872 (58%)]\tLoss: 0.380901\tLR: 0.00009442\n",
            "Train Epoch: 12 [72704/123872 (59%)]\tLoss: 0.337125\tLR: 0.00009437\n",
            "Train Epoch: 12 [72960/123872 (59%)]\tLoss: 0.333210\tLR: 0.00009431\n",
            "Train Epoch: 12 [73216/123872 (59%)]\tLoss: 0.404969\tLR: 0.00009425\n",
            "Train Epoch: 12 [73472/123872 (59%)]\tLoss: 0.331005\tLR: 0.00009419\n",
            "Train Epoch: 12 [73728/123872 (60%)]\tLoss: 0.311058\tLR: 0.00009414\n",
            "Train Epoch: 12 [73984/123872 (60%)]\tLoss: 0.387603\tLR: 0.00009408\n",
            "Train Epoch: 12 [74240/123872 (60%)]\tLoss: 0.320716\tLR: 0.00009402\n",
            "Train Epoch: 12 [74240/123872 (60%)]\tLoss: 0.320716\n",
            "Train Epoch: 12 [74496/123872 (60%)]\tLoss: 0.402637\tLR: 0.00009396\n",
            "Train Epoch: 12 [74752/123872 (60%)]\tLoss: 0.352466\tLR: 0.00009391\n",
            "Train Epoch: 12 [75008/123872 (61%)]\tLoss: 0.317939\tLR: 0.00009385\n",
            "Train Epoch: 12 [75264/123872 (61%)]\tLoss: 0.394222\tLR: 0.00009379\n",
            "Train Epoch: 12 [75520/123872 (61%)]\tLoss: 0.335603\tLR: 0.00009373\n",
            "Train Epoch: 12 [75776/123872 (61%)]\tLoss: 0.356639\tLR: 0.00009368\n",
            "Train Epoch: 12 [76032/123872 (61%)]\tLoss: 0.346838\tLR: 0.00009362\n",
            "Train Epoch: 12 [76288/123872 (62%)]\tLoss: 0.350325\tLR: 0.00009356\n",
            "Train Epoch: 12 [76544/123872 (62%)]\tLoss: 0.395911\tLR: 0.00009351\n",
            "Train Epoch: 12 [76800/123872 (62%)]\tLoss: 0.353038\tLR: 0.00009345\n",
            "Train Epoch: 12 [76800/123872 (62%)]\tLoss: 0.353038\n",
            "Train Epoch: 12 [77056/123872 (62%)]\tLoss: 0.326972\tLR: 0.00009339\n",
            "Train Epoch: 12 [77312/123872 (62%)]\tLoss: 0.369622\tLR: 0.00009334\n",
            "Train Epoch: 12 [77568/123872 (63%)]\tLoss: 0.348301\tLR: 0.00009328\n",
            "Train Epoch: 12 [77824/123872 (63%)]\tLoss: 0.330666\tLR: 0.00009322\n",
            "Train Epoch: 12 [78080/123872 (63%)]\tLoss: 0.316969\tLR: 0.00009317\n",
            "Train Epoch: 12 [78336/123872 (63%)]\tLoss: 0.349713\tLR: 0.00009311\n",
            "Train Epoch: 12 [78592/123872 (63%)]\tLoss: 0.338710\tLR: 0.00009305\n",
            "Train Epoch: 12 [78848/123872 (64%)]\tLoss: 0.351962\tLR: 0.00009300\n",
            "Train Epoch: 12 [79104/123872 (64%)]\tLoss: 0.386097\tLR: 0.00009294\n",
            "Train Epoch: 12 [79360/123872 (64%)]\tLoss: 0.331151\tLR: 0.00009288\n",
            "Train Epoch: 12 [79360/123872 (64%)]\tLoss: 0.331151\n",
            "Train Epoch: 12 [79616/123872 (64%)]\tLoss: 0.324794\tLR: 0.00009283\n",
            "Train Epoch: 12 [79872/123872 (64%)]\tLoss: 0.347410\tLR: 0.00009277\n",
            "Train Epoch: 12 [80128/123872 (65%)]\tLoss: 0.329649\tLR: 0.00009271\n",
            "Train Epoch: 12 [80384/123872 (65%)]\tLoss: 0.294742\tLR: 0.00009266\n",
            "Train Epoch: 12 [80640/123872 (65%)]\tLoss: 0.340038\tLR: 0.00009260\n",
            "Train Epoch: 12 [80896/123872 (65%)]\tLoss: 0.368152\tLR: 0.00009255\n",
            "Train Epoch: 12 [81152/123872 (65%)]\tLoss: 0.381761\tLR: 0.00009249\n",
            "Train Epoch: 12 [81408/123872 (66%)]\tLoss: 0.354725\tLR: 0.00009243\n",
            "Train Epoch: 12 [81664/123872 (66%)]\tLoss: 0.333583\tLR: 0.00009238\n",
            "Train Epoch: 12 [81920/123872 (66%)]\tLoss: 0.361674\tLR: 0.00009232\n",
            "Train Epoch: 12 [81920/123872 (66%)]\tLoss: 0.361674\n",
            "Train Epoch: 12 [82176/123872 (66%)]\tLoss: 0.399863\tLR: 0.00009226\n",
            "Train Epoch: 12 [82432/123872 (67%)]\tLoss: 0.334722\tLR: 0.00009221\n",
            "Train Epoch: 12 [82688/123872 (67%)]\tLoss: 0.352117\tLR: 0.00009215\n",
            "Train Epoch: 12 [82944/123872 (67%)]\tLoss: 0.436386\tLR: 0.00009210\n",
            "Train Epoch: 12 [83200/123872 (67%)]\tLoss: 0.394987\tLR: 0.00009204\n",
            "Train Epoch: 12 [83456/123872 (67%)]\tLoss: 0.416271\tLR: 0.00009199\n",
            "Train Epoch: 12 [83712/123872 (68%)]\tLoss: 0.357641\tLR: 0.00009193\n",
            "Train Epoch: 12 [83968/123872 (68%)]\tLoss: 0.384428\tLR: 0.00009187\n",
            "Train Epoch: 12 [84224/123872 (68%)]\tLoss: 0.315933\tLR: 0.00009182\n",
            "Train Epoch: 12 [84480/123872 (68%)]\tLoss: 0.366326\tLR: 0.00009176\n",
            "Train Epoch: 12 [84480/123872 (68%)]\tLoss: 0.366326\n",
            "Train Epoch: 12 [84736/123872 (68%)]\tLoss: 0.342885\tLR: 0.00009171\n",
            "Train Epoch: 12 [84992/123872 (69%)]\tLoss: 0.398797\tLR: 0.00009165\n",
            "Train Epoch: 12 [85248/123872 (69%)]\tLoss: 0.354351\tLR: 0.00009160\n",
            "Train Epoch: 12 [85504/123872 (69%)]\tLoss: 0.429755\tLR: 0.00009154\n",
            "Train Epoch: 12 [85760/123872 (69%)]\tLoss: 0.353176\tLR: 0.00009149\n",
            "Train Epoch: 12 [86016/123872 (69%)]\tLoss: 0.375744\tLR: 0.00009143\n",
            "Train Epoch: 12 [86272/123872 (70%)]\tLoss: 0.331816\tLR: 0.00009138\n",
            "Train Epoch: 12 [86528/123872 (70%)]\tLoss: 0.342411\tLR: 0.00009132\n",
            "Train Epoch: 12 [86784/123872 (70%)]\tLoss: 0.398644\tLR: 0.00009127\n",
            "Train Epoch: 12 [87040/123872 (70%)]\tLoss: 0.375094\tLR: 0.00009121\n",
            "Train Epoch: 12 [87040/123872 (70%)]\tLoss: 0.375094\n",
            "Train Epoch: 12 [87296/123872 (70%)]\tLoss: 0.334214\tLR: 0.00009116\n",
            "Train Epoch: 12 [87552/123872 (71%)]\tLoss: 0.335897\tLR: 0.00009110\n",
            "Train Epoch: 12 [87808/123872 (71%)]\tLoss: 0.314571\tLR: 0.00009105\n",
            "Train Epoch: 12 [88064/123872 (71%)]\tLoss: 0.314434\tLR: 0.00009099\n",
            "Train Epoch: 12 [88320/123872 (71%)]\tLoss: 0.355596\tLR: 0.00009094\n",
            "Train Epoch: 12 [88576/123872 (71%)]\tLoss: 0.291276\tLR: 0.00009088\n",
            "Train Epoch: 12 [88832/123872 (72%)]\tLoss: 0.362391\tLR: 0.00009083\n",
            "Train Epoch: 12 [89088/123872 (72%)]\tLoss: 0.284975\tLR: 0.00009077\n",
            "Train Epoch: 12 [89344/123872 (72%)]\tLoss: 0.286189\tLR: 0.00009072\n",
            "Train Epoch: 12 [89600/123872 (72%)]\tLoss: 0.400078\tLR: 0.00009066\n",
            "Train Epoch: 12 [89600/123872 (72%)]\tLoss: 0.400078\n",
            "Train Epoch: 12 [89856/123872 (73%)]\tLoss: 0.386396\tLR: 0.00009061\n",
            "Train Epoch: 12 [90112/123872 (73%)]\tLoss: 0.435018\tLR: 0.00009055\n",
            "Train Epoch: 12 [90368/123872 (73%)]\tLoss: 0.341777\tLR: 0.00009050\n",
            "Train Epoch: 12 [90624/123872 (73%)]\tLoss: 0.347541\tLR: 0.00009044\n",
            "Train Epoch: 12 [90880/123872 (73%)]\tLoss: 0.368797\tLR: 0.00009039\n",
            "Train Epoch: 12 [91136/123872 (74%)]\tLoss: 0.370734\tLR: 0.00009033\n",
            "Train Epoch: 12 [91392/123872 (74%)]\tLoss: 0.364377\tLR: 0.00009028\n",
            "Train Epoch: 12 [91648/123872 (74%)]\tLoss: 0.342332\tLR: 0.00009023\n",
            "Train Epoch: 12 [91904/123872 (74%)]\tLoss: 0.406459\tLR: 0.00009017\n",
            "Train Epoch: 12 [92160/123872 (74%)]\tLoss: 0.351348\tLR: 0.00009012\n",
            "Train Epoch: 12 [92160/123872 (74%)]\tLoss: 0.351348\n",
            "Train Epoch: 12 [92416/123872 (75%)]\tLoss: 0.340592\tLR: 0.00009006\n",
            "Train Epoch: 12 [92672/123872 (75%)]\tLoss: 0.333679\tLR: 0.00009001\n",
            "Train Epoch: 12 [92928/123872 (75%)]\tLoss: 0.389990\tLR: 0.00008996\n",
            "Train Epoch: 12 [93184/123872 (75%)]\tLoss: 0.354022\tLR: 0.00008990\n",
            "Train Epoch: 12 [93440/123872 (75%)]\tLoss: 0.280317\tLR: 0.00008985\n",
            "Train Epoch: 12 [93696/123872 (76%)]\tLoss: 0.378501\tLR: 0.00008979\n",
            "Train Epoch: 12 [93952/123872 (76%)]\tLoss: 0.395598\tLR: 0.00008974\n",
            "Train Epoch: 12 [94208/123872 (76%)]\tLoss: 0.353671\tLR: 0.00008969\n",
            "Train Epoch: 12 [94464/123872 (76%)]\tLoss: 0.367084\tLR: 0.00008963\n",
            "Train Epoch: 12 [94720/123872 (76%)]\tLoss: 0.357269\tLR: 0.00008958\n",
            "Train Epoch: 12 [94720/123872 (76%)]\tLoss: 0.357269\n",
            "Train Epoch: 12 [94976/123872 (77%)]\tLoss: 0.399995\tLR: 0.00008952\n",
            "Train Epoch: 12 [95232/123872 (77%)]\tLoss: 0.359766\tLR: 0.00008947\n",
            "Train Epoch: 12 [95488/123872 (77%)]\tLoss: 0.414163\tLR: 0.00008942\n",
            "Train Epoch: 12 [95744/123872 (77%)]\tLoss: 0.383879\tLR: 0.00008936\n",
            "Train Epoch: 12 [96000/123872 (77%)]\tLoss: 0.351350\tLR: 0.00008931\n",
            "Train Epoch: 12 [96256/123872 (78%)]\tLoss: 0.395917\tLR: 0.00008926\n",
            "Train Epoch: 12 [96512/123872 (78%)]\tLoss: 0.390586\tLR: 0.00008920\n",
            "Train Epoch: 12 [96768/123872 (78%)]\tLoss: 0.364125\tLR: 0.00008915\n",
            "Train Epoch: 12 [97024/123872 (78%)]\tLoss: 0.377629\tLR: 0.00008910\n",
            "Train Epoch: 12 [97280/123872 (79%)]\tLoss: 0.302147\tLR: 0.00008904\n",
            "Train Epoch: 12 [97280/123872 (79%)]\tLoss: 0.302147\n",
            "Train Epoch: 12 [97536/123872 (79%)]\tLoss: 0.341763\tLR: 0.00008899\n",
            "Train Epoch: 12 [97792/123872 (79%)]\tLoss: 0.314310\tLR: 0.00008894\n",
            "Train Epoch: 12 [98048/123872 (79%)]\tLoss: 0.329095\tLR: 0.00008888\n",
            "Train Epoch: 12 [98304/123872 (79%)]\tLoss: 0.386429\tLR: 0.00008883\n",
            "Train Epoch: 12 [98560/123872 (80%)]\tLoss: 0.360979\tLR: 0.00008878\n",
            "Train Epoch: 12 [98816/123872 (80%)]\tLoss: 0.359053\tLR: 0.00008873\n",
            "Train Epoch: 12 [99072/123872 (80%)]\tLoss: 0.355119\tLR: 0.00008867\n",
            "Train Epoch: 12 [99328/123872 (80%)]\tLoss: 0.362930\tLR: 0.00008862\n",
            "Train Epoch: 12 [99584/123872 (80%)]\tLoss: 0.329399\tLR: 0.00008857\n",
            "Train Epoch: 12 [99840/123872 (81%)]\tLoss: 0.382832\tLR: 0.00008851\n",
            "Train Epoch: 12 [99840/123872 (81%)]\tLoss: 0.382832\n",
            "Train Epoch: 12 [100096/123872 (81%)]\tLoss: 0.378644\tLR: 0.00008846\n",
            "Train Epoch: 12 [100352/123872 (81%)]\tLoss: 0.373585\tLR: 0.00008841\n",
            "Train Epoch: 12 [100608/123872 (81%)]\tLoss: 0.347399\tLR: 0.00008836\n",
            "Train Epoch: 12 [100864/123872 (81%)]\tLoss: 0.279714\tLR: 0.00008830\n",
            "Train Epoch: 12 [101120/123872 (82%)]\tLoss: 0.351620\tLR: 0.00008825\n",
            "Train Epoch: 12 [101376/123872 (82%)]\tLoss: 0.352636\tLR: 0.00008820\n",
            "Train Epoch: 12 [101632/123872 (82%)]\tLoss: 0.326805\tLR: 0.00008815\n",
            "Train Epoch: 12 [101888/123872 (82%)]\tLoss: 0.374931\tLR: 0.00008809\n",
            "Train Epoch: 12 [102144/123872 (82%)]\tLoss: 0.346058\tLR: 0.00008804\n",
            "Train Epoch: 12 [102400/123872 (83%)]\tLoss: 0.422603\tLR: 0.00008799\n",
            "Train Epoch: 12 [102400/123872 (83%)]\tLoss: 0.422603\n",
            "Train Epoch: 12 [102656/123872 (83%)]\tLoss: 0.394607\tLR: 0.00008794\n",
            "Train Epoch: 12 [102912/123872 (83%)]\tLoss: 0.360442\tLR: 0.00008788\n",
            "Train Epoch: 12 [103168/123872 (83%)]\tLoss: 0.366454\tLR: 0.00008783\n",
            "Train Epoch: 12 [103424/123872 (83%)]\tLoss: 0.364700\tLR: 0.00008778\n",
            "Train Epoch: 12 [103680/123872 (84%)]\tLoss: 0.362499\tLR: 0.00008773\n",
            "Train Epoch: 12 [103936/123872 (84%)]\tLoss: 0.301219\tLR: 0.00008767\n",
            "Train Epoch: 12 [104192/123872 (84%)]\tLoss: 0.381575\tLR: 0.00008762\n",
            "Train Epoch: 12 [104448/123872 (84%)]\tLoss: 0.371230\tLR: 0.00008757\n",
            "Train Epoch: 12 [104704/123872 (85%)]\tLoss: 0.373633\tLR: 0.00008752\n",
            "Train Epoch: 12 [104960/123872 (85%)]\tLoss: 0.342122\tLR: 0.00008747\n",
            "Train Epoch: 12 [104960/123872 (85%)]\tLoss: 0.342122\n",
            "Train Epoch: 12 [105216/123872 (85%)]\tLoss: 0.319615\tLR: 0.00008742\n",
            "Train Epoch: 12 [105472/123872 (85%)]\tLoss: 0.320201\tLR: 0.00008736\n",
            "Train Epoch: 12 [105728/123872 (85%)]\tLoss: 0.350395\tLR: 0.00008731\n",
            "Train Epoch: 12 [105984/123872 (86%)]\tLoss: 0.330877\tLR: 0.00008726\n",
            "Train Epoch: 12 [106240/123872 (86%)]\tLoss: 0.296224\tLR: 0.00008721\n",
            "Train Epoch: 12 [106496/123872 (86%)]\tLoss: 0.315831\tLR: 0.00008716\n",
            "Train Epoch: 12 [106752/123872 (86%)]\tLoss: 0.341079\tLR: 0.00008711\n",
            "Train Epoch: 12 [107008/123872 (86%)]\tLoss: 0.401119\tLR: 0.00008705\n",
            "Train Epoch: 12 [107264/123872 (87%)]\tLoss: 0.332366\tLR: 0.00008700\n",
            "Train Epoch: 12 [107520/123872 (87%)]\tLoss: 0.382790\tLR: 0.00008695\n",
            "Train Epoch: 12 [107520/123872 (87%)]\tLoss: 0.382790\n",
            "Train Epoch: 12 [107776/123872 (87%)]\tLoss: 0.384166\tLR: 0.00008690\n",
            "Train Epoch: 12 [108032/123872 (87%)]\tLoss: 0.381189\tLR: 0.00008685\n",
            "Train Epoch: 12 [108288/123872 (87%)]\tLoss: 0.355253\tLR: 0.00008680\n",
            "Train Epoch: 12 [108544/123872 (88%)]\tLoss: 0.338873\tLR: 0.00008675\n",
            "Train Epoch: 12 [108800/123872 (88%)]\tLoss: 0.389970\tLR: 0.00008669\n",
            "Train Epoch: 12 [109056/123872 (88%)]\tLoss: 0.356704\tLR: 0.00008664\n",
            "Train Epoch: 12 [109312/123872 (88%)]\tLoss: 0.384543\tLR: 0.00008659\n",
            "Train Epoch: 12 [109568/123872 (88%)]\tLoss: 0.347880\tLR: 0.00008654\n",
            "Train Epoch: 12 [109824/123872 (89%)]\tLoss: 0.450688\tLR: 0.00008649\n",
            "Train Epoch: 12 [110080/123872 (89%)]\tLoss: 0.404787\tLR: 0.00008644\n",
            "Train Epoch: 12 [110080/123872 (89%)]\tLoss: 0.404787\n",
            "Train Epoch: 12 [110336/123872 (89%)]\tLoss: 0.380252\tLR: 0.00008639\n",
            "Train Epoch: 12 [110592/123872 (89%)]\tLoss: 0.379078\tLR: 0.00008634\n",
            "Train Epoch: 12 [110848/123872 (89%)]\tLoss: 0.346927\tLR: 0.00008629\n",
            "Train Epoch: 12 [111104/123872 (90%)]\tLoss: 0.404908\tLR: 0.00008624\n",
            "Train Epoch: 12 [111360/123872 (90%)]\tLoss: 0.415635\tLR: 0.00008619\n",
            "Train Epoch: 12 [111616/123872 (90%)]\tLoss: 0.319216\tLR: 0.00008613\n",
            "Train Epoch: 12 [111872/123872 (90%)]\tLoss: 0.342447\tLR: 0.00008608\n",
            "Train Epoch: 12 [112128/123872 (90%)]\tLoss: 0.359428\tLR: 0.00008603\n",
            "Train Epoch: 12 [112384/123872 (91%)]\tLoss: 0.386428\tLR: 0.00008598\n",
            "Train Epoch: 12 [112640/123872 (91%)]\tLoss: 0.362810\tLR: 0.00008593\n",
            "Train Epoch: 12 [112640/123872 (91%)]\tLoss: 0.362810\n",
            "Train Epoch: 12 [112896/123872 (91%)]\tLoss: 0.358450\tLR: 0.00008588\n",
            "Train Epoch: 12 [113152/123872 (91%)]\tLoss: 0.315032\tLR: 0.00008583\n",
            "Train Epoch: 12 [113408/123872 (92%)]\tLoss: 0.318263\tLR: 0.00008578\n",
            "Train Epoch: 12 [113664/123872 (92%)]\tLoss: 0.311551\tLR: 0.00008573\n",
            "Train Epoch: 12 [113920/123872 (92%)]\tLoss: 0.398013\tLR: 0.00008568\n",
            "Train Epoch: 12 [114176/123872 (92%)]\tLoss: 0.349685\tLR: 0.00008563\n",
            "Train Epoch: 12 [114432/123872 (92%)]\tLoss: 0.329113\tLR: 0.00008558\n",
            "Train Epoch: 12 [114688/123872 (93%)]\tLoss: 0.451029\tLR: 0.00008553\n",
            "Train Epoch: 12 [114944/123872 (93%)]\tLoss: 0.304943\tLR: 0.00008548\n",
            "Train Epoch: 12 [115200/123872 (93%)]\tLoss: 0.318802\tLR: 0.00008543\n",
            "Train Epoch: 12 [115200/123872 (93%)]\tLoss: 0.318802\n",
            "Train Epoch: 12 [115456/123872 (93%)]\tLoss: 0.376506\tLR: 0.00008538\n",
            "Train Epoch: 12 [115712/123872 (93%)]\tLoss: 0.385989\tLR: 0.00008533\n",
            "Train Epoch: 12 [115968/123872 (94%)]\tLoss: 0.375715\tLR: 0.00008528\n",
            "Train Epoch: 12 [116224/123872 (94%)]\tLoss: 0.303056\tLR: 0.00008523\n",
            "Train Epoch: 12 [116480/123872 (94%)]\tLoss: 0.318745\tLR: 0.00008518\n",
            "Train Epoch: 12 [116736/123872 (94%)]\tLoss: 0.354040\tLR: 0.00008513\n",
            "Train Epoch: 12 [116992/123872 (94%)]\tLoss: 0.378700\tLR: 0.00008508\n",
            "Train Epoch: 12 [117248/123872 (95%)]\tLoss: 0.342091\tLR: 0.00008503\n",
            "Train Epoch: 12 [117504/123872 (95%)]\tLoss: 0.396838\tLR: 0.00008498\n",
            "Train Epoch: 12 [117760/123872 (95%)]\tLoss: 0.298290\tLR: 0.00008493\n",
            "Train Epoch: 12 [117760/123872 (95%)]\tLoss: 0.298290\n",
            "Train Epoch: 12 [118016/123872 (95%)]\tLoss: 0.361369\tLR: 0.00008488\n",
            "Train Epoch: 12 [118272/123872 (95%)]\tLoss: 0.318292\tLR: 0.00008483\n",
            "Train Epoch: 12 [118528/123872 (96%)]\tLoss: 0.360530\tLR: 0.00008478\n",
            "Train Epoch: 12 [118784/123872 (96%)]\tLoss: 0.369560\tLR: 0.00008473\n",
            "Train Epoch: 12 [119040/123872 (96%)]\tLoss: 0.353825\tLR: 0.00008469\n",
            "Train Epoch: 12 [119296/123872 (96%)]\tLoss: 0.328516\tLR: 0.00008464\n",
            "Train Epoch: 12 [119552/123872 (96%)]\tLoss: 0.340487\tLR: 0.00008459\n",
            "Train Epoch: 12 [119808/123872 (97%)]\tLoss: 0.325296\tLR: 0.00008454\n",
            "Train Epoch: 12 [120064/123872 (97%)]\tLoss: 0.402219\tLR: 0.00008449\n",
            "Train Epoch: 12 [120320/123872 (97%)]\tLoss: 0.451408\tLR: 0.00008444\n",
            "Train Epoch: 12 [120320/123872 (97%)]\tLoss: 0.451408\n",
            "Train Epoch: 12 [120576/123872 (97%)]\tLoss: 0.345927\tLR: 0.00008439\n",
            "Train Epoch: 12 [120832/123872 (98%)]\tLoss: 0.287850\tLR: 0.00008434\n",
            "Train Epoch: 12 [121088/123872 (98%)]\tLoss: 0.338292\tLR: 0.00008429\n",
            "Train Epoch: 12 [121344/123872 (98%)]\tLoss: 0.369887\tLR: 0.00008424\n",
            "Train Epoch: 12 [121600/123872 (98%)]\tLoss: 0.375300\tLR: 0.00008419\n",
            "Train Epoch: 12 [121856/123872 (98%)]\tLoss: 0.385220\tLR: 0.00008415\n",
            "Train Epoch: 12 [122112/123872 (99%)]\tLoss: 0.353517\tLR: 0.00008410\n",
            "Train Epoch: 12 [122368/123872 (99%)]\tLoss: 0.370343\tLR: 0.00008405\n",
            "Train Epoch: 12 [122624/123872 (99%)]\tLoss: 0.294724\tLR: 0.00008400\n",
            "Train Epoch: 12 [122880/123872 (99%)]\tLoss: 0.390984\tLR: 0.00008395\n",
            "Train Epoch: 12 [122880/123872 (99%)]\tLoss: 0.390984\n",
            "Train Epoch: 12 [123136/123872 (99%)]\tLoss: 0.348011\tLR: 0.00008390\n",
            "Train Epoch: 12 [123392/123872 (100%)]\tLoss: 0.318387\tLR: 0.00008385\n",
            "Train Epoch: 12 [108192/123872 (100%)]\tLoss: 0.333632\tLR: 0.00008381\n",
            "\n",
            "Test set: Average loss: 0.0015, Accuracy: 25752/30970 (83.15%)\n",
            "\n",
            "Train Epoch: 13 [0/123872 (0%)]\tLoss: 0.334320\tLR: 0.00008376\n",
            "Train Epoch: 13 [0/123872 (0%)]\tLoss: 0.334320\n",
            "Train Epoch: 13 [256/123872 (0%)]\tLoss: 0.353420\tLR: 0.00008371\n",
            "Train Epoch: 13 [512/123872 (0%)]\tLoss: 0.392471\tLR: 0.00008366\n",
            "Train Epoch: 13 [768/123872 (1%)]\tLoss: 0.405021\tLR: 0.00008361\n",
            "Train Epoch: 13 [1024/123872 (1%)]\tLoss: 0.329757\tLR: 0.00008356\n",
            "Train Epoch: 13 [1280/123872 (1%)]\tLoss: 0.378179\tLR: 0.00008352\n",
            "Train Epoch: 13 [1536/123872 (1%)]\tLoss: 0.352039\tLR: 0.00008347\n",
            "Train Epoch: 13 [1792/123872 (1%)]\tLoss: 0.356304\tLR: 0.00008342\n",
            "Train Epoch: 13 [2048/123872 (2%)]\tLoss: 0.372390\tLR: 0.00008337\n",
            "Train Epoch: 13 [2304/123872 (2%)]\tLoss: 0.314390\tLR: 0.00008332\n",
            "Train Epoch: 13 [2560/123872 (2%)]\tLoss: 0.432932\tLR: 0.00008328\n",
            "Train Epoch: 13 [2560/123872 (2%)]\tLoss: 0.432932\n",
            "Train Epoch: 13 [2816/123872 (2%)]\tLoss: 0.362752\tLR: 0.00008323\n",
            "Train Epoch: 13 [3072/123872 (2%)]\tLoss: 0.371880\tLR: 0.00008318\n",
            "Train Epoch: 13 [3328/123872 (3%)]\tLoss: 0.319087\tLR: 0.00008313\n",
            "Train Epoch: 13 [3584/123872 (3%)]\tLoss: 0.383316\tLR: 0.00008308\n",
            "Train Epoch: 13 [3840/123872 (3%)]\tLoss: 0.376781\tLR: 0.00008304\n",
            "Train Epoch: 13 [4096/123872 (3%)]\tLoss: 0.380613\tLR: 0.00008299\n",
            "Train Epoch: 13 [4352/123872 (4%)]\tLoss: 0.320451\tLR: 0.00008294\n",
            "Train Epoch: 13 [4608/123872 (4%)]\tLoss: 0.335414\tLR: 0.00008289\n",
            "Train Epoch: 13 [4864/123872 (4%)]\tLoss: 0.429343\tLR: 0.00008285\n",
            "Train Epoch: 13 [5120/123872 (4%)]\tLoss: 0.333664\tLR: 0.00008280\n",
            "Train Epoch: 13 [5120/123872 (4%)]\tLoss: 0.333664\n",
            "Train Epoch: 13 [5376/123872 (4%)]\tLoss: 0.414789\tLR: 0.00008275\n",
            "Train Epoch: 13 [5632/123872 (5%)]\tLoss: 0.347341\tLR: 0.00008270\n",
            "Train Epoch: 13 [5888/123872 (5%)]\tLoss: 0.365928\tLR: 0.00008266\n",
            "Train Epoch: 13 [6144/123872 (5%)]\tLoss: 0.407422\tLR: 0.00008261\n",
            "Train Epoch: 13 [6400/123872 (5%)]\tLoss: 0.332500\tLR: 0.00008256\n",
            "Train Epoch: 13 [6656/123872 (5%)]\tLoss: 0.366500\tLR: 0.00008251\n",
            "Train Epoch: 13 [6912/123872 (6%)]\tLoss: 0.386134\tLR: 0.00008247\n",
            "Train Epoch: 13 [7168/123872 (6%)]\tLoss: 0.364518\tLR: 0.00008242\n",
            "Train Epoch: 13 [7424/123872 (6%)]\tLoss: 0.339270\tLR: 0.00008237\n",
            "Train Epoch: 13 [7680/123872 (6%)]\tLoss: 0.320406\tLR: 0.00008233\n",
            "Train Epoch: 13 [7680/123872 (6%)]\tLoss: 0.320406\n",
            "Train Epoch: 13 [7936/123872 (6%)]\tLoss: 0.399067\tLR: 0.00008228\n",
            "Train Epoch: 13 [8192/123872 (7%)]\tLoss: 0.324369\tLR: 0.00008223\n",
            "Train Epoch: 13 [8448/123872 (7%)]\tLoss: 0.422448\tLR: 0.00008219\n",
            "Train Epoch: 13 [8704/123872 (7%)]\tLoss: 0.353705\tLR: 0.00008214\n",
            "Train Epoch: 13 [8960/123872 (7%)]\tLoss: 0.341026\tLR: 0.00008209\n",
            "Train Epoch: 13 [9216/123872 (7%)]\tLoss: 0.313914\tLR: 0.00008205\n",
            "Train Epoch: 13 [9472/123872 (8%)]\tLoss: 0.342974\tLR: 0.00008200\n",
            "Train Epoch: 13 [9728/123872 (8%)]\tLoss: 0.364318\tLR: 0.00008195\n",
            "Train Epoch: 13 [9984/123872 (8%)]\tLoss: 0.392625\tLR: 0.00008191\n",
            "Train Epoch: 13 [10240/123872 (8%)]\tLoss: 0.391677\tLR: 0.00008186\n",
            "Train Epoch: 13 [10240/123872 (8%)]\tLoss: 0.391677\n",
            "Train Epoch: 13 [10496/123872 (8%)]\tLoss: 0.340657\tLR: 0.00008181\n",
            "Train Epoch: 13 [10752/123872 (9%)]\tLoss: 0.338134\tLR: 0.00008177\n",
            "Train Epoch: 13 [11008/123872 (9%)]\tLoss: 0.360739\tLR: 0.00008172\n",
            "Train Epoch: 13 [11264/123872 (9%)]\tLoss: 0.354430\tLR: 0.00008167\n",
            "Train Epoch: 13 [11520/123872 (9%)]\tLoss: 0.378798\tLR: 0.00008163\n",
            "Train Epoch: 13 [11776/123872 (10%)]\tLoss: 0.386416\tLR: 0.00008158\n",
            "Train Epoch: 13 [12032/123872 (10%)]\tLoss: 0.375932\tLR: 0.00008153\n",
            "Train Epoch: 13 [12288/123872 (10%)]\tLoss: 0.374702\tLR: 0.00008149\n",
            "Train Epoch: 13 [12544/123872 (10%)]\tLoss: 0.336692\tLR: 0.00008144\n",
            "Train Epoch: 13 [12800/123872 (10%)]\tLoss: 0.300987\tLR: 0.00008140\n",
            "Train Epoch: 13 [12800/123872 (10%)]\tLoss: 0.300987\n",
            "Train Epoch: 13 [13056/123872 (11%)]\tLoss: 0.372387\tLR: 0.00008135\n",
            "Train Epoch: 13 [13312/123872 (11%)]\tLoss: 0.236693\tLR: 0.00008130\n",
            "Train Epoch: 13 [13568/123872 (11%)]\tLoss: 0.315852\tLR: 0.00008126\n",
            "Train Epoch: 13 [13824/123872 (11%)]\tLoss: 0.370531\tLR: 0.00008121\n",
            "Train Epoch: 13 [14080/123872 (11%)]\tLoss: 0.375278\tLR: 0.00008117\n",
            "Train Epoch: 13 [14336/123872 (12%)]\tLoss: 0.319312\tLR: 0.00008112\n",
            "Train Epoch: 13 [14592/123872 (12%)]\tLoss: 0.363765\tLR: 0.00008108\n",
            "Train Epoch: 13 [14848/123872 (12%)]\tLoss: 0.372816\tLR: 0.00008103\n",
            "Train Epoch: 13 [15104/123872 (12%)]\tLoss: 0.339921\tLR: 0.00008098\n",
            "Train Epoch: 13 [15360/123872 (12%)]\tLoss: 0.312121\tLR: 0.00008094\n",
            "Train Epoch: 13 [15360/123872 (12%)]\tLoss: 0.312121\n",
            "Train Epoch: 13 [15616/123872 (13%)]\tLoss: 0.335819\tLR: 0.00008089\n",
            "Train Epoch: 13 [15872/123872 (13%)]\tLoss: 0.322767\tLR: 0.00008085\n",
            "Train Epoch: 13 [16128/123872 (13%)]\tLoss: 0.338292\tLR: 0.00008080\n",
            "Train Epoch: 13 [16384/123872 (13%)]\tLoss: 0.328901\tLR: 0.00008076\n",
            "Train Epoch: 13 [16640/123872 (13%)]\tLoss: 0.372712\tLR: 0.00008071\n",
            "Train Epoch: 13 [16896/123872 (14%)]\tLoss: 0.330607\tLR: 0.00008067\n",
            "Train Epoch: 13 [17152/123872 (14%)]\tLoss: 0.351187\tLR: 0.00008062\n",
            "Train Epoch: 13 [17408/123872 (14%)]\tLoss: 0.372327\tLR: 0.00008058\n",
            "Train Epoch: 13 [17664/123872 (14%)]\tLoss: 0.366486\tLR: 0.00008053\n",
            "Train Epoch: 13 [17920/123872 (14%)]\tLoss: 0.347522\tLR: 0.00008048\n",
            "Train Epoch: 13 [17920/123872 (14%)]\tLoss: 0.347522\n",
            "Train Epoch: 13 [18176/123872 (15%)]\tLoss: 0.393191\tLR: 0.00008044\n",
            "Train Epoch: 13 [18432/123872 (15%)]\tLoss: 0.347268\tLR: 0.00008039\n",
            "Train Epoch: 13 [18688/123872 (15%)]\tLoss: 0.362735\tLR: 0.00008035\n",
            "Train Epoch: 13 [18944/123872 (15%)]\tLoss: 0.340354\tLR: 0.00008030\n",
            "Train Epoch: 13 [19200/123872 (15%)]\tLoss: 0.396405\tLR: 0.00008026\n",
            "Train Epoch: 13 [19456/123872 (16%)]\tLoss: 0.427799\tLR: 0.00008022\n",
            "Train Epoch: 13 [19712/123872 (16%)]\tLoss: 0.333651\tLR: 0.00008017\n",
            "Train Epoch: 13 [19968/123872 (16%)]\tLoss: 0.379316\tLR: 0.00008013\n",
            "Train Epoch: 13 [20224/123872 (16%)]\tLoss: 0.356473\tLR: 0.00008008\n",
            "Train Epoch: 13 [20480/123872 (17%)]\tLoss: 0.338858\tLR: 0.00008004\n",
            "Train Epoch: 13 [20480/123872 (17%)]\tLoss: 0.338858\n",
            "Train Epoch: 13 [20736/123872 (17%)]\tLoss: 0.298171\tLR: 0.00007999\n",
            "Train Epoch: 13 [20992/123872 (17%)]\tLoss: 0.317727\tLR: 0.00007995\n",
            "Train Epoch: 13 [21248/123872 (17%)]\tLoss: 0.379012\tLR: 0.00007990\n",
            "Train Epoch: 13 [21504/123872 (17%)]\tLoss: 0.305499\tLR: 0.00007986\n",
            "Train Epoch: 13 [21760/123872 (18%)]\tLoss: 0.365402\tLR: 0.00007981\n",
            "Train Epoch: 13 [22016/123872 (18%)]\tLoss: 0.365244\tLR: 0.00007977\n",
            "Train Epoch: 13 [22272/123872 (18%)]\tLoss: 0.329640\tLR: 0.00007973\n",
            "Train Epoch: 13 [22528/123872 (18%)]\tLoss: 0.382153\tLR: 0.00007968\n",
            "Train Epoch: 13 [22784/123872 (18%)]\tLoss: 0.361281\tLR: 0.00007964\n",
            "Train Epoch: 13 [23040/123872 (19%)]\tLoss: 0.393453\tLR: 0.00007959\n",
            "Train Epoch: 13 [23040/123872 (19%)]\tLoss: 0.393453\n",
            "Train Epoch: 13 [23296/123872 (19%)]\tLoss: 0.282648\tLR: 0.00007955\n",
            "Train Epoch: 13 [23552/123872 (19%)]\tLoss: 0.302454\tLR: 0.00007950\n",
            "Train Epoch: 13 [23808/123872 (19%)]\tLoss: 0.274701\tLR: 0.00007946\n",
            "Train Epoch: 13 [24064/123872 (19%)]\tLoss: 0.401466\tLR: 0.00007942\n",
            "Train Epoch: 13 [24320/123872 (20%)]\tLoss: 0.346778\tLR: 0.00007937\n",
            "Train Epoch: 13 [24576/123872 (20%)]\tLoss: 0.380183\tLR: 0.00007933\n",
            "Train Epoch: 13 [24832/123872 (20%)]\tLoss: 0.301578\tLR: 0.00007928\n",
            "Train Epoch: 13 [25088/123872 (20%)]\tLoss: 0.321253\tLR: 0.00007924\n",
            "Train Epoch: 13 [25344/123872 (20%)]\tLoss: 0.355568\tLR: 0.00007920\n",
            "Train Epoch: 13 [25600/123872 (21%)]\tLoss: 0.372489\tLR: 0.00007915\n",
            "Train Epoch: 13 [25600/123872 (21%)]\tLoss: 0.372489\n",
            "Train Epoch: 13 [25856/123872 (21%)]\tLoss: 0.387508\tLR: 0.00007911\n",
            "Train Epoch: 13 [26112/123872 (21%)]\tLoss: 0.395913\tLR: 0.00007907\n",
            "Train Epoch: 13 [26368/123872 (21%)]\tLoss: 0.393335\tLR: 0.00007902\n",
            "Train Epoch: 13 [26624/123872 (21%)]\tLoss: 0.385777\tLR: 0.00007898\n",
            "Train Epoch: 13 [26880/123872 (22%)]\tLoss: 0.334220\tLR: 0.00007894\n",
            "Train Epoch: 13 [27136/123872 (22%)]\tLoss: 0.356384\tLR: 0.00007889\n",
            "Train Epoch: 13 [27392/123872 (22%)]\tLoss: 0.346610\tLR: 0.00007885\n",
            "Train Epoch: 13 [27648/123872 (22%)]\tLoss: 0.372942\tLR: 0.00007881\n",
            "Train Epoch: 13 [27904/123872 (23%)]\tLoss: 0.363268\tLR: 0.00007876\n",
            "Train Epoch: 13 [28160/123872 (23%)]\tLoss: 0.404976\tLR: 0.00007872\n",
            "Train Epoch: 13 [28160/123872 (23%)]\tLoss: 0.404976\n",
            "Train Epoch: 13 [28416/123872 (23%)]\tLoss: 0.316746\tLR: 0.00007868\n",
            "Train Epoch: 13 [28672/123872 (23%)]\tLoss: 0.365351\tLR: 0.00007863\n",
            "Train Epoch: 13 [28928/123872 (23%)]\tLoss: 0.318724\tLR: 0.00007859\n",
            "Train Epoch: 13 [29184/123872 (24%)]\tLoss: 0.334409\tLR: 0.00007855\n",
            "Train Epoch: 13 [29440/123872 (24%)]\tLoss: 0.325188\tLR: 0.00007850\n",
            "Train Epoch: 13 [29696/123872 (24%)]\tLoss: 0.388071\tLR: 0.00007846\n",
            "Train Epoch: 13 [29952/123872 (24%)]\tLoss: 0.372741\tLR: 0.00007842\n",
            "Train Epoch: 13 [30208/123872 (24%)]\tLoss: 0.428519\tLR: 0.00007838\n",
            "Train Epoch: 13 [30464/123872 (25%)]\tLoss: 0.332403\tLR: 0.00007833\n",
            "Train Epoch: 13 [30720/123872 (25%)]\tLoss: 0.345002\tLR: 0.00007829\n",
            "Train Epoch: 13 [30720/123872 (25%)]\tLoss: 0.345002\n",
            "Train Epoch: 13 [30976/123872 (25%)]\tLoss: 0.336936\tLR: 0.00007825\n",
            "Train Epoch: 13 [31232/123872 (25%)]\tLoss: 0.349078\tLR: 0.00007820\n",
            "Train Epoch: 13 [31488/123872 (25%)]\tLoss: 0.354752\tLR: 0.00007816\n",
            "Train Epoch: 13 [31744/123872 (26%)]\tLoss: 0.325121\tLR: 0.00007812\n",
            "Train Epoch: 13 [32000/123872 (26%)]\tLoss: 0.382671\tLR: 0.00007808\n",
            "Train Epoch: 13 [32256/123872 (26%)]\tLoss: 0.397818\tLR: 0.00007803\n",
            "Train Epoch: 13 [32512/123872 (26%)]\tLoss: 0.350729\tLR: 0.00007799\n",
            "Train Epoch: 13 [32768/123872 (26%)]\tLoss: 0.314832\tLR: 0.00007795\n",
            "Train Epoch: 13 [33024/123872 (27%)]\tLoss: 0.316005\tLR: 0.00007791\n",
            "Train Epoch: 13 [33280/123872 (27%)]\tLoss: 0.352830\tLR: 0.00007787\n",
            "Train Epoch: 13 [33280/123872 (27%)]\tLoss: 0.352830\n",
            "Train Epoch: 13 [33536/123872 (27%)]\tLoss: 0.399355\tLR: 0.00007782\n",
            "Train Epoch: 13 [33792/123872 (27%)]\tLoss: 0.394013\tLR: 0.00007778\n",
            "Train Epoch: 13 [34048/123872 (27%)]\tLoss: 0.371357\tLR: 0.00007774\n",
            "Train Epoch: 13 [34304/123872 (28%)]\tLoss: 0.349061\tLR: 0.00007770\n",
            "Train Epoch: 13 [34560/123872 (28%)]\tLoss: 0.363501\tLR: 0.00007766\n",
            "Train Epoch: 13 [34816/123872 (28%)]\tLoss: 0.363376\tLR: 0.00007761\n",
            "Train Epoch: 13 [35072/123872 (28%)]\tLoss: 0.352047\tLR: 0.00007757\n",
            "Train Epoch: 13 [35328/123872 (29%)]\tLoss: 0.391450\tLR: 0.00007753\n",
            "Train Epoch: 13 [35584/123872 (29%)]\tLoss: 0.412833\tLR: 0.00007749\n",
            "Train Epoch: 13 [35840/123872 (29%)]\tLoss: 0.357276\tLR: 0.00007745\n",
            "Train Epoch: 13 [35840/123872 (29%)]\tLoss: 0.357276\n",
            "Train Epoch: 13 [36096/123872 (29%)]\tLoss: 0.359283\tLR: 0.00007740\n",
            "Train Epoch: 13 [36352/123872 (29%)]\tLoss: 0.339258\tLR: 0.00007736\n",
            "Train Epoch: 13 [36608/123872 (30%)]\tLoss: 0.299712\tLR: 0.00007732\n",
            "Train Epoch: 13 [36864/123872 (30%)]\tLoss: 0.351248\tLR: 0.00007728\n",
            "Train Epoch: 13 [37120/123872 (30%)]\tLoss: 0.309750\tLR: 0.00007724\n",
            "Train Epoch: 13 [37376/123872 (30%)]\tLoss: 0.382970\tLR: 0.00007720\n",
            "Train Epoch: 13 [37632/123872 (30%)]\tLoss: 0.320790\tLR: 0.00007716\n",
            "Train Epoch: 13 [37888/123872 (31%)]\tLoss: 0.377816\tLR: 0.00007711\n",
            "Train Epoch: 13 [38144/123872 (31%)]\tLoss: 0.376219\tLR: 0.00007707\n",
            "Train Epoch: 13 [38400/123872 (31%)]\tLoss: 0.291035\tLR: 0.00007703\n",
            "Train Epoch: 13 [38400/123872 (31%)]\tLoss: 0.291035\n",
            "Train Epoch: 13 [38656/123872 (31%)]\tLoss: 0.368769\tLR: 0.00007699\n",
            "Train Epoch: 13 [38912/123872 (31%)]\tLoss: 0.324717\tLR: 0.00007695\n",
            "Train Epoch: 13 [39168/123872 (32%)]\tLoss: 0.362056\tLR: 0.00007691\n",
            "Train Epoch: 13 [39424/123872 (32%)]\tLoss: 0.377421\tLR: 0.00007687\n",
            "Train Epoch: 13 [39680/123872 (32%)]\tLoss: 0.309581\tLR: 0.00007683\n",
            "Train Epoch: 13 [39936/123872 (32%)]\tLoss: 0.328664\tLR: 0.00007678\n",
            "Train Epoch: 13 [40192/123872 (32%)]\tLoss: 0.309969\tLR: 0.00007674\n",
            "Train Epoch: 13 [40448/123872 (33%)]\tLoss: 0.364769\tLR: 0.00007670\n",
            "Train Epoch: 13 [40704/123872 (33%)]\tLoss: 0.338963\tLR: 0.00007666\n",
            "Train Epoch: 13 [40960/123872 (33%)]\tLoss: 0.380391\tLR: 0.00007662\n",
            "Train Epoch: 13 [40960/123872 (33%)]\tLoss: 0.380391\n",
            "Train Epoch: 13 [41216/123872 (33%)]\tLoss: 0.353509\tLR: 0.00007658\n",
            "Train Epoch: 13 [41472/123872 (33%)]\tLoss: 0.333111\tLR: 0.00007654\n",
            "Train Epoch: 13 [41728/123872 (34%)]\tLoss: 0.316083\tLR: 0.00007650\n",
            "Train Epoch: 13 [41984/123872 (34%)]\tLoss: 0.299165\tLR: 0.00007646\n",
            "Train Epoch: 13 [42240/123872 (34%)]\tLoss: 0.338581\tLR: 0.00007642\n",
            "Train Epoch: 13 [42496/123872 (34%)]\tLoss: 0.338347\tLR: 0.00007638\n",
            "Train Epoch: 13 [42752/123872 (35%)]\tLoss: 0.373699\tLR: 0.00007634\n",
            "Train Epoch: 13 [43008/123872 (35%)]\tLoss: 0.386515\tLR: 0.00007630\n",
            "Train Epoch: 13 [43264/123872 (35%)]\tLoss: 0.323084\tLR: 0.00007626\n",
            "Train Epoch: 13 [43520/123872 (35%)]\tLoss: 0.357539\tLR: 0.00007622\n",
            "Train Epoch: 13 [43520/123872 (35%)]\tLoss: 0.357539\n",
            "Train Epoch: 13 [43776/123872 (35%)]\tLoss: 0.377197\tLR: 0.00007618\n",
            "Train Epoch: 13 [44032/123872 (36%)]\tLoss: 0.279058\tLR: 0.00007614\n",
            "Train Epoch: 13 [44288/123872 (36%)]\tLoss: 0.393043\tLR: 0.00007610\n",
            "Train Epoch: 13 [44544/123872 (36%)]\tLoss: 0.388891\tLR: 0.00007606\n",
            "Train Epoch: 13 [44800/123872 (36%)]\tLoss: 0.364992\tLR: 0.00007601\n",
            "Train Epoch: 13 [45056/123872 (36%)]\tLoss: 0.388377\tLR: 0.00007597\n",
            "Train Epoch: 13 [45312/123872 (37%)]\tLoss: 0.379363\tLR: 0.00007594\n",
            "Train Epoch: 13 [45568/123872 (37%)]\tLoss: 0.316378\tLR: 0.00007590\n",
            "Train Epoch: 13 [45824/123872 (37%)]\tLoss: 0.274941\tLR: 0.00007586\n",
            "Train Epoch: 13 [46080/123872 (37%)]\tLoss: 0.352044\tLR: 0.00007582\n",
            "Train Epoch: 13 [46080/123872 (37%)]\tLoss: 0.352044\n",
            "Train Epoch: 13 [46336/123872 (37%)]\tLoss: 0.390247\tLR: 0.00007578\n",
            "Train Epoch: 13 [46592/123872 (38%)]\tLoss: 0.421701\tLR: 0.00007574\n",
            "Train Epoch: 13 [46848/123872 (38%)]\tLoss: 0.298316\tLR: 0.00007570\n",
            "Train Epoch: 13 [47104/123872 (38%)]\tLoss: 0.343116\tLR: 0.00007566\n",
            "Train Epoch: 13 [47360/123872 (38%)]\tLoss: 0.307534\tLR: 0.00007562\n",
            "Train Epoch: 13 [47616/123872 (38%)]\tLoss: 0.393912\tLR: 0.00007558\n",
            "Train Epoch: 13 [47872/123872 (39%)]\tLoss: 0.385754\tLR: 0.00007554\n",
            "Train Epoch: 13 [48128/123872 (39%)]\tLoss: 0.306549\tLR: 0.00007550\n",
            "Train Epoch: 13 [48384/123872 (39%)]\tLoss: 0.384626\tLR: 0.00007546\n",
            "Train Epoch: 13 [48640/123872 (39%)]\tLoss: 0.313868\tLR: 0.00007542\n",
            "Train Epoch: 13 [48640/123872 (39%)]\tLoss: 0.313868\n",
            "Train Epoch: 13 [48896/123872 (39%)]\tLoss: 0.356658\tLR: 0.00007538\n",
            "Train Epoch: 13 [49152/123872 (40%)]\tLoss: 0.387048\tLR: 0.00007534\n",
            "Train Epoch: 13 [49408/123872 (40%)]\tLoss: 0.390800\tLR: 0.00007530\n",
            "Train Epoch: 13 [49664/123872 (40%)]\tLoss: 0.327368\tLR: 0.00007526\n",
            "Train Epoch: 13 [49920/123872 (40%)]\tLoss: 0.356629\tLR: 0.00007522\n",
            "Train Epoch: 13 [50176/123872 (40%)]\tLoss: 0.378147\tLR: 0.00007518\n",
            "Train Epoch: 13 [50432/123872 (41%)]\tLoss: 0.375154\tLR: 0.00007515\n",
            "Train Epoch: 13 [50688/123872 (41%)]\tLoss: 0.297161\tLR: 0.00007511\n",
            "Train Epoch: 13 [50944/123872 (41%)]\tLoss: 0.399938\tLR: 0.00007507\n",
            "Train Epoch: 13 [51200/123872 (41%)]\tLoss: 0.314781\tLR: 0.00007503\n",
            "Train Epoch: 13 [51200/123872 (41%)]\tLoss: 0.314781\n",
            "Train Epoch: 13 [51456/123872 (42%)]\tLoss: 0.377071\tLR: 0.00007499\n",
            "Train Epoch: 13 [51712/123872 (42%)]\tLoss: 0.321911\tLR: 0.00007495\n",
            "Train Epoch: 13 [51968/123872 (42%)]\tLoss: 0.320592\tLR: 0.00007491\n",
            "Train Epoch: 13 [52224/123872 (42%)]\tLoss: 0.328739\tLR: 0.00007487\n",
            "Train Epoch: 13 [52480/123872 (42%)]\tLoss: 0.388217\tLR: 0.00007484\n",
            "Train Epoch: 13 [52736/123872 (43%)]\tLoss: 0.334942\tLR: 0.00007480\n",
            "Train Epoch: 13 [52992/123872 (43%)]\tLoss: 0.393856\tLR: 0.00007476\n",
            "Train Epoch: 13 [53248/123872 (43%)]\tLoss: 0.370388\tLR: 0.00007472\n",
            "Train Epoch: 13 [53504/123872 (43%)]\tLoss: 0.402589\tLR: 0.00007468\n",
            "Train Epoch: 13 [53760/123872 (43%)]\tLoss: 0.401882\tLR: 0.00007464\n",
            "Train Epoch: 13 [53760/123872 (43%)]\tLoss: 0.401882\n",
            "Train Epoch: 13 [54016/123872 (44%)]\tLoss: 0.354764\tLR: 0.00007461\n",
            "Train Epoch: 13 [54272/123872 (44%)]\tLoss: 0.435891\tLR: 0.00007457\n",
            "Train Epoch: 13 [54528/123872 (44%)]\tLoss: 0.436948\tLR: 0.00007453\n",
            "Train Epoch: 13 [54784/123872 (44%)]\tLoss: 0.423546\tLR: 0.00007449\n",
            "Train Epoch: 13 [55040/123872 (44%)]\tLoss: 0.369111\tLR: 0.00007445\n",
            "Train Epoch: 13 [55296/123872 (45%)]\tLoss: 0.357052\tLR: 0.00007441\n",
            "Train Epoch: 13 [55552/123872 (45%)]\tLoss: 0.349245\tLR: 0.00007438\n",
            "Train Epoch: 13 [55808/123872 (45%)]\tLoss: 0.356733\tLR: 0.00007434\n",
            "Train Epoch: 13 [56064/123872 (45%)]\tLoss: 0.339380\tLR: 0.00007430\n",
            "Train Epoch: 13 [56320/123872 (45%)]\tLoss: 0.386200\tLR: 0.00007426\n",
            "Train Epoch: 13 [56320/123872 (45%)]\tLoss: 0.386200\n",
            "Train Epoch: 13 [56576/123872 (46%)]\tLoss: 0.309242\tLR: 0.00007422\n",
            "Train Epoch: 13 [56832/123872 (46%)]\tLoss: 0.370383\tLR: 0.00007419\n",
            "Train Epoch: 13 [57088/123872 (46%)]\tLoss: 0.361374\tLR: 0.00007415\n",
            "Train Epoch: 13 [57344/123872 (46%)]\tLoss: 0.410899\tLR: 0.00007411\n",
            "Train Epoch: 13 [57600/123872 (46%)]\tLoss: 0.300904\tLR: 0.00007407\n",
            "Train Epoch: 13 [57856/123872 (47%)]\tLoss: 0.338407\tLR: 0.00007404\n",
            "Train Epoch: 13 [58112/123872 (47%)]\tLoss: 0.350864\tLR: 0.00007400\n",
            "Train Epoch: 13 [58368/123872 (47%)]\tLoss: 0.288573\tLR: 0.00007396\n",
            "Train Epoch: 13 [58624/123872 (47%)]\tLoss: 0.331277\tLR: 0.00007392\n",
            "Train Epoch: 13 [58880/123872 (48%)]\tLoss: 0.331056\tLR: 0.00007389\n",
            "Train Epoch: 13 [58880/123872 (48%)]\tLoss: 0.331056\n",
            "Train Epoch: 13 [59136/123872 (48%)]\tLoss: 0.341329\tLR: 0.00007385\n",
            "Train Epoch: 13 [59392/123872 (48%)]\tLoss: 0.401496\tLR: 0.00007381\n",
            "Train Epoch: 13 [59648/123872 (48%)]\tLoss: 0.317329\tLR: 0.00007377\n",
            "Train Epoch: 13 [59904/123872 (48%)]\tLoss: 0.458071\tLR: 0.00007374\n",
            "Train Epoch: 13 [60160/123872 (49%)]\tLoss: 0.290253\tLR: 0.00007370\n",
            "Train Epoch: 13 [60416/123872 (49%)]\tLoss: 0.276966\tLR: 0.00007366\n",
            "Train Epoch: 13 [60672/123872 (49%)]\tLoss: 0.321536\tLR: 0.00007363\n",
            "Train Epoch: 13 [60928/123872 (49%)]\tLoss: 0.373000\tLR: 0.00007359\n",
            "Train Epoch: 13 [61184/123872 (49%)]\tLoss: 0.354323\tLR: 0.00007355\n",
            "Train Epoch: 13 [61440/123872 (50%)]\tLoss: 0.373804\tLR: 0.00007352\n",
            "Train Epoch: 13 [61440/123872 (50%)]\tLoss: 0.373804\n",
            "Train Epoch: 13 [61696/123872 (50%)]\tLoss: 0.324524\tLR: 0.00007348\n",
            "Train Epoch: 13 [61952/123872 (50%)]\tLoss: 0.374887\tLR: 0.00007344\n",
            "Train Epoch: 13 [62208/123872 (50%)]\tLoss: 0.347244\tLR: 0.00007341\n",
            "Train Epoch: 13 [62464/123872 (50%)]\tLoss: 0.363982\tLR: 0.00007337\n",
            "Train Epoch: 13 [62720/123872 (51%)]\tLoss: 0.305067\tLR: 0.00007333\n",
            "Train Epoch: 13 [62976/123872 (51%)]\tLoss: 0.349552\tLR: 0.00007330\n",
            "Train Epoch: 13 [63232/123872 (51%)]\tLoss: 0.331148\tLR: 0.00007326\n",
            "Train Epoch: 13 [63488/123872 (51%)]\tLoss: 0.315173\tLR: 0.00007322\n",
            "Train Epoch: 13 [63744/123872 (51%)]\tLoss: 0.405722\tLR: 0.00007319\n",
            "Train Epoch: 13 [64000/123872 (52%)]\tLoss: 0.397701\tLR: 0.00007315\n",
            "Train Epoch: 13 [64000/123872 (52%)]\tLoss: 0.397701\n",
            "Train Epoch: 13 [64256/123872 (52%)]\tLoss: 0.341638\tLR: 0.00007311\n",
            "Train Epoch: 13 [64512/123872 (52%)]\tLoss: 0.315296\tLR: 0.00007308\n",
            "Train Epoch: 13 [64768/123872 (52%)]\tLoss: 0.349109\tLR: 0.00007304\n",
            "Train Epoch: 13 [65024/123872 (52%)]\tLoss: 0.354108\tLR: 0.00007300\n",
            "Train Epoch: 13 [65280/123872 (53%)]\tLoss: 0.376996\tLR: 0.00007297\n",
            "Train Epoch: 13 [65536/123872 (53%)]\tLoss: 0.392311\tLR: 0.00007293\n",
            "Train Epoch: 13 [65792/123872 (53%)]\tLoss: 0.316416\tLR: 0.00007290\n",
            "Train Epoch: 13 [66048/123872 (53%)]\tLoss: 0.336852\tLR: 0.00007286\n",
            "Train Epoch: 13 [66304/123872 (54%)]\tLoss: 0.374541\tLR: 0.00007282\n",
            "Train Epoch: 13 [66560/123872 (54%)]\tLoss: 0.413976\tLR: 0.00007279\n",
            "Train Epoch: 13 [66560/123872 (54%)]\tLoss: 0.413976\n",
            "Train Epoch: 13 [66816/123872 (54%)]\tLoss: 0.316574\tLR: 0.00007275\n",
            "Train Epoch: 13 [67072/123872 (54%)]\tLoss: 0.363368\tLR: 0.00007272\n",
            "Train Epoch: 13 [67328/123872 (54%)]\tLoss: 0.302479\tLR: 0.00007268\n",
            "Train Epoch: 13 [67584/123872 (55%)]\tLoss: 0.315897\tLR: 0.00007264\n",
            "Train Epoch: 13 [67840/123872 (55%)]\tLoss: 0.477145\tLR: 0.00007261\n",
            "Train Epoch: 13 [68096/123872 (55%)]\tLoss: 0.341537\tLR: 0.00007257\n",
            "Train Epoch: 13 [68352/123872 (55%)]\tLoss: 0.380274\tLR: 0.00007254\n",
            "Train Epoch: 13 [68608/123872 (55%)]\tLoss: 0.347325\tLR: 0.00007250\n",
            "Train Epoch: 13 [68864/123872 (56%)]\tLoss: 0.311023\tLR: 0.00007247\n",
            "Train Epoch: 13 [69120/123872 (56%)]\tLoss: 0.365034\tLR: 0.00007243\n",
            "Train Epoch: 13 [69120/123872 (56%)]\tLoss: 0.365034\n",
            "Train Epoch: 13 [69376/123872 (56%)]\tLoss: 0.380080\tLR: 0.00007240\n",
            "Train Epoch: 13 [69632/123872 (56%)]\tLoss: 0.291001\tLR: 0.00007236\n",
            "Train Epoch: 13 [69888/123872 (56%)]\tLoss: 0.296341\tLR: 0.00007233\n",
            "Train Epoch: 13 [70144/123872 (57%)]\tLoss: 0.371796\tLR: 0.00007229\n",
            "Train Epoch: 13 [70400/123872 (57%)]\tLoss: 0.350294\tLR: 0.00007226\n",
            "Train Epoch: 13 [70656/123872 (57%)]\tLoss: 0.322654\tLR: 0.00007222\n",
            "Train Epoch: 13 [70912/123872 (57%)]\tLoss: 0.370405\tLR: 0.00007218\n",
            "Train Epoch: 13 [71168/123872 (57%)]\tLoss: 0.357252\tLR: 0.00007215\n",
            "Train Epoch: 13 [71424/123872 (58%)]\tLoss: 0.363222\tLR: 0.00007211\n",
            "Train Epoch: 13 [71680/123872 (58%)]\tLoss: 0.357021\tLR: 0.00007208\n",
            "Train Epoch: 13 [71680/123872 (58%)]\tLoss: 0.357021\n",
            "Train Epoch: 13 [71936/123872 (58%)]\tLoss: 0.357436\tLR: 0.00007205\n",
            "Train Epoch: 13 [72192/123872 (58%)]\tLoss: 0.315174\tLR: 0.00007201\n",
            "Train Epoch: 13 [72448/123872 (58%)]\tLoss: 0.354416\tLR: 0.00007198\n",
            "Train Epoch: 13 [72704/123872 (59%)]\tLoss: 0.316321\tLR: 0.00007194\n",
            "Train Epoch: 13 [72960/123872 (59%)]\tLoss: 0.329639\tLR: 0.00007191\n",
            "Train Epoch: 13 [73216/123872 (59%)]\tLoss: 0.325193\tLR: 0.00007187\n",
            "Train Epoch: 13 [73472/123872 (59%)]\tLoss: 0.300976\tLR: 0.00007184\n",
            "Train Epoch: 13 [73728/123872 (60%)]\tLoss: 0.343913\tLR: 0.00007180\n",
            "Train Epoch: 13 [73984/123872 (60%)]\tLoss: 0.441675\tLR: 0.00007177\n",
            "Train Epoch: 13 [74240/123872 (60%)]\tLoss: 0.335724\tLR: 0.00007173\n",
            "Train Epoch: 13 [74240/123872 (60%)]\tLoss: 0.335724\n",
            "Train Epoch: 13 [74496/123872 (60%)]\tLoss: 0.349975\tLR: 0.00007170\n",
            "Train Epoch: 13 [74752/123872 (60%)]\tLoss: 0.393930\tLR: 0.00007166\n",
            "Train Epoch: 13 [75008/123872 (61%)]\tLoss: 0.326188\tLR: 0.00007163\n",
            "Train Epoch: 13 [75264/123872 (61%)]\tLoss: 0.345952\tLR: 0.00007160\n",
            "Train Epoch: 13 [75520/123872 (61%)]\tLoss: 0.323650\tLR: 0.00007156\n",
            "Train Epoch: 13 [75776/123872 (61%)]\tLoss: 0.303443\tLR: 0.00007153\n",
            "Train Epoch: 13 [76032/123872 (61%)]\tLoss: 0.347428\tLR: 0.00007149\n",
            "Train Epoch: 13 [76288/123872 (62%)]\tLoss: 0.347033\tLR: 0.00007146\n",
            "Train Epoch: 13 [76544/123872 (62%)]\tLoss: 0.307854\tLR: 0.00007143\n",
            "Train Epoch: 13 [76800/123872 (62%)]\tLoss: 0.377131\tLR: 0.00007139\n",
            "Train Epoch: 13 [76800/123872 (62%)]\tLoss: 0.377131\n",
            "Train Epoch: 13 [77056/123872 (62%)]\tLoss: 0.346554\tLR: 0.00007136\n",
            "Train Epoch: 13 [77312/123872 (62%)]\tLoss: 0.401276\tLR: 0.00007132\n",
            "Train Epoch: 13 [77568/123872 (63%)]\tLoss: 0.277641\tLR: 0.00007129\n",
            "Train Epoch: 13 [77824/123872 (63%)]\tLoss: 0.312946\tLR: 0.00007126\n",
            "Train Epoch: 13 [78080/123872 (63%)]\tLoss: 0.379565\tLR: 0.00007122\n",
            "Train Epoch: 13 [78336/123872 (63%)]\tLoss: 0.395986\tLR: 0.00007119\n",
            "Train Epoch: 13 [78592/123872 (63%)]\tLoss: 0.419821\tLR: 0.00007116\n",
            "Train Epoch: 13 [78848/123872 (64%)]\tLoss: 0.395038\tLR: 0.00007112\n",
            "Train Epoch: 13 [79104/123872 (64%)]\tLoss: 0.399272\tLR: 0.00007109\n",
            "Train Epoch: 13 [79360/123872 (64%)]\tLoss: 0.320831\tLR: 0.00007106\n",
            "Train Epoch: 13 [79360/123872 (64%)]\tLoss: 0.320831\n",
            "Train Epoch: 13 [79616/123872 (64%)]\tLoss: 0.395008\tLR: 0.00007102\n",
            "Train Epoch: 13 [79872/123872 (64%)]\tLoss: 0.379965\tLR: 0.00007099\n",
            "Train Epoch: 13 [80128/123872 (65%)]\tLoss: 0.336003\tLR: 0.00007096\n",
            "Train Epoch: 13 [80384/123872 (65%)]\tLoss: 0.285084\tLR: 0.00007092\n",
            "Train Epoch: 13 [80640/123872 (65%)]\tLoss: 0.362260\tLR: 0.00007089\n",
            "Train Epoch: 13 [80896/123872 (65%)]\tLoss: 0.431497\tLR: 0.00007086\n",
            "Train Epoch: 13 [81152/123872 (65%)]\tLoss: 0.377263\tLR: 0.00007082\n",
            "Train Epoch: 13 [81408/123872 (66%)]\tLoss: 0.367130\tLR: 0.00007079\n",
            "Train Epoch: 13 [81664/123872 (66%)]\tLoss: 0.386920\tLR: 0.00007076\n",
            "Train Epoch: 13 [81920/123872 (66%)]\tLoss: 0.367379\tLR: 0.00007072\n",
            "Train Epoch: 13 [81920/123872 (66%)]\tLoss: 0.367379\n",
            "Train Epoch: 13 [82176/123872 (66%)]\tLoss: 0.296509\tLR: 0.00007069\n",
            "Train Epoch: 13 [82432/123872 (67%)]\tLoss: 0.360160\tLR: 0.00007066\n",
            "Train Epoch: 13 [82688/123872 (67%)]\tLoss: 0.370274\tLR: 0.00007062\n",
            "Train Epoch: 13 [82944/123872 (67%)]\tLoss: 0.396803\tLR: 0.00007059\n",
            "Train Epoch: 13 [83200/123872 (67%)]\tLoss: 0.296351\tLR: 0.00007056\n",
            "Train Epoch: 13 [83456/123872 (67%)]\tLoss: 0.370212\tLR: 0.00007053\n",
            "Train Epoch: 13 [83712/123872 (68%)]\tLoss: 0.311038\tLR: 0.00007049\n",
            "Train Epoch: 13 [83968/123872 (68%)]\tLoss: 0.348288\tLR: 0.00007046\n",
            "Train Epoch: 13 [84224/123872 (68%)]\tLoss: 0.309617\tLR: 0.00007043\n",
            "Train Epoch: 13 [84480/123872 (68%)]\tLoss: 0.422156\tLR: 0.00007040\n",
            "Train Epoch: 13 [84480/123872 (68%)]\tLoss: 0.422156\n",
            "Train Epoch: 13 [84736/123872 (68%)]\tLoss: 0.458156\tLR: 0.00007036\n",
            "Train Epoch: 13 [84992/123872 (69%)]\tLoss: 0.343895\tLR: 0.00007033\n",
            "Train Epoch: 13 [85248/123872 (69%)]\tLoss: 0.342245\tLR: 0.00007030\n",
            "Train Epoch: 13 [85504/123872 (69%)]\tLoss: 0.364120\tLR: 0.00007027\n",
            "Train Epoch: 13 [85760/123872 (69%)]\tLoss: 0.326323\tLR: 0.00007024\n",
            "Train Epoch: 13 [86016/123872 (69%)]\tLoss: 0.310334\tLR: 0.00007020\n",
            "Train Epoch: 13 [86272/123872 (70%)]\tLoss: 0.345049\tLR: 0.00007017\n",
            "Train Epoch: 13 [86528/123872 (70%)]\tLoss: 0.301816\tLR: 0.00007014\n",
            "Train Epoch: 13 [86784/123872 (70%)]\tLoss: 0.343699\tLR: 0.00007011\n",
            "Train Epoch: 13 [87040/123872 (70%)]\tLoss: 0.372954\tLR: 0.00007007\n",
            "Train Epoch: 13 [87040/123872 (70%)]\tLoss: 0.372954\n",
            "Train Epoch: 13 [87296/123872 (70%)]\tLoss: 0.345016\tLR: 0.00007004\n",
            "Train Epoch: 13 [87552/123872 (71%)]\tLoss: 0.394163\tLR: 0.00007001\n",
            "Train Epoch: 13 [87808/123872 (71%)]\tLoss: 0.338434\tLR: 0.00006998\n",
            "Train Epoch: 13 [88064/123872 (71%)]\tLoss: 0.287543\tLR: 0.00006995\n",
            "Train Epoch: 13 [88320/123872 (71%)]\tLoss: 0.362279\tLR: 0.00006992\n",
            "Train Epoch: 13 [88576/123872 (71%)]\tLoss: 0.349527\tLR: 0.00006988\n",
            "Train Epoch: 13 [88832/123872 (72%)]\tLoss: 0.378298\tLR: 0.00006985\n",
            "Train Epoch: 13 [89088/123872 (72%)]\tLoss: 0.388915\tLR: 0.00006982\n",
            "Train Epoch: 13 [89344/123872 (72%)]\tLoss: 0.343043\tLR: 0.00006979\n",
            "Train Epoch: 13 [89600/123872 (72%)]\tLoss: 0.346593\tLR: 0.00006976\n",
            "Train Epoch: 13 [89600/123872 (72%)]\tLoss: 0.346593\n",
            "Train Epoch: 13 [89856/123872 (73%)]\tLoss: 0.397716\tLR: 0.00006973\n",
            "Train Epoch: 13 [90112/123872 (73%)]\tLoss: 0.355548\tLR: 0.00006970\n",
            "Train Epoch: 13 [90368/123872 (73%)]\tLoss: 0.343211\tLR: 0.00006966\n",
            "Train Epoch: 13 [90624/123872 (73%)]\tLoss: 0.337848\tLR: 0.00006963\n",
            "Train Epoch: 13 [90880/123872 (73%)]\tLoss: 0.361709\tLR: 0.00006960\n",
            "Train Epoch: 13 [91136/123872 (74%)]\tLoss: 0.392349\tLR: 0.00006957\n",
            "Train Epoch: 13 [91392/123872 (74%)]\tLoss: 0.327751\tLR: 0.00006954\n",
            "Train Epoch: 13 [91648/123872 (74%)]\tLoss: 0.330114\tLR: 0.00006951\n",
            "Train Epoch: 13 [91904/123872 (74%)]\tLoss: 0.374436\tLR: 0.00006948\n",
            "Train Epoch: 13 [92160/123872 (74%)]\tLoss: 0.289180\tLR: 0.00006945\n",
            "Train Epoch: 13 [92160/123872 (74%)]\tLoss: 0.289180\n",
            "Train Epoch: 13 [92416/123872 (75%)]\tLoss: 0.298680\tLR: 0.00006942\n",
            "Train Epoch: 13 [92672/123872 (75%)]\tLoss: 0.412174\tLR: 0.00006938\n",
            "Train Epoch: 13 [92928/123872 (75%)]\tLoss: 0.384113\tLR: 0.00006935\n",
            "Train Epoch: 13 [93184/123872 (75%)]\tLoss: 0.326859\tLR: 0.00006932\n",
            "Train Epoch: 13 [93440/123872 (75%)]\tLoss: 0.307536\tLR: 0.00006929\n",
            "Train Epoch: 13 [93696/123872 (76%)]\tLoss: 0.289740\tLR: 0.00006926\n",
            "Train Epoch: 13 [93952/123872 (76%)]\tLoss: 0.336854\tLR: 0.00006923\n",
            "Train Epoch: 13 [94208/123872 (76%)]\tLoss: 0.383289\tLR: 0.00006920\n",
            "Train Epoch: 13 [94464/123872 (76%)]\tLoss: 0.282182\tLR: 0.00006917\n",
            "Train Epoch: 13 [94720/123872 (76%)]\tLoss: 0.318064\tLR: 0.00006914\n",
            "Train Epoch: 13 [94720/123872 (76%)]\tLoss: 0.318064\n",
            "Train Epoch: 13 [94976/123872 (77%)]\tLoss: 0.418227\tLR: 0.00006911\n",
            "Train Epoch: 13 [95232/123872 (77%)]\tLoss: 0.288195\tLR: 0.00006908\n",
            "Train Epoch: 13 [95488/123872 (77%)]\tLoss: 0.363845\tLR: 0.00006905\n",
            "Train Epoch: 13 [95744/123872 (77%)]\tLoss: 0.299930\tLR: 0.00006902\n",
            "Train Epoch: 13 [96000/123872 (77%)]\tLoss: 0.371085\tLR: 0.00006899\n",
            "Train Epoch: 13 [96256/123872 (78%)]\tLoss: 0.339650\tLR: 0.00006896\n",
            "Train Epoch: 13 [96512/123872 (78%)]\tLoss: 0.391131\tLR: 0.00006893\n",
            "Train Epoch: 13 [96768/123872 (78%)]\tLoss: 0.345644\tLR: 0.00006890\n",
            "Train Epoch: 13 [97024/123872 (78%)]\tLoss: 0.388854\tLR: 0.00006887\n",
            "Train Epoch: 13 [97280/123872 (79%)]\tLoss: 0.266367\tLR: 0.00006884\n",
            "Train Epoch: 13 [97280/123872 (79%)]\tLoss: 0.266367\n",
            "Train Epoch: 13 [97536/123872 (79%)]\tLoss: 0.407449\tLR: 0.00006881\n",
            "Train Epoch: 13 [97792/123872 (79%)]\tLoss: 0.358487\tLR: 0.00006878\n",
            "Train Epoch: 13 [98048/123872 (79%)]\tLoss: 0.272720\tLR: 0.00006875\n",
            "Train Epoch: 13 [98304/123872 (79%)]\tLoss: 0.422372\tLR: 0.00006872\n",
            "Train Epoch: 13 [98560/123872 (80%)]\tLoss: 0.266184\tLR: 0.00006869\n",
            "Train Epoch: 13 [98816/123872 (80%)]\tLoss: 0.355456\tLR: 0.00006866\n",
            "Train Epoch: 13 [99072/123872 (80%)]\tLoss: 0.292584\tLR: 0.00006863\n",
            "Train Epoch: 13 [99328/123872 (80%)]\tLoss: 0.388354\tLR: 0.00006860\n",
            "Train Epoch: 13 [99584/123872 (80%)]\tLoss: 0.316593\tLR: 0.00006857\n",
            "Train Epoch: 13 [99840/123872 (81%)]\tLoss: 0.327765\tLR: 0.00006854\n",
            "Train Epoch: 13 [99840/123872 (81%)]\tLoss: 0.327765\n",
            "Train Epoch: 13 [100096/123872 (81%)]\tLoss: 0.341558\tLR: 0.00006851\n",
            "Train Epoch: 13 [100352/123872 (81%)]\tLoss: 0.342552\tLR: 0.00006848\n",
            "Train Epoch: 13 [100608/123872 (81%)]\tLoss: 0.318472\tLR: 0.00006845\n",
            "Train Epoch: 13 [100864/123872 (81%)]\tLoss: 0.436283\tLR: 0.00006842\n",
            "Train Epoch: 13 [101120/123872 (82%)]\tLoss: 0.411924\tLR: 0.00006839\n",
            "Train Epoch: 13 [101376/123872 (82%)]\tLoss: 0.290719\tLR: 0.00006836\n",
            "Train Epoch: 13 [101632/123872 (82%)]\tLoss: 0.333889\tLR: 0.00006834\n",
            "Train Epoch: 13 [101888/123872 (82%)]\tLoss: 0.391703\tLR: 0.00006831\n",
            "Train Epoch: 13 [102144/123872 (82%)]\tLoss: 0.291145\tLR: 0.00006828\n",
            "Train Epoch: 13 [102400/123872 (83%)]\tLoss: 0.366550\tLR: 0.00006825\n",
            "Train Epoch: 13 [102400/123872 (83%)]\tLoss: 0.366550\n",
            "Train Epoch: 13 [102656/123872 (83%)]\tLoss: 0.308387\tLR: 0.00006822\n",
            "Train Epoch: 13 [102912/123872 (83%)]\tLoss: 0.435989\tLR: 0.00006819\n",
            "Train Epoch: 13 [103168/123872 (83%)]\tLoss: 0.324298\tLR: 0.00006816\n",
            "Train Epoch: 13 [103424/123872 (83%)]\tLoss: 0.374161\tLR: 0.00006813\n",
            "Train Epoch: 13 [103680/123872 (84%)]\tLoss: 0.339645\tLR: 0.00006810\n",
            "Train Epoch: 13 [103936/123872 (84%)]\tLoss: 0.341162\tLR: 0.00006808\n",
            "Train Epoch: 13 [104192/123872 (84%)]\tLoss: 0.349259\tLR: 0.00006805\n",
            "Train Epoch: 13 [104448/123872 (84%)]\tLoss: 0.352438\tLR: 0.00006802\n",
            "Train Epoch: 13 [104704/123872 (85%)]\tLoss: 0.297142\tLR: 0.00006799\n",
            "Train Epoch: 13 [104960/123872 (85%)]\tLoss: 0.305863\tLR: 0.00006796\n",
            "Train Epoch: 13 [104960/123872 (85%)]\tLoss: 0.305863\n",
            "Train Epoch: 13 [105216/123872 (85%)]\tLoss: 0.271238\tLR: 0.00006793\n",
            "Train Epoch: 13 [105472/123872 (85%)]\tLoss: 0.335547\tLR: 0.00006790\n",
            "Train Epoch: 13 [105728/123872 (85%)]\tLoss: 0.353692\tLR: 0.00006788\n",
            "Train Epoch: 13 [105984/123872 (86%)]\tLoss: 0.299517\tLR: 0.00006785\n",
            "Train Epoch: 13 [106240/123872 (86%)]\tLoss: 0.424212\tLR: 0.00006782\n",
            "Train Epoch: 13 [106496/123872 (86%)]\tLoss: 0.315169\tLR: 0.00006779\n",
            "Train Epoch: 13 [106752/123872 (86%)]\tLoss: 0.367836\tLR: 0.00006776\n",
            "Train Epoch: 13 [107008/123872 (86%)]\tLoss: 0.356417\tLR: 0.00006774\n",
            "Train Epoch: 13 [107264/123872 (87%)]\tLoss: 0.332225\tLR: 0.00006771\n",
            "Train Epoch: 13 [107520/123872 (87%)]\tLoss: 0.302846\tLR: 0.00006768\n",
            "Train Epoch: 13 [107520/123872 (87%)]\tLoss: 0.302846\n",
            "Train Epoch: 13 [107776/123872 (87%)]\tLoss: 0.336511\tLR: 0.00006765\n",
            "Train Epoch: 13 [108032/123872 (87%)]\tLoss: 0.341853\tLR: 0.00006762\n",
            "Train Epoch: 13 [108288/123872 (87%)]\tLoss: 0.365321\tLR: 0.00006760\n",
            "Train Epoch: 13 [108544/123872 (88%)]\tLoss: 0.345829\tLR: 0.00006757\n",
            "Train Epoch: 13 [108800/123872 (88%)]\tLoss: 0.332200\tLR: 0.00006754\n",
            "Train Epoch: 13 [109056/123872 (88%)]\tLoss: 0.309397\tLR: 0.00006751\n",
            "Train Epoch: 13 [109312/123872 (88%)]\tLoss: 0.312225\tLR: 0.00006748\n",
            "Train Epoch: 13 [109568/123872 (88%)]\tLoss: 0.326838\tLR: 0.00006746\n",
            "Train Epoch: 13 [109824/123872 (89%)]\tLoss: 0.328107\tLR: 0.00006743\n",
            "Train Epoch: 13 [110080/123872 (89%)]\tLoss: 0.318998\tLR: 0.00006740\n",
            "Train Epoch: 13 [110080/123872 (89%)]\tLoss: 0.318998\n",
            "Train Epoch: 13 [110336/123872 (89%)]\tLoss: 0.323527\tLR: 0.00006737\n",
            "Train Epoch: 13 [110592/123872 (89%)]\tLoss: 0.266376\tLR: 0.00006735\n",
            "Train Epoch: 13 [110848/123872 (89%)]\tLoss: 0.369413\tLR: 0.00006732\n",
            "Train Epoch: 13 [111104/123872 (90%)]\tLoss: 0.327978\tLR: 0.00006729\n",
            "Train Epoch: 13 [111360/123872 (90%)]\tLoss: 0.394257\tLR: 0.00006727\n",
            "Train Epoch: 13 [111616/123872 (90%)]\tLoss: 0.373317\tLR: 0.00006724\n",
            "Train Epoch: 13 [111872/123872 (90%)]\tLoss: 0.351653\tLR: 0.00006721\n",
            "Train Epoch: 13 [112128/123872 (90%)]\tLoss: 0.381362\tLR: 0.00006718\n",
            "Train Epoch: 13 [112384/123872 (91%)]\tLoss: 0.433476\tLR: 0.00006716\n",
            "Train Epoch: 13 [112640/123872 (91%)]\tLoss: 0.348386\tLR: 0.00006713\n",
            "Train Epoch: 13 [112640/123872 (91%)]\tLoss: 0.348386\n",
            "Train Epoch: 13 [112896/123872 (91%)]\tLoss: 0.353234\tLR: 0.00006710\n",
            "Train Epoch: 13 [113152/123872 (91%)]\tLoss: 0.336527\tLR: 0.00006708\n",
            "Train Epoch: 13 [113408/123872 (92%)]\tLoss: 0.319499\tLR: 0.00006705\n",
            "Train Epoch: 13 [113664/123872 (92%)]\tLoss: 0.382176\tLR: 0.00006702\n",
            "Train Epoch: 13 [113920/123872 (92%)]\tLoss: 0.367762\tLR: 0.00006700\n",
            "Train Epoch: 13 [114176/123872 (92%)]\tLoss: 0.402105\tLR: 0.00006697\n",
            "Train Epoch: 13 [114432/123872 (92%)]\tLoss: 0.310333\tLR: 0.00006694\n",
            "Train Epoch: 13 [114688/123872 (93%)]\tLoss: 0.330270\tLR: 0.00006692\n",
            "Train Epoch: 13 [114944/123872 (93%)]\tLoss: 0.399259\tLR: 0.00006689\n",
            "Train Epoch: 13 [115200/123872 (93%)]\tLoss: 0.319336\tLR: 0.00006686\n",
            "Train Epoch: 13 [115200/123872 (93%)]\tLoss: 0.319336\n",
            "Train Epoch: 13 [115456/123872 (93%)]\tLoss: 0.359722\tLR: 0.00006684\n",
            "Train Epoch: 13 [115712/123872 (93%)]\tLoss: 0.359548\tLR: 0.00006681\n",
            "Train Epoch: 13 [115968/123872 (94%)]\tLoss: 0.324537\tLR: 0.00006678\n",
            "Train Epoch: 13 [116224/123872 (94%)]\tLoss: 0.429236\tLR: 0.00006676\n",
            "Train Epoch: 13 [116480/123872 (94%)]\tLoss: 0.316143\tLR: 0.00006673\n",
            "Train Epoch: 13 [116736/123872 (94%)]\tLoss: 0.302736\tLR: 0.00006671\n",
            "Train Epoch: 13 [116992/123872 (94%)]\tLoss: 0.339668\tLR: 0.00006668\n",
            "Train Epoch: 13 [117248/123872 (95%)]\tLoss: 0.400741\tLR: 0.00006665\n",
            "Train Epoch: 13 [117504/123872 (95%)]\tLoss: 0.333835\tLR: 0.00006663\n",
            "Train Epoch: 13 [117760/123872 (95%)]\tLoss: 0.349444\tLR: 0.00006660\n",
            "Train Epoch: 13 [117760/123872 (95%)]\tLoss: 0.349444\n",
            "Train Epoch: 13 [118016/123872 (95%)]\tLoss: 0.365860\tLR: 0.00006658\n",
            "Train Epoch: 13 [118272/123872 (95%)]\tLoss: 0.309855\tLR: 0.00006655\n",
            "Train Epoch: 13 [118528/123872 (96%)]\tLoss: 0.379277\tLR: 0.00006652\n",
            "Train Epoch: 13 [118784/123872 (96%)]\tLoss: 0.376158\tLR: 0.00006650\n",
            "Train Epoch: 13 [119040/123872 (96%)]\tLoss: 0.409676\tLR: 0.00006647\n",
            "Train Epoch: 13 [119296/123872 (96%)]\tLoss: 0.374535\tLR: 0.00006645\n",
            "Train Epoch: 13 [119552/123872 (96%)]\tLoss: 0.306932\tLR: 0.00006642\n",
            "Train Epoch: 13 [119808/123872 (97%)]\tLoss: 0.338033\tLR: 0.00006640\n",
            "Train Epoch: 13 [120064/123872 (97%)]\tLoss: 0.318617\tLR: 0.00006637\n",
            "Train Epoch: 13 [120320/123872 (97%)]\tLoss: 0.332205\tLR: 0.00006634\n",
            "Train Epoch: 13 [120320/123872 (97%)]\tLoss: 0.332205\n",
            "Train Epoch: 13 [120576/123872 (97%)]\tLoss: 0.370866\tLR: 0.00006632\n",
            "Train Epoch: 13 [120832/123872 (98%)]\tLoss: 0.437268\tLR: 0.00006629\n",
            "Train Epoch: 13 [121088/123872 (98%)]\tLoss: 0.358620\tLR: 0.00006627\n",
            "Train Epoch: 13 [121344/123872 (98%)]\tLoss: 0.355775\tLR: 0.00006624\n",
            "Train Epoch: 13 [121600/123872 (98%)]\tLoss: 0.309186\tLR: 0.00006622\n",
            "Train Epoch: 13 [121856/123872 (98%)]\tLoss: 0.356335\tLR: 0.00006619\n",
            "Train Epoch: 13 [122112/123872 (99%)]\tLoss: 0.346606\tLR: 0.00006617\n",
            "Train Epoch: 13 [122368/123872 (99%)]\tLoss: 0.372691\tLR: 0.00006614\n",
            "Train Epoch: 13 [122624/123872 (99%)]\tLoss: 0.338476\tLR: 0.00006612\n",
            "Train Epoch: 13 [122880/123872 (99%)]\tLoss: 0.342974\tLR: 0.00006609\n",
            "Train Epoch: 13 [122880/123872 (99%)]\tLoss: 0.342974\n",
            "Train Epoch: 13 [123136/123872 (99%)]\tLoss: 0.377072\tLR: 0.00006607\n",
            "Train Epoch: 13 [123392/123872 (100%)]\tLoss: 0.322040\tLR: 0.00006604\n",
            "Train Epoch: 13 [108192/123872 (100%)]\tLoss: 0.321996\tLR: 0.00006602\n",
            "\n",
            "Test set: Average loss: 0.0014, Accuracy: 25957/30970 (83.81%)\n",
            "\n",
            "Train Epoch: 14 [0/123872 (0%)]\tLoss: 0.367770\tLR: 0.00006599\n",
            "Train Epoch: 14 [0/123872 (0%)]\tLoss: 0.367770\n",
            "Train Epoch: 14 [256/123872 (0%)]\tLoss: 0.399997\tLR: 0.00006597\n",
            "Train Epoch: 14 [512/123872 (0%)]\tLoss: 0.340343\tLR: 0.00006594\n",
            "Train Epoch: 14 [768/123872 (1%)]\tLoss: 0.443441\tLR: 0.00006592\n",
            "Train Epoch: 14 [1024/123872 (1%)]\tLoss: 0.317274\tLR: 0.00006590\n",
            "Train Epoch: 14 [1280/123872 (1%)]\tLoss: 0.357360\tLR: 0.00006587\n",
            "Train Epoch: 14 [1536/123872 (1%)]\tLoss: 0.287785\tLR: 0.00006585\n",
            "Train Epoch: 14 [1792/123872 (1%)]\tLoss: 0.354471\tLR: 0.00006582\n",
            "Train Epoch: 14 [2048/123872 (2%)]\tLoss: 0.319273\tLR: 0.00006580\n",
            "Train Epoch: 14 [2304/123872 (2%)]\tLoss: 0.321521\tLR: 0.00006577\n",
            "Train Epoch: 14 [2560/123872 (2%)]\tLoss: 0.329957\tLR: 0.00006575\n",
            "Train Epoch: 14 [2560/123872 (2%)]\tLoss: 0.329957\n",
            "Train Epoch: 14 [2816/123872 (2%)]\tLoss: 0.300776\tLR: 0.00006572\n",
            "Train Epoch: 14 [3072/123872 (2%)]\tLoss: 0.308013\tLR: 0.00006570\n",
            "Train Epoch: 14 [3328/123872 (3%)]\tLoss: 0.324536\tLR: 0.00006568\n",
            "Train Epoch: 14 [3584/123872 (3%)]\tLoss: 0.360228\tLR: 0.00006565\n",
            "Train Epoch: 14 [3840/123872 (3%)]\tLoss: 0.405639\tLR: 0.00006563\n",
            "Train Epoch: 14 [4096/123872 (3%)]\tLoss: 0.341537\tLR: 0.00006560\n",
            "Train Epoch: 14 [4352/123872 (4%)]\tLoss: 0.377485\tLR: 0.00006558\n",
            "Train Epoch: 14 [4608/123872 (4%)]\tLoss: 0.371341\tLR: 0.00006556\n",
            "Train Epoch: 14 [4864/123872 (4%)]\tLoss: 0.356952\tLR: 0.00006553\n",
            "Train Epoch: 14 [5120/123872 (4%)]\tLoss: 0.382695\tLR: 0.00006551\n",
            "Train Epoch: 14 [5120/123872 (4%)]\tLoss: 0.382695\n",
            "Train Epoch: 14 [5376/123872 (4%)]\tLoss: 0.392534\tLR: 0.00006549\n",
            "Train Epoch: 14 [5632/123872 (5%)]\tLoss: 0.347313\tLR: 0.00006546\n",
            "Train Epoch: 14 [5888/123872 (5%)]\tLoss: 0.357213\tLR: 0.00006544\n",
            "Train Epoch: 14 [6144/123872 (5%)]\tLoss: 0.335899\tLR: 0.00006541\n",
            "Train Epoch: 14 [6400/123872 (5%)]\tLoss: 0.372675\tLR: 0.00006539\n",
            "Train Epoch: 14 [6656/123872 (5%)]\tLoss: 0.303522\tLR: 0.00006537\n",
            "Train Epoch: 14 [6912/123872 (6%)]\tLoss: 0.369222\tLR: 0.00006534\n",
            "Train Epoch: 14 [7168/123872 (6%)]\tLoss: 0.337926\tLR: 0.00006532\n",
            "Train Epoch: 14 [7424/123872 (6%)]\tLoss: 0.340319\tLR: 0.00006530\n",
            "Train Epoch: 14 [7680/123872 (6%)]\tLoss: 0.306426\tLR: 0.00006527\n",
            "Train Epoch: 14 [7680/123872 (6%)]\tLoss: 0.306426\n",
            "Train Epoch: 14 [7936/123872 (6%)]\tLoss: 0.295624\tLR: 0.00006525\n",
            "Train Epoch: 14 [8192/123872 (7%)]\tLoss: 0.285368\tLR: 0.00006523\n",
            "Train Epoch: 14 [8448/123872 (7%)]\tLoss: 0.348949\tLR: 0.00006521\n",
            "Train Epoch: 14 [8704/123872 (7%)]\tLoss: 0.368475\tLR: 0.00006518\n",
            "Train Epoch: 14 [8960/123872 (7%)]\tLoss: 0.344591\tLR: 0.00006516\n",
            "Train Epoch: 14 [9216/123872 (7%)]\tLoss: 0.345220\tLR: 0.00006514\n",
            "Train Epoch: 14 [9472/123872 (8%)]\tLoss: 0.360285\tLR: 0.00006511\n",
            "Train Epoch: 14 [9728/123872 (8%)]\tLoss: 0.337724\tLR: 0.00006509\n",
            "Train Epoch: 14 [9984/123872 (8%)]\tLoss: 0.340097\tLR: 0.00006507\n",
            "Train Epoch: 14 [10240/123872 (8%)]\tLoss: 0.368861\tLR: 0.00006505\n",
            "Train Epoch: 14 [10240/123872 (8%)]\tLoss: 0.368861\n",
            "Train Epoch: 14 [10496/123872 (8%)]\tLoss: 0.343586\tLR: 0.00006502\n",
            "Train Epoch: 14 [10752/123872 (9%)]\tLoss: 0.320388\tLR: 0.00006500\n",
            "Train Epoch: 14 [11008/123872 (9%)]\tLoss: 0.301107\tLR: 0.00006498\n",
            "Train Epoch: 14 [11264/123872 (9%)]\tLoss: 0.317391\tLR: 0.00006495\n",
            "Train Epoch: 14 [11520/123872 (9%)]\tLoss: 0.407685\tLR: 0.00006493\n",
            "Train Epoch: 14 [11776/123872 (10%)]\tLoss: 0.336094\tLR: 0.00006491\n",
            "Train Epoch: 14 [12032/123872 (10%)]\tLoss: 0.310686\tLR: 0.00006489\n",
            "Train Epoch: 14 [12288/123872 (10%)]\tLoss: 0.389386\tLR: 0.00006487\n",
            "Train Epoch: 14 [12544/123872 (10%)]\tLoss: 0.366517\tLR: 0.00006484\n",
            "Train Epoch: 14 [12800/123872 (10%)]\tLoss: 0.346378\tLR: 0.00006482\n",
            "Train Epoch: 14 [12800/123872 (10%)]\tLoss: 0.346378\n",
            "Train Epoch: 14 [13056/123872 (11%)]\tLoss: 0.407608\tLR: 0.00006480\n",
            "Train Epoch: 14 [13312/123872 (11%)]\tLoss: 0.302614\tLR: 0.00006478\n",
            "Train Epoch: 14 [13568/123872 (11%)]\tLoss: 0.380228\tLR: 0.00006475\n",
            "Train Epoch: 14 [13824/123872 (11%)]\tLoss: 0.386045\tLR: 0.00006473\n",
            "Train Epoch: 14 [14080/123872 (11%)]\tLoss: 0.332311\tLR: 0.00006471\n",
            "Train Epoch: 14 [14336/123872 (12%)]\tLoss: 0.373410\tLR: 0.00006469\n",
            "Train Epoch: 14 [14592/123872 (12%)]\tLoss: 0.371022\tLR: 0.00006467\n",
            "Train Epoch: 14 [14848/123872 (12%)]\tLoss: 0.321855\tLR: 0.00006464\n",
            "Train Epoch: 14 [15104/123872 (12%)]\tLoss: 0.348135\tLR: 0.00006462\n",
            "Train Epoch: 14 [15360/123872 (12%)]\tLoss: 0.353155\tLR: 0.00006460\n",
            "Train Epoch: 14 [15360/123872 (12%)]\tLoss: 0.353155\n",
            "Train Epoch: 14 [15616/123872 (13%)]\tLoss: 0.328411\tLR: 0.00006458\n",
            "Train Epoch: 14 [15872/123872 (13%)]\tLoss: 0.395318\tLR: 0.00006456\n",
            "Train Epoch: 14 [16128/123872 (13%)]\tLoss: 0.360252\tLR: 0.00006454\n",
            "Train Epoch: 14 [16384/123872 (13%)]\tLoss: 0.372110\tLR: 0.00006451\n",
            "Train Epoch: 14 [16640/123872 (13%)]\tLoss: 0.337736\tLR: 0.00006449\n",
            "Train Epoch: 14 [16896/123872 (14%)]\tLoss: 0.344214\tLR: 0.00006447\n",
            "Train Epoch: 14 [17152/123872 (14%)]\tLoss: 0.386717\tLR: 0.00006445\n",
            "Train Epoch: 14 [17408/123872 (14%)]\tLoss: 0.324426\tLR: 0.00006443\n",
            "Train Epoch: 14 [17664/123872 (14%)]\tLoss: 0.324299\tLR: 0.00006441\n",
            "Train Epoch: 14 [17920/123872 (14%)]\tLoss: 0.324510\tLR: 0.00006439\n",
            "Train Epoch: 14 [17920/123872 (14%)]\tLoss: 0.324510\n",
            "Train Epoch: 14 [18176/123872 (15%)]\tLoss: 0.332983\tLR: 0.00006437\n",
            "Train Epoch: 14 [18432/123872 (15%)]\tLoss: 0.336603\tLR: 0.00006434\n",
            "Train Epoch: 14 [18688/123872 (15%)]\tLoss: 0.399025\tLR: 0.00006432\n",
            "Train Epoch: 14 [18944/123872 (15%)]\tLoss: 0.279619\tLR: 0.00006430\n",
            "Train Epoch: 14 [19200/123872 (15%)]\tLoss: 0.309979\tLR: 0.00006428\n",
            "Train Epoch: 14 [19456/123872 (16%)]\tLoss: 0.349085\tLR: 0.00006426\n",
            "Train Epoch: 14 [19712/123872 (16%)]\tLoss: 0.347125\tLR: 0.00006424\n",
            "Train Epoch: 14 [19968/123872 (16%)]\tLoss: 0.290502\tLR: 0.00006422\n",
            "Train Epoch: 14 [20224/123872 (16%)]\tLoss: 0.314990\tLR: 0.00006420\n",
            "Train Epoch: 14 [20480/123872 (17%)]\tLoss: 0.456183\tLR: 0.00006418\n",
            "Train Epoch: 14 [20480/123872 (17%)]\tLoss: 0.456183\n",
            "Train Epoch: 14 [20736/123872 (17%)]\tLoss: 0.383192\tLR: 0.00006416\n",
            "Train Epoch: 14 [20992/123872 (17%)]\tLoss: 0.411064\tLR: 0.00006414\n",
            "Train Epoch: 14 [21248/123872 (17%)]\tLoss: 0.364737\tLR: 0.00006412\n",
            "Train Epoch: 14 [21504/123872 (17%)]\tLoss: 0.420387\tLR: 0.00006410\n",
            "Train Epoch: 14 [21760/123872 (18%)]\tLoss: 0.333111\tLR: 0.00006407\n",
            "Train Epoch: 14 [22016/123872 (18%)]\tLoss: 0.300355\tLR: 0.00006405\n",
            "Train Epoch: 14 [22272/123872 (18%)]\tLoss: 0.292114\tLR: 0.00006403\n",
            "Train Epoch: 14 [22528/123872 (18%)]\tLoss: 0.359289\tLR: 0.00006401\n",
            "Train Epoch: 14 [22784/123872 (18%)]\tLoss: 0.337492\tLR: 0.00006399\n",
            "Train Epoch: 14 [23040/123872 (19%)]\tLoss: 0.338987\tLR: 0.00006397\n",
            "Train Epoch: 14 [23040/123872 (19%)]\tLoss: 0.338987\n",
            "Train Epoch: 14 [23296/123872 (19%)]\tLoss: 0.352711\tLR: 0.00006395\n",
            "Train Epoch: 14 [23552/123872 (19%)]\tLoss: 0.375067\tLR: 0.00006393\n",
            "Train Epoch: 14 [23808/123872 (19%)]\tLoss: 0.378057\tLR: 0.00006391\n",
            "Train Epoch: 14 [24064/123872 (19%)]\tLoss: 0.356945\tLR: 0.00006389\n",
            "Train Epoch: 14 [24320/123872 (20%)]\tLoss: 0.328261\tLR: 0.00006387\n",
            "Train Epoch: 14 [24576/123872 (20%)]\tLoss: 0.363217\tLR: 0.00006385\n",
            "Train Epoch: 14 [24832/123872 (20%)]\tLoss: 0.347941\tLR: 0.00006383\n",
            "Train Epoch: 14 [25088/123872 (20%)]\tLoss: 0.344725\tLR: 0.00006381\n",
            "Train Epoch: 14 [25344/123872 (20%)]\tLoss: 0.272473\tLR: 0.00006379\n",
            "Train Epoch: 14 [25600/123872 (21%)]\tLoss: 0.313334\tLR: 0.00006377\n",
            "Train Epoch: 14 [25600/123872 (21%)]\tLoss: 0.313334\n",
            "Train Epoch: 14 [25856/123872 (21%)]\tLoss: 0.367714\tLR: 0.00006375\n",
            "Train Epoch: 14 [26112/123872 (21%)]\tLoss: 0.280671\tLR: 0.00006373\n",
            "Train Epoch: 14 [26368/123872 (21%)]\tLoss: 0.310087\tLR: 0.00006372\n",
            "Train Epoch: 14 [26624/123872 (21%)]\tLoss: 0.317320\tLR: 0.00006370\n",
            "Train Epoch: 14 [26880/123872 (22%)]\tLoss: 0.370619\tLR: 0.00006368\n",
            "Train Epoch: 14 [27136/123872 (22%)]\tLoss: 0.390905\tLR: 0.00006366\n",
            "Train Epoch: 14 [27392/123872 (22%)]\tLoss: 0.421705\tLR: 0.00006364\n",
            "Train Epoch: 14 [27648/123872 (22%)]\tLoss: 0.376220\tLR: 0.00006362\n",
            "Train Epoch: 14 [27904/123872 (23%)]\tLoss: 0.340985\tLR: 0.00006360\n",
            "Train Epoch: 14 [28160/123872 (23%)]\tLoss: 0.353746\tLR: 0.00006358\n",
            "Train Epoch: 14 [28160/123872 (23%)]\tLoss: 0.353746\n",
            "Train Epoch: 14 [28416/123872 (23%)]\tLoss: 0.298475\tLR: 0.00006356\n",
            "Train Epoch: 14 [28672/123872 (23%)]\tLoss: 0.350978\tLR: 0.00006354\n",
            "Train Epoch: 14 [28928/123872 (23%)]\tLoss: 0.342949\tLR: 0.00006352\n",
            "Train Epoch: 14 [29184/123872 (24%)]\tLoss: 0.326617\tLR: 0.00006350\n",
            "Train Epoch: 14 [29440/123872 (24%)]\tLoss: 0.326336\tLR: 0.00006348\n",
            "Train Epoch: 14 [29696/123872 (24%)]\tLoss: 0.392115\tLR: 0.00006347\n",
            "Train Epoch: 14 [29952/123872 (24%)]\tLoss: 0.330976\tLR: 0.00006345\n",
            "Train Epoch: 14 [30208/123872 (24%)]\tLoss: 0.301321\tLR: 0.00006343\n",
            "Train Epoch: 14 [30464/123872 (25%)]\tLoss: 0.348622\tLR: 0.00006341\n",
            "Train Epoch: 14 [30720/123872 (25%)]\tLoss: 0.364769\tLR: 0.00006339\n",
            "Train Epoch: 14 [30720/123872 (25%)]\tLoss: 0.364769\n",
            "Train Epoch: 14 [30976/123872 (25%)]\tLoss: 0.367652\tLR: 0.00006337\n",
            "Train Epoch: 14 [31232/123872 (25%)]\tLoss: 0.364672\tLR: 0.00006335\n",
            "Train Epoch: 14 [31488/123872 (25%)]\tLoss: 0.370543\tLR: 0.00006334\n",
            "Train Epoch: 14 [31744/123872 (26%)]\tLoss: 0.313917\tLR: 0.00006332\n",
            "Train Epoch: 14 [32000/123872 (26%)]\tLoss: 0.286900\tLR: 0.00006330\n",
            "Train Epoch: 14 [32256/123872 (26%)]\tLoss: 0.302835\tLR: 0.00006328\n",
            "Train Epoch: 14 [32512/123872 (26%)]\tLoss: 0.310453\tLR: 0.00006326\n",
            "Train Epoch: 14 [32768/123872 (26%)]\tLoss: 0.323792\tLR: 0.00006324\n",
            "Train Epoch: 14 [33024/123872 (27%)]\tLoss: 0.366951\tLR: 0.00006323\n",
            "Train Epoch: 14 [33280/123872 (27%)]\tLoss: 0.358190\tLR: 0.00006321\n",
            "Train Epoch: 14 [33280/123872 (27%)]\tLoss: 0.358190\n",
            "Train Epoch: 14 [33536/123872 (27%)]\tLoss: 0.311604\tLR: 0.00006319\n",
            "Train Epoch: 14 [33792/123872 (27%)]\tLoss: 0.341811\tLR: 0.00006317\n",
            "Train Epoch: 14 [34048/123872 (27%)]\tLoss: 0.350834\tLR: 0.00006315\n",
            "Train Epoch: 14 [34304/123872 (28%)]\tLoss: 0.405515\tLR: 0.00006313\n",
            "Train Epoch: 14 [34560/123872 (28%)]\tLoss: 0.340498\tLR: 0.00006312\n",
            "Train Epoch: 14 [34816/123872 (28%)]\tLoss: 0.346830\tLR: 0.00006310\n",
            "Train Epoch: 14 [35072/123872 (28%)]\tLoss: 0.374719\tLR: 0.00006308\n",
            "Train Epoch: 14 [35328/123872 (29%)]\tLoss: 0.340848\tLR: 0.00006306\n",
            "Train Epoch: 14 [35584/123872 (29%)]\tLoss: 0.440834\tLR: 0.00006305\n",
            "Train Epoch: 14 [35840/123872 (29%)]\tLoss: 0.324893\tLR: 0.00006303\n",
            "Train Epoch: 14 [35840/123872 (29%)]\tLoss: 0.324893\n",
            "Train Epoch: 14 [36096/123872 (29%)]\tLoss: 0.319631\tLR: 0.00006301\n",
            "Train Epoch: 14 [36352/123872 (29%)]\tLoss: 0.279681\tLR: 0.00006299\n",
            "Train Epoch: 14 [36608/123872 (30%)]\tLoss: 0.310418\tLR: 0.00006298\n",
            "Train Epoch: 14 [36864/123872 (30%)]\tLoss: 0.346383\tLR: 0.00006296\n",
            "Train Epoch: 14 [37120/123872 (30%)]\tLoss: 0.317678\tLR: 0.00006294\n",
            "Train Epoch: 14 [37376/123872 (30%)]\tLoss: 0.383241\tLR: 0.00006292\n",
            "Train Epoch: 14 [37632/123872 (30%)]\tLoss: 0.338197\tLR: 0.00006291\n",
            "Train Epoch: 14 [37888/123872 (31%)]\tLoss: 0.377737\tLR: 0.00006289\n",
            "Train Epoch: 14 [38144/123872 (31%)]\tLoss: 0.368014\tLR: 0.00006287\n",
            "Train Epoch: 14 [38400/123872 (31%)]\tLoss: 0.342162\tLR: 0.00006285\n",
            "Train Epoch: 14 [38400/123872 (31%)]\tLoss: 0.342162\n",
            "Train Epoch: 14 [38656/123872 (31%)]\tLoss: 0.386882\tLR: 0.00006284\n",
            "Train Epoch: 14 [38912/123872 (31%)]\tLoss: 0.371447\tLR: 0.00006282\n",
            "Train Epoch: 14 [39168/123872 (32%)]\tLoss: 0.340988\tLR: 0.00006280\n",
            "Train Epoch: 14 [39424/123872 (32%)]\tLoss: 0.350872\tLR: 0.00006279\n",
            "Train Epoch: 14 [39680/123872 (32%)]\tLoss: 0.356515\tLR: 0.00006277\n",
            "Train Epoch: 14 [39936/123872 (32%)]\tLoss: 0.336570\tLR: 0.00006275\n",
            "Train Epoch: 14 [40192/123872 (32%)]\tLoss: 0.326155\tLR: 0.00006274\n",
            "Train Epoch: 14 [40448/123872 (33%)]\tLoss: 0.313073\tLR: 0.00006272\n",
            "Train Epoch: 14 [40704/123872 (33%)]\tLoss: 0.345731\tLR: 0.00006270\n",
            "Train Epoch: 14 [40960/123872 (33%)]\tLoss: 0.307007\tLR: 0.00006269\n",
            "Train Epoch: 14 [40960/123872 (33%)]\tLoss: 0.307007\n",
            "Train Epoch: 14 [41216/123872 (33%)]\tLoss: 0.321441\tLR: 0.00006267\n",
            "Train Epoch: 14 [41472/123872 (33%)]\tLoss: 0.373553\tLR: 0.00006265\n",
            "Train Epoch: 14 [41728/123872 (34%)]\tLoss: 0.372334\tLR: 0.00006264\n",
            "Train Epoch: 14 [41984/123872 (34%)]\tLoss: 0.305885\tLR: 0.00006262\n",
            "Train Epoch: 14 [42240/123872 (34%)]\tLoss: 0.409377\tLR: 0.00006260\n",
            "Train Epoch: 14 [42496/123872 (34%)]\tLoss: 0.339218\tLR: 0.00006259\n",
            "Train Epoch: 14 [42752/123872 (35%)]\tLoss: 0.370334\tLR: 0.00006257\n",
            "Train Epoch: 14 [43008/123872 (35%)]\tLoss: 0.307669\tLR: 0.00006255\n",
            "Train Epoch: 14 [43264/123872 (35%)]\tLoss: 0.340939\tLR: 0.00006254\n",
            "Train Epoch: 14 [43520/123872 (35%)]\tLoss: 0.344593\tLR: 0.00006252\n",
            "Train Epoch: 14 [43520/123872 (35%)]\tLoss: 0.344593\n",
            "Train Epoch: 14 [43776/123872 (35%)]\tLoss: 0.366964\tLR: 0.00006251\n",
            "Train Epoch: 14 [44032/123872 (36%)]\tLoss: 0.350601\tLR: 0.00006249\n",
            "Train Epoch: 14 [44288/123872 (36%)]\tLoss: 0.355301\tLR: 0.00006247\n",
            "Train Epoch: 14 [44544/123872 (36%)]\tLoss: 0.374024\tLR: 0.00006246\n",
            "Train Epoch: 14 [44800/123872 (36%)]\tLoss: 0.353153\tLR: 0.00006244\n",
            "Train Epoch: 14 [45056/123872 (36%)]\tLoss: 0.314531\tLR: 0.00006243\n",
            "Train Epoch: 14 [45312/123872 (37%)]\tLoss: 0.381474\tLR: 0.00006241\n",
            "Train Epoch: 14 [45568/123872 (37%)]\tLoss: 0.363922\tLR: 0.00006240\n",
            "Train Epoch: 14 [45824/123872 (37%)]\tLoss: 0.378316\tLR: 0.00006238\n",
            "Train Epoch: 14 [46080/123872 (37%)]\tLoss: 0.388639\tLR: 0.00006236\n",
            "Train Epoch: 14 [46080/123872 (37%)]\tLoss: 0.388639\n",
            "Train Epoch: 14 [46336/123872 (37%)]\tLoss: 0.313729\tLR: 0.00006235\n",
            "Train Epoch: 14 [46592/123872 (38%)]\tLoss: 0.325099\tLR: 0.00006233\n",
            "Train Epoch: 14 [46848/123872 (38%)]\tLoss: 0.304538\tLR: 0.00006232\n",
            "Train Epoch: 14 [47104/123872 (38%)]\tLoss: 0.373215\tLR: 0.00006230\n",
            "Train Epoch: 14 [47360/123872 (38%)]\tLoss: 0.345804\tLR: 0.00006229\n",
            "Train Epoch: 14 [47616/123872 (38%)]\tLoss: 0.351311\tLR: 0.00006227\n",
            "Train Epoch: 14 [47872/123872 (39%)]\tLoss: 0.371776\tLR: 0.00006226\n",
            "Train Epoch: 14 [48128/123872 (39%)]\tLoss: 0.374763\tLR: 0.00006224\n",
            "Train Epoch: 14 [48384/123872 (39%)]\tLoss: 0.427253\tLR: 0.00006223\n",
            "Train Epoch: 14 [48640/123872 (39%)]\tLoss: 0.344248\tLR: 0.00006221\n",
            "Train Epoch: 14 [48640/123872 (39%)]\tLoss: 0.344248\n",
            "Train Epoch: 14 [48896/123872 (39%)]\tLoss: 0.333808\tLR: 0.00006220\n",
            "Train Epoch: 14 [49152/123872 (40%)]\tLoss: 0.366468\tLR: 0.00006218\n",
            "Train Epoch: 14 [49408/123872 (40%)]\tLoss: 0.314765\tLR: 0.00006217\n",
            "Train Epoch: 14 [49664/123872 (40%)]\tLoss: 0.277934\tLR: 0.00006215\n",
            "Train Epoch: 14 [49920/123872 (40%)]\tLoss: 0.361159\tLR: 0.00006214\n",
            "Train Epoch: 14 [50176/123872 (40%)]\tLoss: 0.332201\tLR: 0.00006212\n",
            "Train Epoch: 14 [50432/123872 (41%)]\tLoss: 0.333898\tLR: 0.00006211\n",
            "Train Epoch: 14 [50688/123872 (41%)]\tLoss: 0.358131\tLR: 0.00006209\n",
            "Train Epoch: 14 [50944/123872 (41%)]\tLoss: 0.332058\tLR: 0.00006208\n",
            "Train Epoch: 14 [51200/123872 (41%)]\tLoss: 0.376270\tLR: 0.00006206\n",
            "Train Epoch: 14 [51200/123872 (41%)]\tLoss: 0.376270\n",
            "Train Epoch: 14 [51456/123872 (42%)]\tLoss: 0.346669\tLR: 0.00006205\n",
            "Train Epoch: 14 [51712/123872 (42%)]\tLoss: 0.356522\tLR: 0.00006203\n",
            "Train Epoch: 14 [51968/123872 (42%)]\tLoss: 0.295962\tLR: 0.00006202\n",
            "Train Epoch: 14 [52224/123872 (42%)]\tLoss: 0.403971\tLR: 0.00006200\n",
            "Train Epoch: 14 [52480/123872 (42%)]\tLoss: 0.380143\tLR: 0.00006199\n",
            "Train Epoch: 14 [52736/123872 (43%)]\tLoss: 0.279221\tLR: 0.00006198\n",
            "Train Epoch: 14 [52992/123872 (43%)]\tLoss: 0.391206\tLR: 0.00006196\n",
            "Train Epoch: 14 [53248/123872 (43%)]\tLoss: 0.386203\tLR: 0.00006195\n",
            "Train Epoch: 14 [53504/123872 (43%)]\tLoss: 0.352323\tLR: 0.00006193\n",
            "Train Epoch: 14 [53760/123872 (43%)]\tLoss: 0.330060\tLR: 0.00006192\n",
            "Train Epoch: 14 [53760/123872 (43%)]\tLoss: 0.330060\n",
            "Train Epoch: 14 [54016/123872 (44%)]\tLoss: 0.350816\tLR: 0.00006191\n",
            "Train Epoch: 14 [54272/123872 (44%)]\tLoss: 0.359017\tLR: 0.00006189\n",
            "Train Epoch: 14 [54528/123872 (44%)]\tLoss: 0.332293\tLR: 0.00006188\n",
            "Train Epoch: 14 [54784/123872 (44%)]\tLoss: 0.337371\tLR: 0.00006186\n",
            "Train Epoch: 14 [55040/123872 (44%)]\tLoss: 0.322690\tLR: 0.00006185\n",
            "Train Epoch: 14 [55296/123872 (45%)]\tLoss: 0.317619\tLR: 0.00006184\n",
            "Train Epoch: 14 [55552/123872 (45%)]\tLoss: 0.351466\tLR: 0.00006182\n",
            "Train Epoch: 14 [55808/123872 (45%)]\tLoss: 0.398573\tLR: 0.00006181\n",
            "Train Epoch: 14 [56064/123872 (45%)]\tLoss: 0.326605\tLR: 0.00006180\n",
            "Train Epoch: 14 [56320/123872 (45%)]\tLoss: 0.377466\tLR: 0.00006178\n",
            "Train Epoch: 14 [56320/123872 (45%)]\tLoss: 0.377466\n",
            "Train Epoch: 14 [56576/123872 (46%)]\tLoss: 0.337078\tLR: 0.00006177\n",
            "Train Epoch: 14 [56832/123872 (46%)]\tLoss: 0.370044\tLR: 0.00006175\n",
            "Train Epoch: 14 [57088/123872 (46%)]\tLoss: 0.285012\tLR: 0.00006174\n",
            "Train Epoch: 14 [57344/123872 (46%)]\tLoss: 0.407437\tLR: 0.00006173\n",
            "Train Epoch: 14 [57600/123872 (46%)]\tLoss: 0.334036\tLR: 0.00006171\n",
            "Train Epoch: 14 [57856/123872 (47%)]\tLoss: 0.379504\tLR: 0.00006170\n",
            "Train Epoch: 14 [58112/123872 (47%)]\tLoss: 0.280970\tLR: 0.00006169\n",
            "Train Epoch: 14 [58368/123872 (47%)]\tLoss: 0.331546\tLR: 0.00006168\n",
            "Train Epoch: 14 [58624/123872 (47%)]\tLoss: 0.354876\tLR: 0.00006166\n",
            "Train Epoch: 14 [58880/123872 (48%)]\tLoss: 0.290427\tLR: 0.00006165\n",
            "Train Epoch: 14 [58880/123872 (48%)]\tLoss: 0.290427\n",
            "Train Epoch: 14 [59136/123872 (48%)]\tLoss: 0.345189\tLR: 0.00006164\n",
            "Train Epoch: 14 [59392/123872 (48%)]\tLoss: 0.360424\tLR: 0.00006162\n",
            "Train Epoch: 14 [59648/123872 (48%)]\tLoss: 0.314792\tLR: 0.00006161\n",
            "Train Epoch: 14 [59904/123872 (48%)]\tLoss: 0.292455\tLR: 0.00006160\n",
            "Train Epoch: 14 [60160/123872 (49%)]\tLoss: 0.369538\tLR: 0.00006158\n",
            "Train Epoch: 14 [60416/123872 (49%)]\tLoss: 0.357032\tLR: 0.00006157\n",
            "Train Epoch: 14 [60672/123872 (49%)]\tLoss: 0.396318\tLR: 0.00006156\n",
            "Train Epoch: 14 [60928/123872 (49%)]\tLoss: 0.363772\tLR: 0.00006155\n",
            "Train Epoch: 14 [61184/123872 (49%)]\tLoss: 0.300621\tLR: 0.00006153\n",
            "Train Epoch: 14 [61440/123872 (50%)]\tLoss: 0.308668\tLR: 0.00006152\n",
            "Train Epoch: 14 [61440/123872 (50%)]\tLoss: 0.308668\n",
            "Train Epoch: 14 [61696/123872 (50%)]\tLoss: 0.416149\tLR: 0.00006151\n",
            "Train Epoch: 14 [61952/123872 (50%)]\tLoss: 0.387988\tLR: 0.00006150\n",
            "Train Epoch: 14 [62208/123872 (50%)]\tLoss: 0.372788\tLR: 0.00006148\n",
            "Train Epoch: 14 [62464/123872 (50%)]\tLoss: 0.413023\tLR: 0.00006147\n",
            "Train Epoch: 14 [62720/123872 (51%)]\tLoss: 0.381136\tLR: 0.00006146\n",
            "Train Epoch: 14 [62976/123872 (51%)]\tLoss: 0.321416\tLR: 0.00006145\n",
            "Train Epoch: 14 [63232/123872 (51%)]\tLoss: 0.319256\tLR: 0.00006144\n",
            "Train Epoch: 14 [63488/123872 (51%)]\tLoss: 0.337393\tLR: 0.00006142\n",
            "Train Epoch: 14 [63744/123872 (51%)]\tLoss: 0.314840\tLR: 0.00006141\n",
            "Train Epoch: 14 [64000/123872 (52%)]\tLoss: 0.381225\tLR: 0.00006140\n",
            "Train Epoch: 14 [64000/123872 (52%)]\tLoss: 0.381225\n",
            "Train Epoch: 14 [64256/123872 (52%)]\tLoss: 0.362178\tLR: 0.00006139\n",
            "Train Epoch: 14 [64512/123872 (52%)]\tLoss: 0.355843\tLR: 0.00006137\n",
            "Train Epoch: 14 [64768/123872 (52%)]\tLoss: 0.426136\tLR: 0.00006136\n",
            "Train Epoch: 14 [65024/123872 (52%)]\tLoss: 0.320899\tLR: 0.00006135\n",
            "Train Epoch: 14 [65280/123872 (53%)]\tLoss: 0.329211\tLR: 0.00006134\n",
            "Train Epoch: 14 [65536/123872 (53%)]\tLoss: 0.293738\tLR: 0.00006133\n",
            "Train Epoch: 14 [65792/123872 (53%)]\tLoss: 0.333950\tLR: 0.00006132\n",
            "Train Epoch: 14 [66048/123872 (53%)]\tLoss: 0.333530\tLR: 0.00006130\n",
            "Train Epoch: 14 [66304/123872 (54%)]\tLoss: 0.323388\tLR: 0.00006129\n",
            "Train Epoch: 14 [66560/123872 (54%)]\tLoss: 0.324288\tLR: 0.00006128\n",
            "Train Epoch: 14 [66560/123872 (54%)]\tLoss: 0.324288\n",
            "Train Epoch: 14 [66816/123872 (54%)]\tLoss: 0.326371\tLR: 0.00006127\n",
            "Train Epoch: 14 [67072/123872 (54%)]\tLoss: 0.382578\tLR: 0.00006126\n",
            "Train Epoch: 14 [67328/123872 (54%)]\tLoss: 0.356303\tLR: 0.00006125\n",
            "Train Epoch: 14 [67584/123872 (55%)]\tLoss: 0.322634\tLR: 0.00006124\n",
            "Train Epoch: 14 [67840/123872 (55%)]\tLoss: 0.322056\tLR: 0.00006122\n",
            "Train Epoch: 14 [68096/123872 (55%)]\tLoss: 0.340531\tLR: 0.00006121\n",
            "Train Epoch: 14 [68352/123872 (55%)]\tLoss: 0.341578\tLR: 0.00006120\n",
            "Train Epoch: 14 [68608/123872 (55%)]\tLoss: 0.327068\tLR: 0.00006119\n",
            "Train Epoch: 14 [68864/123872 (56%)]\tLoss: 0.409423\tLR: 0.00006118\n",
            "Train Epoch: 14 [69120/123872 (56%)]\tLoss: 0.336586\tLR: 0.00006117\n",
            "Train Epoch: 14 [69120/123872 (56%)]\tLoss: 0.336586\n",
            "Train Epoch: 14 [69376/123872 (56%)]\tLoss: 0.332990\tLR: 0.00006116\n",
            "Train Epoch: 14 [69632/123872 (56%)]\tLoss: 0.375313\tLR: 0.00006115\n",
            "Train Epoch: 14 [69888/123872 (56%)]\tLoss: 0.446207\tLR: 0.00006114\n",
            "Train Epoch: 14 [70144/123872 (57%)]\tLoss: 0.329754\tLR: 0.00006113\n",
            "Train Epoch: 14 [70400/123872 (57%)]\tLoss: 0.366727\tLR: 0.00006111\n",
            "Train Epoch: 14 [70656/123872 (57%)]\tLoss: 0.399118\tLR: 0.00006110\n",
            "Train Epoch: 14 [70912/123872 (57%)]\tLoss: 0.354614\tLR: 0.00006109\n",
            "Train Epoch: 14 [71168/123872 (57%)]\tLoss: 0.321627\tLR: 0.00006108\n",
            "Train Epoch: 14 [71424/123872 (58%)]\tLoss: 0.295087\tLR: 0.00006107\n",
            "Train Epoch: 14 [71680/123872 (58%)]\tLoss: 0.353958\tLR: 0.00006106\n",
            "Train Epoch: 14 [71680/123872 (58%)]\tLoss: 0.353958\n",
            "Train Epoch: 14 [71936/123872 (58%)]\tLoss: 0.309887\tLR: 0.00006105\n",
            "Train Epoch: 14 [72192/123872 (58%)]\tLoss: 0.316914\tLR: 0.00006104\n",
            "Train Epoch: 14 [72448/123872 (58%)]\tLoss: 0.379344\tLR: 0.00006103\n",
            "Train Epoch: 14 [72704/123872 (59%)]\tLoss: 0.308448\tLR: 0.00006102\n",
            "Train Epoch: 14 [72960/123872 (59%)]\tLoss: 0.358493\tLR: 0.00006101\n",
            "Train Epoch: 14 [73216/123872 (59%)]\tLoss: 0.359267\tLR: 0.00006100\n",
            "Train Epoch: 14 [73472/123872 (59%)]\tLoss: 0.328974\tLR: 0.00006099\n",
            "Train Epoch: 14 [73728/123872 (60%)]\tLoss: 0.381469\tLR: 0.00006098\n",
            "Train Epoch: 14 [73984/123872 (60%)]\tLoss: 0.360620\tLR: 0.00006097\n",
            "Train Epoch: 14 [74240/123872 (60%)]\tLoss: 0.360989\tLR: 0.00006096\n",
            "Train Epoch: 14 [74240/123872 (60%)]\tLoss: 0.360989\n",
            "Train Epoch: 14 [74496/123872 (60%)]\tLoss: 0.347456\tLR: 0.00006095\n",
            "Train Epoch: 14 [74752/123872 (60%)]\tLoss: 0.369073\tLR: 0.00006094\n",
            "Train Epoch: 14 [75008/123872 (61%)]\tLoss: 0.358493\tLR: 0.00006093\n",
            "Train Epoch: 14 [75264/123872 (61%)]\tLoss: 0.316299\tLR: 0.00006092\n",
            "Train Epoch: 14 [75520/123872 (61%)]\tLoss: 0.368661\tLR: 0.00006091\n",
            "Train Epoch: 14 [75776/123872 (61%)]\tLoss: 0.313552\tLR: 0.00006090\n",
            "Train Epoch: 14 [76032/123872 (61%)]\tLoss: 0.350066\tLR: 0.00006089\n",
            "Train Epoch: 14 [76288/123872 (62%)]\tLoss: 0.278420\tLR: 0.00006088\n",
            "Train Epoch: 14 [76544/123872 (62%)]\tLoss: 0.267165\tLR: 0.00006087\n",
            "Train Epoch: 14 [76800/123872 (62%)]\tLoss: 0.352404\tLR: 0.00006086\n",
            "Train Epoch: 14 [76800/123872 (62%)]\tLoss: 0.352404\n",
            "Train Epoch: 14 [77056/123872 (62%)]\tLoss: 0.375438\tLR: 0.00006085\n",
            "Train Epoch: 14 [77312/123872 (62%)]\tLoss: 0.381351\tLR: 0.00006084\n",
            "Train Epoch: 14 [77568/123872 (63%)]\tLoss: 0.259967\tLR: 0.00006084\n",
            "Train Epoch: 14 [77824/123872 (63%)]\tLoss: 0.384262\tLR: 0.00006083\n",
            "Train Epoch: 14 [78080/123872 (63%)]\tLoss: 0.356319\tLR: 0.00006082\n",
            "Train Epoch: 14 [78336/123872 (63%)]\tLoss: 0.327802\tLR: 0.00006081\n",
            "Train Epoch: 14 [78592/123872 (63%)]\tLoss: 0.356316\tLR: 0.00006080\n",
            "Train Epoch: 14 [78848/123872 (64%)]\tLoss: 0.336143\tLR: 0.00006079\n",
            "Train Epoch: 14 [79104/123872 (64%)]\tLoss: 0.334683\tLR: 0.00006078\n",
            "Train Epoch: 14 [79360/123872 (64%)]\tLoss: 0.356593\tLR: 0.00006077\n",
            "Train Epoch: 14 [79360/123872 (64%)]\tLoss: 0.356593\n",
            "Train Epoch: 14 [79616/123872 (64%)]\tLoss: 0.316871\tLR: 0.00006076\n",
            "Train Epoch: 14 [79872/123872 (64%)]\tLoss: 0.352131\tLR: 0.00006075\n",
            "Train Epoch: 14 [80128/123872 (65%)]\tLoss: 0.310525\tLR: 0.00006074\n",
            "Train Epoch: 14 [80384/123872 (65%)]\tLoss: 0.355157\tLR: 0.00006074\n",
            "Train Epoch: 14 [80640/123872 (65%)]\tLoss: 0.366666\tLR: 0.00006073\n",
            "Train Epoch: 14 [80896/123872 (65%)]\tLoss: 0.310214\tLR: 0.00006072\n",
            "Train Epoch: 14 [81152/123872 (65%)]\tLoss: 0.399515\tLR: 0.00006071\n",
            "Train Epoch: 14 [81408/123872 (66%)]\tLoss: 0.390234\tLR: 0.00006070\n",
            "Train Epoch: 14 [81664/123872 (66%)]\tLoss: 0.301038\tLR: 0.00006069\n",
            "Train Epoch: 14 [81920/123872 (66%)]\tLoss: 0.315581\tLR: 0.00006068\n",
            "Train Epoch: 14 [81920/123872 (66%)]\tLoss: 0.315581\n",
            "Train Epoch: 14 [82176/123872 (66%)]\tLoss: 0.434329\tLR: 0.00006068\n",
            "Train Epoch: 14 [82432/123872 (67%)]\tLoss: 0.337601\tLR: 0.00006067\n",
            "Train Epoch: 14 [82688/123872 (67%)]\tLoss: 0.358551\tLR: 0.00006066\n",
            "Train Epoch: 14 [82944/123872 (67%)]\tLoss: 0.329921\tLR: 0.00006065\n",
            "Train Epoch: 14 [83200/123872 (67%)]\tLoss: 0.358261\tLR: 0.00006064\n",
            "Train Epoch: 14 [83456/123872 (67%)]\tLoss: 0.340623\tLR: 0.00006064\n",
            "Train Epoch: 14 [83712/123872 (68%)]\tLoss: 0.335237\tLR: 0.00006063\n",
            "Train Epoch: 14 [83968/123872 (68%)]\tLoss: 0.339492\tLR: 0.00006062\n",
            "Train Epoch: 14 [84224/123872 (68%)]\tLoss: 0.358995\tLR: 0.00006061\n",
            "Train Epoch: 14 [84480/123872 (68%)]\tLoss: 0.324886\tLR: 0.00006060\n",
            "Train Epoch: 14 [84480/123872 (68%)]\tLoss: 0.324886\n",
            "Train Epoch: 14 [84736/123872 (68%)]\tLoss: 0.396493\tLR: 0.00006060\n",
            "Train Epoch: 14 [84992/123872 (69%)]\tLoss: 0.349395\tLR: 0.00006059\n",
            "Train Epoch: 14 [85248/123872 (69%)]\tLoss: 0.439178\tLR: 0.00006058\n",
            "Train Epoch: 14 [85504/123872 (69%)]\tLoss: 0.323532\tLR: 0.00006057\n",
            "Train Epoch: 14 [85760/123872 (69%)]\tLoss: 0.278147\tLR: 0.00006056\n",
            "Train Epoch: 14 [86016/123872 (69%)]\tLoss: 0.397885\tLR: 0.00006056\n",
            "Train Epoch: 14 [86272/123872 (70%)]\tLoss: 0.364908\tLR: 0.00006055\n",
            "Train Epoch: 14 [86528/123872 (70%)]\tLoss: 0.351087\tLR: 0.00006054\n",
            "Train Epoch: 14 [86784/123872 (70%)]\tLoss: 0.364491\tLR: 0.00006053\n",
            "Train Epoch: 14 [87040/123872 (70%)]\tLoss: 0.277666\tLR: 0.00006053\n",
            "Train Epoch: 14 [87040/123872 (70%)]\tLoss: 0.277666\n",
            "Train Epoch: 14 [87296/123872 (70%)]\tLoss: 0.390982\tLR: 0.00006052\n",
            "Train Epoch: 14 [87552/123872 (71%)]\tLoss: 0.393844\tLR: 0.00006051\n",
            "Train Epoch: 14 [87808/123872 (71%)]\tLoss: 0.333460\tLR: 0.00006051\n",
            "Train Epoch: 14 [88064/123872 (71%)]\tLoss: 0.349950\tLR: 0.00006050\n",
            "Train Epoch: 14 [88320/123872 (71%)]\tLoss: 0.360714\tLR: 0.00006049\n",
            "Train Epoch: 14 [88576/123872 (71%)]\tLoss: 0.345510\tLR: 0.00006048\n",
            "Train Epoch: 14 [88832/123872 (72%)]\tLoss: 0.330171\tLR: 0.00006048\n",
            "Train Epoch: 14 [89088/123872 (72%)]\tLoss: 0.321267\tLR: 0.00006047\n",
            "Train Epoch: 14 [89344/123872 (72%)]\tLoss: 0.354883\tLR: 0.00006046\n",
            "Train Epoch: 14 [89600/123872 (72%)]\tLoss: 0.329833\tLR: 0.00006046\n",
            "Train Epoch: 14 [89600/123872 (72%)]\tLoss: 0.329833\n",
            "Train Epoch: 14 [89856/123872 (73%)]\tLoss: 0.345587\tLR: 0.00006045\n",
            "Train Epoch: 14 [90112/123872 (73%)]\tLoss: 0.360935\tLR: 0.00006044\n",
            "Train Epoch: 14 [90368/123872 (73%)]\tLoss: 0.368825\tLR: 0.00006044\n",
            "Train Epoch: 14 [90624/123872 (73%)]\tLoss: 0.282473\tLR: 0.00006043\n",
            "Train Epoch: 14 [90880/123872 (73%)]\tLoss: 0.375228\tLR: 0.00006042\n",
            "Train Epoch: 14 [91136/123872 (74%)]\tLoss: 0.357978\tLR: 0.00006042\n",
            "Train Epoch: 14 [91392/123872 (74%)]\tLoss: 0.372907\tLR: 0.00006041\n",
            "Train Epoch: 14 [91648/123872 (74%)]\tLoss: 0.308120\tLR: 0.00006040\n",
            "Train Epoch: 14 [91904/123872 (74%)]\tLoss: 0.317153\tLR: 0.00006040\n",
            "Train Epoch: 14 [92160/123872 (74%)]\tLoss: 0.332071\tLR: 0.00006039\n",
            "Train Epoch: 14 [92160/123872 (74%)]\tLoss: 0.332071\n",
            "Train Epoch: 14 [92416/123872 (75%)]\tLoss: 0.319282\tLR: 0.00006038\n",
            "Train Epoch: 14 [92672/123872 (75%)]\tLoss: 0.325751\tLR: 0.00006038\n",
            "Train Epoch: 14 [92928/123872 (75%)]\tLoss: 0.316469\tLR: 0.00006037\n",
            "Train Epoch: 14 [93184/123872 (75%)]\tLoss: 0.368075\tLR: 0.00006037\n",
            "Train Epoch: 14 [93440/123872 (75%)]\tLoss: 0.319146\tLR: 0.00006036\n",
            "Train Epoch: 14 [93696/123872 (76%)]\tLoss: 0.372458\tLR: 0.00006035\n",
            "Train Epoch: 14 [93952/123872 (76%)]\tLoss: 0.342067\tLR: 0.00006035\n",
            "Train Epoch: 14 [94208/123872 (76%)]\tLoss: 0.355089\tLR: 0.00006034\n",
            "Train Epoch: 14 [94464/123872 (76%)]\tLoss: 0.323276\tLR: 0.00006034\n",
            "Train Epoch: 14 [94720/123872 (76%)]\tLoss: 0.384328\tLR: 0.00006033\n",
            "Train Epoch: 14 [94720/123872 (76%)]\tLoss: 0.384328\n",
            "Train Epoch: 14 [94976/123872 (77%)]\tLoss: 0.315444\tLR: 0.00006032\n",
            "Train Epoch: 14 [95232/123872 (77%)]\tLoss: 0.322534\tLR: 0.00006032\n",
            "Train Epoch: 14 [95488/123872 (77%)]\tLoss: 0.288192\tLR: 0.00006031\n",
            "Train Epoch: 14 [95744/123872 (77%)]\tLoss: 0.384425\tLR: 0.00006031\n",
            "Train Epoch: 14 [96000/123872 (77%)]\tLoss: 0.325680\tLR: 0.00006030\n",
            "Train Epoch: 14 [96256/123872 (78%)]\tLoss: 0.299538\tLR: 0.00006030\n",
            "Train Epoch: 14 [96512/123872 (78%)]\tLoss: 0.378315\tLR: 0.00006029\n",
            "Train Epoch: 14 [96768/123872 (78%)]\tLoss: 0.346952\tLR: 0.00006028\n",
            "Train Epoch: 14 [97024/123872 (78%)]\tLoss: 0.334089\tLR: 0.00006028\n",
            "Train Epoch: 14 [97280/123872 (79%)]\tLoss: 0.414996\tLR: 0.00006027\n",
            "Train Epoch: 14 [97280/123872 (79%)]\tLoss: 0.414996\n",
            "Train Epoch: 14 [97536/123872 (79%)]\tLoss: 0.337780\tLR: 0.00006027\n",
            "Train Epoch: 14 [97792/123872 (79%)]\tLoss: 0.350208\tLR: 0.00006026\n",
            "Train Epoch: 14 [98048/123872 (79%)]\tLoss: 0.306777\tLR: 0.00006026\n",
            "Train Epoch: 14 [98304/123872 (79%)]\tLoss: 0.332469\tLR: 0.00006025\n",
            "Train Epoch: 14 [98560/123872 (80%)]\tLoss: 0.352171\tLR: 0.00006025\n",
            "Train Epoch: 14 [98816/123872 (80%)]\tLoss: 0.358304\tLR: 0.00006024\n",
            "Train Epoch: 14 [99072/123872 (80%)]\tLoss: 0.366909\tLR: 0.00006024\n",
            "Train Epoch: 14 [99328/123872 (80%)]\tLoss: 0.393038\tLR: 0.00006023\n",
            "Train Epoch: 14 [99584/123872 (80%)]\tLoss: 0.309319\tLR: 0.00006023\n",
            "Train Epoch: 14 [99840/123872 (81%)]\tLoss: 0.331137\tLR: 0.00006022\n",
            "Train Epoch: 14 [99840/123872 (81%)]\tLoss: 0.331137\n",
            "Train Epoch: 14 [100096/123872 (81%)]\tLoss: 0.338035\tLR: 0.00006022\n",
            "Train Epoch: 14 [100352/123872 (81%)]\tLoss: 0.386640\tLR: 0.00006021\n",
            "Train Epoch: 14 [100608/123872 (81%)]\tLoss: 0.349016\tLR: 0.00006021\n",
            "Train Epoch: 14 [100864/123872 (81%)]\tLoss: 0.328594\tLR: 0.00006020\n",
            "Train Epoch: 14 [101120/123872 (82%)]\tLoss: 0.305295\tLR: 0.00006020\n",
            "Train Epoch: 14 [101376/123872 (82%)]\tLoss: 0.394325\tLR: 0.00006020\n",
            "Train Epoch: 14 [101632/123872 (82%)]\tLoss: 0.329706\tLR: 0.00006019\n",
            "Train Epoch: 14 [101888/123872 (82%)]\tLoss: 0.366426\tLR: 0.00006019\n",
            "Train Epoch: 14 [102144/123872 (82%)]\tLoss: 0.335357\tLR: 0.00006018\n",
            "Train Epoch: 14 [102400/123872 (83%)]\tLoss: 0.361226\tLR: 0.00006018\n",
            "Train Epoch: 14 [102400/123872 (83%)]\tLoss: 0.361226\n",
            "Train Epoch: 14 [102656/123872 (83%)]\tLoss: 0.347530\tLR: 0.00006017\n",
            "Train Epoch: 14 [102912/123872 (83%)]\tLoss: 0.338929\tLR: 0.00006017\n",
            "Train Epoch: 14 [103168/123872 (83%)]\tLoss: 0.362069\tLR: 0.00006017\n",
            "Train Epoch: 14 [103424/123872 (83%)]\tLoss: 0.350629\tLR: 0.00006016\n",
            "Train Epoch: 14 [103680/123872 (84%)]\tLoss: 0.416179\tLR: 0.00006016\n",
            "Train Epoch: 14 [103936/123872 (84%)]\tLoss: 0.384877\tLR: 0.00006015\n",
            "Train Epoch: 14 [104192/123872 (84%)]\tLoss: 0.309779\tLR: 0.00006015\n",
            "Train Epoch: 14 [104448/123872 (84%)]\tLoss: 0.332258\tLR: 0.00006015\n",
            "Train Epoch: 14 [104704/123872 (85%)]\tLoss: 0.356823\tLR: 0.00006014\n",
            "Train Epoch: 14 [104960/123872 (85%)]\tLoss: 0.355054\tLR: 0.00006014\n",
            "Train Epoch: 14 [104960/123872 (85%)]\tLoss: 0.355054\n",
            "Train Epoch: 14 [105216/123872 (85%)]\tLoss: 0.384577\tLR: 0.00006013\n",
            "Train Epoch: 14 [105472/123872 (85%)]\tLoss: 0.333409\tLR: 0.00006013\n",
            "Train Epoch: 14 [105728/123872 (85%)]\tLoss: 0.290569\tLR: 0.00006013\n",
            "Train Epoch: 14 [105984/123872 (86%)]\tLoss: 0.343342\tLR: 0.00006012\n",
            "Train Epoch: 14 [106240/123872 (86%)]\tLoss: 0.319040\tLR: 0.00006012\n",
            "Train Epoch: 14 [106496/123872 (86%)]\tLoss: 0.359718\tLR: 0.00006012\n",
            "Train Epoch: 14 [106752/123872 (86%)]\tLoss: 0.307828\tLR: 0.00006011\n",
            "Train Epoch: 14 [107008/123872 (86%)]\tLoss: 0.386331\tLR: 0.00006011\n",
            "Train Epoch: 14 [107264/123872 (87%)]\tLoss: 0.330413\tLR: 0.00006011\n",
            "Train Epoch: 14 [107520/123872 (87%)]\tLoss: 0.396374\tLR: 0.00006010\n",
            "Train Epoch: 14 [107520/123872 (87%)]\tLoss: 0.396374\n",
            "Train Epoch: 14 [107776/123872 (87%)]\tLoss: 0.302130\tLR: 0.00006010\n",
            "Train Epoch: 14 [108032/123872 (87%)]\tLoss: 0.350847\tLR: 0.00006010\n",
            "Train Epoch: 14 [108288/123872 (87%)]\tLoss: 0.344675\tLR: 0.00006009\n",
            "Train Epoch: 14 [108544/123872 (88%)]\tLoss: 0.467024\tLR: 0.00006009\n",
            "Train Epoch: 14 [108800/123872 (88%)]\tLoss: 0.303345\tLR: 0.00006009\n",
            "Train Epoch: 14 [109056/123872 (88%)]\tLoss: 0.287426\tLR: 0.00006008\n",
            "Train Epoch: 14 [109312/123872 (88%)]\tLoss: 0.336797\tLR: 0.00006008\n",
            "Train Epoch: 14 [109568/123872 (88%)]\tLoss: 0.343947\tLR: 0.00006008\n",
            "Train Epoch: 14 [109824/123872 (89%)]\tLoss: 0.315967\tLR: 0.00006008\n",
            "Train Epoch: 14 [110080/123872 (89%)]\tLoss: 0.377760\tLR: 0.00006007\n",
            "Train Epoch: 14 [110080/123872 (89%)]\tLoss: 0.377760\n",
            "Train Epoch: 14 [110336/123872 (89%)]\tLoss: 0.391478\tLR: 0.00006007\n",
            "Train Epoch: 14 [110592/123872 (89%)]\tLoss: 0.274792\tLR: 0.00006007\n",
            "Train Epoch: 14 [110848/123872 (89%)]\tLoss: 0.312304\tLR: 0.00006006\n",
            "Train Epoch: 14 [111104/123872 (90%)]\tLoss: 0.321172\tLR: 0.00006006\n",
            "Train Epoch: 14 [111360/123872 (90%)]\tLoss: 0.355949\tLR: 0.00006006\n",
            "Train Epoch: 14 [111616/123872 (90%)]\tLoss: 0.319205\tLR: 0.00006006\n",
            "Train Epoch: 14 [111872/123872 (90%)]\tLoss: 0.324279\tLR: 0.00006005\n",
            "Train Epoch: 14 [112128/123872 (90%)]\tLoss: 0.328736\tLR: 0.00006005\n",
            "Train Epoch: 14 [112384/123872 (91%)]\tLoss: 0.382629\tLR: 0.00006005\n",
            "Train Epoch: 14 [112640/123872 (91%)]\tLoss: 0.441844\tLR: 0.00006005\n",
            "Train Epoch: 14 [112640/123872 (91%)]\tLoss: 0.441844\n",
            "Train Epoch: 14 [112896/123872 (91%)]\tLoss: 0.361674\tLR: 0.00006005\n",
            "Train Epoch: 14 [113152/123872 (91%)]\tLoss: 0.300805\tLR: 0.00006004\n",
            "Train Epoch: 14 [113408/123872 (92%)]\tLoss: 0.411173\tLR: 0.00006004\n",
            "Train Epoch: 14 [113664/123872 (92%)]\tLoss: 0.328224\tLR: 0.00006004\n",
            "Train Epoch: 14 [113920/123872 (92%)]\tLoss: 0.377368\tLR: 0.00006004\n",
            "Train Epoch: 14 [114176/123872 (92%)]\tLoss: 0.392807\tLR: 0.00006004\n",
            "Train Epoch: 14 [114432/123872 (92%)]\tLoss: 0.364429\tLR: 0.00006003\n",
            "Train Epoch: 14 [114688/123872 (93%)]\tLoss: 0.358726\tLR: 0.00006003\n",
            "Train Epoch: 14 [114944/123872 (93%)]\tLoss: 0.401653\tLR: 0.00006003\n",
            "Train Epoch: 14 [115200/123872 (93%)]\tLoss: 0.407749\tLR: 0.00006003\n",
            "Train Epoch: 14 [115200/123872 (93%)]\tLoss: 0.407749\n",
            "Train Epoch: 14 [115456/123872 (93%)]\tLoss: 0.339687\tLR: 0.00006003\n",
            "Train Epoch: 14 [115712/123872 (93%)]\tLoss: 0.321547\tLR: 0.00006002\n",
            "Train Epoch: 14 [115968/123872 (94%)]\tLoss: 0.380779\tLR: 0.00006002\n",
            "Train Epoch: 14 [116224/123872 (94%)]\tLoss: 0.354235\tLR: 0.00006002\n",
            "Train Epoch: 14 [116480/123872 (94%)]\tLoss: 0.328001\tLR: 0.00006002\n",
            "Train Epoch: 14 [116736/123872 (94%)]\tLoss: 0.376782\tLR: 0.00006002\n",
            "Train Epoch: 14 [116992/123872 (94%)]\tLoss: 0.354120\tLR: 0.00006002\n",
            "Train Epoch: 14 [117248/123872 (95%)]\tLoss: 0.321544\tLR: 0.00006002\n",
            "Train Epoch: 14 [117504/123872 (95%)]\tLoss: 0.351514\tLR: 0.00006001\n",
            "Train Epoch: 14 [117760/123872 (95%)]\tLoss: 0.360651\tLR: 0.00006001\n",
            "Train Epoch: 14 [117760/123872 (95%)]\tLoss: 0.360651\n",
            "Train Epoch: 14 [118016/123872 (95%)]\tLoss: 0.348734\tLR: 0.00006001\n",
            "Train Epoch: 14 [118272/123872 (95%)]\tLoss: 0.339278\tLR: 0.00006001\n",
            "Train Epoch: 14 [118528/123872 (96%)]\tLoss: 0.327257\tLR: 0.00006001\n",
            "Train Epoch: 14 [118784/123872 (96%)]\tLoss: 0.403562\tLR: 0.00006001\n",
            "Train Epoch: 14 [119040/123872 (96%)]\tLoss: 0.344723\tLR: 0.00006001\n",
            "Train Epoch: 14 [119296/123872 (96%)]\tLoss: 0.332691\tLR: 0.00006001\n",
            "Train Epoch: 14 [119552/123872 (96%)]\tLoss: 0.405871\tLR: 0.00006001\n",
            "Train Epoch: 14 [119808/123872 (97%)]\tLoss: 0.313064\tLR: 0.00006001\n",
            "Train Epoch: 14 [120064/123872 (97%)]\tLoss: 0.341085\tLR: 0.00006001\n",
            "Train Epoch: 14 [120320/123872 (97%)]\tLoss: 0.352549\tLR: 0.00006000\n",
            "Train Epoch: 14 [120320/123872 (97%)]\tLoss: 0.352549\n",
            "Train Epoch: 14 [120576/123872 (97%)]\tLoss: 0.346347\tLR: 0.00006000\n",
            "Train Epoch: 14 [120832/123872 (98%)]\tLoss: 0.386371\tLR: 0.00006000\n",
            "Train Epoch: 14 [121088/123872 (98%)]\tLoss: 0.444297\tLR: 0.00006000\n",
            "Train Epoch: 14 [121344/123872 (98%)]\tLoss: 0.411166\tLR: 0.00006000\n",
            "Train Epoch: 14 [121600/123872 (98%)]\tLoss: 0.342686\tLR: 0.00006000\n",
            "Train Epoch: 14 [121856/123872 (98%)]\tLoss: 0.357659\tLR: 0.00006000\n",
            "Train Epoch: 14 [122112/123872 (99%)]\tLoss: 0.365311\tLR: 0.00006000\n",
            "Train Epoch: 14 [122368/123872 (99%)]\tLoss: 0.375230\tLR: 0.00006000\n",
            "Train Epoch: 14 [122624/123872 (99%)]\tLoss: 0.348574\tLR: 0.00006000\n",
            "Train Epoch: 14 [122880/123872 (99%)]\tLoss: 0.384663\tLR: 0.00006000\n",
            "Train Epoch: 14 [122880/123872 (99%)]\tLoss: 0.384663\n",
            "Train Epoch: 14 [123136/123872 (99%)]\tLoss: 0.350514\tLR: 0.00006000\n",
            "Train Epoch: 14 [123392/123872 (100%)]\tLoss: 0.305185\tLR: 0.00006000\n",
            "Train Epoch: 14 [108192/123872 (100%)]\tLoss: 0.374618\tLR: 0.00006000\n"
          ]
        }
      ],
      "source": [
        "# Main loop to train with different seeds\n",
        "for seed in seeds:\n",
        "    run_train(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8FbrATBFCym"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}